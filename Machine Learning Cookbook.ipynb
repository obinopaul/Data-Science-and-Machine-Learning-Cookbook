{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70928dd7",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db23770f",
   "metadata": {},
   "source": [
    "### Machine Learning Models to Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithms to Master  \n",
    "1. Linear and Multiple Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Decision Trees\n",
    "4. Naive Bayes\n",
    "5. K-Nearest Neighbors\n",
    "6. Support Vector Machines\n",
    "7. Random Forests\n",
    "8. Neural Networks\n",
    "    1. Convolutional Neural Network (CNN)\n",
    "    2. Recurrent Neural Network (RNN)\n",
    "    3. Long Short-Term Memory (LSTM)\n",
    "    4. Generative Adversarial Network (GAN)\n",
    "    5. Deep Belief Network (DBN)\n",
    "    6. Deep Boltzmann Machine (DBM)\n",
    "    7. Autoencoders\n",
    "    8. Restricted Boltzmann Machines (RBM)\n",
    "    9. Hopfield Networks\n",
    "    10. Self-Organizing Maps (SOM)\n",
    "9. Gradient Boosting\n",
    "    1. XGBoost\n",
    "    2. LightGBM\n",
    "    3. CatBoost\n",
    "    4. Gradient Boosting Machines (GBM)\n",
    "    5. Stochastic Gradient Boosting (SGB)\n",
    "    6. Adaboost\n",
    "    7. Gradient Boosted Decision Trees (GBDT)\n",
    "    8. DeepBoost\n",
    "    9. Neural Network Boosting (NNBoost)\n",
    "    10. Gradient Boosted Regression Trees (GBRT)\n",
    "10. Reinforcement Learning\n",
    "11. Dimensionality Reduction Algorithms\n",
    "    1. Principal Component Analysis (PCA)\n",
    "    2. Linear Discriminant Analysis (LDA)\n",
    "    3. Independent Component Analysis (ICA)\n",
    "    4. Non-Negative Matrix Factorization (NMF)\n",
    "    5. Factor Analysis\n",
    "    6. Singular Value Decomposition (SVD)\n",
    "    7. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    8. Uniform Manifold Approximation and Projection (UMAP)\n",
    "    9. Autoencoders\n",
    "    10. Random Projection\n",
    "    11. Feature Selection\n",
    "    12. Locally Linear Embedding (LLE)\n",
    "12. Clustering Algorithms\n",
    "    1. K-Means Clustering\n",
    "    2. Hierarchical Clustering\n",
    "    3. Expectation-Maximization (EM) Clustering\n",
    "    4. Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "    5. Mean-Shift Clustering\n",
    "    6. Gaussian Mixture Model (GMM) Clustering\n",
    "    7. Spectral Clustering\n",
    "    8. Affinity Propagation Clustering\n",
    "    9. Birch Clustering\n",
    "    10. Optics Clustering\n",
    "13. Autoencoders\n",
    "14. Transfer Learning\n",
    "15. Generative Adversarial Networks (GANs)\n",
    "\n",
    "\n",
    "Data Preprocessing:\n",
    "    importing the required libraries\n",
    "    importing the dataset\n",
    "    handling missing data\n",
    "    encoding the categoical data\n",
    "    feature engineering\n",
    "    spliting the dataset into test set and training set\n",
    "    feature scaling \n",
    "    *webscraping with beautifulsoup\n",
    "\n",
    "Developing the Model:\n",
    "    model selection\n",
    "    model evaluation\n",
    "    model persistence\n",
    "    ensemble methods\n",
    "    feature extraction\n",
    "    feature selection\n",
    "    feature engineering\n",
    "    hyperparameter tuning\n",
    "    model ensembling\n",
    "    model stacking\n",
    "    model blending\n",
    "    model bagging\n",
    "    model boosting\n",
    "    model averaging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84f635b6",
   "metadata": {},
   "source": [
    "### Data Pre-Processing in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d98725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\" \n",
    "1. Data Cleaning:\n",
    "    a. Missing values:\n",
    "        Removing the training example:\n",
    "        Filling in missing value manually\n",
    "        Using a standard value to replace the missing value\n",
    "        Using central tendency (mean, median, mode) for attribute to replace the missing value:\n",
    "        Using central tendency (mean, median, mode) for attribute belonging to same class to replace the missing value:\n",
    "        Using the most probable value to fill in the missing value:\n",
    "\n",
    "    b. Noisy Data and Outliers: \n",
    "        Binning: Using binning methods smooths sorted values by using the values around it. The sorted values are then divided \n",
    "            into bins. \n",
    "        Regression:  Linear regression and multiple linear regression can be used to smooth the data, where the values \n",
    "            are conformed to a function.\n",
    "        Outlier analysis: Approaches such as clustering can be used to detect outliers and deal with them.\n",
    "\n",
    "    c. Remove Unwanted Data: Unwanted data is duplicate or irrelevant data. \n",
    "    \n",
    "2. Data Integration:\n",
    "    Data consolidation: The data is physically brought together to one data store. This usually involves Data Warehousing.\n",
    "    Data propagation: Copying data from one location to another using applications is called data propagation\n",
    "    Data virtualization: An interface is used to provide a real-time and unified view of data from multiple sources. \n",
    "\n",
    "3. Data Reduction:\n",
    "    Missing values ratio: Attributes that have more missing values than a threshold are removed.\n",
    "    Low variance filter: Normalized attributes that have variance (distribution) less than a threshold are also removed \n",
    "        because little changes in data means less information.\n",
    "    High correlation filter: Normalized attributes that have correlation coefficients more than a threshold are removed \n",
    "        because similar trends means similar information is carried. A correlation coefficient is usually calculated using \n",
    "        statistical methods such as Pearson’s chi-square value.\n",
    "    Principal component analysis: Principal component analysis, or PCA, is a statistical method that reduces the numbers \n",
    "        of attributes by lumping highly correlated attributes together.\n",
    "\n",
    "4. Data Transformation:\n",
    "    Smoothing: Eliminating noise in the data to see more data patterns.\n",
    "    Attribute/feature construction: New attributes are constructed from the given set of attributes.\n",
    "    Aggregation: Summary and aggregation operations are applied on the given set of attributes to come up with new attributes\n",
    "    Normalization: The data in each attribute is scaled between a smaller range, for example, 0 to 1 or -1 to 1.\n",
    "    Discretization: Raw values of the numeric attributes are replaced by discrete or conceptual intervals, \n",
    "        which can be further organized into higher-level intervals. \n",
    "    Concept hierarchy generation for nominal data: Values for nominal data are generalized to higher-order concepts.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f3a91",
   "metadata": {},
   "source": [
    "### Basic ML notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de243c8c",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# #Cost Function\n",
    "# A cost function, also known as a loss function or objective function, \n",
    "# is a mathematical function that measures the difference between predicted and actual values in machine learning. \n",
    "# The purpose of a cost function is to guide the learning algorithm towards finding the optimal model parameters that minimize \n",
    "# the difference between the predicted and actual values.\n",
    "\n",
    "# The choice of cost function depends on the type of problem and the learning algorithm used. \n",
    "# Here are some common examples of cost functions and their equations:\n",
    "\n",
    "# 1. Mean Squared Error (MSE): This cost function is used for regression problems where the goal is to predict a continuous \n",
    "#     variable. It measures the average squared difference between the predicted and actual values. The equation for MSE is:\n",
    "\n",
    "#         MSE = 1/n * ∑(y - y_pred)^2\n",
    "#         where n is the number of samples, y is the actual value, and y_pred is the predicted value.\n",
    "\n",
    "# 2. Binary Cross-Entropy: This cost function is used for binary classification problems where the output is either 0 or 1. \n",
    "#     It measures the difference between the predicted probability and the actual label. \n",
    "#     The equation for binary cross-entropy is:\n",
    "\n",
    "#         Binary cross-entropy = -1/n * ∑(y * log(y_pred) + (1-y) * log(1-y_pred))\n",
    "#         where n is the number of samples, y is the actual label (0 or 1), and y_pred is the predicted probability.\n",
    "\n",
    "# 2. Categorical Cross-Entropy: This cost function is used for multi-class classification problems where the output \n",
    "#     can be one of several classes. It measures the difference between the predicted probability distribution and the actual \n",
    "#     label. The equation for categorical cross-entropy is:\n",
    "\n",
    "#         Categorical cross-entropy = -1/n * ∑∑(y_ij * log(y_pred_ij))\n",
    "#         where n is the number of samples, y_ij is the actual probability for class j in sample i, and y_pred_ij is the predicted probability for class j in sample i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aa95f",
   "metadata": {},
   "source": [
    "### Generic Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f626874",
   "metadata": {},
   "source": [
    ">> Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982edd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorical encoding\n",
    "There are four techniques to encode or convert the categorical features into numbers. Here are them:\n",
    "\n",
    "Mapping Method\n",
    "Ordinary Encoding\n",
    "Label Encoding\n",
    "Pandas Dummies\n",
    "OneHot Encoding\n",
    "\n",
    "The choice of categorical encoding method depends on several factors such as the type and nature of the data, \n",
    "the number of unique categories in the variable, the type of machine learning algorithm being used, and the performance \n",
    "of the encoding method on the dataset. Here are some general guidelines on when to use each method:\n",
    "\n",
    "One-Hot Encoding\n",
    "One-hot encoding is a useful technique for handling categorical variables with a small number of unique categories. \n",
    "It is particularly useful when the categories are nominal (unordered) or when there is no inherent order or hierarchy \n",
    "among the categories. One-hot encoding can be applied to both linear and tree-based machine learning models. \n",
    "However, one limitation of one-hot encoding is that it can lead to a high-dimensional feature space, which can be \n",
    "computationally expensive and may lead to the curse of dimensionality.\n",
    "\n",
    "Label Encoding\n",
    "Label encoding is a useful technique for handling categorical variables with a large number of unique categories. \n",
    "It is particularly useful when the categories are ordinal (ordered) or when there is an inherent order or hierarchy \n",
    "among the categories. Label encoding can be applied to both linear and tree-based machine learning models. \n",
    "However, one limitation of label encoding is that it may introduce an arbitrary ordering or hierarchy among the \n",
    "categories, which may not be appropriate for some models.\n",
    "\n",
    "In general, it is recommended to use one-hot encoding when dealing with nominal categorical variables and label encoding \n",
    "when dealing with ordinal categorical variables. However, it is important to consider the nature of the data and the \n",
    "performance of the encoding method on the specific dataset before making a decision on which method to use. \n",
    "Additionally, it is often useful to try both encoding methods and compare their performance on the dataset to determine \n",
    "the optimal encoding method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ea973641",
   "metadata": {},
   "source": [
    ">> Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73aef38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logarithmic (only defined for positive numbers) - log(X)\n",
    "#Exponential (square root or power transformations) - \n",
    "#Reciprocal (naturally not defined for zero, also defined for positive values) - 1/X\n",
    "#Box-Cox (defined only for positive values X>0)\n",
    "#Yeo-Johnson (is an adaptation of box-cox that can be used in negative value variables)\n",
    "\n",
    "#NB: if data is positively skewed (right skewed), use (logarithmic, reciprocal, or square root transformation)\n",
    "    #if data is negatively skewed (left skewed), use (Box-Cox or Yeo-Johnson transformations)\n",
    "\n",
    "#check if dataset is normally distributed or not.\n",
    "def diagnostic_plots(df, variable):\n",
    "\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.title(f\"Histogram of {variable}\")\n",
    "\n",
    "    # q-q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot of {variable}\")\n",
    "\n",
    "    # check for skewness\n",
    "    skewness = df[variable].skew()\n",
    "    if skewness > 0:\n",
    "        skew_type = \"positively skewed\"\n",
    "    elif skewness < 0:\n",
    "        skew_type = \"negatively skewed\"\n",
    "    else:\n",
    "        skew_type = \"approximately symmetric\"\n",
    "        \n",
    "    # print message indicating skewness type\n",
    "    print(f\"The variable {variable} is {skew_type} (skewness = {skewness:.2f})\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#log transform \n",
    "def log_transform(df, columns):\n",
    "     \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the natural logarithm function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_log = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_log\n",
    "\n",
    "#reciprocal transformation\n",
    "def reciprocal_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the reciprocal transformation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(lambda x: 1/x, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_recip = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_recip\n",
    "\n",
    "#square root transformation\n",
    "def sqrt_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the square root function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.sqrt, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_sqrt = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_sqrt\n",
    "\n",
    "#exponential transformation\n",
    "def exp_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the exponential function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.exp, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_exp = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_exp\n",
    "\n",
    "#box-cox transformation\n",
    "def boxcox_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the Box-Cox transformation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = PowerTransformer(method='box-cox', standardize=False)\n",
    "    X = df.copy()\n",
    "    X[columns] = transformer.fit_transform(X[columns])\n",
    "    return X\n",
    "\n",
    "\n",
    "#Yeo-Johnson\n",
    "def yeo_johnson_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the Yeo-Johnson transformation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    X = df.copy()\n",
    "    X[columns] = transformer.fit_transform(X[columns])\n",
    "    return X\n",
    "\n",
    "\"\"\"\n",
    "A normal distribution is characterized by a bell-shaped curve that is symmetric around the mean. \n",
    "The mean, median, and mode of a normal distribution are all equal, and approximately 68% of the data falls within one \n",
    "standard deviation of the mean, 95% falls within two standard deviations, and 99.7% falls within three \n",
    "standard deviations.\n",
    "\"\"\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d7eed6f6",
   "metadata": {},
   "source": [
    ">> Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf00c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization in machine learning is the process of transforming continuous variables into discrete or \n",
    "# categorical variables. This process involves dividing the range of a continuous variable into a finite number of \n",
    "# intervals or bins, and then assigning each observation to a particular bin based on the value of the continuous \n",
    "# variable. \n",
    "\n",
    "#Discretization approaches: equal width, equal frequency, K means, Decision Trees\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dba7c5b2",
   "metadata": {},
   "source": [
    ">> Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c684da91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.model_selection import cross_val_score'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training error: 0.0356386368576308\n",
      "Mean validation error: -0.009125646107584522\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Define the feature union (for feature selection or extraction)\n",
    "feature_union = FeatureUnion(\n",
    "    transformer_list=[\n",
    "        ('pca', PCA(n_components=2)),\n",
    "        ('univariate', SelectKBest(chi2, k=1))\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('features', feature_union),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit and predict using the pipeline\n",
    "pipe.fit(iris.data, iris.target)\n",
    "preds = pipe.predict(iris.data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1793d2fa",
   "metadata": {},
   "source": [
    ">> Feature Selection and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccb3d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection is a technique of selecting relevant features from the original feature set, while feature \n",
    "# extraction is a technique of creating new features from the original feature set. Feature selection is typically \n",
    "# used when the original features are already informative, but there are some irrelevant or redundant features that \n",
    "# need to be removed to improve the model performance. \n",
    "\n",
    "# Feature extraction, on the other hand, is used when the original features are not informative enough, and \n",
    "# new features need to be created to capture the underlying patterns in the data.\n",
    "\n",
    "#there are two ways to resolve/prevent curse of dimensionality (dimensionality reduction)\n",
    "#Feature selection\n",
    "    # Removing features with low variance (VarianceThreshold)\n",
    "    # Univariate Feature Selection (SelectKBest, SelectPercentile, GenericUnivariateSelect)\n",
    "    # Recursive Feature Elimination (RFE, RFECV)        RFECV - RFE cross validation\n",
    "    # Feature selection using SelectFromModel (SelectFromModel) - use L1-based (Lasso, Ridge, ElasticNet) or Tree-based\n",
    "    # Sequential Feature Selection (SequentialFeatureSelector) - SFS can be either forward or backward\n",
    "\n",
    "\n",
    "#Feature extraction\n",
    "    # Principal Component Analysis (PCA)\n",
    "    # Independent Component Analysis (ICA)\n",
    "    # t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variance threshold: - # Removing features with low variance\n",
    "# This technique removes all features whose variance is below a certain threshold. \n",
    "# This is done using the VarianceThreshold function from scikit-learn library.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def variance_threshold(X, threshold=0.0):\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    X_new = selector.fit_transform(X)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# SelectKBest:\n",
    "# This technique selects the K best features based on univariate statistical tests. \n",
    "# This is done using the SelectKBest function from scikit-learn library\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def select_k_best(X, y, k=10):\n",
    "    selector = SelectKBest(score_func = f_classif, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# Principal Component Analysis (PCA):\n",
    "# This technique reduces the dimensionality of the data by projecting it onto a lower dimensional space. \n",
    "# This is done using the PCA function from scikit-learn library.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_new = pca.fit_transform(X)\n",
    "    return X_new \n",
    "                    # # Get the loadings of the original variables in each component\n",
    "                    # loadings = pca.components_\n",
    "                    # # Print the names of the columns that were extracted\n",
    "                    # print(\"Columns extracted:\")\n",
    "                    # for i in range(loadings.shape[0]):\n",
    "                    #     max_loading_index = loadings[i].argmax()\n",
    "                    #     column_name = data.columns[max_loading_index]\n",
    "                    #     print(f\"Component {i+1}: {column_name}\")\n",
    "\n",
    "# Independent Component Analysis (ICA):\n",
    "# This technique extracts independent sources from the data by maximizing their statistical independence. \n",
    "# This is done using the FastICA function from scikit-learn library.\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "def ica(X, n_components=2):\n",
    "    ica = FastICA(n_components=n_components)\n",
    "    X_new = ica.fit_transform(X)\n",
    "    return X_new \n",
    "\n",
    "\n",
    "# t-distributed Stochastic Neighbor Embedding (t-SNE):\n",
    "# This technique is used for visualizing high-dimensional data in a low-dimensional space. \n",
    "# This is done using the TSNE function from scikit-learn library.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(X, n_components=2, perplexity=30):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "    X_new = tsne.fit_transform(X)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "#Feature Selection \n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Univariate Feature Selection\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "estimator = RandomForestClassifier()\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "pca = PCA(n_components=2)\n",
    "X_new = pca.fit_transform(X)\n",
    "\n",
    "# backward elimination (you can use ny model of choice) \n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_scaled, y)\n",
    "model = SelectFromModel(lasso, prefit=True) \n",
    "X_new = model.transform(X_scaled)\n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_scaled, y)\n",
    "model = SelectFromModel(ridge, prefit=True)\n",
    "X_new = model.transform(X_scaled)\n",
    "\n",
    "# Elastic Net\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic.fit(X_scaled, y)\n",
    "model = SelectFromModel(elastic, prefit=True)\n",
    "X_new = model.transform(X_scaled)\n",
    "\n",
    "# Tree-based Feature Selection\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "model = SelectFromModel(rf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "# Mutual Information Feature Selection\n",
    "X_new = SelectKBest(score_func=mutual_info_classif, k=2).fit_transform(X, y)\n",
    "\n",
    "# Sequential Feature Selection\n",
    "estimator = RandomForestClassifier()\n",
    "selector = SequentialFeatureSelector(estimator, n_features_to_select=2)\n",
    "selector = selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56c08676",
   "metadata": {},
   "source": [
    ">> ML evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c78d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "classes = digits.target_names #or df['target].unique()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))\n",
    "# A confusion matrix is a table that is often used to evaluate the performance of a machine learning algorithm. \n",
    "# It shows the number of true positives, false positives, true negatives, and false negatives for a given \n",
    "# classification task.\n",
    "\n",
    "# A confusion matrix has two axes: one for the predicted values and one for the actual values. Each axis has two \n",
    "#     categories: positive and negative. Therefore, a confusion matrix for a binary classification task will have \n",
    "#     four cells:\n",
    "# True Positive (TP): the actual value was positive, and the predicted value was also positive.\n",
    "# False Positive (FP): the actual value was negative, but the predicted value was positive.\n",
    "# True Negative (TN): the actual value was negative, and the predicted value was also negative.\n",
    "# False Negative (FN): the actual value was positive, but the predicted value was negative.\n",
    "                Predicted Positive    Predicted Negative\n",
    "Actual Positive         TP                   FN\n",
    "Actual Negative         FP                   TN\n",
    "\n",
    "#Recall\n",
    "# the proportion of true positives among the total number of actual positives. It is calculated as TP / (TP + FN).\n",
    "\n",
    "#Accuracy\n",
    "# the proportion of true results (both true positives and true negatives) among the total number of cases examined. \n",
    "# It is calculated as (TP + TN) / (TP + FP + TN + FN).\n",
    "\n",
    "#Precision\n",
    "# The proportion of true positives among the total number of positive predictions. It is calculated as TP / (TP + FP).\n",
    "\n",
    "#F1-score\n",
    "# the harmonic mean of precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4d27b07",
   "metadata": {},
   "source": [
    ">> Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4642c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd ## For DataFrame operation\n",
    "import numpy as np ## Numerical python for matrix operations\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder ## Preprocessing function\n",
    "import pandas_profiling ## For easy profiling of pandas DataFrame \n",
    "import missingno as msno ## Missing value co-occurance analysis\n",
    "\n",
    "####### Data Exploration ############\n",
    "\n",
    "def print_dim(df):\n",
    "    '''\n",
    "    Function to print the dimensions of a given python dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Data size\n",
    "    '''\n",
    "    print(\"Data size: Rows-{0} Columns-{1}\".format(df.shape[0],df.shape[1]))\n",
    "\n",
    "\n",
    "def print_dataunique(df):\n",
    "    '''\n",
    "    Function to print unique information for each column in a python dataframe\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Column name\n",
    "        - Data type of that column\n",
    "        - Number of unique values in that column\n",
    "        - 5 unique values from that column\n",
    "    '''\n",
    "    counter = 0\n",
    "    for i in df.columns:\n",
    "        x = df.loc[:,i].unique()\n",
    "        print(counter,i,type(df.loc[0,i]), len(x), x[0:5])\n",
    "        counter +=1\n",
    "        \n",
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = pandas_profiling.ProfileReport(df)\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "def view_datatypes_in_perspective(df):\n",
    "    '''\n",
    "    Function to group dataframe columns into three common dtypes and visualize the columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - three unique datatypes (float, object, others(for the rest))\n",
    "    '''\n",
    "    float = 0\n",
    "    float_col = []\n",
    "    object = 0\n",
    "    object_col = []\n",
    "    others = 0\n",
    "    others_col = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype ==  \"float\":\n",
    "            float += 1\n",
    "            float_col.append(col) \n",
    "        elif df[col].dtypes == \"object\":\n",
    "            object += 1\n",
    "            object_col.append(col)\n",
    "        else:\n",
    "            others +=1\n",
    "            others_col.append(col)\n",
    "            others_col.append(smart_home[col].dtype)        \n",
    "    print (f\" float = {float} \\t{float_col}, \\n \\nobject = {object} \\t{object_col}, \\n\\nothers = {others} \\t{others_col} \")\n",
    "\n",
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True:\n",
    "            print(f\"there is {df[col].isnull().sum()} NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)\n",
    "\n",
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "####### Basic helper function ############\n",
    "\n",
    "def join_df(left, right, left_on, right_on=None, method='left'):\n",
    "    '''\n",
    "    Function to outer joins of pandas dataframe\n",
    "    Required Input - \n",
    "        - left = Pandas DataFrame 1\n",
    "        - right = Pandas DataFrame 2\n",
    "        - left_on = Fields in DataFrame 1 to merge on\n",
    "        - right_on = Fields in DataFrame 2 to merge with left_on fields of Dataframe 1\n",
    "        - method = Type of join\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    if right_on is None:\n",
    "        right_on = left_on\n",
    "    return left.merge(right, \n",
    "                      how=method, \n",
    "                      left_on=left_on, \n",
    "                      right_on=right_on, \n",
    "                      suffixes=(\"\",\"_y\"))\n",
    "    \n",
    "####### Pre-processing ############    \n",
    "\n",
    "def drop_allsame(df):\n",
    "    '''\n",
    "    Function to remove any columns which have same value all across\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    to_drop = list()\n",
    "    for i in df.columns:\n",
    "        if len(df.loc[:,i].unique()) == 1:\n",
    "            to_drop.append(i)\n",
    "    return df.drop(to_drop,axis =1)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "#Handling Missing Values\n",
    "----------------------------------------------------\n",
    "#fill Nan Values in the cloudCover column\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(5)\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1)  \n",
    "\n",
    "\n",
    "def treat_missing_categorical(df, columns, how='mode', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in categorical columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mode', any string or numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mode':\n",
    "        for col in columns:\n",
    "            print(\"Filling missing values with mode for column - {0}\".format(col))\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            \n",
    "    elif isinstance(how, str):\n",
    "        for col in columns:\n",
    "            print(\"Filling missing values with '{0}' for column - {1}\".format(how, col))\n",
    "            df[col] = df[col].fillna(how)\n",
    "            \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "            \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(4)\n",
    "\n",
    "\n",
    "#SimpleImputer: This function replaces missing values with a specified strategy.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_missing_values(X, strategy='mean'): #strategy = \"median\", 'most_frequent', 'constant'. (strategy=\"constant\", fill_value=-1)\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X_imputed = pd.DataFrame(X_imputed, \n",
    "                            columns=X.columns, index=X.index )\n",
    "    return X_imputed\n",
    "\n",
    "#MissingIndicator: This function creates a binary indicator for each feature indicating whether the value is missing or not.\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "def create_missing_indicator(X):\n",
    "    indicator = MissingIndicator()\n",
    "    X_missing_indicator = indicator.fit_transform(X)\n",
    "    return X_missing_indicator\n",
    "\n",
    "#KNNImputer: The missing values are estimated as the average value from the closest K neighbours\n",
    "    # multivariate imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "def knn_impute(X, k):\n",
    "    imputer = KNNImputer(n_neighbors=k, # the number of neighbours K\n",
    "                        weights='distance', # the weighting factor\n",
    "                        metric='nan_euclidean', # the metric to find the neighbours\n",
    "                        add_indicator=False, # whether to add a missing indicator\n",
    "                        )\n",
    "    imputed_X = imputer.fit_transform(X)\n",
    "    return imputed_X\n",
    "\n",
    "#IterativeImputer: This function estimates missing values using a predictive model.\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def impute_missing_values_iteratively(X): #or (X, Columns)\n",
    "    imputer = IterativeImputer(\n",
    "        # estimator = RandomForestRegressor() \n",
    "        estimator=BayesianRidge(), # the estimator to predict the NA\n",
    "        initial_strategy='mean', # how will NA be imputed in step 1\n",
    "        max_iter=10, # number of cycles\n",
    "        imputation_order='ascending', # the order in which to impute the variables\n",
    "        n_nearest_features=None, # whether to limit the number of predictors\n",
    "        skip_complete=True, # whether to ignore variables without NA\n",
    "        random_state=0,)\n",
    "        \n",
    "    # select only the columns with missing values to be imputed\n",
    "    # X_cols = X[columns]\n",
    "    X_imputed = imputer.fit_transform(X) #or X_cols\n",
    "    return X_imputed\n",
    "\n",
    "#other predictive models include\n",
    "imputer = IterativeImputer(estimator=BayesianRidge()) #from sklearn.linear_model import BayesianRidge\n",
    "imputer = IterativeImputer(estimator=LinearRegression()) #from sklearn.linear_model import LinearRegression\n",
    "imputer = IterativeImputer(estimator=DecisionTreeRegressor()) #from sklearn.tree import DecisionTreeRegressor\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor()) #from sklearn.ensemble import RandomForestRegressor\n",
    "imputer = IterativeImputer(estimator=KNeighborsRegressor()) #from sklearn.neighbors import KNeighborsRegressor\n",
    "imputer = IterativeImputer(estimator=MLPRegressor()) #from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# IterativeImputer in SKlearn is a class that can estimate missing values in a dataset by modeling each feature with \n",
    "# missing values as a function of the other features. It does this by taking a predictive model and using it to \n",
    "# fill in the missing values iteratively. \n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "def min_max_scaler(df,columns):\n",
    "    '''\n",
    "    Function to do Min-Max scaling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = columns\n",
    "    return data, scaler\n",
    "\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df\n",
    "\n",
    "def z_scaler(df,columns):\n",
    "    '''\n",
    "    Function to standardize features by removing the mean and scaling to unit variance\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = columns\n",
    "    return data, scaler\n",
    "\n",
    "mapping_dict = {        #an example\n",
    "    'First':0,\n",
    "    'Second': 1,\n",
    "    'Third': 2 \n",
    "}\n",
    "def map_encoding(data, feature_name, mapping_dict):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using mapping method.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "        mapping_dict (dict): A dictionary containing the mapping of category values to integers.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Replace the category values with their corresponding integers\n",
    "    encoded_data[feature_name] = encoded_data[feature_name].map(mapping_dict)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "def ordinal_encoding_sklearn(data, feature_name, categories):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using ordinal encoding method with scikit-learn's OrdinalEncoder class.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "        categories (list): The list of categories in the order of their numerical encoding.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Perform ordinal encoding using the OrdinalEncoder class\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[categories])\n",
    "    encoded_data[feature_name] = ordinal_encoder.fit_transform(encoded_data[[feature_name]])\n",
    "    encoded_data[feature_name] = pd.DataFrame(encoded_data, columns=encoded_data.columns, index=encoded_data.index)\n",
    "    \n",
    "    return encoded_data.head\n",
    "\n",
    "\n",
    "def one_hot_encoding_sklearn(data, feature_name):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using one-hot encoding method with scikit-learn's OneHotEncoder class.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Perform one-hot encoding using the OneHotEncoder class\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    encoded_data = pd.DataFrame(one_hot_encoder.fit_transform(encoded_data[[feature_name]]).toarray())\n",
    "    feature_names_out = [f\"{feature_name}_{category}\" for category in one_hot_encoder.categories_[0]]\n",
    "    encoded_data.columns = feature_names_out\n",
    "    encoded_data.index = data.index\n",
    "    encoded_data = pd.concat([data.drop(feature_name, axis=1), encoded_data], axis=1)\n",
    "    encoded_data[feature_name] = pd.DataFrame(encoded_data, columns=encoded_data.columns, index=encoded_data.index)\n",
    "    \n",
    "    return encoded_data.head(3) \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encoding_sklearn(data, feature_name):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using label encoding method with scikit-learn's LabelEncoder class.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Perform label encoding using the LabelEncoder class\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_data[feature_name] = label_encoder.fit_transform(encoded_data[feature_name])\n",
    "    encoded_data[feature_name] = pd.DataFrame(encoded_data, columns=encoded_data.columns, index=encoded_data.index)\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "    \n",
    "def label_encoder(df,columns):\n",
    "    '''\n",
    "    Function to label encode\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be label encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with lable encoded columns\n",
    "        - le_dict = Dictionary of all the column and their label encoders\n",
    "    '''\n",
    "    le_dict = {}\n",
    "    for c in columns:\n",
    "        print(\"Label encoding column - {0}\".format(c))\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df[c].values.astype('str')))\n",
    "        df[c] = lbl.transform(list(df[c].values.astype('str')))\n",
    "        le_dict[c] = lbl\n",
    "    return df, le_dict\n",
    "\n",
    "def one_hot_encoder(df, columns):\n",
    "    '''\n",
    "    Function to do one-hot encoded\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be one-hot encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with one-hot encoded columns\n",
    "    '''\n",
    "    for each in columns:\n",
    "        print(\"One-Hot encoding column - {0}\".format(each))\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df.drop(columns,axis = 1)\n",
    "\n",
    "####### Feature Engineering ############ \n",
    "def create_date_features(df,column, date_format = None, more_features = False, time_features = False): \n",
    "    '''\n",
    "    Function to extract date features\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - date_format = Date parsing format\n",
    "        - columns = Columns name containing date field\n",
    "        - more_features = To get more feature extracted\n",
    "        - time_features = To extract hour from datetime field\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with additional extracted date features\n",
    "    '''\n",
    "    if date_format is None:\n",
    "        df.loc[:,column] = pd.to_datetime(df.loc[:,column])\n",
    "    else:\n",
    "        df.loc[:,column] = pd.to_datetime(df.loc[:,column],format = date_format)\n",
    "    df.loc[:,column+'_Year'] = df.loc[:,column].dt.year\n",
    "    df.loc[:,column+'_Month'] = df.loc[:,column].dt.month.astype('uint8')\n",
    "    df.loc[:,column+'_Week'] = df.loc[:,column].dt.week.astype('uint8')\n",
    "    df.loc[:,column+'_Day'] = df.loc[:,column].dt.day.astype('uint8')\n",
    "    \n",
    "    if more_features:\n",
    "        df.loc[:,column+'_Quarter'] = df.loc[:,column].dt.quarter.astype('uint8')\n",
    "        df.loc[:,column+'_DayOfWeek'] = df.loc[:,column].dt.dayofweek.astype('uint8')\n",
    "        df.loc[:,column+'_DayOfYear'] = df.loc[:,column].dt.dayofyear\n",
    "        \n",
    "    if time_features:\n",
    "        df.loc[:,column+'_Hour'] = df.loc[:,column].dt.hour.astype('uint8')\n",
    "    return df\n",
    "\n",
    "def target_encoder(train_df, col_name, target_name, test_df = None, how='mean'):\n",
    "    '''\n",
    "    Function to do target encoding\n",
    "    Required Input - \n",
    "        - train_df = Training Pandas Dataframe\n",
    "        - test_df = Testing Pandas Dataframe\n",
    "        - col_name = Name of the columns of the source variable\n",
    "        - target_name = Name of the columns of target variable\n",
    "        - how = 'mean' default but can also be 'count'\n",
    "\tExpected Output - \n",
    "\t\t- train_df = Training dataframe with added encoded features\n",
    "\t\t- test_df = Testing dataframe with added encoded features\n",
    "    '''\n",
    "    aggregate_data = train_df.groupby(col_name)[target_name] \\\n",
    "                    .agg([how]) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns={how: col_name+'_'+target_name+'_'+how})\n",
    "    if test_df is None:\n",
    "        return join_df(train_df,aggregate_data,left_on = col_name)\n",
    "    else:\n",
    "        return join_df(train_df,aggregate_data,left_on = col_name), join_df(test_df,aggregate_data,left_on = col_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc13d1e3",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0c2c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_df(df):       #best for time series analysis when your index is in datetime \n",
    "    \"\"\"\n",
    "    Creates an interactive plot of the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data for the PJM Interconnection region.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays an interactive plot of the energy consumption data using Plotly.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_df(my_df)\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    fig = go.Figure(layout=go.Layout(\n",
    "        height=500,\n",
    "        width=800,\n",
    "    ))\n",
    "\n",
    "    for col in df.columns:\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'PJM Energy Consumption',\n",
    "            'font': {'size': 25, 'family': 'Arial', 'color': 'black'}\n",
    "        },\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Energy Consumption (MW)'\n",
    "    )\n",
    "\n",
    "    return fig.show(renderer='svg')\n",
    "\n",
    "\n",
    "\n",
    "def visualize_subplots_boxplots(df: DataFrame, columns: List[str], nrows: int, ncols: int) -> None:\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots containing boxplots of daily average energy consumption.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data.\n",
    "        columns: A list of column names to include in the boxplots.\n",
    "        nrows: The number of rows in the subplot grid.\n",
    "        ncols: The number of columns in the subplot grid.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a grid of subplots containing boxplots of daily average energy consumption.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_subplots_boxplots(my_df, ['Consumption', 'Generation'], 3, 4)\n",
    "    \"\"\"\n",
    "    from typing import List\n",
    "    from pandas import DataFrame\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 12))\n",
    "    fig.suptitle('Hourly Average Energy Consumption', weight='bold', fontsize=25)\n",
    "\n",
    "    # We just need 11 figures, so we delete the last one\n",
    "    if nrows*ncols > len(columns):\n",
    "        fig.delaxes(axes[nrows-1][ncols-1])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.boxplot(data=df, x='Hour', y=col, ax=axes.flatten()[i], color='#cc444b')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"Images/xxx.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "visualize_subplots_boxplots(df=result_2, columns=['AEP_MW', 'COMED_MW', 'DAYTON_MW', 'DEOK_MW', 'DOM_MW', 'DUQ_MW',\n",
    "        'EKPC_MW', 'FE_MW', 'NI_MW', 'PJME_MW', 'PJMW_MW'], nrows=6, ncols=2)\n",
    "\n",
    "\n",
    "\n",
    "def visualize_subplots_barcharts(df: DataFrame, columns: List[str], nrows: int, ncols: int) -> None:\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots containing bar charts of energy consumption.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data.\n",
    "        columns: A list of column names to include in the bar charts.\n",
    "        nrows: The number of rows in the subplot grid.\n",
    "        ncols: The number of columns in the subplot grid.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a grid of subplots containing bar charts of energy consumption.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_subplots_barcharts(my_df, ['Consumption', 'Generation'], 3, 4)\n",
    "    \"\"\"\n",
    "    from typing import List\n",
    "    from pandas import DataFrame\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 12))\n",
    "    fig.suptitle('Energy Consumption Bar Charts', weight='bold', fontsize=25)\n",
    "\n",
    "    # We just need enough figures for each column\n",
    "    if nrows*ncols > len(columns):\n",
    "        fig.delaxes(axes[nrows-1][ncols-1])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.barplot(data=df, x='Hour', y=col, ax=axes.flatten()[i], color='#cc444b')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def moving_average(data: pd.DataFrame, window: int) -> None:\n",
    "    \"\"\"\n",
    "    Calculates and visualizes the moving average of a time series data.\n",
    "\n",
    "    Args:\n",
    "        data: A Pandas DataFrame containing the time series data.\n",
    "        window: An integer representing the window size for calculating the moving average.\n",
    "\n",
    "    Returns:\n",
    "        None. Visualizes the actual data and the moving average.\n",
    "\n",
    "    Example:\n",
    "        >>> moving_average(my_data, 5)\n",
    "    \"\"\"\n",
    "    # calculate the moving average\n",
    "    data['Moving Average'] = data['DAYTON_MW'].rolling(window).mean()\n",
    "    actual = data['DAYTON_MW'][-(window+30):]\n",
    "    ma = data['Moving Average'][-(window+30):]\n",
    "\n",
    "    # plot the actual data and moving average\n",
    "    plt.figure(figsize=(20,8))\n",
    "    actual.plot(label='Actual', lw=4)\n",
    "    ma.plot(label='MA-{}'.format(str(window)), ls='--', lw=2)\n",
    "    plt.title('{}-Days Moving Average'.format(str(window)), weight='bold', fontsize=25, loc= \"center\", pad=20)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_pivot_line(pivot: DataFrame, xlabel: str, ylabel: str, title: str, savepath: str):\n",
    "    \"\"\"\n",
    "    Plots a line chart from a pivot table and saves it to a file.\n",
    "\n",
    "    Args:\n",
    "        pivot: A Pandas DataFrame containing the data to plot in a pivot table format.\n",
    "        xlabel: A string representing the label for the X-axis.\n",
    "        ylabel: A string representing the label for the Y-axis.\n",
    "        title: A string representing the title of the plot.\n",
    "        savepath: A string representing the file path to save the plot image.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays and saves a line chart from the input pivot table.\n",
    "\n",
    "    Example:\n",
    "        >>> plot_pivot_line(my_pivot, \"Year\", \"Deaths\", \"Deaths by Cardiovascular diseases\", \"/path/to/image.png\")\n",
    "    \"\"\"\n",
    "    fig,ax = plt.subplots(nrows = 2, ncols = 2, figsize=(3,2), dpi=100)\n",
    "    pivot.plot(kind='line', ax = ax)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title, weight='bold', fontsize=25, loc= \"center\", pad=20)\n",
    "    plt.show()\n",
    "    fig.savefig(savepath, dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "def plot_boxplot(focus_countries, save_path):\n",
    "    \"\"\"\n",
    "    Plots a boxplot of deaths caused by Cardiovascular diseases for 70+ years, for different entities.\n",
    "\n",
    "    Args:\n",
    "        focus_countries: A Pandas DataFrame containing the relevant data.\n",
    "        save_path: A string containing the path to save the plot image.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays and saves the plot.\n",
    "\n",
    "    Example:\n",
    "        >>> plot_boxplot(my_df, \"C:/myDrive/xx_image.png\")\n",
    "    \"\"\"\n",
    "    sns.boxplot(x=\"Year\", y=\"Deaths 70+ years\", hue='Entity', data=focus_countries, palette='mako')\n",
    "    plt.title('Deaths caused by Cardiovascular deaths for 70+ Years', weight='bold', fontsize=25, loc= \"center\", pad=20)\n",
    "    ax = plt.gca() # Get the Axes object\n",
    "    ax.set_xlim(0, 50) # Set the x-axis range\n",
    "    ax.xaxis.set_ticks(range(0, 55, 5)) # Set the number of x-axis values to display or ax.xaxis.set_ticks([1990, 1995, 2000, 2005, 2010])\n",
    "    ax.set_xticks([1990, 1995, 2000, 2005, 2010, 2015, 2020])\n",
    "    ax.set_xticklabels([\"1990\", \"1995\", \"2000\", \"2005\", \"2010\", \"2015\", \"2020\"])\n",
    "    plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def plot_data_splitting(train, test):\n",
    "    \"\"\"\n",
    "    Plots the training and test sets of a time series.\n",
    "\n",
    "    Args:\n",
    "    train (pandas.DataFrame): DataFrame containing the training set with a DatetimeIndex and a 'PJME_MW' column.\n",
    "    test (pandas.DataFrame): DataFrame containing the test set with a DatetimeIndex and a 'PJME_MW' column.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "\n",
    "    plt.plot(train.index, train['PJME_MW'], label='Training Set')\n",
    "    plt.plot(test.index, test['PJME_MW'], label='Test Set')\n",
    "\n",
    "    plt.title('Data Splitting', weight='bold', fontsize=25, loc= \"center\", pad=20)\n",
    "    plt.axvline('2015-09-01', color='black', ls='--', lw=3) \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_mean_energy_per_month(df):     #you can change the month column to plot for hour, day, year etc. \n",
    "    \"\"\"\n",
    "    Plots the mean energy consumption per month for each column in the given DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a plot of the mean energy consumption per month for each column in the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        >>> plot_mean_energy_per_month(my_df)\n",
    "    \"\"\"\n",
    "    mean_month = df.groupby('month').agg({i: 'mean' for i in df.columns[:-5].tolist()})\n",
    "    mean_month[mean_month.columns[0:13].tolist()].plot(subplots=True, layout=(-1, 3), figsize=(15, 10),\n",
    "                                                        grid=True, rot=45, xlabel=None, marker='o')\n",
    "    plt.savefig(\"Images/mean_energy_per_month.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def generate_energy_plots():\n",
    "    \"\"\"\n",
    "    Generates energy consumption plots for each month of 2016.\n",
    "\n",
    "    Args:\n",
    "        None.\n",
    "\n",
    "    Returns:\n",
    "        None. Saves energy consumption plots for each month to individual PNG files and displays the plots.\n",
    "\n",
    "    Example:\n",
    "        >>> generate_energy_plots()\n",
    "    \"\"\"\n",
    "    # Load energy consumption data\n",
    "    energy_data = pd.read_csv('energy_data.csv', parse_dates=['Date/Time'])\n",
    "    energy_data = energy_data.set_index('Date/Time')\n",
    "    \n",
    "    # Resample to daily frequency\n",
    "    energy_per_day = energy_data.resample('D').sum()\n",
    "    \n",
    "    # Define columns to include in plots\n",
    "    cols_energy = energy_per_day.columns[:-5].tolist()\n",
    "    \n",
    "    for month in range(1, 13):\n",
    "        # Filter the energy and weather data for the current month\n",
    "        start_date = f'2016-{month:02}-01'\n",
    "        end_date = f'2016-{month:02}-' + str(calendar.monthrange(2016, month)[1])\n",
    "        energy_per_day_month = energy_per_day.loc[start_date:end_date].filter(items=cols_energy)\n",
    "\n",
    "        # Generate the plots for the current month\n",
    "        fig_energy = px.line(data_frame=energy_per_day_month, line_dash_sequence=['solid']*15, width=900, height=600, title=f'Energy Consumption - {calendar.month_name[month]}')\n",
    "\n",
    "        # Save the plots to files\n",
    "        fig_energy.write_image(f'Images/energy_{month:02}.png')\n",
    "\n",
    "        # Show the plots\n",
    "        fig_energy.show()\n",
    "\n",
    "def plot_predictions(y_pred, y_test):\n",
    "    \"\"\"\n",
    "    Plots the predicted and actual values on separate scatter plots.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot the actual values\n",
    "    ax1.scatter(range(len(y_test)), y_test, label='Actual Values')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Actual Values')\n",
    "    ax1.set_title('Scatter plot of Actual Values')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot the predicted values\n",
    "    ax2.scatter(range(len(y_pred)), y_pred, label='Predicted Values')\n",
    "    ax2.set_xlabel('Index')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title('Scatter plot of Predicted Values')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13de6381",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fb24615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-Learn Sub-modules\n",
    "\n",
    "# Scikit-Learn library is organized into several sub-modules, each of which contains a set of related functions and classes. \n",
    "# Here are the main sub-modules in scikit-learn:\n",
    "\n",
    "#from sklearn.\"sub-module\" import \"model\"\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "\n",
    "# sklearn.datasets: This sub-module provides a set of standard datasets for machine learning, including iris, \n",
    "#     digits, and breast cancer.\n",
    "        from sklearn.datasets import load_iris\n",
    "        iris_data = load_iris()\n",
    "        iris_features = iris_data.data \n",
    "        iris_target = iris_data.target\n",
    "        \n",
    "        # Convert the data to a DataFrame\n",
    "        df = pd.DataFrame(iris_features, columns=iris_data.feature_names)\n",
    "        \n",
    "        # Add the target variable to the DataFrame\n",
    "        df['target'] = iris_target \n",
    "        \n",
    "        # print(iris_data.DESCR) - Describes the data \n",
    "        # iris_data.data: An array containing the feature values for each instance of the dataset.\n",
    "        # iris_data.target: An array containing the class labels (i.e., 0, 1, or 2) for each instance of the dataset.\n",
    "        # Iris_data.target_names: An array containing the names of the three classes \n",
    "        # iris_data.feature_names: An array containing the names of the attributes \n",
    "        #or\n",
    "        X, y = load_iris(return_X_y=True, as_frame=True) \n",
    "        \n",
    "        from sklearn.datasets import load_digits\n",
    "\n",
    "        X, y = load_digits(return_X_y=True, as_frame=True)\n",
    "        X.shape, y.shape\n",
    "        # Plot the first 10 digits\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(X.iloc[i].values.reshape(8, 8), cmap='gray')\n",
    "        ax.set_title(f\"Digit {y.iloc[i]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# sklearn.model_selection: This sub-module contains functions for model selection, such as splitting data into \n",
    "#     training and test sets, cross-validation, and grid search.\n",
    "\n",
    "# sklearn.preprocessing: This sub-module provides functions for preprocessing data, such as scaling, normalization, \n",
    "#     and encoding categorical variables.\n",
    "\n",
    "# sklearn.feature_extraction: This sub-module contains functions for feature extraction from raw data, \n",
    "#     such as text data, including Bag of Words, CountVectorizer, and TfidfVectorizer.\n",
    "\n",
    "# sklearn.metrics: This sub-module provides functions for evaluating the performance of machine learning models, \n",
    "#     such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "# sklearn.pipeline: This sub-module provides tools for building machine learning pipelines, \n",
    "#     which allows you to chain together multiple steps, such as feature extraction, preprocessing, and model selection.\n",
    "\n",
    "# sklearn.decomposition: This sub-module provides classes for matrix factorization and decomposition, \n",
    "#     such as Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), \n",
    "#     and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "# sklearn.discriminant_analysis: This sub-module provides classes for linear and quadratic discriminant analysis, \n",
    "#     which are used for supervised classification tasks.\n",
    "\n",
    "# sklearn.covariance: This sub-module provides classes for covariance estimation, such as Empirical Covariance and \n",
    "#     Shrunk Covariance.\n",
    "\n",
    "# sklearn.exceptions: This sub-module contains custom exceptions raised by scikit-learn, such as NotFittedError and \n",
    "#     ConvergenceWarning.\n",
    "\n",
    "\n",
    "#Models: \n",
    "\n",
    "\n",
    "# sklearn.linear_model: This sub-module contains classes for linear models, such as linear regression, \n",
    "#     logistic regression, and ridge regression.\n",
    "\n",
    "# sklearn.tree: This sub-module provides classes for decision trees, such as DecisionTreeClassifier and \n",
    "#     DecisionTreeRegressor.\n",
    "\n",
    "# sklearn.ensemble: This sub-module contains classes for ensemble models, such as random forests, AdaBoost, \n",
    "#     and Gradient Boosting.\n",
    "\n",
    "# sklearn.cluster: This sub-module provides classes for clustering, such as KMeans and Hierarchical Clustering.\n",
    "\n",
    "# sklearn.neural_network: This sub-module contains classes for neural networks, such as Multi-Layer Perceptron (MLP) \n",
    "#     and Convolutional Neural Networks (CNNs).\n",
    "\n",
    "# sklearn.svm: This sub-module contains classes for Support Vector Machines (SVMs), such as SVM classifier and regression.\n",
    "\n",
    "# sklearn.manifold: This sub-module provides classes for manifold learning, such as t-SNE and Isomap.\n",
    "\n",
    "# sklearn.naive_bayes: This sub-module provides classes for Naive Bayes models, such as Gaussian Naive Bayes and \n",
    "#     Multinomial Naive Bayes.\n",
    "\n",
    "# sklearn.neighbors: This sub-module provides classes for k-Nearest Neighbors (k-NN) models, \n",
    "#     such as KNeighborsClassifier and KNeighborsRegressor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b17ace5a",
   "metadata": {},
   "source": [
    "### Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c17a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Decision Tree                          (use one hot encoding for this)\n",
    "#steps for Decision tree (classification)\n",
    "    #Start with all examples at the root node\n",
    "    #calculate information gain for all possible features, and pick the one with the highest infrmation gain\n",
    "    #Split the dataset according to selected feature, and create left and right branches of the tree\n",
    "    #Keep repeating splitting process until stopping criteria is met:\n",
    "        #when a node is 100% one class\n",
    "        #when splitting a node will result in the tree exceeding a maximum depth\n",
    "        #information gain from additional splits is less than threshold\n",
    "        #when number of examples in a node is below threshold\n",
    "\n",
    "\n",
    "#Bagging (Bootstrapping + Aggregating (or voting)) i.e., randomly creating samples (subsets) of the dataset with replacement, \n",
    "    # then builds models on the random subsets. The multiple models are combined by taking a majority vote or \n",
    "    # averaging their predictions to make the final prediction or decision\n",
    "    from sklearn.ensemble import BaggingClassifier, VotingClassifier,RandomForestClassifier\n",
    "    bagging_classifier = BaggingClassifier(estimator=RandomForestClassifier(), n_estimators=15,  max_samples=200, max_features=X_train.shape[1])\n",
    "    vot_classifier = VotingClassifier(    \n",
    "                                    estimators=[('log_reg', log_classifier),\n",
    "                                                ('svc', sv_classifier),\n",
    "                                                ('sgd', sgd_classifier)], \n",
    "                                    voting='hard')  #there are several types of voting/aggregation (majority vote,\n",
    "                                                                                                    #average, \n",
    "                                                                                                    # weighted average etc.)\n",
    "#Boosting\n",
    "    #Boosting is a machine learning ensemble technique that combines multiple base models to create a stronger overall model. \n",
    "    # Unlike bagging, which creates subsets of the training data for training base models, The basic idea behind boosting is \n",
    "    # to sequentially train a series of base models, where each subsequent base model focuses on correcting the errors \n",
    "    # made by the previous base models\n",
    "    from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "    #AdaBoost (Adaptive Boosting): \n",
    "    # AdaBoost is a widely used boosting algorithm that assigns higher weights to misclassified samples and adjusts the \n",
    "    # weights of base models based on their accuracy. It places more emphasis on samples that are misclassified by the \n",
    "    # current ensemble and updates the weights of samples and base models accordingly.\n",
    "    \n",
    "    #Gradient Boosting: \n",
    "    # Gradient Boosting is a generalization of AdaBoost that uses gradient descent optimization to minimize the loss \n",
    "    # function of the ensemble model. It sequentially fits the base models to the residuals (i.e., the differences between \n",
    "    # the true labels and the predictions) of the previous base models, resulting in a more accurate and robust model.\n",
    "    \n",
    "    #XGBoost (Extreme Gradient Boosting): XGBoost is a popular implementation of gradient boosting that incorporates \n",
    "    # additional optimizations for improved performance, such as parallelization, regularization, and handling of missing \n",
    "    # values.\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "#Stacking and Blending \n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "    #Stacking refers to training a learning algorithm to combine the predictions of several other algorithms. The \n",
    "    #predictions of the base algorithms are used as input to train the stacking algorithm. This helps to create a \n",
    "    #meta-model that can leverage the predictions of the base models\n",
    "    \n",
    "    #Blending refers to simply averaging the predictions of multiple models. The predictions of the base models are\n",
    "    #combined using a weighted average to get the final prediction. \n",
    "\n",
    "stacking_classifier = StackingClassifier(estimators=[('random forest', RandomForestClassifier()), \n",
    "                                                     ('decision trees', DecisionTreeClassifier()),\n",
    "                                                     ('logistic regression', LogisticRegression())], stack_method='predict'\n",
    "                                         final_estimator=RandomForestClassifier())    \n",
    "        #final_estimator is the metal-model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf3fc6ce",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data:\n",
    "#     Some machine learning models, such as decision trees and random forests, can handle missing data directly, \n",
    "#     while others may require imputation or removal of missing data. For example, models like K-nearest neighbors (KNN) \n",
    "#     and Support Vector Machines (SVM) may be sensitive to missing data and may require imputation or removal of \n",
    "#     missing values before training the model.\n",
    "\n",
    "# Data Imbalance:\n",
    "#     Techniques such as oversampling, undersampling, or using ensemble methods like SMOTE \n",
    "#     (Synthetic Minority Over-sampling Technique) can be used to address data imbalance. Some machine learning models, \n",
    "#     such as decision trees and random forests, can handle imbalanced data well, while others may require handling of \n",
    "#     imbalanced data as a preprocessing step, such as using oversampling or undersampling techniques. \n",
    "#     For example, models like logistic regression, naive Bayes, KNN,    and support vector machines (SVM) may require handling \n",
    "#     of imbalanced data.\n",
    "\n",
    "# Feature Scaling:\n",
    "#     Some machine learning models, such as k-nearest neighbors (KNN) and support vector machines (SVM), are sensitive \n",
    "#     to the scale of features and may require feature scaling.  On the other hand, decision trees and random forests \n",
    "#     are not sensitive to feature scaling and do not require this preprocessing step.\n",
    "\n",
    "# Categorical Data:\n",
    "#     Some models, like decision trees and random forests, can directly handle categorical data without encoding, \n",
    "#     while others, like logistic regression and support vector machines (SVM), may require encoding of categorical data\n",
    "\n",
    "# Outliers:\n",
    "#     Some machine learning models, such as decision trees and random forests, are less sensitive to outliers, while \n",
    "#     others, such as linear regression, SVM, and k-nearest neighbors (KNN), can be affected by outliers and may require \n",
    "#     handling of outliers as a preprocessing step.\n",
    "    \n",
    "# Dimensionality:\n",
    "    #  refers to the number of features or variables in the dataset. High-dimensional data can lead to increased \n",
    "    #  complexity, increased computation time, and reduced model performance. \n",
    "    #  Some machine learning models, such as decision trees and random forests, are less sensitive to \n",
    "    #  high-dimensional data, while others, such as logistic regression and support vector machines (SVM), \n",
    "    #  may require handling of high-dimensional data as a preprocessing step\n",
    "\n",
    "#reduce memory usage of the dataset\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78800008",
   "metadata": {},
   "source": [
    "### Machine Learning Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b16720",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd ## For DataFrame operation\n",
    "import numpy as np ## Numerical python for matrix operations\n",
    "from sklearn.model_selection import KFold, train_test_split ## Creating cross validation sets\n",
    "from sklearn import metrics ## For loss functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Libraries for Regressiion algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb \n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "import lime \n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "model.get_params()  #to get the parameters of the models in order to improve it\n",
    "\n",
    "########### Cross Validation ###########\n",
    "### 1) Train test split\n",
    "def holdout_cv(X,y,size = 0.3, seed = 1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = size, random_state = seed)\n",
    "    X_train = X_train.reset_index(drop='index')\n",
    "    X_test = X_test.reset_index(drop='index')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "### 2) Cross-Validation (K-Fold)\n",
    "def kfold_cv(X,n_folds = 5, seed = 1):\n",
    "    cv = KFold(n_splits = n_folds, random_state = seed, shuffle = True)\n",
    "    return cv.split(X)\n",
    "\n",
    "########### Model Explanation ###########\n",
    "## Variable Importance plot\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "def standardize_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Standardizes the training and testing data using the mean and standard deviation\n",
    "    learned from the training set.\n",
    "    \n",
    "    Args:\n",
    "    - X_train: numpy array or pandas dataframe, training data\n",
    "    - X_test: numpy array or pandas dataframe, testing data\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled: numpy array or pandas dataframe, standardized training data\n",
    "    - X_test_scaled: numpy array or pandas dataframe, standardized testing data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    # Set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the training set\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    # Transform the training and testing sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def mean_normalize(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on both the training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy.ndarray\n",
    "        The training set features as a 2D array.\n",
    "\n",
    "    X_test: numpy.ndarray\n",
    "        The testing set features as a 2D array.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_norm: numpy.ndarray\n",
    "        The mean-normalized training set features as a 2D array.\n",
    "\n",
    "    X_test_norm: numpy.ndarray\n",
    "        The mean-normalized testing set features as a 2D array.\n",
    "    \"\"\"\n",
    "    scaler_mean = StandardScaler(with_mean=True, with_std=False) # set up the scaler\n",
    "    scaler_minmax = RobustScaler(with_centering = False, with_scaling = True,   #use this when working with outliers\n",
    "                                 quantile_range = (0,100))\n",
    "    \n",
    "    scaler_mean.fit(X_train) # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler_minmax.fit(X_train) #fit the scaler to the train set, it will learn the parameters\n",
    "    \n",
    "    X_train_norm = scaler_minmax.transform(scaler_mean.transform(X_train)) # transform train set\n",
    "    X_test_norm = scaler_minmax.transform(scaler_mean.transform(X_test)) # transform test set\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "\n",
    "def scale_min_max(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales the features in X_train and X_test to the range [0, 1] using MinMaxScaler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy array\n",
    "        Training data features\n",
    "        \n",
    "    X_test: numpy array\n",
    "        Test data features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_scaled: numpy array\n",
    "        Scaled training data features\n",
    "        \n",
    "    X_test_scaled: numpy array\n",
    "        Scaled test data features\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    # set up the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "########### Functions for explaination using Lime ###########\n",
    "\n",
    "## Make a prediction function\n",
    "def make_prediction_function(model, type = None):\n",
    "    if type == 'xgb':\n",
    "        predict_fn = lambda x: model.predict(xgb.DMatrix(x)).astype(float)\n",
    "    else:\n",
    "        predict_fn = lambda x: model.predict(x).astype(float)\n",
    "    return predict_fn\n",
    "\n",
    "## Make a lime explainer\n",
    "def make_lime_explainer(df, c_names = [], verbose_val = True):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(df.values,\n",
    "                                                       class_names=c_names,\n",
    "                                                       feature_names = list(df.columns),\n",
    "                                                       kernel_width=3, \n",
    "                                                       verbose=verbose_val,\n",
    "                                                       mode='regression'\n",
    "                                                    )\n",
    "    return explainer\n",
    "\n",
    "## Lime explain function\n",
    "def lime_explain(explainer,predict_fn, df, index = 0, num_features = None,\n",
    "                 show_in_notebook = True, filename = None):\n",
    "    if num_features is not None:\n",
    "        exp = explainer.explain_instance(df.values[index], predict_fn, num_features=num_features)\n",
    "    else:\n",
    "        exp = explainer.explain_instance(df.values[index], predict_fn, num_features=df.shape[1])\n",
    "    \n",
    "    if show_in_notebook:\n",
    "        exp.show_in_notebook(show_all=False)\n",
    "    \n",
    "    if filename is not None:\n",
    "        exp.save_to_file(filename)\n",
    "        \n",
    "########### Algorithms For Regression ###########\n",
    "\n",
    "### Running Xgboost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, \n",
    "           rounds=500, dep=8, eta=0.05,sub_sample=0.7,col_sample=0.7,\n",
    "           min_child_weight_val=1, silent_val = 1):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"\n",
    "    params['eval_metric'] = 'rmse'\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = sub_sample\n",
    "    params[\"min_child_weight\"] = min_child_weight_val\n",
    "    params[\"colsample_bytree\"] = col_sample\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"silent\"] = silent_val\n",
    "    params[\"seed\"] = seed_val\n",
    "    #params[\"max_delta_step\"] = 2\n",
    "    #params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    \n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "    \n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_iteration)\n",
    "    \n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.mean_squared_error(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "        \n",
    "### Running LightGBM\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, \n",
    "           seed_val=0, rounds=500, dep=8, eta=0.05,sub_sample=0.7,\n",
    "           col_sample=0.7,silent_val = 1,min_data_in_leaf_val = 20, bagging_freq = 5):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"regression\"\n",
    "    params['metric'] = 'rmse'\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = min_data_in_leaf_val\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = sub_sample\n",
    "    params[\"feature_fraction\"] = col_sample\n",
    "    params[\"bagging_freq\"] = bagging_freq\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = silent_val\n",
    "    num_rounds = rounds\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.Dataset(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "        \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    \n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    \n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.mean_squared_error(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "        \n",
    "### Running Extra Trees  \n",
    "def runET(train_X, train_y, test_X, test_y=None, test_X2=None, rounds=100, depth=20,\n",
    "          leaf=10, feat=0.2, min_data_split_val=2,seed_val=0,job = -1):\n",
    "\tmodel = ExtraTreesRegressor(\n",
    "                                n_estimators = rounds,\n",
    "                                max_depth = depth,\n",
    "                                min_samples_split = min_data_split_val,\n",
    "                                min_samples_leaf = leaf,\n",
    "                                max_features =  feat,\n",
    "                                n_jobs = job,\n",
    "                                random_state = seed_val)\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\ttrain_preds = model.predict(train_X)\n",
    "\ttest_preds = model.predict(test_X)\n",
    "\t\n",
    "\ttest_preds2 = 0\n",
    "\tif test_X2 is not None:\n",
    "\t\ttest_preds2 = model.predict(test_X2)\n",
    "\t\n",
    "\ttest_loss = 0\n",
    "\tif test_y is not None:\n",
    "\t\ttrain_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "\t\ttest_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "\t\tprint(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "\t\tprint(\"Train and Test loss : \", train_loss, test_loss)\n",
    "\treturn test_preds, test_loss, test_preds2, model\n",
    " \n",
    "### Running Random Forest\n",
    "def runRF(train_X, train_y, test_X, test_y=None, test_X2=None, rounds=100, depth=20, leaf=10,\n",
    "          feat=0.2,min_data_split_val=2,seed_val=0,job = -1):\n",
    "    model = RandomForestRegressor(\n",
    "                                n_estimators = rounds,\n",
    "                                max_depth = depth,\n",
    "                                min_samples_split = min_data_split_val,\n",
    "                                min_samples_leaf = leaf,\n",
    "                                max_features =  feat,\n",
    "                                n_jobs = job,\n",
    "                                random_state = seed_val)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "    \n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running Linear regression\n",
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None):\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running Decision Tree\n",
    "def runDT(train_X, train_y, test_X, test_y=None, test_X2=None, criterion='mse', \n",
    "          depth=None, min_split=2, min_leaf=1):\n",
    "    model = DecisionTreeRegressor(\n",
    "                                criterion = criterion, \n",
    "                                max_depth = depth, \n",
    "                                min_samples_split = min_split, \n",
    "                                min_samples_leaf=min_leaf)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "    \n",
    "### Running K-Nearest Neighbour\n",
    "def runKNN(train_X, train_y, test_X, test_y=None, test_X2=None, \n",
    "           neighbors=5, job = -1):\n",
    "    model = KNeighborsRegressor(\n",
    "                                n_neighbors=neighbors, \n",
    "                                n_jobs=job)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running SVM\n",
    "def runSVC(train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, \n",
    "           eps=0.1, kernel_choice = 'rbf'):\n",
    "    model = SVR(\n",
    "                C=C, \n",
    "                kernel=kernel_choice,  \n",
    "                epsilon=eps)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0947809",
   "metadata": {},
   "source": [
    "### Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd  ## For DataFrame operation\n",
    "import numpy as np  ## Numerical python for matrix operations\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    ")  ## Creating cross validation sets\n",
    "from sklearn import metrics  ## For loss functions\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "## For evaluation\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "from inspect import signature\n",
    "\n",
    "## Libraries for Classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "model.get_params()  #to get the parameters of the models in order to improve it\n",
    "\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42): #split data into train, test, and validation\n",
    "    \"\"\"\n",
    "    This function splits the data into train and test sets, and further splits the train set into training and validation sets.\n",
    "    \n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing the input data.\n",
    "    target_col : str\n",
    "        The name of the target column in the dataframe.\n",
    "    test_size : float, optional (default=0.2)\n",
    "        The proportion of the data to be used for testing.\n",
    "    val_size : float, optional (default=0.2)\n",
    "        The proportion of the training data to be used for validation.\n",
    "    random_state : int, optional (default=42)\n",
    "        The seed used by the random number generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xtrain : pandas DataFrame\n",
    "        The training input data.\n",
    "    ytrain : pandas Series\n",
    "        The training target data.\n",
    "    xvalid : pandas DataFrame\n",
    "        The validation input data.\n",
    "    yvalid : pandas Series\n",
    "        The validation target data.\n",
    "    xtest : pandas DataFrame\n",
    "        The test input data.\n",
    "    ytest : pandas Series\n",
    "        The test target data.\n",
    "    \"\"\" \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "########### Cross Validation ###########\n",
    "### 1) Train test split\n",
    "def holdout_cv(X, y, size=0.3, seed=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=size, random_state=seed\n",
    "    )\n",
    "    X_train = X_train.reset_index(drop=\"index\")\n",
    "    X_test = X_test.reset_index(drop=\"index\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "### 2) Cross-Validation (K-Fold)\n",
    "def kfold_cv(X, n_folds=5, seed=1):\n",
    "    cv = KFold(n_splits=n_folds, random_state=seed, shuffle=True)\n",
    "    return cv.split(X)\n",
    "\n",
    "\n",
    "########### Model Explanation ###########\n",
    "## Plotting AUC ROC curve\n",
    "def plot_roc(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Model (AUC = %0.2f)\" % (roc_auc_score(y_actual, y_pred)),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        linestyle=\"--\",\n",
    "        lw=2,\n",
    "        color=\"r\",\n",
    "        label=\"Luck (AUC = 0.5)\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precisionrecall(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    average_precision = average_precision_score(y_actual, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_actual, y_pred)\n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = (\n",
    "        {\"step\": \"post\"} if \"step\" in signature(plt.fill_between).parameters else {}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.step(recall, precision, color=\"b\", alpha=0.2, where=\"post\")\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color=\"b\", **step_kwargs)\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\"Precision-Recall curve: AP={0:0.2f}\".format(average_precision))\n",
    "\n",
    "\n",
    "## Plotting confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    cmap=plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "\n",
    "## Variable Importance plot\n",
    "def feature_importance(model, X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.title(\"Variable Importance\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Functions for explaination using Lime\n",
    "def make_prediction_function(model):\n",
    "    predict_fn = lambda x: model.predict_proba(x).astype(float)\n",
    "    return predict_fn\n",
    "\n",
    "\n",
    "def make_lime_explainer(df, c_names=[], k_width=3, verbose_val=True):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        df.values,\n",
    "        class_names=c_names,\n",
    "        feature_names=list(df.columns),\n",
    "        kernel_width=3,\n",
    "        verbose=verbose_val,\n",
    "    )\n",
    "    return explainer\n",
    "\n",
    "\n",
    "def lime_explain(\n",
    "    explainer,\n",
    "    predict_fn,\n",
    "    df,\n",
    "    index=0,\n",
    "    num_features=None,\n",
    "    show_in_notebook=True,\n",
    "    filename=None,\n",
    "):\n",
    "    if num_features is not None:\n",
    "        exp = explainer.explain_instance(\n",
    "            df.values[index], predict_fn, num_features=num_features\n",
    "        )\n",
    "    else:\n",
    "        exp = explainer.explain_instance(\n",
    "            df.values[index], predict_fn, num_features=df.shape[1]\n",
    "        )\n",
    "\n",
    "    if show_in_notebook:\n",
    "        exp.show_in_notebook(show_all=False)\n",
    "\n",
    "    if filename is not None:\n",
    "        exp.save_to_file(filename)\n",
    "\n",
    "\n",
    "########### Algorithms For Binary classification ###########\n",
    "\n",
    "### Running Xgboost\n",
    "def runXGB(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    min_child_weight_val=1,\n",
    "    silent_val=1,\n",
    "):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eval_metric\"] = \"auc\"\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = sub_sample\n",
    "    params[\"min_child_weight\"] = min_child_weight_val\n",
    "    params[\"colsample_bytree\"] = col_sample\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"silent\"] = silent_val\n",
    "    params[\"seed\"] = seed_val\n",
    "    # params[\"max_delta_step\"] = 2\n",
    "    # params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [(xgtrain, \"train\"), (xgtest, \"test\")]\n",
    "        model = xgb.train(\n",
    "            plst,\n",
    "            xgtrain,\n",
    "            num_rounds,\n",
    "            watchlist,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "\n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(\n",
    "            xgb.DMatrix(test_X2), ntree_limit=model.best_iteration\n",
    "        )\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "### Running Xgboost classifier for model explaination\n",
    "def runXGBC(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    min_child_weight_val=1,\n",
    "    silent_val=1,\n",
    "):\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        learning_rate=eta,\n",
    "        subsample=sub_sample,\n",
    "        min_child_weight=min_child_weight_val,\n",
    "        colsample_bytree=col_sample,\n",
    "        max_depth=dep,\n",
    "        silent=silent_val,\n",
    "        seed=seed_val,\n",
    "        n_estimators=rounds,\n",
    "    )\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running LightGBM\n",
    "def runLGB(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    feature_names=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    silent_val=1,\n",
    "    min_data_in_leaf_val=20,\n",
    "    bagging_freq=5,\n",
    "    n_thread=20,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params[\"metric\"] = metric\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = min_data_in_leaf_val\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = sub_sample\n",
    "    params[\"feature_fraction\"] = col_sample\n",
    "    params[\"bagging_freq\"] = bagging_freq\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = silent_val\n",
    "    params[\"num_threads\"] = n_thread\n",
    "    num_rounds = rounds\n",
    "\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgtrain,\n",
    "            num_rounds,\n",
    "            valid_sets=[lgtrain, lgtest],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "    else:\n",
    "        lgtest = lgb.Dataset(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "\n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = roc_auc_score(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "### Running LightGBM classifier for model explaination\n",
    "def runLGBC(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    silent_val=1,\n",
    "    min_data_in_leaf_val=20,\n",
    "    bagging_freq=5,\n",
    "    n_thread=20,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        max_depth=dep,\n",
    "        learning_rate=eta,\n",
    "        min_data_in_leaf=min_data_in_leaf_val,\n",
    "        bagging_fraction=sub_sample,\n",
    "        feature_fraction=col_sample,\n",
    "        bagging_freq=bagging_freq,\n",
    "        bagging_seed=seed_val,\n",
    "        verbosity=silent_val,\n",
    "        num_threads=n_thread,\n",
    "        n_estimators=rounds,\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = roc_auc_score(train_y, train_preds)\n",
    "        test_loss = roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test AUC : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Extra Trees\n",
    "def runET(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Logistic Regression\n",
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, penalty=\"l1\"):\n",
    "    model = LogisticRegression(C=C, penalty=penalty, n_jobs=-1)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Decision Tree\n",
    "def runDT(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    criterion=\"gini\",\n",
    "    depth=None,\n",
    "    min_split=2,\n",
    "    min_leaf=1,\n",
    "):\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_split,\n",
    "        min_samples_leaf=min_leaf,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running K-Nearest Neighbour\n",
    "def runKNN(train_X, train_y, test_X, test_y=None, test_X2=None, neighbors=5, job=-1):\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors, n_jobs=job)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running SVM\n",
    "def runSVC(\n",
    "    train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, kernel_choice=\"rbf\"\n",
    "):\n",
    "    model = SVC(C=C, kernel=kernel_choice, probability=True)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddfff7",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a11145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "eng_stop = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def word_grams(text, min=1, max=4):\n",
    "    '''\n",
    "    Function to create N-grams from text\n",
    "    Required Input -\n",
    "        - text = text string for which N-gram needs to be created\n",
    "        - min = minimum number of N\n",
    "        - max = maximum number of N\n",
    "    Expected Output -\n",
    "        - s = list of N-grams \n",
    "    '''\n",
    "    s = []\n",
    "    for n in range(min, max+1):\n",
    "        for ngram in ngrams(text, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_bigrams_df(df, column_names):\n",
    "    \"\"\"\n",
    "    Generate bigrams from specified columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to generate bigrams from.\n",
    "    column_names (list of str): List of column names to generate bigrams from.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with bigrams appended as new columns.\n",
    "    \"\"\"\n",
    "    bigram_columns = []\n",
    "    for col in column_names:\n",
    "        bigram_col = f\"{col}_bigrams\"\n",
    "        bigram_columns.append(bigram_col)\n",
    "        df[bigram_col] = df[col].apply(lambda x: generate_bigrams([x]))\n",
    "    return df[bigram_columns]\n",
    "\n",
    "def make_wordcloud(df,column, bg_color='white', w=1200, h=1000, font_size_max=50, n_words=40,g_min=1,g_max=1):\n",
    "    '''\n",
    "    Function to make wordcloud from a text corpus\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - bg_color = Background color\n",
    "        - w = width\n",
    "        - h = height\n",
    "        - font_size_max = maximum font size allowed\n",
    "        - n_word = maximum words allowed\n",
    "        - g_min = minimum n-grams\n",
    "        - g_max = maximum n-grams\n",
    "    Expected Output -\n",
    "        - World cloud image\n",
    "    '''\n",
    "    text = \"\"\n",
    "    for ind, row in df.iterrows(): \n",
    "        text += row[column] + \" \"\n",
    "    text = text.strip().split(' ') \n",
    "    text = word_grams(text,g_min,g_max)\n",
    "    \n",
    "    text = list(pd.Series(word_grams(text,1,2)).apply(lambda x: x.replace(' ','_')))\n",
    "    \n",
    "    s = \"\"\n",
    "    for i in range(len(text)):\n",
    "        s += text[i] + \" \"\n",
    "\n",
    "    wordcloud = WordCloud(background_color=bg_color, \\\n",
    "                          width=w, \\\n",
    "                          height=h, \\\n",
    "                          max_font_size=font_size_max, \\\n",
    "                          max_words=n_words).generate(s)\n",
    "    wordcloud.recolor(random_state=1)\n",
    "    plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def generate_wordcloud(df, column_names):\n",
    "    \"\"\"\n",
    "    Generates a wordcloud from a pandas DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data\n",
    "    column_names (list): List of column names in the DataFrame to generate the wordcloud from\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    all_words = ' '.join([' '.join(text) for col in column_names for text in df[col]])\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_tokens(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - tokenized list output\n",
    "    '''\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def tokenize_columns(dataframe, columns):\n",
    "    \"\"\"\n",
    "    Tokenize the values in specified columns of a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame to tokenize.\n",
    "        columns (list): A list of column names to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with tokenized values in the specified columns.\n",
    "    \"\"\"\n",
    "    # Download necessary NLTK resources if they haven't been downloaded yet\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Create a new DataFrame to hold the tokenized values\n",
    "    tokenized_df = pd.DataFrame()\n",
    "\n",
    "    # Tokenize the values in each specified column\n",
    "    for col in columns:\n",
    "        # Tokenize the values in the current column using NLTK's word_tokenize function\n",
    "        tokenized_values = dataframe[col].apply(nltk.word_tokenize)\n",
    "\n",
    "        # Add the tokenized values to the new DataFrame\n",
    "        tokenized_df[col] = tokenized_values\n",
    "\n",
    "    # Return the new DataFrame with tokenized values\n",
    "    return tokenized_df\n",
    "\n",
    "#another way\n",
    "--------------------------------------------------------------------------\n",
    "def tokenize(text, sep=' ', preserve_case=False):\n",
    "    \"\"\"\n",
    "    Tokenize a string into a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): String to be tokenized\n",
    "    sep (str, optional): Separator to use for tokenization. Defaults to ' '.\n",
    "    preserve_case (bool, optional): Whether to preserve the case of the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tokens\n",
    "    \"\"\"\n",
    "    if not preserve_case:\n",
    "        text = text.lower()\n",
    "    tokens = text.split(sep)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_df(df, column_names, sep=' ', preserve_case=False):\n",
    "    \"\"\"\n",
    "    Tokenize a pandas dataframe with multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to be tokenized\n",
    "    columns (list of str): List of column names to be tokenized\n",
    "    sep (str, optional): Separator to use for tokenization. Defaults to ' '.\n",
    "    preserve_case (bool, optional): Whether to preserve the case of the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Tokenized dataframe\n",
    "    \"\"\"\n",
    "    for col in column_names:\n",
    "        df[col] = df[col].apply(lambda x: tokenize(x, sep, preserve_case))\n",
    "    return df\n",
    "\n",
    "carbon_google1 = tokenize_df (carbon_google1, column_names =  [\"title\"], sep=' ', preserve_case=False)\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "def bag_of_words_features(df, text_columns, target_columns):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and one or two columns and returns a bag of words representation of the data as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas DataFrame): The DataFrame to extract features from.\n",
    "    column1 (str): The name of the first column to use as input data.\n",
    "    column2 (str, optional): The name of the second column to use as input data. If not provided, only the first column will be used.\n",
    "\n",
    "    Returns:\n",
    "    pandas DataFrame: The bag of words representation of the input data as a DataFrame.\n",
    "    \"\"\"\n",
    "        \n",
    "    text_data = df[text_columns].apply(lambda x: \" \".join([str(i) for i in x]), axis=1)\n",
    "\n",
    "    text_data = text_data.str.lower()\n",
    "    vectorizer = CountVectorizer(max_df=0.90, min_df=4, max_features=1000, stop_words=None)\n",
    "    X_bow = vectorizer.fit_transform(text_data)\n",
    "    # Use the new function to get the feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    df.dropna(subset=[target_column], inplace=True) if target_columns else None\n",
    "\n",
    "    X_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "    \n",
    "    if target_columns:        \n",
    "        y = df[target_columns]\n",
    "        return X_bow, y\n",
    "    \n",
    "    return X_bow\n",
    "\n",
    "def convert_lowercase(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be lowercased\n",
    "    Expected Output -\n",
    "        - text - lower cased text string output\n",
    "    '''\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unwanted_characters(df, columns):\n",
    "    \"\"\"\n",
    "    Remove unwanted characters (including smileys and emojies) from specified columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    columns (list): A list of column names to clean.\n",
    "    unwanted_chars (str): The characters to remove.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    import re \n",
    "    unwanted_chars = '[$#&*@%]'\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\u2764\\ufe0f\" # heart emoji\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "            df[col] = df[col].str.replace(unwanted_chars, '')\n",
    "        else:\n",
    "            print(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string \n",
    "    Expected Output -\n",
    "        - text - text string with punctuation removed\n",
    "    '''\n",
    "    return text.translate(None,string.punctuation)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - list output with stopwords removed\n",
    "    '''\n",
    "    return [word for word in text.split() if word not in eng_stop]\n",
    "\n",
    "def remove_short_words(df, column_names, min_length=3):\n",
    "    \"\"\"Remove short words from columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to modify.\n",
    "    column_names (List[str]): A list of column names to modify.\n",
    "    min_length (int, optional): The minimum length of words to keep. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified DataFrame with short words removed from specified columns.\n",
    "    \"\"\"\n",
    "    for column_name in column_names:\n",
    "        df[column_name] = df[column_name].apply(\n",
    "            lambda x: ' '.join([word for word in x.split() if len(word) >= min_length])\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def convert_stemmer(word):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - word - word which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - word output after stemming\n",
    "    '''\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return porter_stemmer.stem(word)\n",
    "\n",
    "def stem_df(df, column_names):\n",
    "    \"\"\"\n",
    "    Perform stemming on a pandas dataframe with multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to be stemmed\n",
    "    columns (list of str): List of column names to be stemmed\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Stemmed dataframe\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    for col in column_names:\n",
    "        df[col] = df[col].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    return df\n",
    "\n",
    "def convert_lemmatizer(word):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - word - word which needs to be lemmatized\n",
    "    Expected Output -\n",
    "        - word - word output after lemmatizing\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "def create_tf_idf(df, column, train_df = None, test_df = None,n_features = None):\n",
    "    '''\n",
    "    Function to do tf-idf on a pandas dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - train_df(optional) = Train DataFrame\n",
    "        - test_df(optional) = Test DataFrame\n",
    "        - n_features(optional) = Maximum number of features needed\n",
    "    Expected Output -\n",
    "        - train_tfidf = train tf-idf sparse matrix output\n",
    "        - test_tfidf = test tf-idf sparse matrix output\n",
    "        - tfidf_obj = tf-idf model\n",
    "    '''\n",
    "    tfidf_obj = TfidfVectorizer(ngram_range=(1,1), stop_words='english', \n",
    "                                analyzer='word', max_features = n_features)\n",
    "    tfidf_text = tfidf_obj.fit_transform(df.ix[:,column].values)\n",
    "    \n",
    "    if train_df is not None:        \n",
    "        train_tfidf = tfidf_obj.transform(train_df.ix[:,column].values)\n",
    "    else:\n",
    "        train_tfidf = tfidf_text\n",
    "\n",
    "    test_tfidf = None\n",
    "    if test_df is not None:\n",
    "        test_tfidf = tfidf_obj.transform(test_df.ix[:,column].values)\n",
    "\n",
    "    return train_tfidf, test_tfidf, tfidf_obj\n",
    "    \n",
    "def create_countvector(df, column, train_df = None, test_df = None,n_features = None):\n",
    "    '''\n",
    "    Function to do count vectorizer on a pandas dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - train_df(optional) = Train DataFrame\n",
    "        - test_df(optional) = Test DataFrame\n",
    "        - n_features(optional) = Maximum number of features needed\n",
    "    Expected Output -\n",
    "        - train_cvect = train count vectorized sparse matrix output\n",
    "        - test_cvect = test count vectorized sparse matrix output\n",
    "        - cvect_obj = count vectorized model\n",
    "    '''\n",
    "    cvect_obj = CountVectorizer(ngram_range=(1,1), stop_words='english', \n",
    "                                analyzer='word', max_features = n_features)\n",
    "    cvect_text = cvect_obj.fit_transform(df.ix[:,column].values)\n",
    "    \n",
    "    if train_df is not None:\n",
    "        train_cvect = cvect_obj.transform(train_df.ix[:,column].values)\n",
    "    else:\n",
    "        train_cvect = cvect_text\n",
    "        \n",
    "    test_cvect = None\n",
    "    if test_df is not None:\n",
    "        test_cvect = cvect_obj.transform(test_df.ix[:,column].values)\n",
    "\n",
    "    return train_cvect, test_cvect, cvect_obj"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8168156b",
   "metadata": {},
   "source": [
    "### NLP Text Preprocessing Steps for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Tokenization\n",
    "# 2. Stopword Removal\n",
    "# 3. Stemming\n",
    "# 4. Lemmatization\n",
    "# 5. Part-of-speech (POS) tagging\n",
    "# 6. Named Entity Recognition (NER)\n",
    "# 7. Spell Checking and Correction\n",
    "# 8. Removing HTML tags, punctuation, and special characters\n",
    "# 9. Converting to Lowercase\n",
    "# 10. Text Vectorization\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenization\n",
    "# The process of converting a raw text into a sequence of tokens (words, phrases, symbols, etc.) is called tokenization.\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = \"This is a sample text for tokenization.\"\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)\n",
    "\n",
    "# 2. Stopword Removal\n",
    "# Stopwords are commonly used words in a language, such as “the,” “and,” “a,” etc., that do not add much meaning to the \n",
    "# text. Removing these words helps to reduce the noise in the text data.\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    print(tokens)\n",
    "\n",
    "# 3. Stemming\n",
    "# Stemming is the process of reducing a word to its base or root form. For example, the words “jumping”, “jumps”, and \n",
    "# “jumped” would all be reduced to “jump” by a stemming algorithm. The main goal of stemming is to reduce different \n",
    "# forms of a word to a common base form, which can help in tasks like text classification, sentiment analysis, and \n",
    "# information retrieval.\n",
    "\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    print(stemmed_words)\n",
    "\n",
    "# 4. Lemmatization\n",
    "# Lemmatization is the process of reducing words to their base or dictionary form (known as a lemma) so that they can be \n",
    "# analyzed as a single item, rather than multiple different forms. For example, the word “running” can be reduced to \n",
    "# its base form “run” through lemmatization.\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    print(lemmatized_words)\n",
    "\n",
    "# 5. Part-of-speech (POS) tagging\n",
    "# Part-of-speech (POS) tagging is the process of identifying and labeling the part of speech of each word in a sentence, \n",
    "# such as a noun, verb, adjective, adverb, etc. POS tagging is useful in various natural languages processing tasks like \n",
    "# sentiment analysis, text classification, information extraction, and machine translation.\n",
    "\n",
    "    import nltk\n",
    "    # Sample sentence\n",
    "    sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Perform POS tagging\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    # Print the POS tags\n",
    "    print(pos_tags)\n",
    "\n",
    "# 6. Named Entity Recognition (NER)\n",
    "# Named Entity Recognition (NER) is a natural language processing technique that is used to identify and extract the \n",
    "# named entities from a given text. Named entities can be anything like a person, organization, location, product, etc.\n",
    "\n",
    "    import spacy\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Sample text for NER\n",
    "    text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "    # Process the text with the language model\n",
    "    doc = nlp(text)\n",
    "    # Extract named entities from the text\n",
    "    for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# 7. Spell Checking and Correction\n",
    "# Spell checking and correction is the process of identifying and correcting spelling errors in the text. It is an \n",
    "# important step in text preprocessing as it can improve the accuracy of natural language processing algorithms that \n",
    "# are applied to text data.\n",
    "\n",
    "    !pip install pyspellchecker\n",
    "\n",
    "    from spellchecker import SpellChecker\n",
    "    # initialize spell checker\n",
    "    spell = SpellChecker()\n",
    "    # example sentence with spelling errors\n",
    "    sentence = \"Ths sentnce hs spellng erors that nd to b corcted.\"\n",
    "    # tokenize sentence\n",
    "    tokens = sentence.split()\n",
    "    # iterate over tokens and correct spelling errors\n",
    "    for i in range(len(tokens)):\n",
    "    # check if token is misspelled\n",
    "    if not spell.correction(tokens[i]) == tokens[i]:\n",
    "    # replace misspelled token with corrected spelling\n",
    "    tokens[i] = spell.correction(tokens[i])\n",
    "    # join corrected tokens back into sentence\n",
    "    corrected_sentence = ' '.join(tokens)\n",
    "    print(corrected_sentence)\n",
    "\n",
    "# 8. Removing HTML tags, punctuation, and special characters\n",
    "# Removing HTML tags, punctuation, and special characters is necessary for text preprocessing to clean the text data \n",
    "# and make it ready for further processing. HTML tags, punctuation, and special characters do not contribute to the \n",
    "# meaning of the text and can cause issues during text analysis.\n",
    "\n",
    "    import re\n",
    "    import string\n",
    "\n",
    "    def remove_html_tags(text):\n",
    "    clean_text = re.sub('<.*?>', '', text)\n",
    "    return clean_text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "    clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return clean_text\n",
    "\n",
    "    def remove_special_characters(text):\n",
    "    clean_text = re.sub('[^a-zA-Z0–9\\s]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "    text = \"<p>Hello, world!</p>\"\n",
    "    clean_text = remove_html_tags(text)\n",
    "    clean_text = remove_punctuation(clean_text)\n",
    "    clean_text = remove_special_characters(clean_text)\n",
    "    print(clean_text)\n",
    "\n",
    "# 9. Converting to Lowercase\n",
    "# Lowercasing the text is a common preprocessing step in natural language processing (NLP) to make text data \n",
    "# consistent and easier to analyze. This step involves converting all the letters in the text to lowercase so \n",
    "# that words that differ only by the case are treated as the same word.\n",
    "\n",
    "    text = \"This is a sample TEXT for preprocessing\"\n",
    "    text = text.lower()\n",
    "    print(text)\n",
    "\n",
    "# 10. Text Vectorization\n",
    "# Text vectorization is the process of transforming raw text into a numerical representation that can be used by \n",
    "# machine learning algorithms. This is a crucial step in text preprocessing as most machine learning algorithms work \n",
    "# with numerical data. There are several ways to vectorize text, including Bag of Words (BoW), Term Frequency-Inverse \n",
    "# Document Frequency (TF-IDF), and Word Embeddings.\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "    # Example text corpus\n",
    "    corpus = [\"This is the first document.\", \n",
    "    \"This document is the second document.\", \n",
    "    \"And this is the third one.\", \n",
    "    \"Is this the first document?\"]\n",
    "\n",
    "    # Vectorize text using BoW representation\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    print(\"BoW representation:\")\n",
    "    print(X_bow.toarray())\n",
    "    print(\"Vocabulary:\")\n",
    "    print(vectorizer.get_feature_names())\n",
    "\n",
    "    # Vectorize text using TF-IDF representation\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    print(\"TF-IDF representation:\")\n",
    "    print(X_tfidf.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12652f36",
   "metadata": {},
   "source": [
    "### Recommendation Systems (Recsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20910ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from lightfm import LightFM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_interaction_matrix(df,user_col, item_col, rating_col, norm= False, threshold = None):\n",
    "    '''\n",
    "    Function to create an interaction matrix dataframe from transactional type interactions\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame containing user-item interactions\n",
    "        - user_col = column name containing user's identifier\n",
    "        - item_col = column name containing item's identifier\n",
    "        - rating col = column name containing user feedback on interaction with a given item\n",
    "        - norm (optional) = True if a normalization of ratings is needed\n",
    "        - threshold (required if norm = True) = value above which the rating is favorable\n",
    "    Expected output - \n",
    "        - Pandas dataframe with user-item interactions ready to be fed in a recommendation algorithm\n",
    "    '''\n",
    "    interactions = df.groupby([user_col, item_col])[rating_col] \\\n",
    "            .sum().unstack().reset_index(). \\\n",
    "            fillna(0).set_index(user_col)\n",
    "    if norm:\n",
    "        interactions = interactions.applymap(lambda x: 1 if x > threshold else 0)\n",
    "    return interactions\n",
    "\n",
    "def create_user_dict(interactions):\n",
    "    '''\n",
    "    Function to create a user dictionary based on their index and number in interaction dataset\n",
    "    Required Input - \n",
    "        interactions - dataset create by create_interaction_matrix\n",
    "    Expected Output -\n",
    "        user_dict - Dictionary type output containing interaction_index as key and user_id as value\n",
    "    '''\n",
    "    user_id = list(interactions.index)\n",
    "    user_dict = {}\n",
    "    counter = 0 \n",
    "    for i in user_id:\n",
    "        user_dict[i] = counter\n",
    "        counter += 1\n",
    "    return user_dict\n",
    "    \n",
    "def create_item_dict(df,id_col,name_col):\n",
    "    '''\n",
    "    Function to create an item dictionary based on their item_id and item name\n",
    "    Required Input - \n",
    "        - df = Pandas dataframe with Item information\n",
    "        - id_col = Column name containing unique identifier for an item\n",
    "        - name_col = Column name containing name of the item\n",
    "    Expected Output -\n",
    "        item_dict = Dictionary type output containing item_id as key and item_name as value\n",
    "    '''\n",
    "    item_dict ={}\n",
    "    for i in range(df.shape[0]):\n",
    "        item_dict[(df.loc[i,id_col])] = df.loc[i,name_col]\n",
    "    return item_dict\n",
    "\n",
    "def runMF(interactions, n_components=30, loss='warp', k=15, epoch=30,n_jobs = 4):\n",
    "    '''\n",
    "    Function to run matrix-factorization algorithm\n",
    "    Required Input -\n",
    "        - interactions = dataset create by create_interaction_matrix\n",
    "        - n_components = number of embeddings you want to create to define Item and user\n",
    "        - loss = loss function other options are logistic, brp\n",
    "        - epoch = number of epochs to run \n",
    "        - n_jobs = number of cores used for execution \n",
    "    Expected Output  -\n",
    "        Model - Trained model\n",
    "    '''\n",
    "    x = sparse.csr_matrix(interactions.values)\n",
    "    model = LightFM(no_components= n_components, loss=loss,k=k)\n",
    "    model.fit(x,epochs=epoch,num_threads = n_jobs)\n",
    "    return model\n",
    "\n",
    "def sample_recommendation_user(model, interactions, user_id, user_dict, \n",
    "                               item_dict,threshold = 0,nrec_items = 10, show = True):\n",
    "    '''\n",
    "    Function to produce user recommendations\n",
    "    Required Input - \n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "        - user_id = user ID for which we need to generate recommendation\n",
    "        - user_dict = Dictionary type input containing interaction_index as key and user_id as value\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - threshold = value above which the rating is favorable in new interaction matrix\n",
    "        - nrec_items = Number of output recommendation needed\n",
    "    Expected Output - \n",
    "        - Prints list of items the given user has already bought\n",
    "        - Prints list of N recommended items  which user hopefully will be interested in\n",
    "    '''\n",
    "    n_users, n_items = interactions.shape\n",
    "    user_x = user_dict[user_id]\n",
    "    scores = pd.Series(model.predict(user_x,np.arange(n_items)))\n",
    "    scores.index = interactions.columns\n",
    "    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n",
    "    \n",
    "    known_items = list(pd.Series(interactions.loc[user_id,:] \\\n",
    "                                 [interactions.loc[user_id,:] > threshold].index) \\\n",
    "\t\t\t\t\t\t\t\t .sort_values(ascending=False))\n",
    "    \n",
    "    scores = [x for x in scores if x not in known_items]\n",
    "    return_score_list = scores[0:nrec_items]\n",
    "    known_items = list(pd.Series(known_items).apply(lambda x: item_dict[x]))\n",
    "    scores = list(pd.Series(return_score_list).apply(lambda x: item_dict[x]))\n",
    "    if show == True:\n",
    "        print(\"Known Likes:\")\n",
    "        counter = 1\n",
    "        for i in known_items:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "\n",
    "        print(\"\\n Recommended Items:\")\n",
    "        counter = 1\n",
    "        for i in scores:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "    return return_score_list\n",
    "    \n",
    "\n",
    "def sample_recommendation_item(model,interactions,item_id,user_dict,item_dict,number_of_user):\n",
    "    '''\n",
    "    Funnction to produce a list of top N interested users for a given item\n",
    "    Required Input -\n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "        - item_id = item ID for which we need to generate recommended users\n",
    "        - user_dict =  Dictionary type input containing interaction_index as key and user_id as value\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - number_of_user = Number of users needed as an output\n",
    "    Expected Output -\n",
    "        - user_list = List of recommended users \n",
    "    '''\n",
    "    n_users, n_items = interactions.shape\n",
    "    x = np.array(interactions.columns)\n",
    "    scores = pd.Series(model.predict(np.arange(n_users), np.repeat(x.searchsorted(item_id),n_users)))\n",
    "    user_list = list(interactions.index[scores.sort_values(ascending=False).head(number_of_user).index])\n",
    "    return user_list \n",
    "\n",
    "\n",
    "def create_item_emdedding_distance_matrix(model,interactions):\n",
    "    '''\n",
    "    Function to create item-item distance embedding matrix\n",
    "    Required Input -\n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "    Expected Output -\n",
    "        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w items\n",
    "    '''\n",
    "    df_item_norm_sparse = sparse.csr_matrix(model.item_embeddings)\n",
    "    similarities = cosine_similarity(df_item_norm_sparse)\n",
    "    item_emdedding_distance_matrix = pd.DataFrame(similarities)\n",
    "    item_emdedding_distance_matrix.columns = interactions.columns\n",
    "    item_emdedding_distance_matrix.index = interactions.columns\n",
    "    return item_emdedding_distance_matrix\n",
    "\n",
    "def item_item_recommendation(item_emdedding_distance_matrix, item_id, \n",
    "                             item_dict, n_items = 10, show = True):\n",
    "    '''\n",
    "    Function to create item-item recommendation\n",
    "    Required Input - \n",
    "        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w items\n",
    "        - item_id  = item ID for which we need to generate recommended items\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - n_items = Number of items needed as an output\n",
    "    Expected Output -\n",
    "        - recommended_items = List of recommended items\n",
    "    '''\n",
    "    recommended_items = list(pd.Series(item_emdedding_distance_matrix.loc[item_id,:]. \\\n",
    "                                  sort_values(ascending = False).head(n_items+1). \\\n",
    "                                  index[1:n_items+1]))\n",
    "    if show == True:\n",
    "        print(\"Item of interest :{0}\".format(item_dict[item_id]))\n",
    "        print(\"Item similar to the above item:\")\n",
    "        counter = 1\n",
    "        for i in recommended_items:\n",
    "            print(str(counter) + '- ' +  item_dict[i])\n",
    "            counter+=1\n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94ab6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbbd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebe45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
