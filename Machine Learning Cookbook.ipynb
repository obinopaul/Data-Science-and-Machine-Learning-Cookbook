{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70928dd7",
   "metadata": {},
   "source": [
    "### Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b19a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db23770f",
   "metadata": {},
   "source": [
    "### Machine Learning Models to Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Algorithms to Master  \n",
    "1. Linear and Multiple Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Decision Trees\n",
    "4. Naive Bayes\n",
    "5. K-Nearest Neighbors\n",
    "6. Support Vector Machines\n",
    "7. Random Forests\n",
    "8. Neural Networks\n",
    "    1. Convolutional Neural Network (CNN)\n",
    "    2. Recurrent Neural Network (RNN)\n",
    "    3. Long Short-Term Memory (LSTM)\n",
    "    4. Generative Adversarial Network (GAN)\n",
    "    5. Deep Belief Network (DBN)\n",
    "    6. Deep Boltzmann Machine (DBM)\n",
    "    7. Autoencoders\n",
    "    8. Restricted Boltzmann Machines (RBM)\n",
    "    9. Hopfield Networks\n",
    "    10. Self-Organizing Maps (SOM)\n",
    "9. Gradient Boosting\n",
    "    1. XGBoost\n",
    "    2. LightGBM\n",
    "    3. CatBoost\n",
    "    4. Gradient Boosting Machines (GBM)\n",
    "    5. Stochastic Gradient Boosting (SGB)\n",
    "    6. Adaboost\n",
    "    7. Gradient Boosted Decision Trees (GBDT)\n",
    "    8. DeepBoost\n",
    "    9. Neural Network Boosting (NNBoost)\n",
    "    10. Gradient Boosted Regression Trees (GBRT)\n",
    "10. Reinforcement Learning\n",
    "11. Dimensionality Reduction Algorithms\n",
    "    1. Principal Component Analysis (PCA)\n",
    "    2. Linear Discriminant Analysis (LDA)\n",
    "    3. Independent Component Analysis (ICA)\n",
    "    4. Non-Negative Matrix Factorization (NMF)\n",
    "    5. Factor Analysis\n",
    "    6. Singular Value Decomposition (SVD)\n",
    "    7. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    8. Uniform Manifold Approximation and Projection (UMAP)\n",
    "    9. Autoencoders\n",
    "    10. Random Projection\n",
    "    11. Feature Selection\n",
    "    12. Locally Linear Embedding (LLE)\n",
    "12. Clustering Algorithms\n",
    "    1. K-Means Clustering\n",
    "    2. Hierarchical Clustering\n",
    "    3. Expectation-Maximization (EM) Clustering\n",
    "    4. Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "    5. Mean-Shift Clustering\n",
    "    6. Gaussian Mixture Model (GMM) Clustering\n",
    "    7. Spectral Clustering\n",
    "    8. Affinity Propagation Clustering\n",
    "    9. Birch Clustering\n",
    "    10. Optics Clustering\n",
    "13. Autoencoders\n",
    "14. Transfer Learning\n",
    "15. Generative Adversarial Networks (GANs)\n",
    "\n",
    "\n",
    "Data Preprocessing:\n",
    "    importing the required libraries\n",
    "    importing the dataset\n",
    "    handling missing data\n",
    "    encoding the categoical data\n",
    "    feature engineering\n",
    "    spliting the dataset into test set and training set\n",
    "    feature scaling \n",
    "    *webscraping with beautifulsoup\n",
    "\n",
    "Developing the Model:\n",
    "    model selection\n",
    "    model evaluation\n",
    "    model persistence\n",
    "    ensemble methods\n",
    "    feature extraction\n",
    "    feature selection\n",
    "    feature engineering\n",
    "    hyperparameter tuning\n",
    "    model ensembling\n",
    "    model stacking\n",
    "    model blending\n",
    "    model bagging\n",
    "    model boosting\n",
    "    model averaging"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "84f635b6",
   "metadata": {},
   "source": [
    "### Data Pre-Processing in Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79d98725",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\" \n",
    "1. Data Cleaning:\n",
    "    a. Missing values:\n",
    "        Removing the training example:\n",
    "        Filling in missing value manually\n",
    "        Using a standard value to replace the missing value\n",
    "        Using central tendency (mean, median, mode) for attribute to replace the missing value:\n",
    "        Using central tendency (mean, median, mode) for attribute belonging to same class to replace the missing value:\n",
    "        Using the most probable value to fill in the missing value:\n",
    "\n",
    "    b. Noisy Data and Outliers: \n",
    "        Binning: Using binning methods smooths sorted values by using the values around it. The sorted values are then divided \n",
    "            into bins. \n",
    "        Regression:  Linear regression and multiple linear regression can be used to smooth the data, where the values \n",
    "            are conformed to a function.\n",
    "        Outlier analysis: Approaches such as clustering can be used to detect outliers and deal with them.\n",
    "\n",
    "    c. Remove Unwanted Data: Unwanted data is duplicate or irrelevant data. \n",
    "    \n",
    "2. Data Integration:\n",
    "    Data consolidation: The data is physically brought together to one data store. This usually involves Data Warehousing.\n",
    "    Data propagation: Copying data from one location to another using applications is called data propagation\n",
    "    Data virtualization: An interface is used to provide a real-time and unified view of data from multiple sources. \n",
    "\n",
    "3. Data Reduction:\n",
    "    Missing values ratio: Attributes that have more missing values than a threshold are removed.\n",
    "    Low variance filter: Normalized attributes that have variance (distribution) less than a threshold are also removed \n",
    "        because little changes in data means less information.\n",
    "    High correlation filter: Normalized attributes that have correlation coefficients more than a threshold are removed \n",
    "        because similar trends means similar information is carried. A correlation coefficient is usually calculated using \n",
    "        statistical methods such as Pearson’s chi-square value.\n",
    "    Principal component analysis: Principal component analysis, or PCA, is a statistical method that reduces the numbers \n",
    "        of attributes by lumping highly correlated attributes together.\n",
    "\n",
    "4. Data Transformation:\n",
    "    Smoothing: Eliminating noise in the data to see more data patterns.\n",
    "    Attribute/feature construction: New attributes are constructed from the given set of attributes.\n",
    "    Aggregation: Summary and aggregation operations are applied on the given set of attributes to come up with new attributes\n",
    "    Normalization: The data in each attribute is scaled between a smaller range, for example, 0 to 1 or -1 to 1.\n",
    "    Discretization: Raw values of the numeric attributes are replaced by discrete or conceptual intervals, \n",
    "        which can be further organized into higher-level intervals. \n",
    "    Concept hierarchy generation for nominal data: Values for nominal data are generalized to higher-order concepts.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80f3a91",
   "metadata": {},
   "source": [
    "### Basic ML notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de243c8c",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# #Cost Function\n",
    "# A cost function, also known as a loss function or objective function, \n",
    "# is a mathematical function that measures the difference between predicted and actual values in machine learning. \n",
    "# The purpose of a cost function is to guide the learning algorithm towards finding the optimal model parameters that minimize \n",
    "# the difference between the predicted and actual values.\n",
    "\n",
    "# The choice of cost function depends on the type of problem and the learning algorithm used. \n",
    "# Here are some common examples of cost functions and their equations:\n",
    "\n",
    "# 1. Mean Squared Error (MSE): This cost function is used for regression problems where the goal is to predict a continuous \n",
    "#     variable. It measures the average squared difference between the predicted and actual values. The equation for MSE is:\n",
    "\n",
    "#         MSE = 1/n * ∑(y - y_pred)^2\n",
    "#         where n is the number of samples, y is the actual value, and y_pred is the predicted value.\n",
    "\n",
    "# 2. Binary Cross-Entropy: This cost function is used for binary classification problems where the output is either 0 or 1. \n",
    "#     It measures the difference between the predicted probability and the actual label. \n",
    "#     The equation for binary cross-entropy is:\n",
    "\n",
    "#         Binary cross-entropy = -1/n * ∑(y * log(y_pred) + (1-y) * log(1-y_pred))\n",
    "#         where n is the number of samples, y is the actual label (0 or 1), and y_pred is the predicted probability.\n",
    "\n",
    "# 2. Categorical Cross-Entropy: This cost function is used for multi-class classification problems where the output \n",
    "#     can be one of several classes. It measures the difference between the predicted probability distribution and the actual \n",
    "#     label. The equation for categorical cross-entropy is:\n",
    "\n",
    "#         Categorical cross-entropy = -1/n * ∑∑(y_ij * log(y_pred_ij))\n",
    "#         where n is the number of samples, y_ij is the actual probability for class j in sample i, and y_pred_ij is the predicted probability for class j in sample i."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aa95f",
   "metadata": {},
   "source": [
    "### Generic Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4642c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd ## For DataFrame operation\n",
    "import numpy as np ## Numerical python for matrix operations\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler ## Preprocessing function\n",
    "import pandas_profiling ## For easy profiling of pandas DataFrame\n",
    "import missingno as msno ## Missing value co-occurance analysis\n",
    "\n",
    "####### Data Exploration ############\n",
    "\n",
    "def print_dim(df):\n",
    "    '''\n",
    "    Function to print the dimensions of a given python dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Data size\n",
    "    '''\n",
    "    print(\"Data size: Rows-{0} Columns-{1}\".format(df.shape[0],df.shape[1]))\n",
    "\n",
    "\n",
    "def print_dataunique(df):\n",
    "    '''\n",
    "    Function to print unique information for each column in a python dataframe\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Column name\n",
    "        - Data type of that column\n",
    "        - Number of unique values in that column\n",
    "        - 5 unique values from that column\n",
    "    '''\n",
    "    counter = 0\n",
    "    for i in df.columns:\n",
    "        x = df.loc[:,i].unique()\n",
    "        print(counter,i,type(df.loc[0,i]), len(x), x[0:5])\n",
    "        counter +=1\n",
    "        \n",
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = pandas_profiling.ProfileReport(df)\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "def view_datatypes_in_perspective(df):\n",
    "    '''\n",
    "    Function to group dataframe columns into three common dtypes and visualize the columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - three unique datatypes (float, object, others(for the rest))\n",
    "    '''\n",
    "    float = 0\n",
    "    float_col = []\n",
    "    object = 0\n",
    "    object_col = []\n",
    "    others = 0\n",
    "    others_col = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype ==  \"float\":\n",
    "            float += 1\n",
    "            float_col.append(col) \n",
    "        elif df[col].dtypes == \"object\":\n",
    "            object += 1\n",
    "            object_col.append(col)\n",
    "        else:\n",
    "            others +=1\n",
    "            others_col.append(col)\n",
    "            others_col.append(smart_home[col].dtype)        \n",
    "    print (f\" float = {float} \\t{float_col}, \\n \\nobject = {object} \\t{object_col}, \\n\\nothers = {others} \\t{others_col} \")\n",
    "\n",
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True:\n",
    "            print(\"there is NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)\n",
    "\n",
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "####### Basic helper function ############\n",
    "\n",
    "def join_df(left, right, left_on, right_on=None, method='left'):\n",
    "    '''\n",
    "    Function to outer joins of pandas dataframe\n",
    "    Required Input - \n",
    "        - left = Pandas DataFrame 1\n",
    "        - right = Pandas DataFrame 2\n",
    "        - left_on = Fields in DataFrame 1 to merge on\n",
    "        - right_on = Fields in DataFrame 2 to merge with left_on fields of Dataframe 1\n",
    "        - method = Type of join\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    if right_on is None:\n",
    "        right_on = left_on\n",
    "    return left.merge(right, \n",
    "                      how=method, \n",
    "                      left_on=left_on, \n",
    "                      right_on=right_on, \n",
    "                      suffixes=(\"\",\"_y\"))\n",
    "    \n",
    "####### Pre-processing ############    \n",
    "\n",
    "def drop_allsame(df):\n",
    "    '''\n",
    "    Function to remove any columns which have same value all across\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    to_drop = list()\n",
    "    for i in df.columns:\n",
    "        if len(df.loc[:,i].unique()) == 1:\n",
    "            to_drop.append(i)\n",
    "    return df.drop(to_drop,axis =1)\n",
    "\n",
    "#fill Nan Values in the cloudCover column\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(5)\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1)  \n",
    "\n",
    "\n",
    "def treat_missing_categorical(df, columns, how='mode', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in categorical columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mode', any string or numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mode':\n",
    "        for col in columns:\n",
    "            print(\"Filling missing values with mode for column - {0}\".format(col))\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            \n",
    "    elif isinstance(how, str):\n",
    "        for col in columns:\n",
    "            print(\"Filling missing values with '{0}' for column - {1}\".format(how, col))\n",
    "            df[col] = df[col].fillna(how)\n",
    "            \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "            \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(4)\n",
    "\n",
    "\n",
    "def min_max_scaler(df,columns):\n",
    "    '''\n",
    "    Function to do Min-Max scaling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = columns\n",
    "    return data, scaler\n",
    "\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df\n",
    "\n",
    "def z_scaler(df,columns):\n",
    "    '''\n",
    "    Function to standardize features by removing the mean and scaling to unit variance\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = columns\n",
    "    return data, scaler\n",
    "    \n",
    "def label_encoder(df,columns):\n",
    "    '''\n",
    "    Function to label encode\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be label encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with lable encoded columns\n",
    "        - le_dict = Dictionary of all the column and their label encoders\n",
    "    '''\n",
    "    le_dict = {}\n",
    "    for c in columns:\n",
    "        print(\"Label encoding column - {0}\".format(c))\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df[c].values.astype('str')))\n",
    "        df[c] = lbl.transform(list(df[c].values.astype('str')))\n",
    "        le_dict[c] = lbl\n",
    "    return df, le_dict\n",
    "\n",
    "def one_hot_encoder(df, columns):\n",
    "    '''\n",
    "    Function to do one-hot encoded\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be one-hot encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with one-hot encoded columns\n",
    "    '''\n",
    "    for each in columns:\n",
    "        print(\"One-Hot encoding column - {0}\".format(each))\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df.drop(columns,axis = 1)\n",
    "\n",
    "####### Feature Engineering ############\n",
    "def create_date_features(df,column, date_format = None, more_features = False, time_features = False):\n",
    "    '''\n",
    "    Function to extract date features\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - date_format = Date parsing format\n",
    "        - columns = Columns name containing date field\n",
    "        - more_features = To get more feature extracted\n",
    "        - time_features = To extract hour from datetime field\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with additional extracted date features\n",
    "    '''\n",
    "    if date_format is None:\n",
    "        df.loc[:,column] = pd.to_datetime(df.loc[:,column])\n",
    "    else:\n",
    "        df.loc[:,column] = pd.to_datetime(df.loc[:,column],format = date_format)\n",
    "    df.loc[:,column+'_Year'] = df.loc[:,column].dt.year\n",
    "    df.loc[:,column+'_Month'] = df.loc[:,column].dt.month.astype('uint8')\n",
    "    df.loc[:,column+'_Week'] = df.loc[:,column].dt.week.astype('uint8')\n",
    "    df.loc[:,column+'_Day'] = df.loc[:,column].dt.day.astype('uint8')\n",
    "    \n",
    "    if more_features:\n",
    "        df.loc[:,column+'_Quarter'] = df.loc[:,column].dt.quarter.astype('uint8')\n",
    "        df.loc[:,column+'_DayOfWeek'] = df.loc[:,column].dt.dayofweek.astype('uint8')\n",
    "        df.loc[:,column+'_DayOfYear'] = df.loc[:,column].dt.dayofyear\n",
    "        \n",
    "    if time_features:\n",
    "        df.loc[:,column+'_Hour'] = df.loc[:,column].dt.hour.astype('uint8')\n",
    "    return df\n",
    "\n",
    "def target_encoder(train_df, col_name, target_name, test_df = None, how='mean'):\n",
    "    '''\n",
    "    Function to do target encoding\n",
    "    Required Input - \n",
    "        - train_df = Training Pandas Dataframe\n",
    "        - test_df = Testing Pandas Dataframe\n",
    "        - col_name = Name of the columns of the source variable\n",
    "        - target_name = Name of the columns of target variable\n",
    "        - how = 'mean' default but can also be 'count'\n",
    "\tExpected Output - \n",
    "\t\t- train_df = Training dataframe with added encoded features\n",
    "\t\t- test_df = Testing dataframe with added encoded features\n",
    "    '''\n",
    "    aggregate_data = train_df.groupby(col_name)[target_name] \\\n",
    "                    .agg([how]) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns={how: col_name+'_'+target_name+'_'+how})\n",
    "    if test_df is None:\n",
    "        return join_df(train_df,aggregate_data,left_on = col_name)\n",
    "    else:\n",
    "        return join_df(train_df,aggregate_data,left_on = col_name), join_df(test_df,aggregate_data,left_on = col_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13de6381",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fb24615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-Learn Sub-modules\n",
    "\n",
    "# Scikit-Learn library is organized into several sub-modules, each of which contains a set of related functions and classes. \n",
    "# Here are the main sub-modules in scikit-learn:\n",
    "\n",
    "#from sklearn.\"sub-module\" import \"model\"\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "\n",
    "# sklearn.datasets: This sub-module provides a set of standard datasets for machine learning, including iris, \n",
    "#     digits, and breast cancer.\n",
    "        from sklearn.datasets import load_iris\n",
    "        iris_data = load_iris()\n",
    "        iris_features = iris_data.data \n",
    "        iris_target = iris_data.target\n",
    "        \n",
    "        # Convert the data to a DataFrame\n",
    "        df = pd.DataFrame(iris_features, columns=iris_data.feature_names)\n",
    "        \n",
    "        # Add the target variable to the DataFrame\n",
    "        df['target'] = iris_target \n",
    "        \n",
    "        # print(iris_data.DESCR) - Describes the data \n",
    "        # iris_data.data: An array containing the feature values for each instance of the dataset.\n",
    "        # iris_data.target: An array containing the class labels (i.e., 0, 1, or 2) for each instance of the dataset.\n",
    "        # Iris_data.target_names: An array containing the names of the three classes \n",
    "        # iris_data.feature_names: An array containing the names of the attributes \n",
    "# sklearn.model_selection: This sub-module contains functions for model selection, such as splitting data into \n",
    "#     training and test sets, cross-validation, and grid search.\n",
    "\n",
    "# sklearn.preprocessing: This sub-module provides functions for preprocessing data, such as scaling, normalization, \n",
    "#     and encoding categorical variables.\n",
    "\n",
    "# sklearn.feature_extraction: This sub-module contains functions for feature extraction from raw data, \n",
    "#     such as text data, including Bag of Words, CountVectorizer, and TfidfVectorizer.\n",
    "\n",
    "# sklearn.metrics: This sub-module provides functions for evaluating the performance of machine learning models, \n",
    "#     such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "# sklearn.pipeline: This sub-module provides tools for building machine learning pipelines, \n",
    "#     which allows you to chain together multiple steps, such as feature extraction, preprocessing, and model selection.\n",
    "\n",
    "# sklearn.decomposition: This sub-module provides classes for matrix factorization and decomposition, \n",
    "#     such as Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), \n",
    "#     and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "# sklearn.discriminant_analysis: This sub-module provides classes for linear and quadratic discriminant analysis, \n",
    "#     which are used for supervised classification tasks.\n",
    "\n",
    "# sklearn.covariance: This sub-module provides classes for covariance estimation, such as Empirical Covariance and \n",
    "#     Shrunk Covariance.\n",
    "\n",
    "# sklearn.exceptions: This sub-module contains custom exceptions raised by scikit-learn, such as NotFittedError and \n",
    "#     ConvergenceWarning.\n",
    "\n",
    "\n",
    "#Models: \n",
    "\n",
    "\n",
    "# sklearn.linear_model: This sub-module contains classes for linear models, such as linear regression, \n",
    "#     logistic regression, and ridge regression.\n",
    "\n",
    "# sklearn.tree: This sub-module provides classes for decision trees, such as DecisionTreeClassifier and \n",
    "#     DecisionTreeRegressor.\n",
    "\n",
    "# sklearn.ensemble: This sub-module contains classes for ensemble models, such as random forests, AdaBoost, \n",
    "#     and Gradient Boosting.\n",
    "\n",
    "# sklearn.cluster: This sub-module provides classes for clustering, such as KMeans and Hierarchical Clustering.\n",
    "\n",
    "# sklearn.neural_network: This sub-module contains classes for neural networks, such as Multi-Layer Perceptron (MLP) \n",
    "#     and Convolutional Neural Networks (CNNs).\n",
    "\n",
    "# sklearn.svm: This sub-module contains classes for Support Vector Machines (SVMs), such as SVM classifier and regression.\n",
    "\n",
    "# sklearn.manifold: This sub-module provides classes for manifold learning, such as t-SNE and Isomap.\n",
    "\n",
    "# sklearn.naive_bayes: This sub-module provides classes for Naive Bayes models, such as Gaussian Naive Bayes and \n",
    "#     Multinomial Naive Bayes.\n",
    "\n",
    "# sklearn.neighbors: This sub-module provides classes for k-Nearest Neighbors (k-NN) models, \n",
    "#     such as KNeighborsClassifier and KNeighborsRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17ace5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c17a28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78800008",
   "metadata": {},
   "source": [
    "### Machine Learning Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b16720",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd ## For DataFrame operation\n",
    "import numpy as np ## Numerical python for matrix operations\n",
    "from sklearn.model_selection import KFold, train_test_split ## Creating cross validation sets\n",
    "from sklearn import metrics ## For loss functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Libraries for Regressiion algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb \n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "########### Cross Validation ###########\n",
    "### 1) Train test split\n",
    "def holdout_cv(X,y,size = 0.3, seed = 1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = size, random_state = seed)\n",
    "    X_train = X_train.reset_index(drop='index')\n",
    "    X_test = X_test.reset_index(drop='index')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "### 2) Cross-Validation (K-Fold)\n",
    "def kfold_cv(X,n_folds = 5, seed = 1):\n",
    "    cv = KFold(n_splits = n_folds, random_state = seed, shuffle = True)\n",
    "    return cv.split(X)\n",
    "\n",
    "########### Model Explanation ###########\n",
    "## Variable Importance plot\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "########### Functions for explaination using Lime ###########\n",
    "\n",
    "## Make a prediction function\n",
    "def make_prediction_function(model, type = None):\n",
    "    if type == 'xgb':\n",
    "        predict_fn = lambda x: model.predict(xgb.DMatrix(x)).astype(float)\n",
    "    else:\n",
    "        predict_fn = lambda x: model.predict(x).astype(float)\n",
    "    return predict_fn\n",
    "\n",
    "## Make a lime explainer\n",
    "def make_lime_explainer(df, c_names = [], verbose_val = True):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(df.values,\n",
    "                                                       class_names=c_names,\n",
    "                                                       feature_names = list(df.columns),\n",
    "                                                       kernel_width=3, \n",
    "                                                       verbose=verbose_val,\n",
    "                                                       mode='regression'\n",
    "                                                    )\n",
    "    return explainer\n",
    "\n",
    "## Lime explain function\n",
    "def lime_explain(explainer,predict_fn, df, index = 0, num_features = None,\n",
    "                 show_in_notebook = True, filename = None):\n",
    "    if num_features is not None:\n",
    "        exp = explainer.explain_instance(df.values[index], predict_fn, num_features=num_features)\n",
    "    else:\n",
    "        exp = explainer.explain_instance(df.values[index], predict_fn, num_features=df.shape[1])\n",
    "    \n",
    "    if show_in_notebook:\n",
    "        exp.show_in_notebook(show_all=False)\n",
    "    \n",
    "    if filename is not None:\n",
    "        exp.save_to_file(filename)\n",
    "        \n",
    "########### Algorithms For Regression ###########\n",
    "\n",
    "### Running Xgboost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, \n",
    "           rounds=500, dep=8, eta=0.05,sub_sample=0.7,col_sample=0.7,\n",
    "           min_child_weight_val=1, silent_val = 1):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"reg:linear\"\n",
    "    params['eval_metric'] = 'rmse'\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = sub_sample\n",
    "    params[\"min_child_weight\"] = min_child_weight_val\n",
    "    params[\"colsample_bytree\"] = col_sample\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"silent\"] = silent_val\n",
    "    params[\"seed\"] = seed_val\n",
    "    #params[\"max_delta_step\"] = 2\n",
    "    #params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    \n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "    \n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_iteration)\n",
    "    \n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.mean_squared_error(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "        \n",
    "### Running LightGBM\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, \n",
    "           seed_val=0, rounds=500, dep=8, eta=0.05,sub_sample=0.7,\n",
    "           col_sample=0.7,silent_val = 1,min_data_in_leaf_val = 20, bagging_freq = 5):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"regression\"\n",
    "    params['metric'] = 'rmse'\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = min_data_in_leaf_val\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = sub_sample\n",
    "    params[\"feature_fraction\"] = col_sample\n",
    "    params[\"bagging_freq\"] = bagging_freq\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = silent_val\n",
    "    num_rounds = rounds\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.Dataset(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "        \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    \n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    \n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.mean_squared_error(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "        \n",
    "### Running Extra Trees  \n",
    "def runET(train_X, train_y, test_X, test_y=None, test_X2=None, rounds=100, depth=20,\n",
    "          leaf=10, feat=0.2, min_data_split_val=2,seed_val=0,job = -1):\n",
    "\tmodel = ExtraTreesRegressor(\n",
    "                                n_estimators = rounds,\n",
    "                                max_depth = depth,\n",
    "                                min_samples_split = min_data_split_val,\n",
    "                                min_samples_leaf = leaf,\n",
    "                                max_features =  feat,\n",
    "                                n_jobs = job,\n",
    "                                random_state = seed_val)\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\ttrain_preds = model.predict(train_X)\n",
    "\ttest_preds = model.predict(test_X)\n",
    "\t\n",
    "\ttest_preds2 = 0\n",
    "\tif test_X2 is not None:\n",
    "\t\ttest_preds2 = model.predict(test_X2)\n",
    "\t\n",
    "\ttest_loss = 0\n",
    "\tif test_y is not None:\n",
    "\t\ttrain_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "\t\ttest_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "\t\tprint(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "\t\tprint(\"Train and Test loss : \", train_loss, test_loss)\n",
    "\treturn test_preds, test_loss, test_preds2, model\n",
    " \n",
    "### Running Random Forest\n",
    "def runRF(train_X, train_y, test_X, test_y=None, test_X2=None, rounds=100, depth=20, leaf=10,\n",
    "          feat=0.2,min_data_split_val=2,seed_val=0,job = -1):\n",
    "    model = RandomForestRegressor(\n",
    "                                n_estimators = rounds,\n",
    "                                max_depth = depth,\n",
    "                                min_samples_split = min_data_split_val,\n",
    "                                min_samples_leaf = leaf,\n",
    "                                max_features =  feat,\n",
    "                                n_jobs = job,\n",
    "                                random_state = seed_val)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "    \n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running Linear regression\n",
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None):\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running Decision Tree\n",
    "def runDT(train_X, train_y, test_X, test_y=None, test_X2=None, criterion='mse', \n",
    "          depth=None, min_split=2, min_leaf=1):\n",
    "    model = DecisionTreeRegressor(\n",
    "                                criterion = criterion, \n",
    "                                max_depth = depth, \n",
    "                                min_samples_split = min_split, \n",
    "                                min_samples_leaf=min_leaf)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "    \n",
    "### Running K-Nearest Neighbour\n",
    "def runKNN(train_X, train_y, test_X, test_y=None, test_X2=None, \n",
    "           neighbors=5, job = -1):\n",
    "    model = KNeighborsRegressor(\n",
    "                                n_neighbors=neighbors, \n",
    "                                n_jobs=job)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running SVM\n",
    "def runSVC(train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, \n",
    "           eps=0.1, kernel_choice = 'rbf'):\n",
    "    model = SVR(\n",
    "                C=C, \n",
    "                kernel=kernel_choice,  \n",
    "                epsilon=eps)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0947809",
   "metadata": {},
   "source": [
    "### Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd  ## For DataFrame operation\n",
    "import numpy as np  ## Numerical python for matrix operations\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    ")  ## Creating cross validation sets\n",
    "from sklearn import metrics  ## For loss functions\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "## For evaluation\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "from inspect import signature\n",
    "\n",
    "## Libraries for Classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42): #split data into train, test, and validation\n",
    "    \"\"\"\n",
    "    This function splits the data into train and test sets, and further splits the train set into training and validation sets.\n",
    "    \n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing the input data.\n",
    "    target_col : str\n",
    "        The name of the target column in the dataframe.\n",
    "    test_size : float, optional (default=0.2)\n",
    "        The proportion of the data to be used for testing.\n",
    "    val_size : float, optional (default=0.2)\n",
    "        The proportion of the training data to be used for validation.\n",
    "    random_state : int, optional (default=42)\n",
    "        The seed used by the random number generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xtrain : pandas DataFrame\n",
    "        The training input data.\n",
    "    ytrain : pandas Series\n",
    "        The training target data.\n",
    "    xvalid : pandas DataFrame\n",
    "        The validation input data.\n",
    "    yvalid : pandas Series\n",
    "        The validation target data.\n",
    "    xtest : pandas DataFrame\n",
    "        The test input data.\n",
    "    ytest : pandas Series\n",
    "        The test target data.\n",
    "    \"\"\" \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "########### Cross Validation ###########\n",
    "### 1) Train test split\n",
    "def holdout_cv(X, y, size=0.3, seed=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=size, random_state=seed\n",
    "    )\n",
    "    X_train = X_train.reset_index(drop=\"index\")\n",
    "    X_test = X_test.reset_index(drop=\"index\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "### 2) Cross-Validation (K-Fold)\n",
    "def kfold_cv(X, n_folds=5, seed=1):\n",
    "    cv = KFold(n_splits=n_folds, random_state=seed, shuffle=True)\n",
    "    return cv.split(X)\n",
    "\n",
    "\n",
    "########### Model Explanation ###########\n",
    "## Plotting AUC ROC curve\n",
    "def plot_roc(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Model (AUC = %0.2f)\" % (roc_auc_score(y_actual, y_pred)),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        linestyle=\"--\",\n",
    "        lw=2,\n",
    "        color=\"r\",\n",
    "        label=\"Luck (AUC = 0.5)\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precisionrecall(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    average_precision = average_precision_score(y_actual, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_actual, y_pred)\n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = (\n",
    "        {\"step\": \"post\"} if \"step\" in signature(plt.fill_between).parameters else {}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.step(recall, precision, color=\"b\", alpha=0.2, where=\"post\")\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color=\"b\", **step_kwargs)\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\"Precision-Recall curve: AP={0:0.2f}\".format(average_precision))\n",
    "\n",
    "\n",
    "## Plotting confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    cmap=plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "\n",
    "## Variable Importance plot\n",
    "def feature_importance(model, X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.title(\"Variable Importance\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Functions for explaination using Lime\n",
    "def make_prediction_function(model):\n",
    "    predict_fn = lambda x: model.predict_proba(x).astype(float)\n",
    "    return predict_fn\n",
    "\n",
    "\n",
    "def make_lime_explainer(df, c_names=[], k_width=3, verbose_val=True):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        df.values,\n",
    "        class_names=c_names,\n",
    "        feature_names=list(df.columns),\n",
    "        kernel_width=3,\n",
    "        verbose=verbose_val,\n",
    "    )\n",
    "    return explainer\n",
    "\n",
    "\n",
    "def lime_explain(\n",
    "    explainer,\n",
    "    predict_fn,\n",
    "    df,\n",
    "    index=0,\n",
    "    num_features=None,\n",
    "    show_in_notebook=True,\n",
    "    filename=None,\n",
    "):\n",
    "    if num_features is not None:\n",
    "        exp = explainer.explain_instance(\n",
    "            df.values[index], predict_fn, num_features=num_features\n",
    "        )\n",
    "    else:\n",
    "        exp = explainer.explain_instance(\n",
    "            df.values[index], predict_fn, num_features=df.shape[1]\n",
    "        )\n",
    "\n",
    "    if show_in_notebook:\n",
    "        exp.show_in_notebook(show_all=False)\n",
    "\n",
    "    if filename is not None:\n",
    "        exp.save_to_file(filename)\n",
    "\n",
    "\n",
    "########### Algorithms For Binary classification ###########\n",
    "\n",
    "### Running Xgboost\n",
    "def runXGB(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    min_child_weight_val=1,\n",
    "    silent_val=1,\n",
    "):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eval_metric\"] = \"auc\"\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = sub_sample\n",
    "    params[\"min_child_weight\"] = min_child_weight_val\n",
    "    params[\"colsample_bytree\"] = col_sample\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"silent\"] = silent_val\n",
    "    params[\"seed\"] = seed_val\n",
    "    # params[\"max_delta_step\"] = 2\n",
    "    # params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [(xgtrain, \"train\"), (xgtest, \"test\")]\n",
    "        model = xgb.train(\n",
    "            plst,\n",
    "            xgtrain,\n",
    "            num_rounds,\n",
    "            watchlist,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "\n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(\n",
    "            xgb.DMatrix(test_X2), ntree_limit=model.best_iteration\n",
    "        )\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "### Running Xgboost classifier for model explaination\n",
    "def runXGBC(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    min_child_weight_val=1,\n",
    "    silent_val=1,\n",
    "):\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        learning_rate=eta,\n",
    "        subsample=sub_sample,\n",
    "        min_child_weight=min_child_weight_val,\n",
    "        colsample_bytree=col_sample,\n",
    "        max_depth=dep,\n",
    "        silent=silent_val,\n",
    "        seed=seed_val,\n",
    "        n_estimators=rounds,\n",
    "    )\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running LightGBM\n",
    "def runLGB(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    feature_names=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    silent_val=1,\n",
    "    min_data_in_leaf_val=20,\n",
    "    bagging_freq=5,\n",
    "    n_thread=20,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params[\"metric\"] = metric\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = min_data_in_leaf_val\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = sub_sample\n",
    "    params[\"feature_fraction\"] = col_sample\n",
    "    params[\"bagging_freq\"] = bagging_freq\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = silent_val\n",
    "    params[\"num_threads\"] = n_thread\n",
    "    num_rounds = rounds\n",
    "\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgtrain,\n",
    "            num_rounds,\n",
    "            valid_sets=[lgtrain, lgtest],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "    else:\n",
    "        lgtest = lgb.Dataset(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "\n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = roc_auc_score(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "### Running LightGBM classifier for model explaination\n",
    "def runLGBC(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    silent_val=1,\n",
    "    min_data_in_leaf_val=20,\n",
    "    bagging_freq=5,\n",
    "    n_thread=20,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        max_depth=dep,\n",
    "        learning_rate=eta,\n",
    "        min_data_in_leaf=min_data_in_leaf_val,\n",
    "        bagging_fraction=sub_sample,\n",
    "        feature_fraction=col_sample,\n",
    "        bagging_freq=bagging_freq,\n",
    "        bagging_seed=seed_val,\n",
    "        verbosity=silent_val,\n",
    "        num_threads=n_thread,\n",
    "        n_estimators=rounds,\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = roc_auc_score(train_y, train_preds)\n",
    "        test_loss = roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test AUC : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Extra Trees\n",
    "def runET(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Logistic Regression\n",
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, penalty=\"l1\"):\n",
    "    model = LogisticRegression(C=C, penalty=penalty, n_jobs=-1)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Decision Tree\n",
    "def runDT(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    criterion=\"gini\",\n",
    "    depth=None,\n",
    "    min_split=2,\n",
    "    min_leaf=1,\n",
    "):\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_split,\n",
    "        min_samples_leaf=min_leaf,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running K-Nearest Neighbour\n",
    "def runKNN(train_X, train_y, test_X, test_y=None, test_X2=None, neighbors=5, job=-1):\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors, n_jobs=job)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running SVM\n",
    "def runSVC(\n",
    "    train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, kernel_choice=\"rbf\"\n",
    "):\n",
    "    model = SVC(C=C, kernel=kernel_choice, probability=True)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edddfff7",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a11145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "eng_stop = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def word_grams(text, min=1, max=4):\n",
    "    '''\n",
    "    Function to create N-grams from text\n",
    "    Required Input -\n",
    "        - text = text string for which N-gram needs to be created\n",
    "        - min = minimum number of N\n",
    "        - max = maximum number of N\n",
    "    Expected Output -\n",
    "        - s = list of N-grams \n",
    "    '''\n",
    "    s = []\n",
    "    for n in range(min, max+1):\n",
    "        for ngram in ngrams(text, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_bigrams_df(df, column_names):\n",
    "    \"\"\"\n",
    "    Generate bigrams from specified columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to generate bigrams from.\n",
    "    column_names (list of str): List of column names to generate bigrams from.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with bigrams appended as new columns.\n",
    "    \"\"\"\n",
    "    bigram_columns = []\n",
    "    for col in column_names:\n",
    "        bigram_col = f\"{col}_bigrams\"\n",
    "        bigram_columns.append(bigram_col)\n",
    "        df[bigram_col] = df[col].apply(lambda x: generate_bigrams([x]))\n",
    "    return df[bigram_columns]\n",
    "\n",
    "def make_wordcloud(df,column, bg_color='white', w=1200, h=1000, font_size_max=50, n_words=40,g_min=1,g_max=1):\n",
    "    '''\n",
    "    Function to make wordcloud from a text corpus\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - bg_color = Background color\n",
    "        - w = width\n",
    "        - h = height\n",
    "        - font_size_max = maximum font size allowed\n",
    "        - n_word = maximum words allowed\n",
    "        - g_min = minimum n-grams\n",
    "        - g_max = maximum n-grams\n",
    "    Expected Output -\n",
    "        - World cloud image\n",
    "    '''\n",
    "    text = \"\"\n",
    "    for ind, row in df.iterrows(): \n",
    "        text += row[column] + \" \"\n",
    "    text = text.strip().split(' ') \n",
    "    text = word_grams(text,g_min,g_max)\n",
    "    \n",
    "    text = list(pd.Series(word_grams(text,1,2)).apply(lambda x: x.replace(' ','_')))\n",
    "    \n",
    "    s = \"\"\n",
    "    for i in range(len(text)):\n",
    "        s += text[i] + \" \"\n",
    "\n",
    "    wordcloud = WordCloud(background_color=bg_color, \\\n",
    "                          width=w, \\\n",
    "                          height=h, \\\n",
    "                          max_font_size=font_size_max, \\\n",
    "                          max_words=n_words).generate(s)\n",
    "    wordcloud.recolor(random_state=1)\n",
    "    plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def generate_wordcloud(df, column_names):\n",
    "    \"\"\"\n",
    "    Generates a wordcloud from a pandas DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data\n",
    "    column_names (list): List of column names in the DataFrame to generate the wordcloud from\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    all_words = ' '.join([' '.join(text) for col in column_names for text in df[col]])\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_tokens(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - tokenized list output\n",
    "    '''\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def tokenize_columns(dataframe, columns):\n",
    "    \"\"\"\n",
    "    Tokenize the values in specified columns of a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame to tokenize.\n",
    "        columns (list): A list of column names to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with tokenized values in the specified columns.\n",
    "    \"\"\"\n",
    "    # Download necessary NLTK resources if they haven't been downloaded yet\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Create a new DataFrame to hold the tokenized values\n",
    "    tokenized_df = pd.DataFrame()\n",
    "\n",
    "    # Tokenize the values in each specified column\n",
    "    for col in columns:\n",
    "        # Tokenize the values in the current column using NLTK's word_tokenize function\n",
    "        tokenized_values = dataframe[col].apply(nltk.word_tokenize)\n",
    "\n",
    "        # Add the tokenized values to the new DataFrame\n",
    "        tokenized_df[col] = tokenized_values\n",
    "\n",
    "    # Return the new DataFrame with tokenized values\n",
    "    return tokenized_df\n",
    "\n",
    "#another way\n",
    "--------------------------------------------------------------------------\n",
    "def tokenize(text, sep=' ', preserve_case=False):\n",
    "    \"\"\"\n",
    "    Tokenize a string into a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): String to be tokenized\n",
    "    sep (str, optional): Separator to use for tokenization. Defaults to ' '.\n",
    "    preserve_case (bool, optional): Whether to preserve the case of the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tokens\n",
    "    \"\"\"\n",
    "    if not preserve_case:\n",
    "        text = text.lower()\n",
    "    tokens = text.split(sep)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_df(df, column_names, sep=' ', preserve_case=False):\n",
    "    \"\"\"\n",
    "    Tokenize a pandas dataframe with multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to be tokenized\n",
    "    columns (list of str): List of column names to be tokenized\n",
    "    sep (str, optional): Separator to use for tokenization. Defaults to ' '.\n",
    "    preserve_case (bool, optional): Whether to preserve the case of the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Tokenized dataframe\n",
    "    \"\"\"\n",
    "    for col in column_names:\n",
    "        df[col] = df[col].apply(lambda x: tokenize(x, sep, preserve_case))\n",
    "    return df\n",
    "\n",
    "carbon_google1 = tokenize_df (carbon_google1, column_names =  [\"title\"], sep=' ', preserve_case=False)\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "def bag_of_words_features(df, text_columns, target_columns):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and one or two columns and returns a bag of words representation of the data as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas DataFrame): The DataFrame to extract features from.\n",
    "    column1 (str): The name of the first column to use as input data.\n",
    "    column2 (str, optional): The name of the second column to use as input data. If not provided, only the first column will be used.\n",
    "\n",
    "    Returns:\n",
    "    pandas DataFrame: The bag of words representation of the input data as a DataFrame.\n",
    "    \"\"\"\n",
    "        \n",
    "    text_data = df[text_columns].apply(lambda x: \" \".join([str(i) for i in x]), axis=1)\n",
    "\n",
    "    text_data = text_data.str.lower()\n",
    "    vectorizer = CountVectorizer(max_df=0.90, min_df=4, max_features=1000, stop_words=None)\n",
    "    X_bow = vectorizer.fit_transform(text_data)\n",
    "    # Use the new function to get the feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    df.dropna(subset=[target_column], inplace=True) if target_columns else None\n",
    "\n",
    "    X_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "    \n",
    "    if target_columns:        \n",
    "        y = df[target_columns]\n",
    "        return X_bow, y\n",
    "    \n",
    "    return X_bow\n",
    "\n",
    "def convert_lowercase(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be lowercased\n",
    "    Expected Output -\n",
    "        - text - lower cased text string output\n",
    "    '''\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unwanted_characters(df, columns):\n",
    "    \"\"\"\n",
    "    Remove unwanted characters (including smileys and emojies) from specified columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    columns (list): A list of column names to clean.\n",
    "    unwanted_chars (str): The characters to remove.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    import re \n",
    "    unwanted_chars = '[$#&*@%]'\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\u2764\\ufe0f\" # heart emoji\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "            df[col] = df[col].str.replace(unwanted_chars, '')\n",
    "        else:\n",
    "            print(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string \n",
    "    Expected Output -\n",
    "        - text - text string with punctuation removed\n",
    "    '''\n",
    "    return text.translate(None,string.punctuation)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - list output with stopwords removed\n",
    "    '''\n",
    "    return [word for word in text.split() if word not in eng_stop]\n",
    "\n",
    "def remove_short_words(df, column_names, min_length=3):\n",
    "    \"\"\"Remove short words from columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to modify.\n",
    "    column_names (List[str]): A list of column names to modify.\n",
    "    min_length (int, optional): The minimum length of words to keep. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified DataFrame with short words removed from specified columns.\n",
    "    \"\"\"\n",
    "    for column_name in column_names:\n",
    "        df[column_name] = df[column_name].apply(\n",
    "            lambda x: ' '.join([word for word in x.split() if len(word) >= min_length])\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def convert_stemmer(word):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - word - word which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - word output after stemming\n",
    "    '''\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return porter_stemmer.stem(word)\n",
    "\n",
    "def stem_df(df, column_names):\n",
    "    \"\"\"\n",
    "    Perform stemming on a pandas dataframe with multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to be stemmed\n",
    "    columns (list of str): List of column names to be stemmed\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Stemmed dataframe\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    for col in column_names:\n",
    "        df[col] = df[col].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    return df\n",
    "\n",
    "def convert_lemmatizer(word):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - word - word which needs to be lemmatized\n",
    "    Expected Output -\n",
    "        - word - word output after lemmatizing\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "def create_tf_idf(df, column, train_df = None, test_df = None,n_features = None):\n",
    "    '''\n",
    "    Function to do tf-idf on a pandas dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - train_df(optional) = Train DataFrame\n",
    "        - test_df(optional) = Test DataFrame\n",
    "        - n_features(optional) = Maximum number of features needed\n",
    "    Expected Output -\n",
    "        - train_tfidf = train tf-idf sparse matrix output\n",
    "        - test_tfidf = test tf-idf sparse matrix output\n",
    "        - tfidf_obj = tf-idf model\n",
    "    '''\n",
    "    tfidf_obj = TfidfVectorizer(ngram_range=(1,1), stop_words='english', \n",
    "                                analyzer='word', max_features = n_features)\n",
    "    tfidf_text = tfidf_obj.fit_transform(df.ix[:,column].values)\n",
    "    \n",
    "    if train_df is not None:        \n",
    "        train_tfidf = tfidf_obj.transform(train_df.ix[:,column].values)\n",
    "    else:\n",
    "        train_tfidf = tfidf_text\n",
    "\n",
    "    test_tfidf = None\n",
    "    if test_df is not None:\n",
    "        test_tfidf = tfidf_obj.transform(test_df.ix[:,column].values)\n",
    "\n",
    "    return train_tfidf, test_tfidf, tfidf_obj\n",
    "    \n",
    "def create_countvector(df, column, train_df = None, test_df = None,n_features = None):\n",
    "    '''\n",
    "    Function to do count vectorizer on a pandas dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - train_df(optional) = Train DataFrame\n",
    "        - test_df(optional) = Test DataFrame\n",
    "        - n_features(optional) = Maximum number of features needed\n",
    "    Expected Output -\n",
    "        - train_cvect = train count vectorized sparse matrix output\n",
    "        - test_cvect = test count vectorized sparse matrix output\n",
    "        - cvect_obj = count vectorized model\n",
    "    '''\n",
    "    cvect_obj = CountVectorizer(ngram_range=(1,1), stop_words='english', \n",
    "                                analyzer='word', max_features = n_features)\n",
    "    cvect_text = cvect_obj.fit_transform(df.ix[:,column].values)\n",
    "    \n",
    "    if train_df is not None:\n",
    "        train_cvect = cvect_obj.transform(train_df.ix[:,column].values)\n",
    "    else:\n",
    "        train_cvect = cvect_text\n",
    "        \n",
    "    test_cvect = None\n",
    "    if test_df is not None:\n",
    "        test_cvect = cvect_obj.transform(test_df.ix[:,column].values)\n",
    "\n",
    "    return train_cvect, test_cvect, cvect_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12652f36",
   "metadata": {},
   "source": [
    "### Recommendation Systems (Recsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20910ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from lightfm import LightFM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_interaction_matrix(df,user_col, item_col, rating_col, norm= False, threshold = None):\n",
    "    '''\n",
    "    Function to create an interaction matrix dataframe from transactional type interactions\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame containing user-item interactions\n",
    "        - user_col = column name containing user's identifier\n",
    "        - item_col = column name containing item's identifier\n",
    "        - rating col = column name containing user feedback on interaction with a given item\n",
    "        - norm (optional) = True if a normalization of ratings is needed\n",
    "        - threshold (required if norm = True) = value above which the rating is favorable\n",
    "    Expected output - \n",
    "        - Pandas dataframe with user-item interactions ready to be fed in a recommendation algorithm\n",
    "    '''\n",
    "    interactions = df.groupby([user_col, item_col])[rating_col] \\\n",
    "            .sum().unstack().reset_index(). \\\n",
    "            fillna(0).set_index(user_col)\n",
    "    if norm:\n",
    "        interactions = interactions.applymap(lambda x: 1 if x > threshold else 0)\n",
    "    return interactions\n",
    "\n",
    "def create_user_dict(interactions):\n",
    "    '''\n",
    "    Function to create a user dictionary based on their index and number in interaction dataset\n",
    "    Required Input - \n",
    "        interactions - dataset create by create_interaction_matrix\n",
    "    Expected Output -\n",
    "        user_dict - Dictionary type output containing interaction_index as key and user_id as value\n",
    "    '''\n",
    "    user_id = list(interactions.index)\n",
    "    user_dict = {}\n",
    "    counter = 0 \n",
    "    for i in user_id:\n",
    "        user_dict[i] = counter\n",
    "        counter += 1\n",
    "    return user_dict\n",
    "    \n",
    "def create_item_dict(df,id_col,name_col):\n",
    "    '''\n",
    "    Function to create an item dictionary based on their item_id and item name\n",
    "    Required Input - \n",
    "        - df = Pandas dataframe with Item information\n",
    "        - id_col = Column name containing unique identifier for an item\n",
    "        - name_col = Column name containing name of the item\n",
    "    Expected Output -\n",
    "        item_dict = Dictionary type output containing item_id as key and item_name as value\n",
    "    '''\n",
    "    item_dict ={}\n",
    "    for i in range(df.shape[0]):\n",
    "        item_dict[(df.loc[i,id_col])] = df.loc[i,name_col]\n",
    "    return item_dict\n",
    "\n",
    "def runMF(interactions, n_components=30, loss='warp', k=15, epoch=30,n_jobs = 4):\n",
    "    '''\n",
    "    Function to run matrix-factorization algorithm\n",
    "    Required Input -\n",
    "        - interactions = dataset create by create_interaction_matrix\n",
    "        - n_components = number of embeddings you want to create to define Item and user\n",
    "        - loss = loss function other options are logistic, brp\n",
    "        - epoch = number of epochs to run \n",
    "        - n_jobs = number of cores used for execution \n",
    "    Expected Output  -\n",
    "        Model - Trained model\n",
    "    '''\n",
    "    x = sparse.csr_matrix(interactions.values)\n",
    "    model = LightFM(no_components= n_components, loss=loss,k=k)\n",
    "    model.fit(x,epochs=epoch,num_threads = n_jobs)\n",
    "    return model\n",
    "\n",
    "def sample_recommendation_user(model, interactions, user_id, user_dict, \n",
    "                               item_dict,threshold = 0,nrec_items = 10, show = True):\n",
    "    '''\n",
    "    Function to produce user recommendations\n",
    "    Required Input - \n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "        - user_id = user ID for which we need to generate recommendation\n",
    "        - user_dict = Dictionary type input containing interaction_index as key and user_id as value\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - threshold = value above which the rating is favorable in new interaction matrix\n",
    "        - nrec_items = Number of output recommendation needed\n",
    "    Expected Output - \n",
    "        - Prints list of items the given user has already bought\n",
    "        - Prints list of N recommended items  which user hopefully will be interested in\n",
    "    '''\n",
    "    n_users, n_items = interactions.shape\n",
    "    user_x = user_dict[user_id]\n",
    "    scores = pd.Series(model.predict(user_x,np.arange(n_items)))\n",
    "    scores.index = interactions.columns\n",
    "    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n",
    "    \n",
    "    known_items = list(pd.Series(interactions.loc[user_id,:] \\\n",
    "                                 [interactions.loc[user_id,:] > threshold].index) \\\n",
    "\t\t\t\t\t\t\t\t .sort_values(ascending=False))\n",
    "    \n",
    "    scores = [x for x in scores if x not in known_items]\n",
    "    return_score_list = scores[0:nrec_items]\n",
    "    known_items = list(pd.Series(known_items).apply(lambda x: item_dict[x]))\n",
    "    scores = list(pd.Series(return_score_list).apply(lambda x: item_dict[x]))\n",
    "    if show == True:\n",
    "        print(\"Known Likes:\")\n",
    "        counter = 1\n",
    "        for i in known_items:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "\n",
    "        print(\"\\n Recommended Items:\")\n",
    "        counter = 1\n",
    "        for i in scores:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "    return return_score_list\n",
    "    \n",
    "\n",
    "def sample_recommendation_item(model,interactions,item_id,user_dict,item_dict,number_of_user):\n",
    "    '''\n",
    "    Funnction to produce a list of top N interested users for a given item\n",
    "    Required Input -\n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "        - item_id = item ID for which we need to generate recommended users\n",
    "        - user_dict =  Dictionary type input containing interaction_index as key and user_id as value\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - number_of_user = Number of users needed as an output\n",
    "    Expected Output -\n",
    "        - user_list = List of recommended users \n",
    "    '''\n",
    "    n_users, n_items = interactions.shape\n",
    "    x = np.array(interactions.columns)\n",
    "    scores = pd.Series(model.predict(np.arange(n_users), np.repeat(x.searchsorted(item_id),n_users)))\n",
    "    user_list = list(interactions.index[scores.sort_values(ascending=False).head(number_of_user).index])\n",
    "    return user_list \n",
    "\n",
    "\n",
    "def create_item_emdedding_distance_matrix(model,interactions):\n",
    "    '''\n",
    "    Function to create item-item distance embedding matrix\n",
    "    Required Input -\n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "    Expected Output -\n",
    "        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w items\n",
    "    '''\n",
    "    df_item_norm_sparse = sparse.csr_matrix(model.item_embeddings)\n",
    "    similarities = cosine_similarity(df_item_norm_sparse)\n",
    "    item_emdedding_distance_matrix = pd.DataFrame(similarities)\n",
    "    item_emdedding_distance_matrix.columns = interactions.columns\n",
    "    item_emdedding_distance_matrix.index = interactions.columns\n",
    "    return item_emdedding_distance_matrix\n",
    "\n",
    "def item_item_recommendation(item_emdedding_distance_matrix, item_id, \n",
    "                             item_dict, n_items = 10, show = True):\n",
    "    '''\n",
    "    Function to create item-item recommendation\n",
    "    Required Input - \n",
    "        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w items\n",
    "        - item_id  = item ID for which we need to generate recommended items\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - n_items = Number of items needed as an output\n",
    "    Expected Output -\n",
    "        - recommended_items = List of recommended items\n",
    "    '''\n",
    "    recommended_items = list(pd.Series(item_emdedding_distance_matrix.loc[item_id,:]. \\\n",
    "                                  sort_values(ascending = False).head(n_items+1). \\\n",
    "                                  index[1:n_items+1]))\n",
    "    if show == True:\n",
    "        print(\"Item of interest :{0}\".format(item_dict[item_id]))\n",
    "        print(\"Item similar to the above item:\")\n",
    "        counter = 1\n",
    "        for i in recommended_items:\n",
    "            print(str(counter) + '- ' +  item_dict[i])\n",
    "            counter+=1\n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94ab6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fbbd09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ebe45a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
