{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/prashant111/code \n",
    "\n",
    "#for all the notebook tutorials on Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c2c80",
   "metadata": {},
   "source": [
    "#### Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3b5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Operators \n",
    "    # - Arithmetic operators: +, -, *, **, /, // (floor division), % (modulus) \n",
    "    # - Comparison operators: ==, !=, >, <, >=, <= (returns True or False) \n",
    "    # - Logical operators: and, or, not \n",
    "    # - Identity Operators: is, is not\n",
    "    # - Assignment operators: =, +=, -=, *=, /=, //=, %=, &=, |=, \n",
    "    # - Membership operators: in, not in \n",
    "    # - Bitwise shift operators: <<, >> \n",
    "    # - Unary operators: +, -, ~, not \n",
    "    # - String concatenation operator: \"+\" \n",
    "\n",
    "\n",
    "# # Lambda Function\n",
    "    # - The lambda function can be used to create small anonymous functions.\n",
    "    #   Syntax: lambda arguments : expression\n",
    "        # Example: f = lambda x : 2*x+3\n",
    "        #          print(f(5)) -> prints 13\n",
    "\n",
    "\n",
    "# Data types\n",
    "    myInt = int() # 1. Integer: A whole number like 4203987654.\n",
    "    myFloat = float() # 2. Floats/Doubles: Decimal numbers with a decimal point such as 3.14159.\n",
    "    myString = \"\" # 3. String: Text enclosed by quotes \"example\" or 'example'.\n",
    "    myBool = False # 4. Boolean: True of False.\n",
    "    myList = [] # 5. Lists: ordered collection of items inside square brackets [].\n",
    "    myTuple = () # 6. Tuples: ordered collection of immutable items inside parentheses ().\n",
    "    mySet = set() # 7. Sets: unordered collection of unique elements {}.\n",
    "    myDict = {} # 8. Dictionaries: key : value pairs {}\n",
    "\n",
    "        # String to Other Types\n",
    "        int_value = int(\"123\")  # String to Integer: Converts \"123\" to 123\n",
    "        float_value = float(\"123.45\")  # String to Float: Converts \"123.45\" to 123.45\n",
    "        list_value = list(\"abc\")  # String to List: Converts \"abc\" to ['a', 'b', 'c']\n",
    "        tuple_value = tuple(\"abc\")  # String to Tuple: Converts \"abc\" to ('a', 'b', 'c')\n",
    "        dict_value = eval(\"{'key': 'value'}\")  # String to Dictionary: Converts \"{'key': 'value'}\" to {'key': 'value'}\n",
    "\n",
    "        # Integer/Float to Other Types\n",
    "        str_from_int_float = str(123)  # Integer/Float to String: Converts 123 to \"123\"\n",
    "        float_from_int = float(123)  # Integer to Float: Converts 123 to 123.0\n",
    "        int_from_float = int(123.45)  # Float to Integer: Converts 123.45 to 123\n",
    "\n",
    "        # List to Other Types\n",
    "        str_from_list = ''.join(['a', 'b', 'c'])  # List to String: Converts ['a', 'b', 'c'] to \"abc\"\n",
    "        tuple_from_list = tuple(['a', 'b', 'c'])  # List to Tuple: Converts ['a', 'b', 'c'] to ('a', 'b', 'c')\n",
    "        dict_from_list = dict([('key1', 'value1'), ('key2', 'value2')])  # List to Dictionary: Converts [('key1', 'value1'), ('key2', 'value2')] to {'key1': 'value1', 'key2': 'value2'}\n",
    "        numpy_array_from_list = np.array([1, 2, 3])  # List to NumPy Array: Converts [1, 2, 3] to NumPy array\n",
    "\n",
    "        # Tuple to Other Types\n",
    "        str_from_tuple = ''.join(('a', 'b', 'c'))  # Tuple to String: Converts ('a', 'b', 'c') to \"abc\"\n",
    "        list_from_tuple = list(('a', 'b', 'c'))  # Tuple to List: Converts ('a', 'b', 'c') to ['a', 'b', 'c']\n",
    "        dict_from_tuple = dict((('key1', 'value1'), ('key2', 'value2')))  # Tuple to Dictionary: Converts (('key1', 'value1'), ('key2', 'value2')) to {'key1': 'value1', 'key2': 'value2'}\n",
    "\n",
    "        # Dictionary to Other Types\n",
    "        list_from_dict = list({'key': 'value'}.items())  # Dictionary to List: Converts {'key': 'value'} to [('key', 'value')]\n",
    "        tuple_from_dict = tuple({'key': 'value'}.items())  # Dictionary to Tuple: Converts {'key': 'value'} to (('key', 'value'),)\n",
    "        str_from_dict = str({'key': 'value'})  # Dictionary to String: Converts {'key': 'value'} to \"{'key': 'value'}\"\n",
    "\n",
    "        # Converting to and from JSON\n",
    "        json_str_from_dict = json.dumps({'key': 'value'})  # Dictionary to JSON String: Converts {'key': 'value'} to '{\"key\": \"value\"}'\n",
    "        dict_from_json_str = json.loads('{\"key\": \"value\"}')  # JSON String to Dictionary: Converts '{\"key\": \"value\"}' to {'key': 'value'}\n",
    "        \n",
    "        # Converting to and from Pandas DataFrame\n",
    "        df_from_dict = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]})  # Dictionary to DataFrame\n",
    "        dict_from_df = df_from_dict.to_dict()  # DataFrame to Dictionary\n",
    "        list_from_series = pd.Series([1, 2, 3]).tolist()  # Converts Series to list\n",
    "        dict_from_series = pd.Series([1, 2, 3]).to_dict()  # Converts Series to dictionary\n",
    "        numpy_array_from_df_column = df['A'].to_numpy()  # Converts specific DataFrame column to NumPy array\n",
    "\n",
    "        # Converting to and from NumPy Arrays\n",
    "        numpy_array_from_list_tuple = np.array([1, 2, 3])  # List/Tuple to NumPy Array\n",
    "        list_from_numpy_array = numpy_array_from_list_tuple.tolist()  # NumPy Array to List\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Control Flow Statements\n",
    "    # 1. if statement: checks condition and executes code block if true.\n",
    "    # 2. elif statement: else if clause to an if statement.\n",
    "    # 3. else statement: executes when no other conditions are met.\n",
    "    # 4. while loop: repeats until specified condition is false.\n",
    "    # 5. break statement: breaks out of current closest looping structure.\n",
    "    # 6. continue statement: skips rest of current iteration and goes back to beginning of next iteration.\n",
    "    # 7. for loops: iterate over each item in something iterable(list, tuple etc.).\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterable:\n",
    "# Iterable: An object is called iterable if we can get an iterator from it. Examples of iterables include all sequence types \n",
    "# (such as list, str, tuple) and some non-sequence types like dict, file objects, and objects of any classes you define with an __iter__() \n",
    "# or __getitem__() method.\n",
    "\n",
    "# Iterator:\n",
    "# Iterator: An iterator is an object that can be iterated upon and which returns data, one element at a time when next() is called on it. \n",
    "# In Python, an iterator must implement two methods:  \n",
    "    # __iter__() which returns the iterator object itself. This is used in for and in statements.\n",
    "    # __next__() which returns the next value from the iterator. If there is no more items to return, it should raise StopIteration exception.\n",
    "    my_list = [\"go\", \"hello\", 23, 10, 12, 19]\n",
    "    my_iter = iter(my_list)  # Create an iterator\n",
    "\n",
    "    for _ in range(4):\n",
    "        print(next(my_iter))\n",
    "    \n",
    "    \n",
    "    \n",
    "#basic Python Functions and Operations\n",
    "# built in functions\n",
    "\n",
    "int, bool, tuple, str, chr, list, set, dict, \n",
    "all, any, dir, type, help,\n",
    "enumerate, filter, zip, zip(*zipped)-> unzip, input, isinstance, iter or __iter__(), next or __next__(),\n",
    "len, map, min, max, pow, print, range, reversed, round, sorted, sum, super, \n",
    "\n",
    "#functions of list, strings, dictionary, tuples etc\n",
    "dir(list)\n",
    "dir(str)\n",
    "dir(dict)\n",
    "dir(tuple)\n",
    "\n",
    "# python built-in functions\n",
    "import builtins\n",
    "dir(builtins)\n",
    "\n",
    "# other python libraries\n",
    "import pkgutil\n",
    "standard_lib = sorted([module for _, module, _ in pkgutil.iter_modules()])  # sorted list of all standard library module names.\n",
    "\n",
    "for module in standard_lib:\n",
    "    print(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "####################################### LIST ############################################\n",
    "\n",
    "#List Manipulation\n",
    "val = [10, 12, 23, 24, 22]  # Sample List for reference\n",
    "\n",
    "\" \".join(my_list)   # convert a list to string\n",
    "my_string.split(\" \") # convert a string to list | or list(my_string)\n",
    "del val[1]  # Removing an item by index (delete)\n",
    "first_item = val[0]  # Accessing an item\n",
    "first_two = val[:2]  # Slicing the list\n",
    "list('Hello') # Creates a list from the string.\n",
    "list(range(5)) # Creates a list from the range object.\n",
    "list.append('Hello') # Adds an item to the end of the list.\n",
    "list.count('Hello') # Counts the occurrences of an item in the list.\n",
    "list.extend('Hello') # Adds all items of an iterable to the end of the list.\n",
    "list.index('Hello') # Returns the index of an item in the list.\n",
    "list.insert(0, 'Hello') # Inserts an item at a specified position in the list.\n",
    "list.pop() # Removes an item from the list.\n",
    "list.remove('Hello') # Removes the first occurrence of an item from the list.\n",
    "list.reverse() # Reverses the order of the items in the list.\n",
    "list.sort() # Sorts the items in the list.\n",
    "val.sort(key=lambda x: x[0])  # Sorting the list by the first element\n",
    "for i in val:\n",
    "    if isinstance(i, list):\n",
    "        print(\"List\")\n",
    "print(\",\".join(sorted(list(set(words)))))\n",
    "\n",
    "\n",
    "# List Operations\n",
    "val.clear()  # Clearing the list\n",
    "copied = val.copy()  # Copying the list\n",
    "repeated = val * 2  # Multiplying the list (repeating it)\n",
    "\n",
    "\n",
    "#List Comprehensions (with Lambda Functions)\n",
    "        # Expression: [expression for item in iterable if condition]\n",
    "filtered = [item for item in val if item[0] == 20]  # Filtering the list\n",
    "[num for sublist in val for num in sublist]  # Flattening the list\n",
    "[[x+1, y+1] for x, y in val]  # Increase each element\n",
    "[i**3 if i %2 else i**2 for i in numbers]\n",
    "are_all_greater_than_20 = all(x[0] > 20 for x in val)  # Check if all items meet a condition\n",
    "is_any_greater_than_50 = any(x[1] > 50 for x in val)  # Check if any item meets a condition\n",
    "flattened_even = [y for x in val for y in x if y % 2 == 0]  # Nested list comprehension\n",
    "\n",
    "\n",
    "# Advanced List Processing\n",
    "another_list = [[30, 40], [40, 50]]  # Combining with another list using zip\n",
    "combined = list(zip(val, another_list))\n",
    "first_elements, second_elements = zip(*val)  # Unzipping a list of pairs\n",
    "as_dict = dict(val)  # Converting list of pairs to a dictionary\n",
    "\n",
    "\n",
    "# List Statistics and Queries\n",
    "count = val.count([20, 45])  # Counting occurrences\n",
    "length = len(val)  # Length of the list\n",
    "exists = [20, 45] in val  # Checking if an item exists in the list\n",
    "maximum = max(val, key=lambda x: x[0])  # Finding the maximum based on the first element\n",
    "minimum = min(val, key=lambda x: x[0])  # Finding the minimum based on the first element\n",
    "total = sum(item[0] for item in val)  # Summing the first elements\n",
    "unique_set = set(tuple(item) for item in val)  # Converting list to a set (for unique values) - set is an unordered collection of unique items\n",
    "\n",
    "\n",
    "# Functional Programming with Lambda\n",
    "        # Expression: lambda arguments: expression\n",
    "            add = lambda x, y: x + y\n",
    "filtered_lambda = list(filter(lambda x: x[0] > 20, val))  # Filtering using a lambda function\n",
    "mapped_lambda = list(map(lambda x: [x[0]*2, x[1]*2], val))  # Mapping using a lambda function\n",
    "max_num = lambda x, y: x if x > y else y  # Using lambda to find the max value\n",
    "\n",
    "\n",
    "\n",
    "########################################### STRING #################################################\n",
    "\n",
    "# String Manipulation\n",
    "my_string = \"Hello World\"\n",
    "reversed_joined_string = \" \".join(reversed(string.split()))  # Split, reverse, and join to a string\n",
    "new_string = string[:4] + string[4+1:]    # using string concatenation to delete an item\n",
    "new_string = string.replace('0', \"\") # using replace to delete an item\n",
    "new_string = reversed(string)  # Reverse order strings\n",
    "\n",
    "# Advanced List and String Manipulation\n",
    "joined = ', '.join(map(str, val))  # Joining the list into a string\n",
    "str_val = str(val)  # Converting to string and back to list\n",
    "back_to_list = eval(str_val)\n",
    "k = 2  # Rotate by 2 positions\n",
    "rotated = val[k:] + val[:k]  # Rotate the list\n",
    "half = len(val)//2  # Split the list into two halves\n",
    "first_half, second_half = val[:half], val[half:]\n",
    "\n",
    "\n",
    "my_string.lower() # Converts all characters in the string to lowercase. Frequently used for case-insensitive comparisons.\n",
    "my_string.upper() # Converts all characters in the string to uppercase. Often used for standardizing text input.\n",
    "my_string.replace() # Replaces occurrences of a specified substring with another substring.\n",
    "my_string.split() # Divides a string into a list based on a specified delimiter.\n",
    "my_string.join() # Combines a list of strings into a single string, using a specified separator.\n",
    "my_string.find() # Searches for a substring within a string and returns the lowest index where the substring is found.\n",
    "my_string.strip() # Removes leading and trailing characters (spaces by default) from a string.\n",
    "my_string.startswith() # Checks if the string starts with the specified substring.\n",
    "my_string.endswith() # Checks if the string ends with the specified substring.\n",
    "my_string.count() #  Counts occurrences of a substring within the string.\n",
    "my_string.format() # Formats the string in a specified format.\n",
    "my_string.capitalize() # Capitalizes the first character of the string.\n",
    "my_string.title() # Converts the first character of each word to uppercase and the rest to lowercase.\n",
    "\n",
    "\n",
    "my_string.isalnum()   # Checks if all characters in the text are alphanumeric. \n",
    "my_string.isalpha()   # Checks if all characters in the text are letters.\n",
    "my_string.isnumeric() # Checks if all characters in the text are numeric.\n",
    "my_string.isdigit()   # Checks if all characters in the text are digits.\n",
    "my_string.is_integer() # Checks if a floating-point number is an integer.\n",
    "my_string.isdecimal() # Checks if all characters in the text are decimals.\n",
    "my_string.isascii()   # Checks if all characters in the text are ASCII. (ord('A) or chr(65))\n",
    "my_string.isupper()   # Checks if all characters in the text are uppercase.\n",
    "my_string.islower()   # Checks if all characters in the text are lowercase.\n",
    "my_string.isinstance()    # Checks if an object is an instance of a specified type.\n",
    "my_string.issubclass()    # Checks if a class is a subclass of a specified class.\n",
    "my_string.isidentifier()  # Checks if the string is a valid identifier.\n",
    "my_string.isprintable()   # Checks if all characters in the text are printable.\n",
    "my_string.isspace()   # Checks if all characters in the text are whitespaces.\n",
    "my_string.istitle()   # Checks if the string follows the title case style.\n",
    "\n",
    "\n",
    "\n",
    "################################################# DICTIONARY #################################################\n",
    "\n",
    "# Dictionary \n",
    "    my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
    "    new_dict = {'a': 1, 'b': 2}\n",
    "    my_dict_2 = {'b': 3, 'c': 4}\n",
    "\n",
    "new_dict.update(dict2) # merge dictionaries (similar to append or extend for lists)\n",
    "my_dict.items()    # provides a view on the key-value pairs.\n",
    "my_dict.keys() # provides a view on the keys.\n",
    "my_dict.values()   # provides a view on the values.\n",
    "my_dict[key]    # accessing values\n",
    "my_dict[key] = value    # adding or updating\n",
    "dict1.copy()   # copying a dictionary\n",
    "dict1.clear() # clearing all items\n",
    "dict1.pop('a', 'default value if key not found')   # removes a key-value pair and returns the value.\n",
    "dict1.get('b', 'default value')    # access the value associated with a key, while providing a default value if the key is not found.\n",
    "del dict1['b'] #delete a key\n",
    "{value : key for key, value in dict_one.items() if condition}    #swap keys and values in a dictionary\n",
    "\n",
    "\n",
    "# Using external moduls\n",
    "import itertools\n",
    "chained = list(itertools.chain.from_iterable(val))  # Using itertools.chain to flatten\n",
    "combinations = list(itertools.combinations(val, 2))  # Using itertools.combinations\n",
    "permutations = list(itertools.permutations(val, 2))  # Using itertools.permutations\n",
    "\n",
    "sorted_val = sorted(val, key=lambda x: x[0])  # Using sorted and itertools.groupby\n",
    "grouped = {k: list(g) for k, g in itertools.groupby(sorted_val, key=lambda x: x[0])}\n",
    "\n",
    "import random\n",
    "random.shuffle(val)  # Shuffling the list\n",
    "sample = random.sample(val, 2)  # Taking random samples\n",
    "\n",
    "\n",
    "#Others\n",
    "from functools import reduce\n",
    "total_sum = reduce(lambda acc, x: acc + x[0] + x[1], val, 0)  # Using reduce to sum elements\n",
    "\n",
    "for idx, item in enumerate(val):  # Using enumerate to get index and item\n",
    "    print(idx, item)\n",
    "\n",
    "index_of_first_greater_than_20 = next((i for i, x in enumerate(val) if x[0] > 20), None)  # Finding index with a condition\n",
    "\n",
    "import copy\n",
    "deep_copied = copy.deepcopy(val)  # Deep copy of the list\n",
    "\n",
    "\n",
    "\n",
    "# working with time\n",
    "import time\n",
    "from datetime import datetime, time\n",
    "import pytz #this is much easier to work with time\n",
    "\n",
    "nigeria_timezone = pytz.timezone('Africa/Lagos')    # Set the timezone to Nigeria\n",
    "current_time = datetime.now(nigeria_timezone)   # Get the current time\n",
    "current_time = datetime.datetime.now(nigeria_timezone) # Get the current time in Nigeria\n",
    "formatted_time = current_time.strftime(\"%H:%M:%S\")  # Format the time as desired\n",
    "start_time = time.time()\n",
    "end_time = = time.time()\n",
    "print(\"Time taken by function is %s seconds\" % (end_time - start_time))\n",
    "\n",
    "\n",
    "\n",
    "#Getting Help and Documentation\n",
    "?list.pop  # Get help on the pop method of a list\n",
    "?max # or max? - if it is a baseline function\n",
    "??list.pop  # View the source code of the pop method if possible\n",
    "help(list.pop)  # Get help on the pop method of a list\n",
    "print(list.pop.__doc__)  # Print the documentation string of the pop method\n",
    "print(dir(list))  # List all attributes and methods of the list class\n",
    "\n",
    "import numpy as np  #checking documentation from external libraries\n",
    "help(np.some_function)\n",
    "\n",
    "# Each datatype has its own unique methods, hence its own way of finding documentation\n",
    "my_string = \"Hello, World!\"     # for string\n",
    "help(str.upper)\n",
    "help(my_list.append)    # for list\n",
    "help(my_dict.get)   # for dictionary\n",
    "help(my_tuple.index)   # for tuple\n",
    "help(my_float.is_integer)  # for floating point number\n",
    "\n",
    "\n",
    "import builtins\n",
    "\n",
    "# List all built-in functions and classes\n",
    "builtins_list = dir(builtins)\n",
    "print(builtins_list)\n",
    "print(dir(list))\n",
    "print(dir(str))\n",
    "print(dir(dict))\n",
    "print(dir(tuple))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89347b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lambda Functions: Anonymous functions defined using the lambda keyword.\n",
    "    # Expression: lambda arguments: expression\n",
    "add = lambda x, y: x + y\n",
    "result = add(3, 5)  # result is 8\n",
    "\n",
    "\n",
    "# List Comprehensions: Concise syntax for creating lists.\n",
    "    # Expression: [expression for item in iterable if condition]\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "squares = [x ** 2 for x in numbers if x % 2 == 0]\n",
    "# squares will be [4, 16]\n",
    "\n",
    "\n",
    "# Generators: Functions that yield values one at a time using the yield keyword.\n",
    "    # Expression: def generator_function(): yield value\n",
    "def count_up_to(n):\n",
    "    i = 1\n",
    "    while i <= n:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "gen = count_up_to(5)\n",
    "for num in gen:\n",
    "    print(num)  # Prints 1, 2, 3, 4, 5\n",
    "\n",
    "# Classes:\n",
    "class Person:\n",
    "    def __init__(self, height, shoe_size, shirt_size, pant_size):\n",
    "        self.height = height\n",
    "        self.shoe_size = shoe_size\n",
    "        self.shirt_size = shirt_size\n",
    "        self.pant_size = pant_size\n",
    "\n",
    "    def school(self):   # object functions (methods)\n",
    "        if (self.height > 6) and (self.pant_size) > 35:\n",
    "            return \"University of Oklahoma'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def work(self, name): # object function (methods)\n",
    "        print(\"{name} is going to work.\")\n",
    "\n",
    "Paul = Person(6.5, 13, \"XXL\", 38)  #instantiating the class (an object)\n",
    "Paul.height # assessing an attribute in the object\n",
    "Paul.school()       # calling a method of the class\n",
    "dir(class_name) # this gives you a list of all the methods of the class.\n",
    "\n",
    "\n",
    "# Decorators: Functions that modify other functions or methods.\n",
    "    # Expression: @decorator_function\n",
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def say_hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "say_hello()\n",
    "\n",
    "\n",
    "def fibonacci (numb):\n",
    "    if numb < 1:\n",
    "        return \"enter a valid integer > 0\"\n",
    "    elif numb == 1:\n",
    "        return [0]\n",
    "    elif numb == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        fibonacci = [0, 1]\n",
    "        for i in range(2, numb):\n",
    "            fibonacci.append(fibonacci[-1] + fibonacci[-2])\n",
    "        return fibonacci\n",
    "        \n",
    "\n",
    "print(fibonacci(9))\n",
    "# Context Managers: Used with the with statement to manage resources.\n",
    "    # Expression: with context_manager as resource:\n",
    "with open(\"file.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "# File is automatically closed when exiting the 'with' block.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_library (module)\n",
    "|\n",
    "|--- Animal (class)\n",
    "|    |\n",
    "|    |--- __init__(self, name) (method)\n",
    "|    |    |--- self.name (attribute)\n",
    "|    |\n",
    "|    |--- speak(self) (method)\n",
    "|\n",
    "|--- Dog (class, inherits from Animal)\n",
    "|    |\n",
    "|    |--- __init__(self, name, breed) (method)\n",
    "|    |    |--- self.breed (attribute)\n",
    "|    |\n",
    "|    |--- speak(self) (method)\n",
    "|    |--- fetch(self, item) (method)\n",
    "|\n",
    "|--- Cat (class, inherits from Animal)\n",
    "     |\n",
    "     |--- __init__(self, name, color) (method)\n",
    "     |    |--- self.color (attribute)\n",
    "     |\n",
    "     |--- speak(self) (method)\n",
    "     |--- purr(self) (method)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Animal base class\n",
    "class Animal:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # All animals will have a name\n",
    "\n",
    "    def introduce(self):\n",
    "        return f\"I am an animal and my name is {self.name}.\"\n",
    "\n",
    "# Define the Dog class which inherits from Animal\n",
    "class Dog(Animal):\n",
    "    def __init__(self, name, breed):\n",
    "        super().__init__(name)  # Call the superclass constructor to set the name\n",
    "        self.breed = breed  # Dog-specific attribute\n",
    "\n",
    "    def introduce(self):\n",
    "        # Override the method to include breed specific introduction\n",
    "        return f\"I am a {self.breed} dog and my name is {self.name}.\"\n",
    "\n",
    "# Create an instance of Dog\n",
    "my_dog = Dog(\"Buddy\", \"Golden Retriever\")\n",
    "\n",
    "# Call the introduce method\n",
    "print(my_dog.introduce())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489178c2",
   "metadata": {},
   "source": [
    "> Random Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a5dc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(a=None, version=2) # Initialize the random number generator with an optional integer or system time.\n",
    "random.random() # Return the next random floating point number between 0.0 and 1.0.\n",
    "random.uniform(a, b) # Generate a random float r, such that a <= r <= b.\n",
    "random.randint(a, b) # Return a random integer N such that a <= N <= b.\n",
    "random.randrange(start, stop[, step]) # Return a randomly selected element from range(start, stop, step).\n",
    "random.choice(seq) # Return a random element from the non-empty sequence seq.\n",
    "random.choices(population, weights=None, *, cum_weights=None, k=1) # Return a k sized list of elements chosen from the population with replacement. \n",
    "random.shuffle(x[, random]) # Shuffle the sequence x in place. The optional argument random is a 0-argument function returning a random float in [0.0, 1.0).\n",
    "random.sample(population, k) # Return a k length list of unique elements chosen from the population sequence.\n",
    "random.getstate() # Return an object capturing the current internal state of the generator.\n",
    "random.setstate(state) # Restore the internal state of the generator to a state previously captured with getstate().\n",
    "random.getrandbits(k) # Return a python integer with k random bits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy \n",
    "#1D - Vectors\n",
    "#2D - Matrices\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1,2,3], [1,2,3]])\n",
    "arr\n",
    "arr[~np.isnan(arr)] #filter array 'arr' where there are no missing values.\n",
    "\n",
    "\n",
    "#Numpy built-in functions\n",
    "    #creating arrays\n",
    "np.arange(0,11,2) #(start, stop, step)\n",
    "np.linspace(1,5,10) #(start, stop, number of items)\n",
    "np.zeros((3,3)) #(row, col)\n",
    "np.zeros_like(arr)\n",
    "np.ones((2,3)) #(row, col)\n",
    "np.ones_like(arr)\n",
    "np.eye(4) #identity matrix\n",
    "np.empty_like()\n",
    "\n",
    "\n",
    "#creating arrays of random numbers\n",
    "np.random.rand(5) #random samples of a uniform distribution from 0-1\n",
    "    np.random.rand (5,5) #2D random samples from 0-1\n",
    "np.random.randn(2) #random samples from a standard normal distribution centered around zero (0)\n",
    "np.random.randint(5) #returns random integers from a low to a high number\n",
    "    np.random.randint(1,100,10) #(low, high, size) #inclusive on the low end, exclusive on the high end\n",
    "np.random.permutation(np.arange(10)) #to shuffle\n",
    "np.random.choice(a=(3, 10), size=(3,5), replace=False) # returns a random sample from a range of 'a' values for the inputted size. \n",
    "np.random.sample(size=(1, 5), k = 5) #returns k random samples of the inputted size \n",
    "\n",
    "\n",
    "#attributes and methods of Array\n",
    "arr = np.array(range(10))\n",
    "arr.reshape(5,5) #reshape an array (row, col)\n",
    "arr.shape #find the shape of an array\n",
    "\n",
    "\n",
    "#Numpy Indexing and Slicing\n",
    "arr[1:3] = array([1,2]) #for 1D #inclusive on the low end and exclusive on the high end\n",
    "arr[1:2, 0:1] #for 2-D (row, col)\n",
    "arr[0][2] #[row][col]\n",
    "\n",
    "arr[1:2] = 10 #Broadcasting an array #its always best to make a copy before broadcasting an array\n",
    "    arr.copy() #to copy an array\n",
    "\n",
    "\n",
    "#Conditional selection\n",
    "arr[arr>3]\n",
    "arr[(arr<8) & (arr > 4)] #and\n",
    "arr[(arr>5) | (arr < 3)] #or\n",
    "\n",
    "\n",
    "#Useful functions in Numpy\n",
    "np.where(arr==3), np.argwhere(arr==3) or np.unravel_index(3, arr) #find index of 3 in array \"arr\"\n",
    "np.repeat(3,5) or np.tile(arr,5) #repeat number 3, 5 times. #repeat - 1D; tile - 2D\n",
    "    np.tile(arr, (2,2)) #repeat array \"arr\" in 2 rows and 2 cols\n",
    "# we can also have: np.where(condition, x, y), which means \"Replace elements in x with y where the condition is False.\"\n",
    "\n",
    "#plot\n",
    "np.polyfit(x,y)\n",
    "np.polyval(p,x)\n",
    "np.histogram(a, bins=10, range=None, normed=None)\n",
    "\n",
    "np.full((2,4), fill_value=2) #fill an array of 2 rows, 4 cols with the value 2\n",
    "np.unique(arr, return_counts=True) #return_counts = If True, also return the number of times each unique item appears\n",
    "np.T(arr) or arr.T (transpose)\n",
    "\n",
    "#maths\n",
    "np.mean()\n",
    "np.describe()\n",
    "np.sum()\n",
    "np.sort()\n",
    "np.flatten(arr) #flatten from x-dim to 1D\n",
    "np.add(arr_1, arr_2)\n",
    "np.subtract(x,y)\n",
    "np.multiply(x,y)\n",
    "np.divide(x,y)\n",
    "np.min(arr)\n",
    "np.max(arr)\n",
    "np.power(arr)\n",
    "np.sqrt(arr)\n",
    "np.sin(arr)\n",
    "np.floor(arr)\n",
    "np.round(arr)\n",
    "np.std(arr)\n",
    "np.abs(arr)\n",
    "np.var(arr)\n",
    "\n",
    "np.digitize(x, bins, right=False) #to xxxx\n",
    "np.shape or np.reshape()\n",
    "np.expand_dims(ar, axis=0) or np.flatten(arr, axis=0) or np.squeeze(arr, axis=0)\n",
    "np.count_nonzero(arr) #count all non-zero elements\n",
    "np.argwhere(arr)\n",
    "np.argmin(arr) or np.argmax(arr)\n",
    "arr.clip(0,5) #to keep all values of an array within a range\n",
    "np.put(arr, [1,2], [6,7]) #(arr, (indices = [where to replace]), (value = [what to replace it with]))\n",
    "np.astype(int) #change the datatype to integer\n",
    "np.astype(float) #change the datatype to float\n",
    "np.transpose(image, (1, 2, 0)) \n",
    "\n",
    "#set operations\n",
    "np.intersect1d(arr1,arr2) #(arr1 n arr2 ) returns all unique values from the two arrays\n",
    "np.setdiff1d(a,b) #returns al unique elements of a not in b\n",
    "np.setxor1d(a,b) #same with above but in sorted order\n",
    "np.union1d(a,b) (a U b)\n",
    "\n",
    "#splitting\n",
    "np.hsplit(arr, 2) #split the data horizontally (rows) into two equal parts\n",
    "np.vsplit (arr, 3) #split the data vertically (cols) into three equal parts\n",
    "\n",
    "#stacking\n",
    "np.hstack((a, b)) #stack/appends b unto a horizontally (rows)\n",
    "np.vstack((a,b)) #stack/appends b unto a vertically (cols)\n",
    "\n",
    "#comparing\n",
    "np.allclose(a,b, tolerance) #compares two array base in tolerance level\n",
    "np.equal(a,b) #compares elements of both arrays\n",
    "\n",
    "#print options\n",
    "np.set_printoptions(precision=2) #show floats within two decimal points\n",
    "np.set_printoptions(threshold=np.inf) #print array to its max (np.inf - positive infinity)\n",
    "np.printoptions(linewidth = 100) #increase the number of elements in a line\n",
    "\n",
    "#save and load array\n",
    "np.savetxt(\"array.txt\", arr)\n",
    "np.loadtxt(\"array.txt\")\n",
    "\n",
    "#multiply\n",
    "np.multiply(a,b) #multiply two arrays of the same shape\n",
    "np.dot(a,b) #multiply two arrays of different shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.Dataframe()\n",
    "\n",
    "# Most used Pandas DataFrame methods - dir(pd.DataFrame)\n",
    "\n",
    "df.head() / df.tail() # Display the first or last few rows of the DataFrame.\n",
    "df.loc[] / df.iloc[] # Access a group of rows and columns by label(s) or integer index.\n",
    "df.columns.get_loc(col) / df.columns.get_index(col) # Get the index of a column in the DataFrame.\n",
    "df.at[3, 'Column B'] / df.iat[-1, 1] # Access a single value for a row/column label pair. \n",
    "df.columns() \n",
    "df.index() / df.reset_index() / df.set_index()  # Set or Reset the index of the DataFrame, and use the default one. \n",
    "& | # and, or df[(df[\"col_4\"] > 0) & (df[\"col_7\" < 7])] \n",
    "pd.isnull() | pd.notnull()  # Detect missing values (NaN in numeric arrays, None/NaN in object arrays). \n",
    "df.melt(id_vars='xx', var_name='xx', value_name='xx')   # Unpivot a DataFrame from wide format to long format. \n",
    "\n",
    "df.groupby() / df.groupby().get_group()  # Group DataFrame using a mapper or by a series of columns.\n",
    "pd.merge(df_1, df_2, how=\"inner\", on = \" \") # Merge DataFrame objects with a database-style join. - (merges on columns keys)\n",
    "df.concat() / pd.concat([df1, df2, df2], axis = 1)\n",
    "df.join(df_2, on=\"inner\") # Join columns with other DataFrame either on index or on a key column.   - (merges on index keys)\n",
    "df['Name'].str.contains('Sum')  // df.filter(like='Sum', axis=1) // df['Name'].str.startswith('Sum')   # filter a specific column/row (df.filter) or its contents (str.contains)\n",
    "df.sort_values() # Sort by the values along either axis.\n",
    "df.value_counts() # Return a Series containing counts of unique values.\n",
    "df.describe() # Generate descriptive statistics.\n",
    "df.info() # Provide a concise summary of the DataFrame.\n",
    "df.unique(), nunique() # Find unique values / count of unique values.\n",
    "df.sum(), mean(), median(), std(), count() # Compute the sum / mean / median / standard deviation of the values.\n",
    "df.dtypes / df.astype(float) / df.select_dtypes(include=[\"float64\", \"int64\"]) # check the datatype / Cast a pandas object to a specified dtype.\n",
    "pd.to_datetime(df[\"dob\"], infer_datetime_format=True)\n",
    "df.duplicated()\n",
    "df.isin()\n",
    "df.itterrows()  # Iterate over DataFrame rows as (index, Series) pairs.\n",
    "df.drop() / df.pop() / dropna() / df.drop_duplicates() # Remove rows or columns / rows with missing values.\n",
    "df.fillna() / df.ffill# Fill NA/NaN values.\n",
    "df.isna(), isnull() # Detect missing values.\n",
    "df.shape() / df.size()\n",
    "df.copy()\n",
    "df.rename() / df.replace() # Rename or Replace values given in 'to_replace' with 'value'.\n",
    "df.filter(items=[\"lab1\", \"lab2\"], regex='e$', axis=1) # Subset rows or columns of DataFrame according to labels in the specified index.\n",
    "df.apply() / df.applymap(lambda x:x[1:]) # Apply a function along an axis of the DataFrame. use applymap if you want to apply to only an element.\n",
    "df.map() #\n",
    "df.to_csv(), to_excel(), to_json(), to_sql() # Methods to save DataFrame in different formats.\n",
    "df.query('col1 > 30 and col2 == \"Male\"') / df.query('col1.str.startswith(\"J\")')  # pay attention to the single, double quotes and backsticks\n",
    "df.query('index2 == \"Hotel room\" and `col3` <= 300') # Query the columns of a DataFrame with a boolean expression\n",
    "df.apply(pd.to_numeric, errors='coerce') # Convert the DataFrame to numeric values, and coerce the non-convertible values to NaN.\n",
    "pd.to_numeric, pd.to_datetime, pd.to_timedelta, pd.wide_to_long, pd.to_pickle\n",
    "df.to_csv, df.to_dict, df.to_excel, df.to_markdown, df.to_string, df.to_numpy, df.to_pickle, df.to_sql, df.to_stata, df.to_timestamp,\n",
    "df.xs('value2', level='index2') #cross-section method - very helpful for multi-index dataframes \n",
    "\n",
    "\n",
    "df.corr()\n",
    "df.pivot_table() # Create a spreadsheet-style pivot table as a DataFrame.\n",
    "df.plot() # Make plots of DataFrame using matplotlib / plotly.\n",
    "df.copy() # Make a copy of this object's indices and data.\n",
    "df.sample() # Return a random sample of items from an axis of the object.\n",
    "df.notna() / notnull() # Detect existing (non-missing) values.\n",
    "df.idxmax(), idxmin() # Return index of first occurrence of maximum / minimum over requested axis.\n",
    "df.melt() # Unpivot a DataFrame from wide format to long format.\n",
    "df.transpose() # Transpose index and columns of the DataFrame.\n",
    "df.clip() # Trim values at input threshold(s).\n",
    "df = df[~((df['column1'] == value1) & (df['column2'] == value2))]   #How to remove specific row based on condition\n",
    "\n",
    "\n",
    "#How to add an empty column to DataFrame\n",
    "df[\"Blank_Column\"] = \" \" // np.nan // None\n",
    "df2 = df.assign(Blank_Column=\" \", NaN_Column = np.nan, None_Column=None) # Add an empty columns using the assign() method\n",
    "df2 = df.reindex(columns = df.columns.tolist() + [\"None_Column\", \"None_Column_2\"]) # Add multiple columns with NaN , uses columns param\n",
    "df2 = df.reindex(df.columns.tolist() + [\"None_Column\", \"None_Column_2\"],axis=1) # Add multiple columns with NaN, , uses axis param \n",
    "df.insert(0,\"Blank_Column\", \" \") # Using insert(), add empty column at first position\n",
    "df[\"Blank_Column\"] = df.apply(lambda _: ' ', axis=1) # Using apply() & lambda function\n",
    "\n",
    "\n",
    "# Convert column to datetime\n",
    "df['date'] = pd.to_datetime(df['date_column'])  # Convert column to datetime\n",
    "# Generate a range of dates\n",
    "date_range = pd.date_range(start='2023-06-01', end='2023-06-08')  # Date range from start to end\n",
    "date_range = pd.date_range(start='2023-06-01', periods=8)  # Date range with 8 periods\n",
    "# Create a single timestamp \n",
    "ts = pd.Timestamp('2023-06-08')  # Create a single timestamp\n",
    "# Create a period\n",
    "period = pd.Period('2023-06')  # Create a single period\n",
    "# Create a range of periods\n",
    "period_range = pd.period_range('2023-06', periods=5, freq='M')  # Range of periods, monthly frequency\n",
    "# Datetime properties\n",
    "year = ts.year  # Get year\n",
    "month = ts.month  # Get month\n",
    "day = ts.day  # Get day\n",
    "hour = ts.hour  # Get hour\n",
    "minute = ts.minute  # Get minute\n",
    "second = ts.second  # Get second\n",
    "weekday = ts.weekday()  # Get day of the week (Monday=0, Sunday=6)\n",
    "day_name = ts.day_name()  # Get day name\n",
    "month_name = ts.month_name()  # Get month name\n",
    "# Handling timezones\n",
    "ts_utc = ts.tz_localize('UTC')  # Localize to UTC\n",
    "ts_est = ts_utc.tz_convert('US/Eastern')  # Convert to Eastern time\n",
    "# Time span calculations\n",
    "delta = pd.Timestamp('2023-06-08') - pd.Timestamp('2023-06-01')  # Calculate time delta\n",
    "# Convert to timedelta\n",
    "time_delta = pd.to_timedelta('1 days 06:05:01.00003')  # Convert string to timedelta\n",
    "time_delta = pd.to_timedelta('15.5us')  # Convert microseconds to timedelta\n",
    "\n",
    "# Offset aliases\n",
    "from pandas.tseries.offsets import BDay\n",
    "ts_shifted = ts + BDay(5)  # Add 5 business days\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization (Matplotlib, Searborn etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "markdown_table = \"\"\"\n",
    "| Plot Type             | Complexity Level | Data Type               | Variable Type | Description |\n",
    "|-----------------------|------------------|-------------------------|---------------|-------------|\n",
    "| Histogram             | Basic            | Numeric                 | Univariate    | Shows the distribution of a single numeric variable. |\n",
    "| Bar Chart             | Basic/Intermediate | Categorical           | Univariate/Bivariate | Shows numeric comparison across categories; can be extended to show multiple categories (stacked bar). |\n",
    "| Box Plot              | Intermediate     | Numeric vs Categorical  | Bivariate     | Displays the distribution of a numeric variable across different categories. |\n",
    "| Violin Plot           | Intermediate     | Numeric vs Categorical  | Bivariate     | Similar to box plots but with a kernel density estimation for better distribution visualization. |\n",
    "| Scatter Plot          | Basic/Intermediate | Numeric               | Bivariate     | Visualizes the relationship between two numeric variables. |\n",
    "| Line Plot             | Basic/Intermediate | Numeric               | Bivariate/Multivariate | Used for numeric x and y data; often for time series. |\n",
    "| Heatmap               | Intermediate     | Categorical/Numeric    | Bivariate/Multivariate | Visualizes data in matrix format; effective for correlation matrices, cross-tabulations. |\n",
    "| Pair Plot             | Intermediate     | Numeric                | Multivariate  | Showcases pairwise relationships in a dataset. |\n",
    "| KDE Plot              | Intermediate     | Numeric                | Bivariate/Multivariate | Kernel Density Estimation for visualizing the distribution of one or more variables. |\n",
    "| Joint Plot            | Intermediate     | Numeric                | Bivariate     | Combines scatter and histogram plots for joint distributions. |\n",
    "| Hexbin Plot           | Intermediate     | Numeric                | Bivariate     | Represents bivariate data using hexagons. |\n",
    "| Bubble Chart          | Intermediate     | Numeric                | Bivariate/Multivariate | Scatter plot variation where bubble size adds a third dimension. |\n",
    "| Swarm Plot            | Intermediate     | Numeric vs Categorical | Bivariate     | Similar to strip plot, but adjusts points to avoid overlap. |\n",
    "| Strip Plot            | Intermediate     | Numeric vs Categorical | Bivariate     | Displays individual data points as strips, providing a clear visualization of distribution. |\n",
    "| Facet Grid            | Intermediate     | Any                    | Multivariate  | Enables plotting of multiple plots on a grid for complex data comparisons. |\n",
    "| Ridge Plot            | Intermediate/Advanced | Numeric vs Categorical | Multivariate | Overlapping KDE plots for visualizing distributions across different categories. |\n",
    "| Parallel Coordinates  | Advanced         | Numeric/Categorical    | Multivariate  | Visualizes data points in terms of their features; good for comparing many variables. |\n",
    "| Contour Plot          | Advanced         | Numeric                | Bivariate     | Represents three-dimensional data in two dimensions using contours. |\n",
    "| Radar Chart             | Advanced           | Numeric/Categorical     | Multivariate        | Displays multivariate data in terms of a two-dimensional chart with one axis for each variable, typically used for performance analysis. |\n",
    "| Treemap                 | Intermediate       | Categorical/Numeric     | Multivariate        | Visualizes hierarchical data using nested rectangles, where size and color can represent different dimensions. |\n",
    "| Sunburst Chart          | Intermediate       | Categorical/Numeric     | Multivariate        | Visual representation of a hierarchy in a radial layout, useful for showing levels of a tree diagram. |\n",
    "| Sankey Diagram          | Advanced           | Categorical/Numeric     | Multivariate        | Visualizes the flow from one set of values to another, commonly used in data flow and financial transaction mapping. |\n",
    "| Choropleth Map          | Intermediate/Advanced | Geospatial Data      | Multivariate        | Maps where areas are shaded in proportion to a statistical variable; useful in geographical data visualization. |\n",
    "| 3D Scatter Plot         | Advanced           | Numeric                 | Multivariate        | A three-dimensional scatter plot, useful for visualizing multivariate data in 3D space. |\n",
    "| 3D Surface Plot         | Advanced           | Numeric                 | Multivariate        | Represents three-dimensional data as a surface in 3D space. |\n",
    "| Streamgraph             | Advanced           | Numeric/Categorical     | Multivariate        | A type of stacked area graph which is displaced around a central axis, resulting in a flowing, organic shape. |\n",
    "| Word Cloud              | Intermediate       | Textual Data            | Univariate          | Visual representation of text data where size of each word indicates its frequency or importance. |\n",
    "| Polar Chart             | Intermediate       | Numeric/Categorical     | Multivariate        | Displays data in a circular graph, where each variable is represented along a separate axis. |\n",
    "| Network/Graph Diagram   | Advanced           | Categorical/Numeric     | Multivariate        | Visualizes relationships and flows between nodes and connections in a network. |\n",
    "| Candlestick Chart       | Advanced           | Numeric                 | Bivariate/Multivariate | Used in financial analysis to represent the price movements of stocks, derivatives, etc. |\n",
    "| Gantt Chart             | Intermediate       | Categorical/Numeric     | Multivariate        | A type of bar chart that illustrates a project schedule or timelines. |\n",
    "| Density Plot            | Intermediate       | Numeric                 | Bivariate/Multivariate | Similar to KDE Plot, visualizes the distribution of a continuous variable. |\n",
    "| Dot Plot                | Intermediate       | Numeric/Categorical     | Bivariate/Multivariate | Represents data points as dots along an axis; useful for small to medium-sized datasets. |\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Plots\n",
    "    # Bar Chart\n",
    "    # Line Chart\n",
    "    # Pie Chart\n",
    "    # Scatter Plot\n",
    "    # Histogram\n",
    "    # Heat Map\n",
    "    # Box Plot and Violin Plot\n",
    "    # Density Plot and KDE plots\n",
    "\n",
    "\n",
    "# Bar Chart:\n",
    "    # Column Chart: Vertical bar chart.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(df_bar['Category'], df_bar['Value1'], color='b')\n",
    "        # sns.barplot(data=df, x='category', y='value', hue='sub_category') # to add hue\n",
    "        # sns.pairplot(data=df, hue='sub_category')\n",
    "        plt.title('Horizontal Bar Chart')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    # Stacked Bar Chart: Bars divided into sub-parts to show cumulative effect.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(df_bar['Category'], df_bar['Value1'], color='b', label='Value1')\n",
    "        plt.bar(df_bar['Category'], df_bar['Value2'], color='r', bottom=df_bar['Value1'], label='Value2')\n",
    "\n",
    "    # Grouped Bar Chart: Bars for different groups placed next to each other for comparison.\n",
    "        x = np.arange(len(df_bar['Category']))\n",
    "        width = 0.35\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(x - width/2, df_bar['Value1'], width, label='Value1')\n",
    "        plt.bar(x + width/2, df_bar['Value2'], width, label='Value2')\n",
    "        plt.xticks(x, df_bar['Category'])\n",
    "        \n",
    "    # Horizontal Bar Chart: Bars are displayed horizontally instead of vertically.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.barh(df_bar['Category'], df_bar['Value1'], color='b')\n",
    "        plt.title('Horizontal Bar Chart')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "# Line Chart:\n",
    "    # Basic Line Chart: Single line showing a trend over time.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(df_line['A'], marker='o', color='b')\n",
    "    # sns.lineplot(data=df, x='time', y='value', hue='category') # to add hue\n",
    "    \n",
    "    # Multi-Line Chart: Multiple lines representing different categories for comparison.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for column in df_line.columns:\n",
    "        plt.plot(df_line[column], marker='o', label=column)\n",
    "        plt.legend()\n",
    "    \n",
    "    # Area Chart: Similar to a line chart but with the area below the line filled in.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.fill_between(df_line.index, df_line['A'], color='skyblue', alpha=0.5)\n",
    "    plt.plot(df_line['A'], color='Slateblue', alpha=0.6)\n",
    "\n",
    "    # Stacked Area Chart: Multiple area charts stacked on top of one another.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.stackplot(df_line.index, df_line['A'], df_line['B'], df_line['C'], labels=['A', 'B', 'C'])\n",
    "\n",
    "    \n",
    "# Pie Chart:\n",
    "    sizes = df['gender'].value_counts() # Count the occurrences of each gender\n",
    "\n",
    "    # Basic Pie Chart: Standard pie with different slices.\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    \n",
    "    # Doughnut Chart: Similar to a pie chart but with a round hole in the center.\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, wedgeprops=dict(width=0.3))\n",
    "\n",
    "    # Exploded Pie Chart: Pie slices slightly separated from each other.\n",
    "    explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'B')\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    \n",
    "    \n",
    "# Scatter Plot:\n",
    "    # Basic Scatter Plot: Plots individual data points on a two-dimensional graph.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df_scatter['X'], df_scatter['Y'])   #hue = df_scatter['Z']\n",
    "    # sns.scatterplot(data=df, x='variable_x', y='variable_y', hue='category') # to add hue \n",
    "    \n",
    "    # Bubble Chart: Similar to scatter plots but uses bubble size as an additional variable.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df_scatter['X'], df_scatter['Y'], s=df_scatter['Size'], sizes=(20, 200), alpha=0.5, edgecolors='w', linewidth=0.5)   # hue = df_scatter['Z']\n",
    "    \n",
    "    # 3D Scatter Plot: Uses three dimensions to plot data points.\n",
    "    \n",
    "    \n",
    "# Histogram:\n",
    "    # Frequency Histogram: Shows the frequency of each data bin.\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(hist_data, bins=30, color='skyblue', edgecolor='black')\n",
    "    # sns.histplot(df[df['fire_occured'] == 'Yes']['RH'], color=\"red\", label='Fire Days', kde=True)\n",
    "    plt.title('Distribution of Relative Humidity on Days With and Without Forest Fires')\n",
    "    plt.xlabel('Relative Humidity (RH)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Density Histogram: Similar to a frequency histogram but shows the probability density.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(hist_data, bins=30, density=True, color='green', alpha=0.6, edgecolor='black')\n",
    "\n",
    "    # Cumulative Histogram: Displays the cumulative count of data points.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(hist_data, bins=30, cumulative=True, color='orange', alpha=0.6, edgecolor='black')\n",
    "\n",
    "    \n",
    "# Heat Map:\n",
    "    # Geographical Heat Map: Displays data density on maps.\n",
    "    # Matrix Heat Map: Represents data in a matrix format with colors indicating magnitude.\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df.select_dtypes(exclude='object').corr(), annot=True, cmap='viridis')\n",
    "\n",
    "    # Tree Map: Uses nested rectangles to represent hierarchical data with color and size.\n",
    "    import squarify\n",
    "    colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    squarify.plot(sizes=df['Value'], label=df['Category'], color=colors, alpha=0.6)\n",
    "    plt.title('Tree Map')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Box Plots and Violin Plots\n",
    "    np.random.seed(0)\n",
    "    box_violin_data = np.random.randn(100, 3)\n",
    "    df_box_violin = pd.DataFrame(box_violin_data, columns=['A', 'B', 'C'])\n",
    "    \n",
    "    # Box Plots: used to show the distribution of quantitative data and effectively highlights the median, quartiles, and outlier\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(data=df_box_violin)\n",
    "    # sns.boxplot(data=df, x='category', y='value', hue='sub_category') # to add hue\n",
    "\n",
    "    # Violin Plot: combines features of the box plot with a kernel density estimation\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.violinplot(data=df_box_violin)\n",
    "    # sns.violinplot(data=df, x='category', y='value', hue='sub_category', split=True) # to add hue\n",
    "\n",
    "\n",
    "# Density/KDE Plots: visualizes the distribution of data over a continuous interval\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data, kde=True, color='blue', bins=30)\n",
    "    # or\n",
    "    df_density['Value'].plot(kind='density', color='blue')\n",
    "    # or\n",
    "    sns.kdeplot(df_density['Value'], shade=True, color='green') # use this\n",
    "    \n",
    "\n",
    "\n",
    "# WordCloud\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Join all the strings in the column to create one large string\n",
    "    text = ' '.join(df['Text'])\n",
    "\n",
    "    # Create a word cloud object\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = None, \n",
    "                    min_font_size = 10).generate(text)\n",
    "\n",
    "    # Plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05763535",
   "metadata": {},
   "source": [
    "#### PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66056042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quickstart\n",
    "# Install on macOS:\n",
    "\n",
    "# brew install apache-spark && pip install pyspark\n",
    "\n",
    "# Create your first DataFrame:\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# I/O options: https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/io.html\n",
    "df = spark.read.csv('/path/to/your/input/file')\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Basics\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Show a preview\n",
    "df.show()\n",
    "\n",
    "# Show preview of first / last n rows\n",
    "df.head(5)\n",
    "df.tail(5)\n",
    "\n",
    "# Show preview as JSON (WARNING: in-memory)\n",
    "df = df.limit(10) # optional\n",
    "print(json.dumps([row.asDict(recursive=True) for row in df.collect()], indent=2))\n",
    "\n",
    "# Limit actual DataFrame to n rows (non-deterministic)\n",
    "df = df.limit(5)\n",
    "\n",
    "# Get columns\n",
    "df.columns\n",
    "\n",
    "# Get columns + column types\n",
    "df.dtypes\n",
    "\n",
    "# Get schema\n",
    "df.schema\n",
    "\n",
    "# Get row count\n",
    "df.count()\n",
    "\n",
    "# Get column count\n",
    "len(df.columns)\n",
    "\n",
    "# Write output to disk\n",
    "df.write.csv('/path/to/your/output/file')\n",
    "\n",
    "# Get results (WARNING: in-memory) as list of PySpark Rows\n",
    "df = df.collect()\n",
    "\n",
    "# Get results (WARNING: in-memory) as list of Python dicts\n",
    "dicts = [row.asDict(recursive=True) for row in df.collect()]\n",
    "\n",
    "# Convert (WARNING: in-memory) to Pandas DataFrame\n",
    "df = df.toPandas()\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Common Patterns\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Importing Functions & Types\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Easily reference these as F.my_function() and T.my_type() below\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Filtering\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Filter on equals condition\n",
    "df = df.filter(df.is_adult == 'Y')\n",
    "\n",
    "# Filter on >, <, >=, <= condition\n",
    "df = df.filter(df.age > 25)\n",
    "\n",
    "# Multiple conditions require parentheses around each condition\n",
    "df = df.filter((df.age > 25) & (df.is_adult == 'Y'))\n",
    "\n",
    "# Compare against a list of allowed values\n",
    "df = df.filter(col('first_name').isin([3, 4, 7]))\n",
    "\n",
    "# Sort results\n",
    "df = df.orderBy(df.age.asc()))\n",
    "df = df.orderBy(df.age.desc()))\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Joins\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Left join in another dataset\n",
    "df = df.join(person_lookup_table, 'person_id', 'left')\n",
    "\n",
    "# Match on different columns in left & right datasets\n",
    "df = df.join(other_table, df.id == other_table.person_id, 'left')\n",
    "\n",
    "# Match on multiple columns\n",
    "df = df.join(other_table, ['first_name', 'last_name'], 'left')\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Column Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Add a new static column\n",
    "df = df.withColumn('status', F.lit('PASS'))\n",
    "\n",
    "# Construct a new dynamic column\n",
    "df = df.withColumn('full_name', F.when(\n",
    "    (df.fname.isNotNull() & df.lname.isNotNull()), F.concat(df.fname, df.lname)\n",
    ").otherwise(F.lit('N/A'))\n",
    "\n",
    "# Pick which columns to keep, optionally rename some\n",
    "df = df.select(\n",
    "    'name',\n",
    "    'age',\n",
    "    F.col('dob').alias('date_of_birth'),\n",
    ")\n",
    "\n",
    "# Remove columns\n",
    "df = df.drop('mod_dt', 'mod_username')\n",
    "\n",
    "# Rename a column\n",
    "df = df.withColumnRenamed('dob', 'date_of_birth')\n",
    "\n",
    "# Keep all the columns which also occur in another dataset\n",
    "df = df.select(*(F.col(c) for c in df2.columns))\n",
    "\n",
    "# Batch Rename/Clean Columns\n",
    "for col in df.columns:\n",
    "    df = df.withColumnRenamed(col, col.lower().replace(' ', '_').replace('-', '_'))\n",
    "    \n",
    "# --------------------------------------------------------------------------------------\n",
    "# Casting & Coalescing Null Values & Duplicates\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Cast a column to a different type\n",
    "df = df.withColumn('price', df.price.cast(T.DoubleType()))\n",
    "\n",
    "# Replace all nulls with a specific value\n",
    "df = df.fillna({\n",
    "    'first_name': 'Tom',\n",
    "    'age': 0,\n",
    "})\n",
    "\n",
    "# Take the first value that is not null\n",
    "df = df.withColumn('last_name', F.coalesce(df.last_name, df.surname, F.lit('N/A')))\n",
    "\n",
    "# Drop duplicate rows in a dataset (distinct)\n",
    "df = df.dropDuplicates() # or\n",
    "df = df.distinct()\n",
    "\n",
    "# Drop duplicate rows, but consider only specific columns\n",
    "df = df.dropDuplicates(['name', 'height'])\n",
    "\n",
    "# Replace empty strings with null (leave out subset keyword arg to replace in all columns)\n",
    "df = df.replace({\"\": None}, subset=[\"name\"])\n",
    "\n",
    "# Convert Python/PySpark/NumPy NaN operator to null\n",
    "df = df.replace(float(\"nan\"), None)\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# String Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# String Filters\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Contains - col.contains(string)\n",
    "df = df.filter(df.name.contains('o'))\n",
    "\n",
    "# Starts With - col.startswith(string)\n",
    "df = df.filter(df.name.startswith('Al'))\n",
    "\n",
    "# Ends With - col.endswith(string)\n",
    "df = df.filter(df.name.endswith('ice'))\n",
    "\n",
    "# Is Null - col.isNull()\n",
    "df = df.filter(df.is_adult.isNull())\n",
    "\n",
    "# Is Not Null - col.isNotNull()\n",
    "df = df.filter(df.first_name.isNotNull())\n",
    "\n",
    "# Like - col.like(string_with_sql_wildcards)\n",
    "df = df.filter(df.name.like('Al%'))\n",
    "\n",
    "# Regex Like - col.rlike(regex)\n",
    "df = df.filter(df.name.rlike('[A-Z]*ice$'))\n",
    "\n",
    "# Is In List - col.isin(*cols)\n",
    "df = df.filter(df.name.isin('Bob', 'Mike'))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# String Functions\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Substring - col.substr(startPos, length)\n",
    "df = df.withColumn('short_id', df.id.substr(0, 10))\n",
    "\n",
    "# Trim - F.trim(col)\n",
    "df = df.withColumn('name', F.trim(df.name))\n",
    "\n",
    "# Left Pad - F.lpad(col, len, pad)\n",
    "# Right Pad - F.rpad(col, len, pad)\n",
    "df = df.withColumn('id', F.lpad('id', 4, '0'))\n",
    "\n",
    "# Left Trim - F.ltrim(col)\n",
    "# Right Trim - F.rtrim(col)\n",
    "df = df.withColumn('id', F.ltrim('id'))\n",
    "\n",
    "# Concatenate - F.concat(*cols)\n",
    "df = df.withColumn('full_name', F.concat('fname', F.lit(' '), 'lname'))\n",
    "\n",
    "# Concatenate with Separator/Delimiter - F.concat_ws(delimiter, *cols)\n",
    "df = df.withColumn('full_name', F.concat_ws('-', 'fname', 'lname'))\n",
    "\n",
    "# Regex Replace - F.regexp_replace(str, pattern, replacement)[source]\n",
    "df = df.withColumn('id', F.regexp_replace(id, '0F1(.*)', '1F1-$1'))\n",
    "\n",
    "# Regex Extract - F.regexp_extract(str, pattern, idx)\n",
    "df = df.withColumn('id', F.regexp_extract(id, '[0-9]*', 0))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Number Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Round - F.round(col, scale=0)\n",
    "df = df.withColumn('price', F.round('price', 0))\n",
    "\n",
    "# Floor - F.floor(col)\n",
    "df = df.withColumn('price', F.floor('price'))\n",
    "\n",
    "# Ceiling - F.ceil(col)\n",
    "df = df.withColumn('price', F.ceil('price'))\n",
    "\n",
    "# Absolute Value - F.abs(col)\n",
    "df = df.withColumn('price', F.abs('price'))\n",
    "\n",
    "# X raised to power Y  F.pow(x, y)\n",
    "df = df.withColumn('exponential_growth', F.pow('x', 'y'))\n",
    "\n",
    "# Select smallest value out of multiple columns  F.least(*cols)\n",
    "df = df.withColumn('least', F.least('subtotal', 'total'))\n",
    "\n",
    "# Select largest value out of multiple columns  F.greatest(*cols)\n",
    "df = df.withColumn('greatest', F.greatest('subtotal', 'total'))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Date & Timestamp Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Add a column with the current date\n",
    "df = df.withColumn('current_date', F.current_date())\n",
    "\n",
    "# Convert a string of known format to a date (excludes time information)\n",
    "df = df.withColumn('date_of_birth', F.to_date('date_of_birth', 'yyyy-MM-dd'))\n",
    "\n",
    "# Convert a string of known format to a timestamp (includes time information)\n",
    "df = df.withColumn('time_of_birth', F.to_timestamp('time_of_birth', 'yyyy-MM-dd HH:mm:ss'))\n",
    "\n",
    "# Get year from date:       F.year(col)\n",
    "# Get month from date:      F.month(col)\n",
    "# Get day from date:        F.dayofmonth(col)\n",
    "# Get hour from date:       F.hour(col)\n",
    "# Get minute from date:     F.minute(col)\n",
    "# Get second from date:     F.second(col)\n",
    "df = df.filter(F.year('date_of_birth') == F.lit('2017'))\n",
    "\n",
    "# Add & subtract days\n",
    "df = df.withColumn('three_days_after', F.date_add('date_of_birth', 3))\n",
    "df = df.withColumn('three_days_before', F.date_sub('date_of_birth', 3))\n",
    "\n",
    "# Add & Subtract months\n",
    "df = df.withColumn('next_month', F.add_month('date_of_birth', 1))\n",
    "\n",
    "# Get number of days between two dates\n",
    "df = df.withColumn('days_between', F.datediff('start', 'end'))\n",
    "\n",
    "# Get number of months between two dates\n",
    "df = df.withColumn('months_between', F.months_between('start', 'end'))\n",
    "\n",
    "# Keep only rows where date_of_birth is between 2017-05-10 and 2018-07-21\n",
    "df = df.filter(\n",
    "    (F.col('date_of_birth') >= F.lit('2017-05-10')) &\n",
    "    (F.col('date_of_birth') <= F.lit('2018-07-21'))\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Array Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Column Array - F.array(*cols)\n",
    "df = df.withColumn('full_name', F.array('fname', 'lname'))\n",
    "\n",
    "# Empty Array - F.array(*cols)\n",
    "df = df.withColumn('empty_array_column', F.array([]))\n",
    "\n",
    "# Get element at index  col.getItem(n)\n",
    "df = df.withColumn('first_element', F.col(\"my_array\").getItem(0))\n",
    "\n",
    "# Array Size/Length  F.size(col)\n",
    "df = df.withColumn('array_length', F.size('my_array'))\n",
    "\n",
    "# Flatten Array  F.flatten(col)\n",
    "df = df.withColumn('flattened', F.flatten('my_array'))\n",
    "\n",
    "# Unique/Distinct Elements  F.array_distinct(col)\n",
    "df = df.withColumn('unique_elements', F.array_distinct('my_array'))\n",
    "\n",
    "# Map over & transform array elements  F.transform(col, func: col -> col)\n",
    "df = df.withColumn('elem_ids', F.transform(F.col('my_array'), lambda x: x.getField('id')))\n",
    "\n",
    "# Return a row per array element  F.explode(col)\n",
    "df = df.select(F.explode('my_array'))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Struct Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Make a new Struct column (similar to Python's `dict()`)  F.struct(*cols)\n",
    "df = df.withColumn('my_struct', F.struct(F.col('col_a'), F.col('col_b')))\n",
    "\n",
    "# Get item from struct by key  col.getField(str)\n",
    "df = df.withColumn('col_a', F.col('my_struct').getField('col_a'))\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Aggregation Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Row Count:                F.count()\n",
    "# Sum of Rows in Group:     F.sum(*cols)\n",
    "# Mean of Rows in Group:    F.mean(*cols)\n",
    "# Max of Rows in Group:     F.max(*cols)\n",
    "# Min of Rows in Group:     F.min(*cols)\n",
    "# First Row in Group:       F.alias(*cols)\n",
    "df = df.groupBy('gender').agg(F.max('age').alias('max_age_by_gender'))\n",
    "\n",
    "# Collect a Set of all Rows in Group:       F.collect_set(col)\n",
    "# Collect a List of all Rows in Group:      F.collect_list(col)\n",
    "df = df.groupBy('age').agg(F.collect_set('name').alias('person_names'))\n",
    "\n",
    "# Just take the lastest row for each combination (Window Functions)\n",
    "from pyspark.sql import Window as W\n",
    "\n",
    "window = W.partitionBy(\"first_name\", \"last_name\").orderBy(F.desc(\"date\"))\n",
    "df = df.withColumn(\"row_number\", F.row_number().over(window))\n",
    "df = df.filter(F.col(\"row_number\") == 1)\n",
    "df = df.drop(\"row_number\")\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Advanced Operations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Repartitioning\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Repartition  df.repartition(num_output_partitions)\n",
    "df = df.repartition(1)\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# UDFs (User Defined Functions\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "# Multiply each row's age column by two\n",
    "times_two_udf = F.udf(lambda x: x * 2)\n",
    "df = df.withColumn('age', times_two_udf(df.age))\n",
    "\n",
    "# Randomly choose a value to use as a row's name\n",
    "import random\n",
    "\n",
    "random_name_udf = F.udf(lambda: random.choice(['Bob', 'Tom', 'Amy', 'Jenna']))\n",
    "df = df.withColumn('name', random_name_udf())\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------------------------\n",
    "# Useful Functions / Transformations\n",
    "# --------------------------------------------------------------------------------------\n",
    "\n",
    "def flatten(df: DataFrame, delimiter=\"_\") -> DataFrame:\n",
    "    '''\n",
    "    Flatten nested struct columns in `df` by one level separated by `delimiter`, i.e.:\n",
    "\n",
    "    df = [ {'a': {'b': 1, 'c': 2} } ]\n",
    "    df = flatten(df, '_')\n",
    "    -> [ {'a_b': 1, 'a_c': 2} ]\n",
    "    '''\n",
    "    flat_cols = [name for name, type in df.dtypes if not type.startswith(\"struct\")]\n",
    "    nested_cols = [name for name, type in df.dtypes if type.startswith(\"struct\")]\n",
    "\n",
    "    flat_df = df.select(\n",
    "        flat_cols\n",
    "        + [F.col(nc + \".\" + c).alias(nc + delimiter + c) for nc in nested_cols for c in df.select(nc + \".*\").columns]\n",
    "    )\n",
    "    return flat_df\n",
    "\n",
    "\n",
    "def lookup_and_replace(df1, df2, df1_key, df2_key, df2_value):\n",
    "    '''\n",
    "    Replace every value in `df1`'s `df1_key` column with the corresponding value\n",
    "    `df2_value` from `df2` where `df1_key` matches `df2_key`\n",
    "\n",
    "    df = lookup_and_replace(people, pay_codes, id, pay_code_id, pay_code_desc)\n",
    "    '''\n",
    "    return (\n",
    "        df1\n",
    "        .join(df2[[df2_key, df2_value]], df1[df1_key] == df2[df2_key], 'left')\n",
    "        .withColumn(df1_key, F.coalesce(F.col(df2_value), F.col(df1_key)))\n",
    "        .drop(df2_key)\n",
    "        .drop(df2_value)\n",
    "           )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-Learn Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here are some of the popular modules within scikit-learn (sklearn) grouped by functionality:\n",
    "\n",
    "dir(sklearn)\n",
    "# Data Processing and Preprocessing:\n",
    "    sklearn.preprocessing # Contains functions and classes for data preprocessing and feature scaling.\n",
    "    sklearn.impute #contains functions to input missing values (e.g. IterativeImputer(estimator = RandomForest())\n",
    "\n",
    "# Machine Learning Models:\n",
    "    sklearn.linear_model # Includes linear regression, logistic regression, and other linear models.\n",
    "    sklearn.cluster # Provides clustering algorithms such as K-Means and hierarchical clustering.\n",
    "    sklearn.tree # Contains decision tree and ExtraTree.\n",
    "    sklearn.svm # Support Vector Machines for classification and regression.\n",
    "    sklearn.ensemble # Ensemble methods like AdaBoost, Bagging, and Gradient Boosting, RandomForest.\n",
    "    sklearn.neighbors # Nearest Neighbors algorithms.\n",
    "    sklearn.naive_bayes # Naive Bayes classifiers.\n",
    "    sklearn.discriminant_analysis # Linear and Quadratic Discriminant Analysis.\n",
    "    sklearn.neural_network # Neural network-based models.\n",
    "    sklearn.decomposition # Matrix decomposition techniques like PCA and NMF.\n",
    "    sklearn.kernel_approximation # Approximation techniques for kernel methods.\n",
    "    sklearn.isotonic # Isotonic regression.\n",
    "\n",
    "# Model Evaluation and Selection:\n",
    "    sklearn.model_selection # Tools for model selection, hyperparameter tuning, and cross-validation.(sklearn.model_selection.cross_validate)\n",
    "    sklearn.metrics # Metrics for evaluating model performance (e.g., accuracy, ROC-AUC, etc.).\n",
    "    sklearn.feature_selection #used to select the most relevant features from a dataset for training machine learning models\n",
    "    sklearn.feature_extraction #contains functions for feature extraction from raw data, such as text data, including Bag of Words, CountVectorizer, and TfidfVectorizer\n",
    "\n",
    "# Data Manipulation and Utility:\n",
    "    sklearn.base # Base classes and utility functions.\n",
    "    sklearn.utils # Various utility functions and classes.\n",
    "\n",
    "# Other:\n",
    "    sklearn.pipeline # provides tools for building machine learning pipelines, which allows you to chain together multiple steps.\n",
    "    sklearn.datasets    # datasets\n",
    "    sklearn.exceptions # Custom exceptions.\n",
    "    sklearn.externals # External dependencies.\n",
    "    sklearn.manifold # Dimensionality reduction techniques.\n",
    "    sklearn.logger and logging # Logging utilities.\n",
    "    sklearn.config_context # Context for configuration settings.\n",
    "    sklearn.get_config # Function to get global configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed569b",
   "metadata": {},
   "source": [
    "> How to find any function in SKlearn, Pytorch Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9303e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkgutil\n",
    "import importlib\n",
    "import sklearn  # Replace with your preferred library if needed\n",
    "\n",
    "\n",
    "def find_instances_in_library(search_term, library=sklearn):\n",
    "    \"\"\"\n",
    "    Finds all occurrences of a search term within a specified library.\n",
    "\n",
    "    Parameters:\n",
    "        search_term (str): The term to search for (case insensitive).\n",
    "        library (module): The library/module to search in. Defaults to sklearn.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of matching instances in the format 'module_name.attribute_name'.\n",
    "    \"\"\"\n",
    "    search_term_lower = search_term.lower()\n",
    "    matches = []\n",
    "\n",
    "    for importer, modname, ispkg in pkgutil.walk_packages(path=library.__path__, prefix=library.__name__ + '.'):\n",
    "        try:\n",
    "            module = importlib.import_module(modname)\n",
    "            for attribute_name in dir(module):\n",
    "                if search_term_lower in attribute_name.lower():\n",
    "                    matches.append(f'{modname}.{attribute_name}')\n",
    "        except (ImportError, AttributeError, SystemError, TypeError) as e:\n",
    "            # Suppress errors and skip problematic modules\n",
    "            pass\n",
    "\n",
    "    return matches\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Get user input for the search term\n",
    "    search_term = input(\"Enter the term to search for in sklearn (or another library): \").strip()\n",
    "\n",
    "    # Perform the search\n",
    "    matches = find_instances_in_library(search_term)\n",
    "\n",
    "    # Display results\n",
    "    if matches:\n",
    "        print(f'\\nInstances matching \"{search_term}\" found in:')\n",
    "        for match in matches:\n",
    "            print(f'  - {match}')\n",
    "    else:\n",
    "        print(f'\\nNo instances matching \"{search_term}\" found in the library.')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37017fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e5d7e71",
   "metadata": {},
   "source": [
    "#### OpenCV Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d92031",
   "metadata": {},
   "source": [
    "> os  | zipfile   | PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating System\n",
    "import os\n",
    "\n",
    "list(os.scandir(path))  # Returns an iterator of all the entries in the directory given by path.\n",
    "os.listdir(path='.') # Lists files and directories in a given directory. Useful for accessing image datasets.\n",
    "os.path.join(path, *paths) # Combines paths intelligently; key for building file paths in data processing.\n",
    "os.getcwd() # Gets the current working directory, often used in setting up paths for data input/output.\n",
    "os.chdir(path) # Changes the current working directory, useful in scripts that require relative paths.\n",
    "os.path.exists(path) # Checks if a given path exists, important for verifying data paths or model checkpoint existence.\n",
    "os.makedirs(path, mode=0o777, exist_ok=False) # Creates directories recursively, useful for creating structured directories to store \n",
    "    # processed images or models.\n",
    "os.rename(src, dst) # Renames files, can be used in organizing or re-labeling datasets.\n",
    "os.path.isdir(path) / os.path.isfile(path) # Checks if a path is a directory or a file, respectively. These are crucial in handling \n",
    "    # data files and directories.\n",
    "os.path.getsize(path) # Gets the size of a file, which can be important when handling large datasets or models.\n",
    "os.walk(top, topdown=True, onerror=None, followlinks=False) # Traverses directory trees, essential for processing datasets stored in \n",
    "    # nested directory structures. \n",
    "    for dirpath, dirnames, filenames in os.walk(file_path):\n",
    "        pass \n",
    "os.path.split(path) # Splits a pathname into head and tail components, useful for parsing file paths.\n",
    "os.remove(path) / os.unlink(path) # Removes files, which can be useful for cleaning up temporary files or outputs.\n",
    "\n",
    "\n",
    "=====================================================================================================================================\n",
    "# when working with datasets that have been zipped \n",
    "import zipfile \n",
    "from zipfile import ZipFile, is_zipfile \n",
    "\n",
    "data_path = 'lung-and-colon-cancer-histopathological-images.zip'\n",
    "\n",
    "zipfile.is_zipfile(data_path)    # check if the file is a zip filed.\n",
    "with ZipFile(data_path,'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('The data set has been extracted.')\n",
    "  \n",
    "  \n",
    "===================================================================================================================================\n",
    "# using the PIL library (PIL - Python Imaging Library)  \n",
    "from PIL import Image\n",
    "\n",
    "Image.open(filepath) # Opens and identifies the given image file.\n",
    "Image.save(filename, format) # Saves the image under the given filename and format.\n",
    "Image.resize((width, height)) # Resizes the image to the specified width and height.\n",
    "Image.crop((left, top, right, bottom)) # Crops the image using the given coordinates.\n",
    "Image.rotate(angle) # Rotates the image by the given angle.\n",
    "Image.convert(mode) # Converts the image to the specified mode (e.g., \"RGB\", \"L\").\n",
    "Image.filter(filter) # Applies a specified filter to the image (e.g., ImageFilter.BLUR).\n",
    "Image.transpose(method) # Transposes the image (e.g., flipping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565be77",
   "metadata": {},
   "source": [
    "> opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273243f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import PIL \n",
    "\n",
    "# cv2 image shape (387, 620, 3) \n",
    "# torch tensor shape (3, 387, 620); channel in RGB\n",
    "# transposed_image = np.transpose(image, (1, 2, 0))     or img.permute(1,2,0)\n",
    "\n",
    "# PIL.Image.open(str(img))       to view an image           img = image path\n",
    "# plt.imshow(img)                to view an image            img = np array, expects RGB not BGR\n",
    "# cv2.imread(str(img))           to read an image            img = image path, output = np array, channel format in BGR\n",
    "# T.ToTensor()                   to transform an image       expects an PIL Image or ndarray \n",
    "\n",
    "\n",
    "img = np.zeros(shape=(480, 640)) \n",
    "\n",
    "# Core Operations\n",
    "\n",
    "    img = cv2.imread(img, flag) # Reading images (flag: cv2.IMREAD_COLOR or 1, cv2.IMREAD_GRAYSCALE or 0, cv2.IMREAD_UNCHANGED or -1)\n",
    "        img.shape   # returns a tuple of rows, columns, and channels\n",
    "        img.size    # returns the total number of pixels (row*col*channels)\n",
    "        img.copy()  # copy an image into a new variable. or use (np.copy(img))\n",
    "        np.copy(img)\n",
    "        img.dtype   # returns the image datatype \n",
    "    cv2.imshow(window_name, img)  # Showing Images\n",
    "    cropped_img = img[100:300, 100:300] # Image Cropping\n",
    "    cv2.imwrite(filename, img) # saving an image\n",
    "    cv2.waitKey(0) & 0xFF  # Wait for a key press  \n",
    "    cv2.destroyAllWindows() / cv2.destroyWindow() # Close all OpenCV windows \n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert Color Spaces - Convert to grayscale\n",
    "    resized_img = cv2.resize(img, (width, height))  # Resize to a specific size\n",
    "    cv2.namedWindow('image')    #set the window to be 'image'\n",
    "    b, g, r = cv2.split(img)\n",
    "    b, g, r = img[y, x, 0], img[y, x, 1], img[y, x, 2]\n",
    "    img = cv2.merge((b,g,r)) \n",
    "    cv2.inRange(image, lower_bound, upper_bound)    # used for color segmentation. It filters an image to include only pixels within a specified color range, creating a binary mask\n",
    "    cv2.createTrackbar('trackbar_name', 'window_name', value, count, onChange)  # Create Trackbar\n",
    "    pos = cv2.getTrackbarPos('trackbar_name', 'window_name')    # Get Trackbar Position\n",
    "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])   # calculate histogram for a gray-scale image\n",
    "    hist_blue = cv2.calcHist([color_image], [0], None, [256], [0, 256])     # calculate histogram for the blue channel of a colored image\n",
    "        plt.plot(hist_blue)     # plot the histogram\n",
    "    np.interp(brightness_scale, [0, 100], [1, 4])       # scales  values in range [0-100] into [1-4]\n",
    "    np.clip(a, a_min, a_max, out=None, **kwargs)        # Clip (limit) the values \"a\" in an array into (a_min, a_max).\n",
    "    np.transpose(img, axes=(0,1,2))     # Returns an array with axes transposed (useful for converting from cv2 to pytorch style)\n",
    "\n",
    "# Working with Images\n",
    "    \n",
    "    # Drawing Shapes and Text on Images\n",
    "\n",
    "        cv2.line(img, (start_coordinates), (end_coordinates), (color_in_bgr), line_thickness)                     # Draw Line\n",
    "        cv2.rectangle(img, (top-left corner coordinates), (bottom-right corner coordinates), color, thickness)    # Draw Rectangle\n",
    "        cv2.circle(img, (center_coordinates), (circle_radius), (color_in_bgr), stroke_thickness)     # Draw Circle\n",
    "        cv2.polylines(img, [pts], isClosed, color, thickness)         # Draw Polygon\n",
    "        cv2.putText(img, text, orgin_coordinate, font, font_scale, (color, thickness, line_type))         # Write Text\n",
    "        cvzone.putTextRect(img, text, pos, scale=3, thickness=3, colorT=(255, 255, 255), colorR=(255, 0, 255), font=1,)    #text with rectangle background\n",
    "        cv2.arrowedLine(img, (start_coordinates), (end_coordinates), (color_in_bgr), line_thickness)      # arrowed line \n",
    "                            # thickness = any digit or use [cv2.FILLED or -1] to fill the shape\n",
    "        cv2.fillPoly(img, [np.array([pts])], color=(200, 245, 0)) # pts is a numpy list of array with all the x,y points in the shape ~ pts.reshape((-1, 1, 2))\n",
    "        cv2.fillConvexPoly(mask, points, color)     # Fill Concave polygon using convex hull algorithm\n",
    "        # draw polygon - https://roboflow.github.io/polygonzone/ \n",
    "\n",
    "    # Arithmetic Operations on Images\n",
    "\n",
    "        cv2.add(image1, image2) # Image Addition\n",
    "        cv2.addWeighted(image1, weight1, image2, weight2, gammaValue)    #Image Alpha Blending\n",
    "        cv2.subtract(image1, image2)    #Image Subtraction\n",
    "        cv2.bitwise_and(image1, image2, destination, mask)   # Bitwise And\n",
    "        cv2.bitwise_or(image1, image2, destination, mask)   # Bitwise Or\n",
    "        cv2.bitwise_not(image, destination, mask)   # Bitwise Not\n",
    "        cv2.bitwise_xor(image1, image2, destination, mask)  # Bitwise Xor\n",
    "        \n",
    "    # Morphological Operations on Images:   (Manipulates image shape/structure using kernels; includes erosion, dilation, opening, and closing)\n",
    "\n",
    "        cv2.erode(img, kernel, iterations=1)  # Erosion:      Shrinks bright regions and enlarges dark regions by removing pixels at boundaries.\n",
    "        cv2.dilate(img, kernel, iterations=1) # Dilation:     Expands bright areas and shrinks dark regions by adding pixels to boundaries.\n",
    "        cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=1)   #Opening:       Removes small bright spots (noise) by erosion followed by dilation\n",
    "        cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=1)   # Closing:      Fills small dark spots and small holes by dilation followed by erosion.\n",
    "        cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel) #Morphological Gradient:    Highlights edges and boundaries by subtracting an eroded image from a dilated image\n",
    "        cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)   # Top Hat:      Isolates small elements and details brighter than their surroundings.\n",
    "        cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel) # Black Hat:    Isolates dark elements and details in bright surroundings.\n",
    "\n",
    "    # Smoothing Images: (Reduces noise and details by applying filters, like Gaussian blur or median blur, to soften images.)\n",
    "\n",
    "        cv2.filter2D(img, depth, kernel) # Convolve an Image:                                 Applies a custom kernel for convolution, allowing for diverse linear filtering effects.     \n",
    "        cv2.blur(img, shapeOfTheKernel)   # Averaging Filtering:                              Smoothens image using a simple average of neighboring pixels within the kernel.\n",
    "        cv2.getGaussianKernel(ksize, sigma[, ktype])    # Create Gaussian Kernel:               Generates a Gaussian kernel, used for more advanced blurring techniques.\n",
    "        cv2.GaussianBlur(img, shapeOfTheKernel, sigmaX )  # Gaussian Blur:                    Reduces noise using a Gaussian filter, effective for Gaussian noise.\n",
    "        cv2.medianBlur(img, kernel size)  # Median Blur:                                      useful for dealing with salt and pepper noise). Removes salt-and-pepper noise, replacing each pixel with the median of neighboring pixels\n",
    "        cv2.bilateralFilter(img, diameter, sigmaColor, sigmaSpace)    # Bilateral Blur:       Preserves edges while reducing noise, using a non-linear, edge-preserving approach.\n",
    "\n",
    "    # Geometric Transformations on Image:  \n",
    "\n",
    "        res = cv2.resize(img,(2width, 2height), interpolation)  # Scaling (scaling types: cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LINEAR)\n",
    "        cv2.warpAffine(img, M, (width, height))         # affine transformation to an image (where M is the transformation matrix)\n",
    "                M = np.float32([[1, 0, new_width], [0, 1, new_height]])     #  Translation\n",
    "                M = cv2.getRotationMatrix2D(center, angle, scale)   # Rotation\n",
    "                M = cv2.getAffineTransform(pts1, pts2)      # Affine Transformation\n",
    "                M = cv2.getPerspectiveTransform(src, dst)      # Perspective Transformation\n",
    "        cv2.warpPerspective(img, M, dsize)\n",
    "        \n",
    "    # Image Thresholding:  (apply to GRAYSCALE) (Converts images to binary using a set threshold, useful for separating foreground from background)\n",
    "\n",
    "        retval, thresholded_image = cv2.threshold(img, thresh, maxvalue, thresholdingTechnique)    # Simple Threshold\n",
    "        retval, thresholded_image = cv2.adaptiveThreshold(img, maxvalue, adaptivemethod, thresholdingTechnique, blocksize, constant)   # Adaptive Threshold\n",
    "        retval, bw_image = cv2.threshold(img, thresh, maxvalue, thresholdingTechnique)    # Otsu Thresholding\n",
    "        \n",
    "            # Thresholding technique\n",
    "            cv2.THRESH_BINARY   # If pixel intensity is greater than the set threshold, the value is set to 255, else set to 0\n",
    "            cv2.THRESH_BINARY_INV   # Inverted or Opposite case of cv2.THRESH_BINARY\n",
    "            cv2.THRESH_TRUNC        # If the pixel intensity value > threshold, it is truncated to the threshold. set pixel values == threshold\n",
    "            cv2.THRESH_TOZERO       # Pixel intensity is set to 0, for all the pixels intensity, less than the threshold value\n",
    "            cv2.THRESH_TOZERO_INV   # Inverted or Opposite case of cv2.THRESH_TOZERO\n",
    "            cv2.THRESH_OTSU\n",
    "            cv2.THRESH_TRIANGLE \n",
    "            \n",
    "            # Adaptive Threshold Methods\n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C      # Calculates the threshold as the mean of neighboring area minus a constant. Useful for varying lighting conditions.\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C  # Uses a Gaussian-weighted sum of the neighborhood values for thresholding, ideal for finer, localized changes.\n",
    "                                                    \n",
    "    # Edge/Line and Object Detection (Features):   (use GRAYSCALE - Identifies sharp changes in intensity to detect object boundaries and lines in images.)\n",
    "\n",
    "        cv2.Canny(img, thresh_lower, thresh_upper, aperture_size, L2Gradient)   # Canny Edge Detection \n",
    "        cv2.HoughLines(img, rho, theta, threshold)    # Houghline Method for Line Detection using the standard Hough transform   (rho: Distance resolution in pixels., theta: Angle resolution in radians., threshold: Minimum votes to consider a line)\n",
    "        cv2.HoughLinesP(img, rho, theta, threshold, ..)    # Finds line segments in a binary image using the probabilistic Hough transform\n",
    "        cv2.HoughCircles(img, cv2.HOUGH_GRADIENT, 1, 20, param1 = 50, param2 = 30, minRadius = 1, maxRadius = 40) #Houghline Method for Circle Detection\n",
    "        cv2.cornerHarris(img, blockSize, kSize, k, borderType)    # Harris Corner Method for Corner Detection\n",
    "        cv2.goodFeaturesToTrack(img, max_corner, quality_level, min_distance)   # Shi-Tomasi Method for Corner Detection\n",
    "        cv2.drawKeypoints(img, key_points, output_image, colour, flag)  # Keypoints Detection\n",
    "        \n",
    "        # Image gradients:      Computes image gradients to emphasize texture and edges\n",
    "        cv2.Sobel(img, ddepth, dx, dy, dst=None, ksize=None, scale=None)       # Sobel Operator, useful for edge detection in both X and Y directions.         (ddepth: Depth of the destination image. dx and dy: Order of the derivative x and y)\n",
    "        cv2.Laplacian(img, ddepth=-1, dst=None, ksize=None)          # Calculates the Laplacian of the image, highlighting regions of rapid intensity change\n",
    "        cv2.Scharr(img, ddepth=-1, dx, dy)    # A variation of Sobel, more sensitive to edges than the standard Sobel operator.\n",
    "\n",
    "        #  Contour Detection:           Used to detect and extract contours from binary images. Essential for shape analysis, object detection, and recognition tasks.\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)  #  Finds contours in a binary image.    Parameters: source image, contour retrieval mode, contour approximation method.\n",
    "        contours, Area, BoundingBox, Center = cvzone.Utils.findContours(img, imgPre, minArea=1000, maxArea=inf,)    # Finds Contours in an image.\n",
    "        cv2.drawContours(img, contours, contourIdx, color, thickness) # Draw contour      contourIdx = -1\n",
    "        area = cv2.contourArea(contours) # contour area\n",
    "        x, y, w, h = cv2.boundingRect(contours)  # Bounding Rectangle: \n",
    "        length = cv2.arcLength(contours, True)   # Contour Perimeter:\n",
    "        approx = cv2.approxPolyDP(contours, epsilon, True)   # Approximating Contours\n",
    "        \n",
    "        # Template Matching:        used for finding the location of a template image within a larger image. It's mainly used in applications where you need to find specific objects or features in an image\n",
    "        result = cv2.matchTemplate(img, template, method)     # Perform Template Matching:    \n",
    "        h, w = template.shape \n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)  # locate the match \n",
    "        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]: \n",
    "            top_left = min_loc \n",
    "        else: \n",
    "            top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "        cv2.rectangle(img, top_left, bottom_right, color, thickness)  # draw a rectangle around the match \n",
    "\n",
    "        \n",
    "    # Image Pyramids:   (Creates a multi-scale representation of an image, useful for scaling images up or down)\n",
    "\n",
    "        cv2.pyrDown(layer)  # Lower Gaussian Pyramid \n",
    "        cv2.pyrUp(layer)    # Higher Gaussian Pyramid\n",
    "        \n",
    "    # Changing the Colorspace of Images\n",
    "\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR to RGB: - often used when switching from cv2 to pytorch, PIL or Sklearn.\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # BGR to Grayscale: \n",
    "        cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # RGB to Grayscale: \n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2HSV)  # BGR to HSV (H-Hue , S-Saturation, V-Value represents intensity)\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2LAB)  # BGR to Lab (L-Lightness. A-color from Green to Magenta. B colors from Blue to Yellow) \n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb) # BGR to YCrCb Color (Y-Luminance or Luma component, Cb and Cr are Chroma components)\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)    # Track Blue (color) Object\n",
    "            lower_blue = np.array([110,50,50])\n",
    "            upper_blue = np.array([130,255,255])\n",
    "            cv2.inRange(hsv, lower_blue, upper_blue)                                \n",
    "                                        \n",
    "\n",
    "# Working With Videos\n",
    "\n",
    "    cv2.VideoCapture('file_name.mp4')       # Playing a Video \n",
    "    PIL.Image.open(filename, mode)  # Create a Video from Multiple Images\n",
    "    cap = cv2.VideoCapture(File_path)     # Extracting Images from Video\n",
    "        cap.read()\n",
    "        cap.imwrite('filename.png', img)    # write the image to a file \n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(\"file_name.mp4\")  # Replace with your video file path\n",
    "    cap = cv2.VideoCapture(0)  # '0' is the default camera. Use '1', '2', etc. for other cameras\n",
    "        ret, frame = cap.read()  # Read Frames from the Video - 'ret' is a boolean indicating success, 'frame' is the current frame\n",
    "        cv2.flip(frame, 1)  # Flip the frame horizontally (1), vertical (0), both (-1) \n",
    "        cv2.imshow('Frame Title', frame)  # Display the current frame\n",
    "        cap.release()  # Release the video file or capturing device\n",
    "        if cap.isOpened():  # Check if VideoCapture Object is Opened\n",
    "        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # Get Video Properties - width\n",
    "        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # Get Video Properties - height\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)             # Get Video Properties - fps\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)      # set Video Properties\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)     # set Video Properties\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Define the codec (like XVID, MP4V, etc.)\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))  # Filename, codec, FPS, resolution\n",
    "            out.write(frame)  # Write a frame to the output video\n",
    "            out.release()  # Release the VideoWriter object\n",
    "    cv2.namedWindow('image')    #set the window to be 'image'\n",
    "\n",
    "            # Video Properties\n",
    "            3 - cv2.CAP_PROP_FRAME_WIDTH    #  Set the width of the video frames\n",
    "            4 - cv2.CAP_PROP_FRAME_HEIGHT   #  Set the height of the video frames\n",
    "            5 - cv2.CAP_PROP_FPS            # Set the frame rate of the video capture.\n",
    "            10 - cv2.CAP_PROP_BRIGHTNESS    # Adjust the brightness of the video (if the camera supports this setting).\n",
    "            11 - cv2.CAP_PROP_CONTRAST      # Adjust the contrast of the video (if the camera supports this setting).\n",
    "            12 - cv2.CAP_PROP_SATURATION    # Adjust the saturation of the video (if the camera supports this setting).\n",
    "            15 - cv2.CAP_PROP_EXPOSURE      # Adjust the exposure of the camera (if the camera supports this setting).\n",
    "            39 - cv2.CAP_PROP_AUTOFOCUS     # Enable or disable autofocus, if supported (0 or 1).\n",
    "            \n",
    "            # Mouse Events\n",
    "            cv2.EVENT_LBUTTONDOWN   # Triggered when the left mouse button is pressed\n",
    "            cv2.EVENT_LBUTTONUP     # Occurs when the left mouse button is released. Often used in conjunction with EVENT_LBUTTONDOWN for actions like drag-and-drop.\n",
    "            cv2.EVENT_RBUTTONDOWN   # Triggered when the right mouse button is pressed\n",
    "            cv2.EVENT_MOUSEMOVE     # Occurs when the mouse is moved over the window. This is frequently used for tracking mouse movement, real-time drawing, or interactive applications\n",
    "            cv2.EVENT_LBUTTONDBLCLK # Triggered on a double-click of the left mouse button. Commonly used for more complex interactions like object selection or opening properties\n",
    "            cv2.EVENT_MOUSEMOVE and flags == cv2.EVENT_FLAG_LBUTTON     #if the mouse is moved while the left button is pressed down.\n",
    "            \n",
    "# Machine Learning and Deep Learning with OpenCV\n",
    "\n",
    "    net = cv2.dnn.readNet(model, config, framework) \n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "\n",
    "    knn = cv2.ml.KNearest_create()\n",
    "    knn.train(trainData, responses)\n",
    "\n",
    "    svm = cv2.ml.SVM_create()\n",
    "    svm.train(data, cv2.ml.ROW_SAMPLE, labels)\n",
    "\n",
    "# Camera Calibration and 3D Reconstruction\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (width, height)) # Finding Chessboard Corners:\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)  # Calibrate Camera:\n",
    "\n",
    "\n",
    "# Object Detection and Tracking\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')     # Face Detection: Using Haar Cascades. download from opencv github \n",
    "    faces = face_cascade.detectMultiScale(image, scaleFactor, minNeighbors)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)       # Feature matching \n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    \n",
    "    sift = cv2.SIFT_create()            # Advanced feature detection \n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "  \n",
    "    \n",
    "    \n",
    "# Others \n",
    "    cv2.setMouseCallback('image', mouse_callback)   # Mouse Callback Function: For capturing mouse events\n",
    "            def mouse_callback(event, x, y, flags, param):\n",
    "                    \"\"\"\n",
    "                    Mouse event callback.\n",
    "                    Parameters: event (int), x (int), y (int), flags (int), param (int)\n",
    "                    \"\"\"\n",
    "                    # Left mouse button down event\n",
    "                    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "                        # Draw a blue rectangle around the mouse click position\n",
    "                        cv2.rectangle(img, (x, y), (x + 20, y + 20), (255, 0, 0), cv2.FILLED)\n",
    "                        cv2.imshow('image', img)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834b568",
   "metadata": {},
   "source": [
    "> CVZone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cvzone\n",
    "pip install mediapipe\n",
    "\n",
    "import cvzone \n",
    "\n",
    "# Others:\n",
    "    cvzone.cornerRect(img, bbox, l=30, t=5, rt=1, colorR=(255, 0, 255), colorC=(0, 255, 0))\n",
    "    cvzone.downloadImageFromUrl(url, keepTransparency=True)         # download image from url \n",
    "    cvzone.findContours(img, imgPre, minArea=1000, maxArea=inf,)    # Finds Contours in an image.\n",
    "    cvzone.overlayPNG(img, imgPNG, pos=[-30, 50])                   # overlay image\n",
    "    cvzone.putTextRect(img, text, pos, scale=3, thickness=3, colorT=(255, 255, 255), colorR=(255, 0, 255), font=1,)    #text with rectangle background\n",
    "    cvzone.rotateImage(img, angle, scale=1, keepSize=False)         # rotate an image 60 deg.\n",
    "    cvzone.stackImages(imgList, cols, scale)                        # Stack Images together to display in a single window\n",
    "    \n",
    "# Face Detection: Easily detect faces in an image or video stream.\n",
    "    from cvzone.FaceDetectionModule import FaceDetector\n",
    "    detector = FaceDetector(minDetectionCon=0.5, modelSelection=0)\n",
    "            img, bboxs = detector.findFaces(img, draw=False)        # # bbox contains 'id', 'bbox', 'score', 'center'\n",
    "\n",
    "# Hand Tracking: Track hand landmarks in real-time.\n",
    "    from cvzone.HandTrackingModule import HandDetector\n",
    "    detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "            hands, img = detector.findHands(img, draw = False)      # Finds hands in a BGR image    (hand1[\"lmList\"]  # List of 21 landmarks for the first hand)\n",
    "            length, info, img = detector.findDistance()         # Find the distance between two landmarks input should be (x1,y1) (x2,y2)\n",
    "            fingers = fingersUp()   # Finds how many fingers are open and returns in a list \n",
    "\n",
    "# Pose Estimation: Identify human body positions.\n",
    "    from cvzone.PoseModule import PoseDetector\n",
    "    detector = PoseDetector(staticMode=False, modelComplexity=1, smoothLandmarks=True, enableSegmentation=False, smoothSegmentation=True,\n",
    "                        detectionCon=0.5,\n",
    "                        trackCon=0.5)\n",
    "            img = detector.findPose(img)            # Find the human pose in the frame\n",
    "            lmList, bboxInfo = detector.findPosition(img, draw=True, bboxWithHands=False)   # Find the landmarks, bounding box, and center of the body in the frame\n",
    "            length, img, info = detector.findDistance(lmList[11][0:2], lmList[15][0:2], img, color, scale)     # Calculate the distance between landmarks\n",
    "            angle, img = detector.findAngle(lmList[11][0:2], lmList[13][0:2], lmList[15][0:2], img, color, scale)   # Calculate the angle between landmarks\n",
    "            isCloseAngle50 = detector.angleCheck(myAngle=angle, targetAngle=50, offset=10)      # Check if the angle is close to 50 degrees with an offset of 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f67b9b",
   "metadata": {},
   "source": [
    "> Image Annotation on Colab and Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78801ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function that loads an image before adding it to the widget\n",
    "\n",
    "import base64\n",
    "\n",
    "def encode_image(filepath):\n",
    "    with open(filepath, 'rb') as f:\n",
    "        image_bytes = f.read()\n",
    "    encoded = str(base64.b64encode(image_bytes), 'utf-8')\n",
    "    return \"data:image/jpg;base64,\"+encoded\n",
    "     \n",
    "\n",
    "file_image = r\"C:\\Users\\pault\\Documents\\5. Projects\\1. Bird Species Classification\\data\\train\\ABYSSINIAN GROUND HORNBILL\\014.jpg\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8afffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IS_COLAB = False\n",
    "\n",
    "if IS_COLAB:\n",
    "    from google.colab import output\n",
    "    output.enable_custom_widget_manager()\n",
    "\n",
    "from jupyter_bbox_widget import BBoxWidget\n",
    "\n",
    "widget = BBoxWidget()\n",
    "widget.image = encode_image(file_image)\n",
    "widget\n",
    "     \n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d925aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "widget.bboxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335f04b",
   "metadata": {},
   "source": [
    "> Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn             # neural networks\n",
    "import torch.optim as optim       # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import torch.nn.functional as F   # layers, activations and more\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms   # composable transforms\n",
    "from torchvision import datasets, models, transforms     # vision datasets, architectures & transforms\n",
    "import torch.tensor as tensor \n",
    "\n",
    "\n",
    "## torch\n",
    "  # **Tensor Creation**\n",
    "    torch.arange(start, end, step)  # Tensor containing numbers from start to end with a step.\n",
    "    torch.zeros(size)  # Tensor filled with zeros.\n",
    "      torch.zeros_like(tensor)  # Tensor of zeros with the same shape as the input tensor.\n",
    "    torch.ones(size)  # Tensor filled with ones.\n",
    "    torch.tensor(data, dtype = None, device = None, requires_grad = False)  # Create a tensor from provided data.\n",
    "      tensor.ndim # Number of dimensions.\n",
    "      tensor.shape\n",
    "      tensor.size()\n",
    "      tensor.dtype\n",
    "      tensor.device\n",
    "    torch.randn(size)  # Tensor with random numbers from a normal distribution (mean=0, std=1).\n",
    "    torch.rand(size)  # Tensor with random numbers from a uniform distribution [0, 1).\n",
    "    torch.randint(low, high, size)  # Tensor with random integers in the range [low, high).\n",
    "    torch.arange(start, end, step)  # Tensor containing numbers from start to end with a step.\n",
    "    torch.linspace(start, end, steps)  # Tensor with `steps` equally spaced points between start and end.\n",
    "    torch.full(size, fill_value)  # Tensor filled with a specified value.\n",
    "    torch.empty(size)  # Tensor with uninitialized values.\n",
    "    torch.eye(n, m=None)  # Identity matrix (n x n if `m` not specified).\n",
    "    torch.diag(input)  # Tensor with input as diagonal elements.\n",
    "    torch.zeros_like(tensor)  # Tensor of zeros with the same shape as the input tensor.\n",
    "    torch.ones_like(tensor)  # Tensor of ones with the same shape as the input tensor.\n",
    "    torch.full_like(tensor, fill_value)  # Tensor filled with a specified value, same shape as input.\n",
    "    torch.empty_like(tensor)  # Uninitialized tensor, same shape as input.\n",
    "    torch.manual_seed(seed)  # Set the seed for generating random numbers.\n",
    "    \n",
    "    \n",
    "  # **Operations**\n",
    "    torch.matmul(input, other)  # Matrix multiplication of `input` and `other`.\n",
    "    torch.cat(tensors, dim=0)  # Concatenates a sequence of tensors along a specified dimension `dim`.\n",
    "    torch.stack(tensors, dim=0)  # Stacks tensors along a new dimension `dim`.\n",
    "    torch.flatten(input, start_dim=0, end_dim=-1)  # Flattens a contiguous range of dims into a 1D tensor.\n",
    "    torch.permute(input, dims)  # Permutes the dimensions of a tensor.\n",
    "    torch.argmax(input, dim=None)  # Returns the indices of the maximum values along a dimension.\n",
    "    torch.argmin(input, dim=None)  # Returns the indices of the minimum values along a dimension.\n",
    "    torch.reshape(input, shape)  # Reshapes a tensor to the specified shape.\n",
    "    tensor.view(*shape)  # Reshapes a tensor.\n",
    "    torch.transpose(input, dim0, dim1)  # Swaps two dimensions of a tensor.\n",
    "    torch.t(input)  # Computes the transpose of a 2D tensor.\n",
    "    torch.unsqueeze(input, dim)  # Inserts a dimension of 1 along a specified dimension `dim`.\n",
    "    torch.squeeze(input, dim=None)  # Removes dimensions of size 1 from the shape of a tensor.\n",
    "    torch.min(input) | torch.max(input)  # Returns the minimum/maximum value of the input tensor.\n",
    "    torch.clamp(input, min, max)  # Clips (limits) the values of a tensor.\n",
    "    torch.where(condition, x, y)  # Returns elements from `x` or `y` based on `condition`.\n",
    "    torch.eq(input, other)  # Computes element-wise equality.\n",
    "    torch.isclose(input, other, rtol=1e-05, atol=1e-08, equal_nan=False)  # Compares two tensors element-wise.\n",
    "\n",
    "    torch.chunk(tensors, chunks, dim=0)  # Splits tensor into chunks along dimension `dim`.\n",
    "    torch.split(tensors, split_size_or_sections, dim=0)  # Splits tensor into chunks along dimension `dim`.\n",
    "    torch.unique(input, sorted=False, return_inverse=False, return_counts=False, dim=None)  # Returns unique elements of the input tensor.\n",
    "    torch.topk(input, k, dim=-1, largest=True, sorted=True)  # Returns the `k` largest elements of the given tensor.\n",
    "    torch.argsort(input, dim=-1, descending=False)  # Returns the indices that would sort the tensor.\n",
    "    torch.type(dtype)  # Casts a tensor to a new data type.\n",
    "    torch.mul(input, other)  # Element-wise multiplication of two tensors.\n",
    "    torch.abs(input)  # Computes the element-wise absolute value of the input tensor.\n",
    "    torch.pow(input, exponent)  # Raises input elements to the power of `exponent`.\n",
    "    torch.exp(input)  # Computes the element-wise exponential of the input tensor.\n",
    "    torch.log(input)  # Computes the element-wise natural logarithm of the input tensor.\n",
    "\n",
    "    torch.stack(tensors, dim=0)  # Stacks tensors along a new dimension `dim`.\n",
    "    torch.vstack(tensors)  # Stacks tensors vertically (row-wise).\n",
    "    torch.hstack(tensors)  # Stacks tensors horizontally (column-wise).\n",
    "    torch.dot(input, other)  # Computes the dot product of two 1D tensors.\n",
    "    torch.transpose(input, dim0, dim1)  # Swaps two dimensions of a tensor. (Duplicate in PyTorch)\n",
    "    torch.diagonal(input, offset=0, dim1=0, dim2=1)  # Extracts diagonal elements of a tensor.\n",
    "    torch.mean(input, dim=None)  # Computes the mean of the tensors elements along the specified dimension.\n",
    "    torch.prod(input, dim=None)  # Computes the product of the elements of the input tensor.\n",
    "    torch.cumsum(input, dim)  # Returns the cumulative sum of elements along a dimension.\n",
    "    torch.cumprod(input, dim)  # Returns the cumulative product of elements along a dimension.\n",
    "\n",
    "\n",
    "  # **Conversion**\n",
    "    torch.from_numpy(ndarray)  # Converts a numpy array to a tensor (shares memory).\n",
    "    tensor.numpy()  # Converts a tensor to a numpy array (shares memory).\n",
    "\n",
    "    \n",
    "  # Device Management**   torch.device` - Device interface (CPU/GPU).\n",
    "    torch.device(type, index=None)  # Device interface for CPU/GPU (e.g., `torch.device('cuda:0')`).\n",
    "    torch.cuda.is_available()  # Checks if CUDA (GPU) is available.\n",
    "    tensor.to(device)  # Moves a tensor to the specified device (e.g., `tensor.to('cuda')`).\n",
    "      device = 'cuda' if torch.cuda.is_available() else 'cpu'  # Set device to GPU if available.\n",
    "      tensor.cpu()  # Moves a tensor to CPU memory.\n",
    "      tensor.cuda()  # Moves a tensor to CUDA (GPU) memory.\n",
    "\n",
    "\n",
    "## torch.nn\n",
    "  # Layers**\n",
    "    nn.Linear(in_features, out_features, bias=True)  # Fully connected layer from `in_features` to `out_features`.\n",
    "    nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True)  # 2D convolutional layer.\n",
    "    nn.MaxPool2d(kernel_size, stride=None, padding=0)  # 2D max pooling layer with `kernel_size`.\n",
    "    nn.Dropout(p=0.5)  # Randomly zeroes elements with probability `p` for regularization.\n",
    "    nn.BatchNorm2d(num_features)  # Batch normalization over a 4D input.\n",
    "    \n",
    "  # Activation Functions**\n",
    "    nn.ReLU(inplace=False)  # Applies the Rectified Linear Unit (ReLU) function.\n",
    "    nn.Sigmoid()  # Applies the sigmoid activation function. for binary classification\n",
    "    nn.Tanh()  # Applies the hyperbolic tangent function.\n",
    "    # Additional Activation Functions:\n",
    "    nn.ELU(alpha=1.0, inplace=False)  # Exponential Linear Unit.\n",
    "    nn.SELU(inplace=False)  # Scaled Exponential Linear Unit.\n",
    "    nn.PReLU(num_parameters=1, init=0.25)  # Parametric ReLU.\n",
    "    nn.LeakyReLU(negative_slope=0.01, inplace=False)  # Leaky ReLU.\n",
    "    nn.GELU()  # Gaussian Error Linear Unit.\n",
    "    nn.Softplus(beta=1, threshold=20)  # Smooth approximation of ReLU.\n",
    "    nn.Softmax(dim=None)  # Normalizes the input tensor along dimension `dim`. for multi-class classification\n",
    "\n",
    "      \n",
    "  # Loss Functions**\n",
    "    nn.CrossEntropyLoss(weight=None, ignore_index=-100, reduction='mean')  # Loss for classification tasks.\n",
    "    nn.MSELoss(reduction='mean')  # Mean Squared Error loss for regression tasks.\n",
    "    nn.BCELoss(weight=None, reduction='mean')  # Binary Cross-Entropy Loss.\n",
    "    # Additional Loss Functions:\n",
    "    nn.L1Loss(reduction='mean')  # Mean absolute error loss.\n",
    "    nn.BCEWithLogitsLoss(pos_weight=None, reduction='mean')  # Combines sigmoid with binary cross-entropy.\n",
    "    nn.KLDivLoss(reduction='batchmean')  # Kullback-Leibler divergence loss.\n",
    "    nn.NLLLoss(weight=None, ignore_index=-100, reduction='mean')  # Negative log-likelihood loss.\n",
    "    nn.CTCLoss(blank=0, reduction='mean', zero_infinity=False)  # Connectionist Temporal Classification loss.\n",
    "\n",
    "      \n",
    "  # Utilities**\n",
    "    nn.Module()  # Base class for all neural network models.\n",
    "    nn.Sequential(*modules)  # A sequential container for modules.\n",
    "    nn.Parameter(tensor, requires_grad=True)  # A kind of tensor that is considered a model parameter.\n",
    "    nn.init.xavier_uniform_(tensor)  # Initializes weights with Xavier uniform distribution.\n",
    "    nn.init.kaiming_uniform_(tensor, mode='fan_in', nonlinearity='leaky_relu')  # Initializes weights with He initialization.\n",
    "\n",
    "\n",
    "## torch.optim\n",
    "  # Optimizers**\n",
    "    optim.SGD(params, lr, momentum=0, weight_decay=0)  # Stochastic Gradient Descent optimizer.\n",
    "    optim.Adam(params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0)  # Adam optimizer.\n",
    "    optim.Adagrad(params, lr=1e-2, lr_decay=0, weight_decay=0)  # Adagrad optimizer.\n",
    "    optim.RMSprop(params, lr=1e-2, alpha=0.99, eps=1e-8, weight_decay=0, momentum=0)  # RMSprop optimizer.\n",
    "    optimizer.step()  # Update model weights.\n",
    "\n",
    "\n",
    "## Learning Rate Scheduling\n",
    "    scheduler = optim.X(optimizer,...)      # create lr scheduler\n",
    "    scheduler.step()                        # update lr after optimizer updates weights\n",
    "    optim.lr_scheduler.X                    # lr scheduler classes\n",
    "        optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)  # Apply a custom learning rate function.\n",
    "        optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1)  # Decay LR every `step_size` epochs.\n",
    "        optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1)  # Decay LR at specified milestones.\n",
    "        optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)  # Exponentially decay LR.\n",
    "        optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0)  # Cosine annealing schedule.\n",
    "        optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)  # Reduce LR on plateau.\n",
    "        optim.lr_scheduler.CyclicLR(optimizer, base_lr, max_lr)  # Cyclic learning rate schedule.\n",
    "        optim.lr_scheduler.OneCycleLR(optimizer, max_lr, total_steps)  # One-cycle learning rate policy.\n",
    "        optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0, T_mult=1)  # Cosine annealing with restarts.\n",
    "\n",
    "## Datasets and Data Utilities \n",
    "    torch.utils.data.random_split(dataset, lengths)  # Randomly split a dataset into non-overlapping new datasets.\n",
    "    torch.utils.data.Subset(dataset, indices)  # Subset of a dataset at specified indices.\n",
    "    torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, drop_last=False)  # Load batches of data.\n",
    "    torch.utils.data.TensorDataset(X, y)  # Create a labeled dataset from tensors.\n",
    "    torch.utils.data.DatasetFolder(root, loader, extensions)  # Load datasets with folder structure.\n",
    "    torch.utils.data.Dataset()  # Abstract class for custom datasets.\n",
    "    torchvision.datasets.MNIST(root, train=True, download=True, transform=None)  # Load MNIST dataset.\n",
    "    \n",
    "    \n",
    "## torch.nn.functional\n",
    "  # Activation Functions**\n",
    "    F.relu(input, inplace=False)  # Rectified Linear Unit activation function.\n",
    "    F.sigmoid(input)  # Sigmoid activation function. for binary classification\n",
    "    F.softmax(input, dim=None)  # Softmax activation function. for multi-class classification. \n",
    "    F.tanh(input)  # Hyperbolic tangent activation function.\n",
    "\n",
    "    \n",
    "  # Loss Functions**\n",
    "    F.cross_entropy(input, target, weight=None, ignore_index=-100, reduction='mean')  # Classification loss.\n",
    "    F.mse_loss(input, target, reduction='mean')  # Mean Squared Error loss.\n",
    "    F.binary_cross_entropy(input, target, weight=None, reduction='mean')  # Binary Cross-Entropy Loss.\n",
    "\n",
    "    \n",
    "  # Convolutional Operations**\n",
    "    F.conv2d(input, weight, bias=None, stride=1, padding=0, dilation=1, groups=1)  # Apply 2D convolution.\n",
    "    F.max_pool2d(input, kernel_size, stride=None, padding=0, dilation=1)  # Apply 2D max pooling.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## torchvision\n",
    "  # Datasets**\n",
    "    torchvision.datasets.MNIST(root, train=True, download=True, transform=None)  # Dataset of handwritten digits.\n",
    "    torchvision.datasets.CIFAR10(root, train=True, download=True, transform=None)  # 32x32 images in 10 classes.\n",
    "    torchvision.datasets.ImageFolder(root, transform=None)  # Generic data loader for folder-arranged images.\n",
    "    torchvision.datasets.FakeData(size, transform=None)  # Fake dataset for debugging and testing.\n",
    "    \n",
    "  # Models**\n",
    "    torchvision.models.resnet50(pretrained=True)  # Pretrained ResNet-50 model.\n",
    "    torchvision.models.vgg16(pretrained=True)  # Pretrained VGG-16 model.\n",
    "    torchvision.models.alexnet(pretrained=True)  # Pretrained AlexNet model.\n",
    "\n",
    "  # Transforms**\n",
    "    torchvision.transforms.Resize(size)  # Resize an image to the specified size.\n",
    "    torchvision.transforms.CenterCrop(size)  # Crop the center of an image to the specified size.\n",
    "    torchvision.transforms.ToTensor()  # Convert a PIL image or numpy array to tensor.\n",
    "    torchvision.transforms.ToPILImage()  # Convert a tensor to a PIL image.\n",
    "    torchvision.transforms.Normalize(mean, std)  # Normalize a tensor image with mean and std.\n",
    "    torchvision.transforms.RandomCrop(size)  # Crop the image randomly.\n",
    "    torchvision.transforms.RandomHorizontalFlip(p=0.5)  # Randomly flip the image horizontally.\n",
    "    torchvision.transforms.RandomVerticalFlip(p=0.5)  # Randomly flip the image vertically.\n",
    "    torchvision.transforms.Compose(transforms)  # Compose several transforms together.\n",
    "\n",
    "\n",
    "\n",
    "## torchvision.ops\n",
    "  # Operations**\n",
    "    torchvision.ops.boxes # Operations on bounding boxes.\n",
    "    torchvision.ops.nms  # Non-maximum suppression. removes overlapping bounding boxes.\n",
    "    torchvision.ops.batched_nms # Performs NMS on a batch of boxes from different images, grouped by image ids.\n",
    "    torchvision.ops.roi_pool  # Region of Interest pooling, used to extract fixed-size features from variable-sized RoIs.\n",
    "    torchvision.ops.roi_align  # Region of Interest alignment, more accurate than RoI pooling\n",
    "    torchvision.ops.box_iou  # Compute intersection over union of two sets of boxes\n",
    "    torchvision.ops.sigmoid_focal_loss  # Sigmoid focal loss for dense object detection.\n",
    "    torchvision.ops.box_area  # Compute the area of a set of boxes.\n",
    "    torchvision.ops.box_convert  # Convert boxes between different formats.\n",
    "    torchvision.ops.generalized_box_iou # Compute generalized intersection over union of two sets of boxes.\n",
    "    torchvision.ops.clip_boxes_to_image # Clip boxes to an image with a specific size.\n",
    "    torchvision.ops.remove_small_boxes  # Remove boxes that are smaller than a specified size. \n",
    "    torchvision.ops.masks_to_boxes  # Convert masks to bounding boxes.\n",
    "    \n",
    "## torchvision.utils\n",
    "  # Utilities**\n",
    "    torchvision.utils.make_grid  # Make a grid of images.\n",
    "    torchvision.utils.save_image  # Save a tensor image to disk.\n",
    "    torchvision.utils.draw_bounding_boxes  # Draw bounding boxes on an image.\n",
    "    torchvision.utils.draw_segmentation_masks # Draw segmentation masks on an image.\n",
    "    torchvision.utils.draw_keypoints # Draw keypoints on an image.\n",
    "\n",
    "\n",
    "# Initialization Methods: These are used to initialize the weights of the neural network layers.\n",
    "    nn.init.uniform_: #Uniform initialization of weights.\n",
    "    nn.init.normal_: #Normal initialization of weights.\n",
    "    nn.init.xavier_uniform_: #Xavier uniform initialization of weights.\n",
    "    nn.init.xavier_normal_: #Xavier normal initialization of weights.\n",
    "    nn.init.kaiming_uniform_: #Kaiming uniform initialization of weights.\n",
    "    nn.init.kaiming_normal_: #Kaiming normal initialization of weights.\n",
    "    nn.init.constant_: #Constant initialization of weights.\n",
    "    nn.init.zeros_: #Zero initialization of weights.\n",
    "    nn.init.ones_: #One initialization of weights.\n",
    "    nn.init.eye_: #Identity initialization of weights.\n",
    "    nn.init.orthogonal_: #Orthogonal initialization of weights.\n",
    "    nn.init.sparse_: #Sparse initialization of weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aeeadd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch Model Functions Cheat Sheet ---\n",
    "\n",
    "# Model Functions\n",
    "# model.parameters() - Returns an iterator over all the parameters (weights and biases) in the model\n",
    "for param in model.parameters():  # Iterate through all model parameters (weights and biases)\n",
    "    print(param.size())  # Print the size of each parameter\n",
    "\n",
    "# model.named_parameters() - Returns a generator over both the parameter name and the parameter itself\n",
    "for name, param in model.named_parameters():  # Iterate through all model parameters with their names\n",
    "    print(f\"{name}: {param.size()}\")  # Print the name and size of each parameter\n",
    "\n",
    "# model.state_dict() - Returns a dictionary containing the model's state (layers and parameters)\n",
    "state_dict = model.state_dict()  # Get the state dictionary (model's layers and parameters)\n",
    "\n",
    "# model.load_state_dict(state_dict) - Loads a state dictionary into the model (used for loading model weights)\n",
    "model.load_state_dict(torch.load('path_to_state_dict.pth'))  # Load model weights from a checkpoint\n",
    "\n",
    "# model.eval() - Sets the model to evaluation mode (useful during inference)\n",
    "model.eval()  # Set the model to evaluation mode (disables dropout, batchnorm updates)\n",
    "\n",
    "# model.train() - Sets the model to training mode (useful during training)\n",
    "model.train()  # Set the model to training mode (enables dropout, batchnorm updates)\n",
    "\n",
    "# model.to(device) - Moves the model's parameters and buffers to the specified device (e.g., GPU)\n",
    "model.to(device)  # Move model to GPU/CPU\n",
    "\n",
    "# model.zero_grad() - Sets gradients of all model parameters to zero (commonly used before .backward())\n",
    "model.zero_grad()  # Zero out all the gradients before the backward pass\n",
    "\n",
    "# model.children() - Returns an iterator over immediate child modules of the model\n",
    "for child in model.children():  # Iterate through immediate child modules (layers)\n",
    "    print(child)  # Print each child module\n",
    "\n",
    "# model.modules() - Returns an iterator over all modules in the model (recursively)\n",
    "for module in model.modules():  # Iterate through all modules in the model (recursively)\n",
    "    print(module)  # Print each module\n",
    "\n",
    "# --- Helper Functions ---\n",
    "# help(nn.Module) - Displays the documentation for nn.Module (or any other module like nn.Linear)\n",
    "help(nn.Module)  # Get documentation for nn.Module class (or any other class)\n",
    "\n",
    "# Freeze a few layers (e.g., first six layers):\n",
    "for name, param in resnet.named_parameters():  # Iterate through model parameters\n",
    "    if \"layer\" in name and int(name.split(\".\")[1]) < 6:  # Freeze layers up to layer 6\n",
    "        param.requires_grad = False  # Freeze the parameter (no gradients during backprop)\n",
    "\n",
    "# Print the number of trainable parameters:\n",
    "num_trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)  # Count trainable parameters\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")  # Print the count of trainable parameters\n",
    "\n",
    "# Freeze specific layer parameters (e.g., freeze weights and bias of `fc1`):\n",
    "model.fc1.weight.requires_grad = False  # Freeze weights of fc1 layer\n",
    "model.fc1.bias.requires_grad = False  # Freeze bias of fc1 layer\n",
    "\n",
    "# Freeze all layers except input:\n",
    "for param in model.named_parameters():  # Iterate through model parameters\n",
    "    if 'input' not in param[0]:  # Skip input layers\n",
    "        param[1].requires_grad = False  # Freeze all parameters except input\n",
    "\n",
    "# Unfreeze all layers:\n",
    "for param in model.parameters():  # Iterate through all model parameters\n",
    "    param.requires_grad = True  # Unfreeze all layers\n",
    "\n",
    "# --- Weights & Biases ---\n",
    "# Access weights and biases of each layer:\n",
    "model.layers.input_bn.weight  # Access the weights of the input batch norm layer\n",
    "model.layers.input_bn.bias    # Access the biases of the input batch norm layer\n",
    "\n",
    "# Print weights of trainable parameters:\n",
    "for name, param in model.named_parameters():  # Iterate through trainable parameters\n",
    "    if param.requires_grad:  # Only print trainable parameters\n",
    "        print(name, param.data)  # Print parameter name and data (weights)\n",
    "\n",
    "# Get the shape of weights for each module:\n",
    "for module in model.modules():  # Iterate through all modules in the model\n",
    "    if hasattr(module, 'weight'):  # Check if the module has a weight attribute\n",
    "        print(module.weight.shape)  # Print the shape of the module's weights\n",
    "\n",
    "# --- State Dict & Saving/Loading Models ---\n",
    "# Save model state dictionary:\n",
    "state_dict = model.state_dict()  # Get the model's state dictionary (parameters & buffers)\n",
    "torch.save(state_dict, 'model_checkpoint.pth')  # Save state dictionary to a file\n",
    "\n",
    "# Load model state dictionary from checkpoint:\n",
    "state_dict = torch.load('model_checkpoint.pth')  # Load state dictionary from a file\n",
    "model.load_state_dict(state_dict)  # Load state dictionary into the model\n",
    "\n",
    "# --- Model Architecture Visualization ---\n",
    "# Count total parameters in the model:\n",
    "total_params = sum(p.numel() for p in model.parameters())  # Count total parameters\n",
    "print(f\"Total parameters: {total_params}\")  # Print the total number of parameters\n",
    "\n",
    "# Visualize model architecture using `torchsummary`:\n",
    "from torchsummary import summary  # Import summary module\n",
    "summary(model, input_size=(28, 28))  # Print a summary of the model architecture\n",
    "\n",
    "# Visualize weight initialization for layers:\n",
    "layers = [name.split('.')[-1] for name, _ in model.named_modules()]  # Get the last part of the layer names (excluding the prefix)\n",
    "for name in layers:  \n",
    "    # Check if the layer has weights (not all layers will have them)\n",
    "    layer = dict(model.named_modules())[name]  # Access the layer by its name\n",
    "    if hasattr(layer, 'weight'):  # Check if the layer has a 'weight' attribute\n",
    "        weight_i = layer.weight.detach()  # Detach the weights from the computation graph\n",
    "        plt.hist(weight_i.cpu().numpy().flatten(), bins=30, edgecolor='black')  # Plot histogram of weight distribution\n",
    "        plt.title(f'Weight Initialization - {name}')  # Title for the plot\n",
    "        plt.xlabel('Weight Values')  # Label for x-axis\n",
    "        plt.ylabel('Frequency')  # Label for y-axis\n",
    "        plt.show()  # Show the plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9c545",
   "metadata": {},
   "source": [
    "# Understanding the Size of Data as It Passes Through Layers\n",
    "\n",
    "When you're building a neural network, especially a Convolutional Neural Network (CNN), it's crucial to understand how the dimensions of the data change as it passes through different layers (such as convolutions, batch normalization, pooling, etc.). Here's a breakdown of how to calculate the size of the data (tensor shape) as it goes through different types of layers:\n",
    "\n",
    "## 1. Convolutional Layer (Conv2d)\n",
    "\n",
    "Convolution layers operate on 2D spatial data, and the size of the output depends on the input size, the filter (kernel) size, the stride, and the padding.\n",
    "\n",
    "### Formula:\n",
    "For a convolutional layer, the output size in height and width is calculated using the following formula:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left( \\frac{\\text{Input size} + 2 \\times \\text{Padding} - \\text{Kernel size}}{\\text{Stride}} \\right) + 1\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **Input size**: Height or width of the input\n",
    "- **Padding**: The amount of padding added to the input (defaults to 0 if not specified)\n",
    "- **Kernel size**: Size of the convolution filter (height or width)\n",
    "- **Stride**: Step size of the filter moving across the input\n",
    "\n",
    "### Example:\n",
    "Suppose the input image size is `(1, 8, 8)`, meaning 1 channel with 8x8 spatial dimensions. Now, let's pass it through a convolution layer with a kernel size of 3, stride of 2, and padding of 1.\n",
    "\n",
    "Using the formula for both height and width:\n",
    "\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left( \\frac{8 + 2 \\times 1 - 3}{2} \\right) + 1 = \\left( \\frac{8}{2} \\right) + 1 = 4 + 1 = 5\n",
    "$$\n",
    "\n",
    "So, the output size will be `(1, 5, 5)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Batch Normalization (BatchNorm2d)\n",
    "\n",
    "Batch normalization normalizes the input to the layer (scaling and shifting) but does not change the spatial dimensions of the input. It only normalizes the values.\n",
    "\n",
    "### Formula:\n",
    "Output size = Input size\n",
    "\n",
    "\n",
    "So, after a batch normalization layer, the size of the tensor remains the same as the input size.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Dropout Layer\n",
    "\n",
    "The dropout layer randomly sets some fraction of the inputs to zero during training to prevent overfitting. This layer also does not change the size of the tensorit only modifies the values.\n",
    "\n",
    "### Formula:\n",
    "Output size = Input size\n",
    "\n",
    "\n",
    "The only difference is that some values are randomly set to 0 during training, but the shape remains the same.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Max Pooling (MaxPool2d)\n",
    "\n",
    "Max pooling reduces the spatial size of the input by downsampling the feature map, usually by taking the maximum value in a fixed-size window.\n",
    "\n",
    "### Formula:\n",
    "For max pooling, the output size is calculated similarly to the convolutional layer:\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left( \\frac{\\text{Input size} - \\text{Kernel size}}{\\text{Stride}} \\right) + 1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "Where:\n",
    "- **Input size**: Height or width of the input\n",
    "- **Kernel size**: Size of the pooling window (usually 2x2 or 3x3)\n",
    "- **Stride**: Step size the pooling window moves over the input (usually 2)\n",
    "\n",
    "### Example:\n",
    "Suppose the input to the pooling layer is `(1, 8, 8)`, and the pooling window is `2x2` with a stride of 2. Then the output size will be:\n",
    "\n",
    "$$\n",
    "\\text{Output size} = \\left( \\frac{8 - 2}{2} \\right) + 1 = \\left( \\frac{6}{2} \\right) + 1 = 3 + 1 = 4\n",
    "$$\n",
    "\n",
    "So, the output will be `(1, 4, 4)`.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Fully Connected (Linear) Layer\n",
    "\n",
    "Before feeding data into a fully connected (linear) layer, we need to flatten the multi-dimensional tensor into a 1D vector. The output size for the fully connected layer depends on the number of input features.\n",
    "\n",
    "### How to Flatten:\n",
    "To flatten a tensor of size `(batch_size, channels, height, width)`, we multiply the channels, height, and width:\n",
    "\n",
    "Flattened size = channels  height  width\n",
    "\n",
    "\n",
    "So, if the input tensor to the fully connected layer is `(32, 256, 4, 4)`, where 32 is the batch size, 256 is the number of channels, and `4x4` is the spatial dimension, the flattened size will be:\n",
    "\n",
    "\n",
    "$$\n",
    "256 \\times 4 \\times 4 = 4096\n",
    "$$\n",
    "\n",
    "\n",
    "This will be the input size to the fully connected layer.\n",
    "\n",
    "### How to Calculate the Size Before the Linear Layer:\n",
    "\n",
    "To determine the size of the tensor just before entering the linear layer, follow these steps:\n",
    "\n",
    "1. **Start with the input size**: This is the original size of your image, e.g., `(batch_size, channels, height, width)`.\n",
    "2. **Apply each convolution**: For each convolutional layer, apply the formula above for each dimension (height and width) to compute the output size.\n",
    "3. **Apply batch normalization and dropout**: These layers dont change the spatial dimensions, so just pass the tensor forward without modifying the size.\n",
    "4. **Apply pooling layers**: Use the formula for pooling layers to compute the new spatial dimensions.\n",
    "5. **Flatten the tensor**: After all the convolution and pooling layers, flatten the tensor to a 1D vector (for each image in the batch).\n",
    "6. **Input size for the linear layer**: This is the size of the tensor after flattening. The linear layer will take this flattened vector as input.\n",
    "\n",
    "### Example Calculation for a CNN\n",
    "Suppose you have an image input of size `(1, 8, 8)`, and you pass it through a series of layers:\n",
    "\n",
    "#### Convolution Layer 1:\n",
    "- **Input size**: `(1, 8, 8)`\n",
    "- **Kernel size**: 3, **Stride**: 2, **Padding**: 1\n",
    "- **Output size**:\n",
    "\n",
    "$$\n",
    "\\left( \\frac{8 + 2 \\times 1 - 3}{2} \\right) + 1 = \\frac{8}{2} + 1 = 4 + 1 = 5\n",
    "$$\n",
    "\n",
    "So, the output size is `(64, 4, 4)`.\n",
    "\n",
    "#### Convolution Layer 2:\n",
    "- **Input size**: `(64, 4, 4)`\n",
    "- **Kernel size**: 3, **Stride**: 2, **Padding**: 1\n",
    "- **Output size**:\n",
    "\n",
    "$$\n",
    "\\left( \\frac{4 + 2 \\times 1 - 3}{2} \\right) + 1 = \\frac{4}{2} + 1 = 2 + 1 = 3\n",
    "$$\n",
    "\n",
    "So, the output size is `(128, 2, 2)`.\n",
    "\n",
    "#### Convolution Layer 3:\n",
    "- **Input size**: `(128, 2, 2)`\n",
    "- **Kernel size**: 3, **Stride**: 2, **Padding**: 1\n",
    "- **Output size**:\n",
    "\n",
    "$$\n",
    "\\left( \\frac{2 + 2 \\times 1 - 3}{2} \\right) + 1 = \\frac{2}{2} + 1 = 1 + 1 = 2\n",
    "$$\n",
    "\n",
    "So, the output size is `(256, 1, 1)`.\n",
    "\n",
    "#### Flattening:\n",
    "- **Flatten the tensor**: \n",
    "\n",
    "$$\n",
    "256 \\times 1 \\times 1 = 256\n",
    "$$\n",
    "\n",
    "#### Linear Layer:\n",
    "- The input to the linear layer is a tensor of size `(batch_size, 256)`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Recap of Key Formulas:\n",
    "\n",
    "- **Convolution**:\n",
    "  \n",
    "  $$ \\text{Output size} = \\left( \\frac{\\text{Input size} + 2 \\times \\text{Padding} - \\text{Kernel size}}{\\text{Stride}} \\right) + 1 $$\n",
    "\n",
    "- **Max Pooling**:\n",
    "  \n",
    "  $$ \\text{Output size} = \\left( \\frac{\\text{Input size} - \\text{Kernel size}}{\\text{Stride}} \\right) + 1 $$\n",
    "\n",
    "- **Batch Normalization / Dropout**: No change in size\n",
    "\n",
    "- **Flattening**: Multiply the dimensions `(channels  height  width)` to get the size that goes into the fully connected layer.\n",
    "\n",
    "This process will help you compute the size at any point in the network. Let me know if you'd like more clarification or further examples!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b657c096",
   "metadata": {},
   "source": [
    "> Pytorch Object Detection Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3732f86e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "31f91a04",
   "metadata": {},
   "source": [
    "> Pytorch Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b54ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Classification Data\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import PIL\n",
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "class Dataset (torch.utils.data.Dataset):\n",
    "    def __init__(self, image_path, transforms):\n",
    "        self.image_path = image_path\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.data_classes = []\n",
    "        self.data_images = []\n",
    "        self.data_labels = []\n",
    "        for dirpath, subdir, filenames in os.walk(image_path):\n",
    "            for file_dir in subdir:\n",
    "                self.data_classes.append(file_dir)\n",
    "            for img in filenames:\n",
    "                # print(img)\n",
    "                image_path = os.path.join(dirpath, img)\n",
    "                self.data_images.append(image_path)\n",
    "                label_class = dirpath.split(os.sep)[-1]\n",
    "                self.data_labels.append(self.data_classes.index(label_class))\n",
    "               \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.data_images[idx]\n",
    "        image = PIL.Image.open(image_path).convert('RGB') \n",
    "        label = self.data_labels[idx]\n",
    "        \n",
    "        if self.transforms:\n",
    "            image = self.transforms(image)\n",
    "        else:\n",
    "            image = T.ToTensor()(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "\n",
    "# Load Object Detection data\n",
    "\n",
    "class ObjectDetectionDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with all the images and labels.\n",
    "            split (string): One of 'train', 'test', or 'valid' to specify the dataset split.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "        self.root_dir = os.path.join(root_dir, split)\n",
    "        self.transform = transform\n",
    "        self.images_dir = os.path.join(self.root_dir, 'images')\n",
    "        self.labels_dir = os.path.join(self.root_dir, 'labels')\n",
    "        self.image_files = [f for f in os.listdir(self.images_dir) if os.path.isfile(os.path.join(self.images_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.images_dir, self.image_files[idx])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label_name = os.path.splitext(self.image_files[idx])[0] + '.txt'\n",
    "        label_path = os.path.join(self.labels_dir, label_name)\n",
    "        boxes = self.parse_labels(label_path)\n",
    "\n",
    "        sample = {'image': image, 'boxes': boxes} \n",
    "\n",
    "        return sample\n",
    "\n",
    "    def parse_labels(self, label_path):\n",
    "        \"\"\"\n",
    "        Parse the label file and return the target format expected by the model.\n",
    "        The label file contains lines in the format: class x_center y_center width height.\n",
    "        \"\"\"\n",
    "        boxes = []\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                class_label, x_center, y_center, width, height = map(float, line.split())\n",
    "                boxes.append([class_label, x_center, y_center, width, height])\n",
    "        return torch.tensor(boxes)\n",
    "\n",
    "# Usage example\n",
    "dataset = ObjectDetectionDataset(root_dir='/path/to/your/data', split='train')\n",
    "sample = dataset[0]  # Get the first sample\n",
    "print(sample)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246134b",
   "metadata": {},
   "source": [
    "> Pytorch Image Models (TIMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e569b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from timm.utils import model_parameters\n",
    "\n",
    "# List all available models\n",
    "all_models = timm.list_models()\n",
    "\n",
    "# List models pre-trained on ImageNet\n",
    "pretrained_models = timm.list_models(pretrained=True)\n",
    "all_densenet_models = timm.list_models('*densenet*')    # search for model architectures using Wildcard. this will list all densenet models\n",
    "\n",
    "# Create a model with pretrained weights\n",
    "model = timm.create_model('resnet50', pretrained=True)\n",
    "\n",
    "# Create a model with a specific number of output classes (e.g., 10)\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(optimizer, t_initial=len(train_loader)*num_epochs)\n",
    "\n",
    "# Counting the number of parameters\n",
    "num_params = model_parameters(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572ca46",
   "metadata": {},
   "source": [
    "> HuggingFace Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e497328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core CV tasks in Hugging Face:\n",
    "    # Image classification\n",
    "    # Image segmentation\n",
    "    # (Zero-shot) object detection\n",
    "    # Video classification\n",
    "    # Depth estimation\n",
    "    # Image-to-image synthesis\n",
    "    # Unconditional image generation\n",
    "    # Zero-shot image classification\n",
    "    \n",
    "# Tasks that lie at the intersection of vision and language:\n",
    "    # Image-to-text (image captioning, OCR)\n",
    "    # Text-to-image\n",
    "    # Document question-answering\n",
    "    # Visual question-answering\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Inference with Hugging Face Transformers\n",
    "#----------------------------------------------------------------------\n",
    "# Load a pre-trained model\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "\n",
    "# Load the Fine-Tuned Model\n",
    "\n",
    "# Specify the path to your fine-tuned model\n",
    "fine_tuned_model_path = \"./models\"  # Update this path if different\n",
    "\n",
    "# Load the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(fine_tuned_model_path)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(fine_tuned_model_path)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Function to create the pipeline with optional API token\n",
    "def create_summarization_pipeline(model, tokenizer, use_api=False, api_token=None):\n",
    "    if use_api and api_token:\n",
    "        # Example placeholder for API token usage\n",
    "        # This depends on the specific API you're integrating with\n",
    "        # For Hugging Face API, you might use the `use_auth_token` parameter\n",
    "        summarizer = pipeline(\n",
    "            \"summarization\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            use_auth_token=api_token\n",
    "        )\n",
    "    else:\n",
    "        summarizer = pipeline(\n",
    "            \"summarization\", \n",
    "            model=model, \n",
    "            tokenizer=tokenizer, \n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "    return summarizer\n",
    "\n",
    "# Create the pipeline\n",
    "api_token = os.getenv(\"HUGGINGFACE_API_TOKEN\")  # Ensure your API token is set as an environment variable\n",
    "summarizer = create_summarization_pipeline(model, tokenizer, use_api=True, api_token=api_token)\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# Perform summarization\n",
    "summary = summarizer(text, max_length=150, min_length=40, do_sample=False)\n",
    "\n",
    "print(\"Original Abstract:\\n\", text)\n",
    "print(\"\\nSummarized Abstract:\\n\", summary[0]['summary_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face Transformers Cheatsheet\n",
    "\n",
    "## **Installation and Environment Setup**\n",
    "!pip install transformers datasets accelerate evaluate bitsandbytes peft  # Additional libraries for advanced usage\n",
    "# transformers: Core HF library\n",
    "# datasets: For loading and processing datasets\n",
    "# accelerate: For distributed & efficient training\n",
    "# evaluate: For evaluation metrics\n",
    "# bitsandbytes: For model quantization and memory optimization\n",
    "# peft: Parameter-Efficient Fine-Tuning (e.g., LoRA)\n",
    "\n",
    "## **Core Modules**\n",
    "from transformers import AutoModel, AutoTokenizer, pipeline                                 # Core components\n",
    "from transformers import AutoModelForSequenceClassification                                 # Model for text classification tasks\n",
    "from transformers import AutoModelForCausalLM, GenerationConfig                             # Model for text generation\n",
    "from transformers import AutoModelForTokenClassification                                    # Model for token classification\n",
    "from transformers import AutoModelForQuestionAnswering, QuestionAnsweringPipeline           # Model for question answering\n",
    "from transformers import AutoModelForSeq2SeqLM, SummarizationPipeline                       # Model for text summarization\n",
    "from transformers import AutoModelForImageClassification                                    # Model for image classification \n",
    "from transformers import TranslationPipeline                                                # Translation pipeline\n",
    "from transformers import AutoModelForImageClassification, ImageClassificationPipeline      # Image classification pipeline\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor, AutomaticSpeechRecognitionPipeline # ASR pipeline\n",
    "from transformers import DataCollatorForLanguageModeling   # Data collator for language modeling tasks\n",
    "\n",
    "from transformers import TrainingArguments, Trainer         # Fine-tuning utilities\n",
    "from transformers import convert_graph_to_onnx              # Convert model to ONNX format\n",
    "from transformers import AutoConfig                         # Model configuration\n",
    "from datasets import load_dataset                           # Datasets library for loading data\n",
    "from accelerate import Accelerator\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "\n",
    "## **Configuration and Model Initialization**\n",
    "config = AutoConfig.from_pretrained(\"bert-base-uncased\")          # Load model configuration\n",
    "config.output_attentions = True                                   # Enable attention outputs\n",
    "config.gradient_checkpointing = True                              # Enable gradient checkpointing for memory savings\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    config=config\n",
    ") # Initialize model with custom config (e.g. enabling attention outputs, gradient checkpointing)\n",
    "\n",
    "\n",
    "## **Tokenization**\n",
    "# Tokenizer loading and usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")  # Load tokenizer for BERT\n",
    "tokens = tokenizer.tokenize(\"Hello, world!\")  # Tokenize input text\n",
    "ids = tokenizer.encode(\"Hello, world!\")       # Encode text into token IDs\n",
    "decoded = tokenizer.decode(ids)               # Decode token IDs back to text\n",
    "\n",
    "# Tokenization with additional options\n",
    "tokenized = tokenizer(\"Hello, Hugging Face!\",                  # Input text\n",
    "                      padding=\"max_length\",                   # Add padding to max length\n",
    "                      truncation=True,                        # Truncate to max length\n",
    "                      return_tensors=\"pt\")                    # Return PyTorch tensors\n",
    "input_ids = tokenized[\"input_ids\"]                            # Extract input IDs\n",
    "attention_mask = tokenized[\"attention_mask\"]                  # Extract attention mask\n",
    "\n",
    "# Special tokens\n",
    "cls_token = tokenizer.cls_token          # CLS token for BERT models\n",
    "sep_token = tokenizer.sep_token          # SEP token for separating sentences\n",
    "pad_token = tokenizer.pad_token          # Padding token\n",
    "\n",
    "## **Pipeline API**\n",
    "# Load a pipeline for a specific task\n",
    "sentiment = pipeline(\"sentiment-analysis\")  # Sentiment analysis pipeline\n",
    "qa = pipeline(\"question-answering\")         # Question answering pipeline\n",
    "ner = pipeline(\"ner\")                       # Named entity recognition pipeline\n",
    "summarizer = pipeline(\"summarization\")      # Summarization pipeline\n",
    "translator = pipeline(\"translation_en_to_fr\")  # Translation pipeline (English to French)\n",
    "\n",
    "# Using pipelines\n",
    "sentiment(\"I love Hugging Face!\")  # Perform sentiment analysis\n",
    "qa({\"question\": \"What is Hugging Face?\", \n",
    "    \"context\": \"Hugging Face is an open-source AI community.\"})  # Answer question\n",
    "\n",
    "## **Model Loading**\n",
    "# Load pretrained models\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")               # Generic model\n",
    "classification_model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\")  # For classification tasks\n",
    "causal_lm_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")       # For text generation tasks\n",
    "\n",
    "# Saving and reloading models\n",
    "model.save_pretrained(\"path/to/dir\")  # Save model to directory\n",
    "tokenizer.save_pretrained(\"path/to/dir\")  # Save tokenizer\n",
    "model = AutoModel.from_pretrained(\"path/to/dir\")  # Reload saved model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"path/to/dir\")  # Reload saved tokenizer\n",
    "\n",
    "## **Model Inference**\n",
    "# Perform inference with a classification model\n",
    "inputs = tokenizer(\"This is amazing!\", return_tensors=\"pt\")  # Prepare input\n",
    "outputs = classification_model(**inputs)                    # Run model on input\n",
    "logits = outputs.logits                                      # Extract logits\n",
    "predictions = logits.argmax(dim=-1)                         # Get predicted class\n",
    "\n",
    "# Text generation with GPT-2\n",
    "input_text = \"Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")  # Encode input text\n",
    "generated_ids = causal_lm_model.generate(input_ids, max_length=50, num_beams=5)  # Generate text\n",
    "output_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)  # Decode generated text\n",
    "\n",
    "## **Fine-Tuning**\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",      # Directory for saving results\n",
    "    evaluation_strategy=\"epoch\", # Evaluate at the end of each epoch\n",
    "    learning_rate=2e-5,          # Learning rate\n",
    "    per_device_train_batch_size=8,  # Batch size per device\n",
    "    num_train_epochs=3           # Number of training epochs\n",
    ")\n",
    "\n",
    "# Define a Trainer instance\n",
    "trainer = Trainer(\n",
    "    model=classification_model,     # Model to fine-tune\n",
    "    args=training_args,             # Training arguments\n",
    "    train_dataset=train_dataset,    # Training dataset\n",
    "    eval_dataset=val_dataset        # Validation dataset\n",
    ")\n",
    "\n",
    "# Start fine-tuning\n",
    "trainer.train()  # Train the model\n",
    "trainer.evaluate()  # Evaluate the model\n",
    "\n",
    "## **Utility Functions**\n",
    "# View model configuration\n",
    "model.config  # Access model configuration\n",
    "model.num_parameters()  # Get number of parameters in the model\n",
    "\n",
    "# Convert model to ONNX format\n",
    "from transformers.convert_graph_to_onnx import convert\n",
    "convert(\"path/to/model\", \"output/model.onnx\", framework=\"pt\")  # Convert to ONNX\n",
    "\n",
    "## **Advanced Features**\n",
    "# Mixed precision training\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",   # Directory for saving results\n",
    "    fp16=True                 # Enable mixed precision training\n",
    ")\n",
    "\n",
    "# Using multiple GPUs\n",
    "model = model.to(\"cuda:0\")  # Move model to GPU 0\n",
    "inputs = inputs.to(\"cuda:0\")  # Move inputs to GPU 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8950269",
   "metadata": {},
   "source": [
    "> HuggingFace Hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3ac3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pip install huggingface_hub\n",
    "!huggingface-cli login       # Login to Hugging Face account. then copy and paste the API token\n",
    "\n",
    "# For files larger than 5gig\n",
    "git lfs install\n",
    "huggingface-cli lfs-enable-largefiles .\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Downloading Models from Huggingface hub Client Library\n",
    "#----------------------------------------------------------------------\n",
    "from huggingface_hub import hf_hub_download\n",
    "import joblib\n",
    "from llama_cpp import Llama\n",
    "\n",
    "REPO_ID = \"TheBloke/\n",
    "-2-13B-chat-GGML\"\n",
    "FILENAME = \"llama-2-13B-chat.ggmlv3.q5_1.bin\"\n",
    "\n",
    "model_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
    "\n",
    "llm = Llama(model_path)\n",
    "prompt = \"Explain the theory of relativity in simple terms.\"\n",
    "output = llm(prompt, max_tokens=128)\n",
    "print(output[\"choices\"][0][\"text\"])\n",
    "#----------------------------------------------------------------------\n",
    "\n",
    "# Downloading Models from Huggingface hub using Git\n",
    "git lfs install\n",
    "!git clone https://huggingface.co/YOUR_USERNAME/YOUR_REPO_ID\n",
    "\n",
    "# Pushing model to your own account\n",
    "model.push_to_hub(\"my-awesome-model\")\n",
    "\n",
    "# Pushing your tokenizer\n",
    "tokenizer.push_to_hub(\"my-awesome-model\")\n",
    "\n",
    "# Pushing all things after training\n",
    "trainer.push_to_hub()\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Huggingface Hub Inference Client\n",
    "#----------------------------------------------------------------------\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(\n",
    "    \"cardiffnlp/twitter-roberta-base-sentiment-latest\",\n",
    "    token=\"hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx\",\n",
    ")\n",
    "\n",
    "client.text_classification(\"Today is a great day\")\n",
    "\n",
    "\n",
    "#others\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "client = InferenceClient(api_key=\"hf_***\")\n",
    "\n",
    "messages = \"\\\"Can you please let us know more details about your \\\"\"\n",
    "\n",
    "stream = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-11B-Vision-Instruct\", \n",
    "\tmessages=messages, \n",
    "\tmax_tokens=500,\n",
    "\tstream=True\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].delta.content, end=\"\")\n",
    "\n",
    "#----------------------------------------------------------------------\n",
    "# Documentation\n",
    "#----------------------------------------------------------------------\n",
    "https://huggingface.co/docs/api-inference/en/parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb73c54",
   "metadata": {},
   "source": [
    "> Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d745b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running Large Language Models (LLMs) Locally on Any Hardware\n",
    "\n",
    "    # Software tools: Tools: C++ toolchain, CMake, Ninja, Git, Python 3.13+.\n",
    "    # .gguf stands for GGML Unified Format.  Optimized for efficient inference on any hardware. \n",
    "\n",
    "# Building llama.cpp\n",
    "    # Install Dependencies\n",
    "        # Windows (Using MSYS):\n",
    "        pacman -S git mingw-w64-ucrt-x86_64-cmake mingw-w64-ucrt-x86_64-ninja mingw-w64-ucrt-x86_64-vulkan-devel mingw-w64-ucrt-x86_64-shaderc\n",
    "        \n",
    "        # Windows (Using winget):\n",
    "        winget install cmake git.git ninja-build.ninja python.python.3.13\n",
    "        \n",
    "        # Ubuntu/Debian:\n",
    "        sudo apt-get update\n",
    "        sudo apt-get install git cmake ninja-build python3 python3-pip\n",
    "        sudo apt-get install libvulkan-dev # Install Vulkan SDK if using Vulkan backend\n",
    "        \n",
    "        \n",
    "    # Clone and Build llama.cpp\n",
    "    git clone https://github.com/ggerganov/llama.cpp.git\n",
    "    cd llama.cpp\n",
    "    git submodule update --init --recursive\n",
    "    cmake -S . -B build -G Ninja -DCMAKE_BUILD_TYPE=Release -DLLAMA_BUILD_TESTS=OFF -DLLAMA_BUILD_EXAMPLES=ON -DLLAMA_BUILD_SERVER=ON # Generate build files\n",
    "    cmake --build build --config Release -j$(nproc) # Build the project\n",
    "    cmake --install build --config Release  # Install binaries\n",
    "\n",
    "\n",
    "# Getting a Model\n",
    "    GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/HuggingFaceTB/SmolLM2-1.7B-Instruct      #or \"set GIT_LFS_SKIP_SMUDGE=1\" if using cmd.exe\n",
    "    cd SmolLM2-1.7B-Instruct\n",
    "    # Download model.safetensors manually from HuggingFace\n",
    "\n",
    "\n",
    "# Converting HuggingFace Model to GGUF\n",
    "    # Setup Python Virtual Environment\n",
    "    python -m venv ~/llama-cpp-venv\n",
    "    source ~/llama-cpp-venv/bin/activate  # Linux/MSYS\n",
    "    pip install --upgrade pip wheel setuptools\n",
    "    pip install --upgrade -r llama.cpp/requirements/requirements-convert_hf_to_gguf.txt # Install dependencies\n",
    "    python llama.cpp/convert_hf_to_gguf.py --help   # Check available options\n",
    "\n",
    "\n",
    "    # Convert HuggingFace model to GGUF\n",
    "    python llama.cpp/convert_hf_to_gguf.py SmolLM2-1.7B-Instruct --outfile ./SmolLM2.gguf\n",
    "\n",
    "\n",
    "# Quantizing the Model (Q8_0, Q4_4, Q2_6, Q1_7) - i.e 8 bits, 4 bits, 2 bits.\n",
    "    # Quantize the model\n",
    "    llama-quantize SmolLM2.gguf SmolLM2.q8.gguf Q8_0 $(nproc)\n",
    "    # or\n",
    "    llama-quantize SmolLM2.gguf SmolLM2.q8.gguf Q8_0 N  # N for automatic thread count. change N to number of cores in your system.\n",
    "\n",
    "\n",
    "# Running llama.cpp Server\n",
    "    # Start the llama.cpp server\n",
    "    llama-server -m SmolLM2.q8.gguf\n",
    "    \n",
    "    # Access the server at http://localhost:8000 (Access Web UI)\n",
    "    llama-server -m SmolLM2.q8.gguf --host 0.0.0.0 --port 8000\n",
    "\n",
    "\n",
    "# Other llama.cpp Tools\n",
    "    # Benchmark Performance\n",
    "    llama-bench --flash-attn 1 --model ./SmolLM2.q8.gguf    # enable --flash-attn 1 for Flash Attention optimization\n",
    "    \n",
    "    # llama-cli\n",
    "        # Text Completion\n",
    "        llama-cli --flash-attn --model ./SmolLM2.q8.gguf --prompt \"The highest mountain on earth\"\n",
    "\n",
    "        # Interactive chat\n",
    "        llama-cli --flash-attn --model ./SmolLM2.q8.gguf --prompt \"You are a helpful assistant\" --conversation\n",
    "\n",
    "\n",
    "# Help\n",
    "    llama-server --help\n",
    "    llama-bench --help\n",
    "    llama-infer --help\n",
    "    llama-cli --help\n",
    "    llama-quantize --help\n",
    "        \n",
    "    \n",
    "    \n",
    "#-------------------------- EXTRAS --------------------------#\n",
    "    \n",
    "\n",
    "\n",
    "#  **llama.cpp Configuration Options Cheat Sheet**\n",
    "\n",
    "# -----------------------------------\n",
    "# 1. Common Parameters\n",
    "# -----------------------------------\n",
    "--threads           # Number of CPU threads to use (default: auto)\n",
    "--threads-batch     # Number of CPU threads for batching (default: auto)\n",
    "--ctx-size          # Prompt context size (number of tokens) (default: model limit)\n",
    "--predict           # Number of tokens to generate (default: -1 for unlimited)\n",
    "--batch-size        # Batch size for processing (default: system/model dependent)\n",
    "--ubatch-size       # Unbounded batch size for processing (default: system/model dependent)\n",
    "--flash-attn        # Enable Flash Attention optimization (0: disabled, 1: enabled)\n",
    "--mlock             # Prevent model from being swapped to disk (default: disabled)\n",
    "--no-mmap           # Disable memory mapping of the model file (default: memory mapping enabled)\n",
    "--gpu-layers        # Number of model layers to offload to GPU (default: 0 for no offloading)\n",
    "\n",
    "# -----------------------------------\n",
    "# 2. Sampling Parameters\n",
    "# -----------------------------------\n",
    "--temp              # Temperature: Controls randomness of output (0.2-2.0 recommended)\n",
    "--top_k             # Top-K: Keeps top K most probable tokens (e.g., 40)\n",
    "--top_p             # Top-P: Nucleus sampling with cumulative probability P (e.g., 0.95)\n",
    "--repeat_penalty    # Repetition Penalty: Reduces likelihood of repeating tokens (e.g., 1.2)\n",
    "--dry               # DRY: Prevents unwanted token repetition (enabled with --dry)\n",
    "--mirostat          # Mirostat: Controlled perplexity sampling (0: disabled, 1 or 2: enabled)\n",
    "--mirostat_lr       # Mirostat Learning Rate (e.g., 0.1)\n",
    "--mirostat_ent      # Mirostat Target Entropy (e.g., 5.0)\n",
    "\n",
    "# -----------------------------------\n",
    "# 3. Environment Variables\n",
    "# -----------------------------------\n",
    "LLAMA_ARG_MODEL     # Path to the model file (e.g., \"./model.gguf\")\n",
    "LLAMA_ARG_HOST      # Host IP address for the server (e.g., \"0.0.0.0\")\n",
    "LLAMA_ARG_PORT      # Port number for the server (e.g., 8000)\n",
    "\n",
    "# -----------------------------------\n",
    "# 4. Example Usage\n",
    "# -----------------------------------\n",
    "# Running llama-server with custom configurations\n",
    "llama-server \\\n",
    "  --model ./model.q8.gguf \\\n",
    "  --host 0.0.0.0 \\\n",
    "  --port 8000 \\\n",
    "  --threads 8 \\\n",
    "  --ctx-size 4096 \\\n",
    "  --predict 100 \\\n",
    "  --batch-size 512 \\\n",
    "  --ubatch-size 256 \\\n",
    "  --flash-attn 1 \\\n",
    "  --mlock \\\n",
    "  --gpu-layers 12\n",
    "\n",
    "# Running llama-cli with sampling parameters\n",
    "llama-cli \\\n",
    "  --model ./model.q8.gguf \\\n",
    "  --prompt \"Once upon a time\" \\\n",
    "  --temp 0.7 \\\n",
    "  --top_k 40 \\\n",
    "  --top_p 0.95 \\\n",
    "  --repeat_penalty 1.2 \\\n",
    "  --dry \\\n",
    "  --mirostat 2 \\\n",
    "  --mirostat_lr 0.1 \\\n",
    "  --mirostat_ent 5.0\n",
    "\n",
    "# -----------------------------------\n",
    "# 5. Setting Environment Variables\n",
    "# -----------------------------------\n",
    "# Linux/macOS (Bash/Zsh/MSYS)\n",
    "export LLAMA_ARG_MODEL=\"./model.gguf\"\n",
    "export LLAMA_ARG_HOST=\"0.0.0.0\"\n",
    "export LLAMA_ARG_PORT=8000\n",
    "\n",
    "# Windows PowerShell\n",
    "$env:LLAMA_ARG_MODEL=\"./model.gguf\"\n",
    "$env:LLAMA_ARG_HOST=\"0.0.0.0\"\n",
    "$env:LLAMA_ARG_PORT=8000\n",
    "\n",
    "# Windows Command Prompt\n",
    "set LLAMA_ARG_MODEL=./model.gguf\n",
    "set LLAMA_ARG_HOST=0.0.0.0\n",
    "set LLAMA_ARG_PORT=8000\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0927a719",
   "metadata": {},
   "source": [
    "##  **Overview of Common Model File Formats**\n",
    "\n",
    "Understanding different model file formats is essential for effectively managing, sharing, and deploying machine learning models across various frameworks and environments. Below is a comprehensive overview of the most commonly used model file formats.\n",
    "\n",
    "###  **Common Model File Formats**\n",
    "\n",
    "| **File Extension** | **Full Name / Description**    | **Primary Frameworks/Use Cases**          | **Key Characteristics**                                                                                      |\n",
    "|--------------------|--------------------------------|--------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| `.bin`             | **Binary File**                | PyTorch, HuggingFace                        | Stores model weights in a binary format. Commonly used with PyTorch and HuggingFace Transformers.            |\n",
    "| `.pt` / `.pth`     | **PyTorch Model File**         | PyTorch                                     | PyTorch-specific formats for saving entire models (`.pt`) or just state dictionaries (`.pth`).               |\n",
    "| `.safetensors`     | **Safe Tensors**               | HuggingFace, Transformers                   | A secure, fast, and memory-efficient format for storing model weights. Prevents arbitrary code execution.     |\n",
    "| `.h5`              | **HDF5 File**                   | TensorFlow, Keras                           | Hierarchical Data Format version 5. Used extensively with Keras and TensorFlow for saving models and weights.  |\n",
    "| `.onnx`            | **Open Neural Network Exchange**| Multiple (e.g., PyTorch, TensorFlow)         | An open standard for representing machine learning models, facilitating interoperability between frameworks.  |\n",
    "| `.ckpt`            | **Checkpoint File**            | TensorFlow, PyTorch, HuggingFace            | Stores training checkpoints, including model weights and optimizer states. Used for resuming training.       |\n",
    "| `.pb`              | **Protocol Buffer**            | TensorFlow                                  | TensorFlow's native format for serialized models. Used for deploying models in production environments.     |\n",
    "| `.json`            | **JSON File**                  | Various                                     | Often used to store model architecture/configuration rather than weights. Common in Keras and some HuggingFace models. |\n",
    "| `.pkl`             | **Pickle File**                | PyTorch, scikit-learn                        | Python-specific serialization format. Can store entire models or specific objects. Not recommended for untrusted sources due to security risks. |\n",
    "| `.gguf`            | **GGML Unified Format**        | llama.cpp, GGML-based Projects               | Optimized for efficient inference with support for various quantization schemes. Ensures fast loading and low memory footprint. Ideal for deployment in resource-constrained environments. |\n",
    "\n",
    "---\n",
    "\n",
    "###  **Summary of Model File Formats**\n",
    "\n",
    "| **File Extension** | **Primary Frameworks/Use Cases**          | **Key Characteristics**                                                                                      |\n",
    "|--------------------|--------------------------------------------|--------------------------------------------------------------------------------------------------------------|\n",
    "| `.bin`             | PyTorch, HuggingFace                        | Stores model weights in a binary format. Commonly used with PyTorch and HuggingFace Transformers.            |\n",
    "| `.pt` / `.pth`     | PyTorch                                     | PyTorch-specific formats for saving entire models (`.pt`) or just state dictionaries (`.pth`).               |\n",
    "| `.safetensors`     | HuggingFace, Transformers                   | A secure, fast, and memory-efficient format for storing model weights. Prevents arbitrary code execution.     |\n",
    "| `.h5`              | TensorFlow, Keras                           | Hierarchical Data Format version 5. Used extensively with Keras and TensorFlow for saving models and weights.  |\n",
    "| `.onnx`            | Multiple (e.g., PyTorch, TensorFlow)         | An open standard for representing machine learning models, facilitating interoperability between frameworks.  |\n",
    "| `.ckpt`            | TensorFlow, PyTorch, HuggingFace            | Stores training checkpoints, including model weights and optimizer states. Used for resuming training.       |\n",
    "| `.pb`              | TensorFlow                                  | TensorFlow's native format for serialized models. Used for deploying models in production environments.     |\n",
    "| `.json`            | Various                                     | Often used to store model architecture/configuration rather than weights. Common in Keras and some HuggingFace models. |\n",
    "| `.pkl`             | PyTorch, scikit-learn                        | Python-specific serialization format. Can store entire models or specific objects. Not recommended for untrusted sources due to security risks. |\n",
    "| `.gguf`            | llama.cpp, GGML-based Projects               | Optimized for efficient inference with support for various quantization schemes. Ensures fast loading and low memory footprint. Ideal for deployment in resource-constrained environments. |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd94e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5182eeca",
   "metadata": {},
   "source": [
    "> Fast API\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "    <img src=\"fastapi.png\" alt=\"Image for FastAPI\" style=\"width: 50%; height: auto;\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de6b3ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beeff78f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a063c54e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34442d8e",
   "metadata": {},
   "source": [
    "> Open AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1dc1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Storing API KEys\n",
    "\n",
    "# Step 1: Install the `dotenv` package\n",
    "!pip install python-dotenv\n",
    "\n",
    "# Step 2: Create a `.env` file in the project directory\n",
    "# Add your API key to the `.env` file\n",
    "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "# Step 3: Load the API key in your Python script\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI # Install the `openai` package\n",
    "from textwrap import dedent\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"No OPENAI_API_KEY found in environment variables.\")\n",
    "\n",
    "# Step 4: Use the API key to make requests\n",
    "client = OpenAI(api_key=api_key)\n",
    "MODEL = \"gpt-4o-2024-11-20\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87103717",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the system prompt and user question\n",
    "math_tutor_prompt = \"\"\"\n",
    "You are a math tutor that helps students understand algebra and calculus concepts.\n",
    "Provide clear and concise explanations with examples.\n",
    "\"\"\"\n",
    "question = \"Can you explain the Fundamental Theorem of Calculus?\"\n",
    "\n",
    "# Create a chat completion request\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,  # Specifies the AI model to use for generating completions\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": dedent(math_tutor_prompt)  # Sets the context or behavior of the assistant\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": question  # The user's input or question to the assistant\n",
    "        }\n",
    "    ],\n",
    "        # # Use the \"system\" role to set context, \"user\" for inputs, and \"assistant\" for responses.\n",
    "    temperature=0.7,  # Controls the randomness of the output (0.0 = deterministic, 1.0 = more random)\n",
    "    max_tokens=150,  # Limits the maximum number of tokens in the generated response\n",
    "    top_p=0.9,  # Implements nucleus sampling; considers the smallest set of tokens with cumulative probability >= top_p\n",
    "    n=1,  # Number of completions to generate for each prompt. Useful for getting multiple responses\n",
    "    stream=False,  # If True, streams back partial progress as the AI generates tokens. Useful for real-time applications\n",
    "    stop=[\"\\n\", \"User:\"],  # Specifies one or more stopping sequences where the API will cease generating further tokens. Can be a single string or a list of strings\n",
    "    presence_penalty=0.5,  # Encourages the model to introduce new topics (range: -2.0 to 2.0). Positive values encourage new topics, negative values discourage\n",
    "    frequency_penalty=0.3,  # Reduces the likelihood of the model repeating the same line verbatim (range: -2.0 to 2.0). Positive values reduce repetition, negative values increase it\n",
    "    logit_bias={\"50256\": -100},  # Modify the likelihood of specified tokens appearing in the completion. Structure: {token_id: bias}\n",
    "        # In this case, it prevents the model from using the end-of-text token. Bias can be between -100 and 100. Negative values discourage, positive values encourage\n",
    "    user=\"user-1234\",  # A unique identifier for the end-user, helps OpenAI monitor and detect abuse\n",
    "    response_format=\"json\"  # Specifies the format of the response (e.g., \"json\", \"text\")\n",
    ")\n",
    "\n",
    "# Print the assistant's response\n",
    "print(response.choices[0])\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# for Multimodal (Image + Text) Conversations\n",
    "# -----------------------------------------------\n",
    "\n",
    "# Function to encode the image\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')\n",
    "\n",
    "# Path to your image\n",
    "image_path = \"path_to_your_image.jpg\"\n",
    "\n",
    "# Getting the base64 string\n",
    "base64_image = encode_image(image_path)\n",
    "\n",
    "messages=[\n",
    "    {\n",
    "        \"role\": \"system\", \n",
    "        \"content\": dedent(math_tutor_prompt)  # Sets the context or behavior of the assistant\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\", \n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
    "                    },\n",
    "            },\n",
    "            { \n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                \"url\": \"https://wisconsin-madison-the-nature-boardwalk.jpg\",\n",
    "                },\n",
    "            },\n",
    "                    ]  # The user's input or question to the assistant\n",
    "    }\n",
    "        ],\n",
    "# -----------------------------------------------\n",
    "# Detailed Explanation of Parameters\n",
    "\n",
    "# 1. model\n",
    "# Specifies the AI model to use for generating completions.\n",
    "# Example Models: \"gpt-4\", \"gpt-3.5-turbo\"\n",
    "# Higher models like GPT-4 are more capable but may be more expensive.\n",
    "\n",
    "# 2. messages\n",
    "# Represents the conversation history. Each message must have a 'role' and 'content'.\n",
    "# Roles:\n",
    "    \"system\": # Sets the behavior or context of the assistant.\n",
    "    \"user\": # The input from the user.\n",
    "    \"assistant\": # The response from the AI (usually not included in the request).\n",
    "    \"developer\": # Messages from the developer (e.g., debugging information).\n",
    "        \n",
    "# Example:\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Tell me a joke about data science.\"}\n",
    "]\n",
    "\n",
    "# 13. response_format\n",
    "# Specifies the format of the response. Common formats include \"json\", \"text\", etc.\n",
    "# Not all endpoints support this parameter. Check OpenAI's documentation for compatibility.\n",
    "# Example:\n",
    "# response_format=\"json\", \"text\", \"html\", \"markdown\" etc.\n",
    "    # custom method to validate the response format\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"math_reasoning\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"steps\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"explanation\": {\"type\": \"string\"},\n",
    "                                    \"output\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"explanation\", \"output\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            }\n",
    "                        },\n",
    "                        \"final_answer\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"required\": [\"steps\", \"final_answer\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5345d8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# OpenAI Python Client: Attributes Cheat Sheet\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Using ChatCompletion API\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "prompt1 = \"You are a helpful assistant.\"\n",
    "prompt2 = \"Hello, who won the world series in 2020?\"\n",
    "prompt3 = \"Translate the following English text to French: 'Hello, world!'\"\n",
    "\n",
    "# Create a chat completion request\n",
    "response = openai.ChatCompletion.create(\n",
    "    model = \"gpt-4\",\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt2}\n",
    "        {\"role\": \"user\", \"content\": prompt2},\n",
    "        {\"role\": \"user\", \"content\": prompt3}\n",
    "    ]\n",
    "        )\n",
    "\n",
    "print(response.choices[0].message.content)  # Print the response message content\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# Using OpenAI Python Client: Attributes Cheat Sheet\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# 1. Authentication & Configuration\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# 'api_key'\n",
    "# Your unique API key for authenticating requests to OpenAI.\n",
    "client.api_key = \"your-api-key-here\"\n",
    "\n",
    "# 'organization'\n",
    "# Specifies the organization ID associated with your API key. # Useful for managing resources across multiple teams.\n",
    "client.organization = \"org-1234567890abcdef\"\n",
    "\n",
    "# 'custom_auth'\n",
    "# Allows you to set custom authentication headers. Useful for advanced authentication mechanisms or proxies.\n",
    "client.custom_auth = {\"Authorization\": \"Bearer your-custom-token\"}\n",
    "\n",
    "# 'base_url'\n",
    "# The base URL for API requests. Change this if you're using a different endpoint.\n",
    "# **Default:** \"https://api.openai.com/v1\"\n",
    "client.base_url = \"https://api.openai.com/v1\"\n",
    "\n",
    "# 'default_headers'\n",
    "# Default HTTP headers sent with each request. You can modify or add headers as needed.\n",
    "client.default_headers.update({\"Custom-Header\": \"value\"})\n",
    "\n",
    "# 'default_query'\n",
    "# Default query parameters appended to each request. Useful for setting global query options.\n",
    "client.default_query = {\"timeout\": 30}\n",
    "\n",
    "# 'user_agent'\n",
    "# Specifies the User-Agent header for HTTP requests. Useful for identifying your application.\n",
    "client.user_agent = \"MyApp/1.0\"\n",
    "\n",
    "# 'platform_headers'\n",
    "# Headers specific to the platform or environment.\n",
    "# **Usage:**\n",
    "client.platform_headers = {\"X-Platform\": \"PythonClient\"}\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# 2. API Interaction Attributes\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# 'chat'\n",
    "# Access to Chat Completion endpoints. Used to create chat-based interactions.\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, who won the world series in 2020?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 'completions'\n",
    "# Access to Completion endpoints for generating text completions based on prompts.\n",
    "completion = client.completions.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=\"Translate the following English text to French: 'Hello, world!'\",\n",
    "    max_tokens=60\n",
    ")\n",
    "\n",
    "# 'embeddings'\n",
    "# Access to Embedding endpoints for generating vector representations of text.\n",
    "embedding = client.embeddings.create(\n",
    "    input=\"OpenAI provides powerful AI tools.\",\n",
    "    model=\"text-embedding-ada-002\",\n",
    "    encoding_format=\"float\"\n",
    ")\n",
    "\n",
    "# 'files'\n",
    "# Manage file uploads and operations, such as uploading data for fine-tuning.\n",
    "    client.files.create(file=open(\"data.jsonl\", \"rb\"), purpose=\"fine-tune\") # Upload a file for fine-tuning\n",
    "    client.files.list()  # List all uploaded files\n",
    "    client.files.retrieve(\"file-abc123\")  # Retrieve details for a specific file\n",
    "    client.files.delete(\"file-abc123\")  # Delete a file\n",
    "    client.files.content(\"file-abc123\")  # Get the content of a file\n",
    "    \n",
    "    \n",
    "# 'fine_tuning'\n",
    "# Access to Fine-Tuning endpoints for customizing models on your dataset.\n",
    "fine_tune = client.fine_tuning.jobs.create(training_file=\"file-abc123\", model=\"davinci\",\n",
    "                                           validation_file = \"file-abc124\", language=\"en\", n_epochs=3,\n",
    "                                           batch_size=32, max_tokens=2048, learning_rate=5e-5)\n",
    "\n",
    "            client.fine_tuning.jobs.list()  # List all fine-tuning jobs\n",
    "            client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"job-123\")  # List events for a specific fine-tuning job\n",
    "            client.fine_tuning.jobs.retrieve(\"ftjob-abc123\")    # Retrieve details for a specific fine-tuning job\n",
    "            client.fine_tuning.jobs.cancel(\"ftjob-abc123\")  # Cancel a fine-tuning job\n",
    "\n",
    "\n",
    "# 'images'\n",
    "# Access to Image Generation endpoints for creating images from text prompts.\n",
    "client.images.generate(model=\"dall-e-3\", prompt=\"A sunset over a mountain range.\", n=1, size=\"1024x1024\")\n",
    "client.images.edit(model=\"dall-e-3\", prompt=\"A sunset over a mountain range.\", image=open(\"otter.png\", \"rb\"), \n",
    "                   mask=open(\"mask.png\", \"rb\"), n=1, size=\"1024x1024\")\n",
    "\n",
    "\n",
    "# 'audio'\n",
    "# Access to Audio endpoints for tasks like speech-to-text and text-to-speech.\n",
    "import openai\n",
    "from pathlib import Path\n",
    "\n",
    "speech_file_path = Path(__file__).parent / \"speech.mp3\"\n",
    "response = openai.audio.speech.create(model=\"tts-1\", voice=\"alloy\", input=\"The quick brown fox jumped over the lazy dog.\") # Create speech\n",
    "transcription = client.audio.transcriptions.create( # Transcribe audio file to text\n",
    "            file=open(\"audio.mp3\", \"rb\"),\n",
    "            model=\"whisper-1\"\n",
    "        )\n",
    "transcript = client.audio.translations.create(  # Translate audio file to text\n",
    "        model=\"whisper-1\",\n",
    "        file=open(\"audio.mp3\", \"rb\")\n",
    "        )\n",
    "\n",
    "\n",
    "# 'uploads'\n",
    "# Handle file uploads for various purposes, such as fine-tuning or moderation.\n",
    "uploaded_file = client.uploads.create(\n",
    "    file=open(\"data.jsonl\", \"rb\"),\n",
    "    purpose=\"fine-tune\"\n",
    ")\n",
    "\n",
    "# 'moderations'\n",
    "# Access to Moderation endpoints for content filtering and safety checks.\n",
    "moderation = client.moderations.create(input=\"I want to harm someone.\")\n",
    "\n",
    "# 'models'\n",
    "# Retrieve information about available models and their capabilities.\n",
    "models = client.models.list().data\n",
    "print(models)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# 3. HTTP Methods\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# 'get', 'post', 'put', 'delete', 'patch'\n",
    "# ---------------------------------------\n",
    "# Direct access to HTTP methods for making custom API requests.\n",
    "# Example: Fetch a specific model's details using GET\n",
    "model_details = client.get(\"/models/gpt-4\")\n",
    "print(model_details)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# 4. Request Management\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# 'request'\n",
    "# Low-level method to make custom API requests.\n",
    "response = client.request(\n",
    "    method=\"POST\",\n",
    "    url=\"/completions\",\n",
    "    json={\n",
    "        \"model\": \"gpt-4\",\n",
    "        \"prompt\": \"Say hello in Spanish.\",\n",
    "        \"max_tokens\": 5\n",
    "    }\n",
    ")\n",
    "print(response.json())\n",
    "\n",
    "# 'max_retries'\n",
    "# Sets the maximum number of retry attempts for failed requests.\n",
    "# **Default:** 3\n",
    "client.max_retries = 5\n",
    "\n",
    "# 'timeout'\n",
    "# Specifies the timeout duration for API requests in seconds.\n",
    "# **Default:** None (no timeout)\n",
    "client.timeout = 60\n",
    "\n",
    "# 'is_closed'\n",
    "# Boolean indicating whether the client session is closed.\n",
    "if client.is_closed:\n",
    "    print(\"Client session is closed.\")\n",
    "\n",
    "# 'close'\n",
    "# Closes the client session to free up resources.\n",
    "client.close()\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# 5. Advanced Features\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# 'with_options'\n",
    "# Creates a new client instance with overridden options.\n",
    "new_client = client.with_options(timeout=30, max_retries=5)\n",
    "\n",
    "# 'with_raw_response'\n",
    "# Returns the raw HTTP response instead of parsed JSON.\n",
    "raw_response = client.with_raw_response().completions.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=\"Hello!\",\n",
    "    max_tokens=5\n",
    ")\n",
    "print(raw_response.content)\n",
    "\n",
    "# 'with_streaming_response'\n",
    "# Enables streaming of responses for real-time applications.\n",
    "stream = client.with_streaming_response().completions.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=\"Write a poem.\",\n",
    "    max_tokens=50,\n",
    "    stream=True\n",
    ")\n",
    "for chunk in stream:\n",
    "    print(chunk.choices[0].text, end='')\n",
    "\n",
    "# 'copy'\n",
    "# Create a copy of the client instance. (Advanced usage)\n",
    "copied_client = client.copy()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6acf727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supervised Method JSON Entry for Fine-Tuning API  \n",
    " \n",
    "JSONL Entry \n",
    " \n",
    " messages (array) \n",
    "    [0] System Message (optional) \n",
    "       role: \"system\" \n",
    "       content: \"System-level instructions or context.\" \n",
    "    \n",
    "    [1] User Message \n",
    "       role: \"user\" \n",
    "       content: \"I am unable to receive my order after payment.\" \n",
    "       name: \"Optional User Name\" \n",
    "    \n",
    "    [2] Assistant Message \n",
    "       role: \"assistant\" \n",
    "       content: \"I'm sorry to hear that you're experiencing issues with your order. Let me assist you with that.\"\n",
    "       weight: 1 or 0\n",
    "       refusal: null\n",
    "       audio: null\n",
    "       name: \"Optional Assistant Name\"\n",
    "   \n",
    "    [3] Tool Message (if applicable)\n",
    "        role: \"tool\"\n",
    "        content: \"Response from tool execution.\"\n",
    "        tool_call_id: \"call_1\"\n",
    "        name: \"Optional Tool Name\"\n",
    "\n",
    " tools (array)\n",
    "    [0] Tool Definition 1\n",
    "       type: \"function\"\n",
    "       function\n",
    "           name: \"provide_order_number\"\n",
    "           description: \"Retrieve and provide the customer's order number.\"\n",
    "           parameters (object)\n",
    "              type: \"object\"\n",
    "              properties\n",
    "                 user_id\n",
    "                    type: \"string\"\n",
    "                    description: \"The unique identifier for the user.\"\n",
    "                 order_id\n",
    "                     type: \"string\"\n",
    "                     description: \"The unique identifier for the order.\"\n",
    "              required: [\"user_id\", \"order_id\"]\n",
    "           strict: null\n",
    "   \n",
    "    [1] Tool Definition 2\n",
    "       type: \"function\"\n",
    "       function\n",
    "           name: \"generate_complaint_link\"\n",
    "           description: \"Provide a link for the customer to lodge a formal complaint.\"\n",
    "           parameters (object)\n",
    "              type: \"object\"\n",
    "              properties\n",
    "                 user_id\n",
    "                     type: \"string\"\n",
    "                     description: \"The unique identifier for the user.\"\n",
    "              required: [\"user_id\"]\n",
    "           strict: null\n",
    "   \n",
    "    [2] Tool Definition 3\n",
    "       type: \"function\"\n",
    "       function\n",
    "           name: \"notify_supply_chain\"\n",
    "           description: \"Send a notice to the supply chain regarding delivery issues.\"\n",
    "           parameters (object)\n",
    "              type: \"object\"\n",
    "              properties\n",
    "                 order_id\n",
    "                    type: \"string\"\n",
    "                    description: \"The unique identifier for the order.\"\n",
    "                 issue\n",
    "                     type: \"string\"\n",
    "                     description: \"Description of the delivery issue.\"\n",
    "              required: [\"order_id\", \"issue\"]\n",
    "           strict: null\n",
    "   \n",
    "    [3] Tool Definition 4\n",
    "        type: \"function\"\n",
    "        function\n",
    "            name: \"connect_to_human\"\n",
    "            description: \"Connect the customer to a human support agent.\"\n",
    "            parameters (object)\n",
    "               type: \"object\"\n",
    "               properties\n",
    "                  user_id\n",
    "                      type: \"string\"\n",
    "                      description: \"The unique identifier for the user.\"\n",
    "               required: [\"user_id\"]\n",
    "            strict: null\n",
    "\n",
    " parallel_tool_calls (boolean): true or false \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ea9a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# Fine-Tuning OpenAI Models\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Fine-tuning involves training a pre-trained model on a custom dataset to specialize it for specific tasks or improve its performance in particular domains.\n",
    "\n",
    "# **Data Formatting**\n",
    "# - **Text Models:** JSON Lines (JSONL) format, where each line is a JSON object containing `prompt` and `completion` fields.\n",
    "    # Understanding JSONL Formats\n",
    "        # 1. Basic JSONL Format\n",
    "        # 1. Supervised Method (Chat Models)\n",
    "        # 2. Preference Method (DPO) for Chat Models\n",
    "        # 3. Completion Models\n",
    "        \n",
    "        \n",
    "    # 1. Basic JSONL Format\n",
    "    {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"What's the capital of France?\"}, {\"role\": \"assistant\", \"content\": \"Paris, as if everyone doesn't know that already.\"}]}\n",
    "    {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"Who wrote 'Romeo and Juliet'?\"}, {\"role\": \"assistant\", \"content\": \"Oh, just some guy named William Shakespeare. Ever heard of him?\"}]}\n",
    "    {\"messages\": [{\"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\"}, {\"role\": \"user\", \"content\": \"How far is the Moon from Earth?\"}, {\"role\": \"assistant\", \"content\": \"Around 384,400 kilometers. Give or take a few, like that really matters.\"}]}\n",
    "    \n",
    "        # 1b. Basic JSONL Format with Image\n",
    "        {\n",
    "    \"messages\": [\n",
    "        { \"role\": \"system\", \"content\": \"You are an assistant that identifies uncommon cheeses.\" },\n",
    "        { \"role\": \"user\", \"content\": \"What is this cheese?\" },\n",
    "        { \"role\": \"user\", \"content\": [\n",
    "            {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": \"https://upload.wikimedia.org/wikipedia/commons/3/36/Danbo_Cheese.jpg\"\n",
    "            }\n",
    "            }\n",
    "        ]\n",
    "        },\n",
    "        { \"role\": \"assistant\", \"content\": \"Danbo\" }\n",
    "    ]\n",
    "    }\n",
    "        \n",
    "    # A. Supervised Method (Chat Models)\n",
    "    # Used for training chat-based models where each training example consists of a conversation between a user and an assistant, potentially involving tool calls or function executions.\n",
    "\n",
    "    # Structure: \n",
    "    \n",
    "        # (messages, parallel_tool_calls, tools)\n",
    "\n",
    "{\n",
    "  \"messages\": [ # Conversation messages\n",
    "    { \"role\": \"system\", \"content\": \"Marv is a factual chatbot that is also sarcastic.\" },\n",
    "    { \"role\": \"user\", \"content\": \"What is the weather in San Francisco?\" },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"tool_calls\": [\n",
    "        {\n",
    "          \"id\": \"call_id\",\n",
    "          \"type\": \"function\",   # Type of tool call (e.g., \"function\", \"tool\")\n",
    "          \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"arguments\": \"{\\\"location\\\": \\\"San Francisco, USA\\\", \\\"format\\\": \\\"celsius\\\"}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"parallel_tool_calls\": false,\n",
    "  \"tools\": [\n",
    "    {\n",
    "      \"type\": \"function\",\n",
    "      \"function\": {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather\",\n",
    "        \"parameters\": {\n",
    "          \"type\": \"object\",\n",
    "          \"properties\": {\n",
    "            \"location\": {\n",
    "              \"type\": \"string\",\n",
    "              \"description\": \"The city and country, e.g., San Francisco, USA\"\n",
    "            },\n",
    "            \"format\": { \"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"] }\n",
    "          },\n",
    "          \"required\": [\"location\", \"format\"]\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "    \n",
    "        \n",
    "    # B. Preference Method (DPO) for Chat Models\n",
    "        # Used for training models based on preferred and non-preferred completions, allowing the model to learn from comparative feedback. \n",
    "        \n",
    "    {\n",
    "    \"input\": {\n",
    "        \"messages\": [\n",
    "        { \"role\": \"user\", \"content\": \"What is the weather in San Francisco?\" }\n",
    "        ]\n",
    "    },\n",
    "    \"preferred_completion\": [\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The weather in San Francisco is 70 degrees Fahrenheit.\"\n",
    "        }\n",
    "    ],\n",
    "    \"non_preferred_completion\": [\n",
    "        {\n",
    "        \"role\": \"assistant\",\n",
    "        \"content\": \"The weather in San Francisco is 21 degrees Celsius.\"\n",
    "        }\n",
    "    ]\n",
    "    }\n",
    "    \n",
    "       \n",
    "    # C. Completion Models\n",
    "        # Used for models that generate text completions based on prompts. This is suitable for straightforward text generation tasks \n",
    "        # without the complexity of multi-turn dialogues or tool integrations.\n",
    "      \n",
    "    {\n",
    "    \"prompt\": \"Translate the following English text to French:\\n\\nHello, how are you?\",\n",
    "    \"completion\": \"Bonjour, comment a va?\"\n",
    "    }\n",
    "    {\"prompt\": \"Translate the following English text to French:\\n\\nHello, how are you?\", \"completion\": \"Bonjour, comment a va?\"}\n",
    "    {\"prompt\": \"Translate the following English text to French:\\n\\nGood morning!\", \"completion\": \"Bonjour!\"}\n",
    "\n",
    "\n",
    "\n",
    "# **Step 2: Upload Your Training Data**\n",
    "# Upload the prepared file to OpenAI.\n",
    "\n",
    "def upload_file(file_path, purpose):\n",
    "    \"\"\"\n",
    "    Uploads a file to OpenAI for fine-tuning.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the training data file.\n",
    "        purpose (str): Purpose of the file (e.g., \"fine-tune\").\n",
    "\n",
    "    Returns:\n",
    "        dict: Uploaded file information.\n",
    "    \"\"\"\n",
    "    response = client.files.create(\n",
    "        file=open(file_path, \"rb\"),\n",
    "        purpose=purpose\n",
    "    )\n",
    "    return response\n",
    "\n",
    "# Example usage:\n",
    "uploaded_file = upload_file(\"training_data.jsonl\", \"fine-tune\")\n",
    "\n",
    "\n",
    "# **Step 3: Create a Fine-Tune Job**\n",
    "# Initiate the fine-tuning process using the uploaded file.\n",
    "\n",
    "def create_fine_tune_job(training_file_id, model=\"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Creates a fine-tune job for the specified model.\n",
    "\n",
    "    Args:\n",
    "        training_file_id (str): ID of the uploaded training file.\n",
    "        model (str): Base model to fine-tune.\n",
    "\n",
    "    Returns:\n",
    "        dict: Fine-tune job information.\n",
    "    \"\"\"\n",
    "    response = client.fine_tuning.create(\n",
    "        training_file=training_file_id,\n",
    "        model=model\n",
    "    )\n",
    "    return response\n",
    "\n",
    "fine_tune_job = create_fine_tune_job(uploaded_file['id'])\n",
    "\n",
    "# **Step 4: Monitor the Fine-Tune Job**\n",
    "# Track the status of the fine-tuning process until completion.\n",
    "\n",
    "def monitor_fine_tune_job(fine_tune_job_id):\n",
    "    \"\"\"\n",
    "    Monitors the status of a fine-tune job.\n",
    "\n",
    "    Args:\n",
    "        fine_tune_job_id (str): ID of the fine-tune job.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated fine-tune job information.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        job = client.fine_tuning.retrieve(id=fine_tune_job_id)\n",
    "        status = job['status']\n",
    "        print(f\"Fine-Tune Job Status: {status}\")\n",
    "        if status in ['succeeded', 'failed']:\n",
    "            break\n",
    "        time.sleep(60)  # Wait for 1 minute before checking again\n",
    "    return job\n",
    "\n",
    "# Example usage:\n",
    "final_job = monitor_fine_tune_job(fine_tune_job['id'])\n",
    "\n",
    "\n",
    "# **Step 5: Use the Fine-Tuned Model**\n",
    "# Once the fine-tuning is complete, use the fine-tuned model for generating responses.\n",
    "\n",
    "def generate_with_finetuned_model(model, prompt, max_tokens=100):\n",
    "    \"\"\"\n",
    "    Generates text using a fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        model (str): Fine-tuned model name.\n",
    "        prompt (str): Input prompt for the model.\n",
    "        max_tokens (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        str: Generated text.\n",
    "    \"\"\"\n",
    "    response = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens\n",
    "    )\n",
    "    return response['choices'][0]['text']\n",
    "\n",
    "# Example usage:\n",
    "fine_tuned_model = \"ft-your-fine-tuned-model-name\"\n",
    "output = generate_with_finetuned_model(fine_tuned_model, \"Translate the following text to French:\\n\\nGood night!\")\n",
    "# print(output)\n",
    "\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 4. Code Snippets for Different Model Types\n",
    "# Example Fine-Tuning Code:\n",
    "def fine_tune_chat_model(training_file_path, base_model=\"gpt-3.5-turbo\"):\n",
    "    uploaded_file = upload_file(training_file_path, \"fine-tune\")\n",
    "    fine_tune_job = create_fine_tune_job(uploaded_file['id'], model=base_model)\n",
    "    final_job = monitor_fine_tune_job(fine_tune_job['id'])\n",
    "    if final_job['status'] == 'succeeded':\n",
    "        return final_job['fine_tuned_model']\n",
    "    else:\n",
    "        raise Exception(\"Fine-tuning failed.\")\n",
    "\n",
    "# -----------------------------------------------\n",
    "# 5. Case Studies\n",
    "\n",
    "# **Case Study 1: Custom Chatbot for Customer Support**\n",
    "\n",
    "# **Objective:** Create a chatbot fine-tuned to handle customer support queries for an e-commerce platform.\n",
    "\n",
    "# **Data Preparation:**\n",
    "# Prepare a JSONL file (`customer_support.jsonl`) with `prompt` and `completion` pairs.\n",
    "\n",
    "'''\n",
    "{\"prompt\": \"User: I forgot my password. Can you help me reset it?\\nAssistant:\", \"completion\": \" Of course! Please click on the 'Forgot Password' link on the login page and follow the instructions to reset your password.\"}\n",
    "{\"prompt\": \"User: Where is my order?\\nAssistant:\", \"completion\": \" Let me check that for you. Could you please provide your order number?\"}\n",
    "'''\n",
    "\n",
    "'''\n",
    "{\"prompt\": \"Patient Name: John Doe\\nAge: 45\\nSymptoms: Headache, dizziness\\n\\nDoctor:\", \"completion\": \" Based on the symptoms presented, further diagnostic tests such as MRI and blood work are recommended to determine the underlying cause.\"}\n",
    "{\"prompt\": \"Patient Name: Jane Smith\\nAge: 60\\nSymptoms: Shortness of breath, chest pain\\n\\nDoctor:\", \"completion\": \" Immediate ECG and stress tests are advised to rule out cardiac issues. Please schedule an appointment at the nearest clinic.\"}\n",
    "'''\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39073ee",
   "metadata": {},
   "source": [
    "> Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e991f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Storing API KEys\n",
    "\n",
    "# Step 1: Install the `dotenv` package\n",
    "!pip install python-dotenv\n",
    "\n",
    "# Step 2: Create a `.env` file in the project directory\n",
    "# Add your API key to the `.env` file\n",
    "OPENAI_API_KEY = \"YOUR_API_KEY\"\n",
    "\n",
    "# Step 3: Load the API key in your Python script\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import openai # Install the `openai` package\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    raise ValueError(\"No OPENAI_API_KEY found in environment variables.\")\n",
    "client = openai.OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac672aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LANGCHAIN FRAMEWORK  ADVANCED TREE\n",
    "\n",
    "1. INSTALLATION\n",
    "    pip install langchain\n",
    "    Additional Dependencies:\n",
    "       OpenAI (pip install openai)\n",
    "       Pinecone (pip install pinecone-client)  for vector storage\n",
    "       PyPDF2 or unstructured  for document loading\n",
    "       FAISS / Weaviate / Milvus clients  for advanced vector indexing\n",
    "    Environment Setup Tips:\n",
    "        Python >= 3.7 recommended\n",
    "\n",
    "2. CORE COMPONENTS\n",
    "    2.1 PROMPT TEMPLATES\n",
    "       from langchain.prompts import PromptTemplate\n",
    "       Example:\n",
    "         prompt = PromptTemplate(\n",
    "             input_variables=[\"input_text\"],\n",
    "             template=\"Translate to French: {input_text}\"\n",
    "         )\n",
    "       Advanced:\n",
    "           Dynamically injecting variables, multi-part prompts, Jinja-like templating\n",
    "\n",
    "    2.2 EXAMPLE SELECTORS\n",
    "       from langchain.prompts.example_selector import SemanticSimilarityExampleSelector\n",
    "       Example:\n",
    "         selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "             examples=my_examples,\n",
    "             embeddings=OpenAIEmbeddings(),\n",
    "             k=3\n",
    "         )\n",
    "       Purpose:\n",
    "           Dynamically pick best example prompt snippets based on user query\n",
    "\n",
    "    2.3 CHAT MODELS\n",
    "       from langchain.chat_models import ChatOpenAI\n",
    "       Example:\n",
    "         chat = ChatOpenAI(temperature=0.5, model_name=\"gpt-3.5-turbo\")\n",
    "       Usage:\n",
    "           Engaging in multi-turn conversation flows, role-based messages\n",
    "\n",
    "    2.4 MEMORY INTEGRATION\n",
    "        from langchain.memory import ConversationBufferMemory\n",
    "        Example:\n",
    "          memory = ConversationBufferMemory(return_messages=True)\n",
    "          # pass memory into a chain or agent to track conversation\n",
    "        Advanced:\n",
    "            Summarizing memory (ConversationBufferWindowMemory), VectorStoreRetrieverMemory\n",
    "\n",
    "3. DOCUMENT RETRIEVAL & VECTOR STORES\n",
    "    DOCUMENT DIRECTORY LOADERS\n",
    "       from langchain.document_loaders import DirectoryLoader\n",
    "       Example:\n",
    "         loader = DirectoryLoader(path=\"my_docs/\", glob=\"**/*.pdf\")\n",
    "         docs = loader.load()\n",
    "       Automated bulk loading of PDFs, text, markdown, etc.\n",
    "\n",
    "    DOCUMENT LOADERS\n",
    "       from langchain.document_loaders import PDFLoader, CSVLoader, WebBaseLoader, etc.\n",
    "       Example:\n",
    "         pdf_loader = PDFLoader(\"report.pdf\")\n",
    "         data = pdf_loader.load()\n",
    "       Custom loaders for proprietary formats or APIs\n",
    "\n",
    "    VECTOR STORES\n",
    "       from langchain.vectorstores import FAISS, Pinecone, Weaviate, Milvus\n",
    "       Example:\n",
    "         store = FAISS.from_texts([\"Doc1\", \"Doc2\"], embedding=OpenAIEmbeddings())\n",
    "         results = store.similarity_search(\"my query\", k=3)\n",
    "       Index management, advanced search, metadata filtering\n",
    "\n",
    "    RETRIEVERS\n",
    "       from langchain.chains import RetrievalQA\n",
    "       Example:\n",
    "         qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(), chain_type=\"stuff\",\n",
    "                                          retriever=store.as_retriever())\n",
    "       pipeline to query embeddings for context, often used in RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "    ADVANCED RETRIEVAL TECHNIQUES\n",
    "        Re-ranking, MMR (Maximal Marginal Relevance)\n",
    "        Hybrid retrieval (keyword + vector search)\n",
    "        Chaining retrieval with summarization or question decomposition\n",
    "\n",
    "4. EMBEDDING\n",
    "    from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings, etc.\n",
    "    Example:\n",
    "      embeddings = OpenAIEmbeddings(openai_api_key=\"...\")\n",
    "    Usage:\n",
    "      - Convert text into dense vectors for semantic similarity\n",
    "    Advanced:\n",
    "        Fine-tuning or using custom embeddings, multilingual embeddings\n",
    "\n",
    "5. HANDLE TEXT DATA\n",
    "    TEXT SPLITTER\n",
    "        from langchain.text_splitter import RecursiveCharacterTextSplitter, NLTKTextSplitter\n",
    "        Example:\n",
    "          splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=50)\n",
    "          chunks = splitter.split_text(\"Large text block...\")\n",
    "        Key for chunking documents into smaller pieces for vector storage or LLM consumption\n",
    "\n",
    "6. AGENTS & TOOLS\n",
    "    AGENTS\n",
    "       from langchain.agents import initialize_agent, AgentType\n",
    "       Example:\n",
    "         tools = [...]\n",
    "         agent = initialize_agent(\n",
    "             tools=tools, llm=ChatOpenAI(),\n",
    "             agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
    "         )\n",
    "       Agents automatically decide when/ how to use tools, parse outputs, and produce answers\n",
    "\n",
    "    ADVANCED FEATURES\n",
    "       Custom Tools\n",
    "       Self-critique or reflection loops\n",
    "       Constraint-based agenting (Guardrails)\n",
    "\n",
    "    Q&A SYSTEMS\n",
    "       from langchain.chains import ConversationalRetrievalChain, RetrievalQA\n",
    "       Pipeline examples:\n",
    "         - LLM + retriever + memory for interactive Q&A\n",
    "       Pattern:\n",
    "           User question  embed & retrieve docs  pass to LLM  final response\n",
    "\n",
    "    CALLBACKS\n",
    "        from langchain.callbacks import StdOutCallbackHandler, WandbCallbackHandler, TracerCallbackHandler\n",
    "        Usage:\n",
    "          callback = StdOutCallbackHandler()\n",
    "          # monitor token usage, intermediate thoughts, error logs\n",
    "        Integrate with tools like Weights & Biases for advanced telemetry\n",
    "\n",
    "7. EVALUATION & MONITORING\n",
    "    Logging & Debugging\n",
    "       LLMonitor, TracerCallbackHandler\n",
    "       Observing chain steps in detail\n",
    "    Evaluation\n",
    "       from langchain.evaluation import QAEvaluator, EmbeddingDistanceEvaluator\n",
    "       Evaluate model outputs vs. reference answers or consistency checks\n",
    "    Monitoring\n",
    "        Real-time introspection of chain states\n",
    "        Performance metrics, cost analysis\n",
    "\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94932f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "### **Installation**\n",
    "\n",
    "   # Install the LangChain library to access its core functionality.\n",
    "   pip install langchain langchain_community\n",
    "    \n",
    "   # Install dependencies required for specific integrations, like OpenAI or Pydantic.\n",
    "   pip install openai pydantic\n",
    "\n",
    "\n",
    "### **Key Features**\n",
    "\n",
    "#### **Debug LLM Applications**\n",
    "# Enable verbose logging to debug issues within your LangChain-based application.\n",
    "import langchain\n",
    "langchain.verbose = True\n",
    "\n",
    "\n",
    "#### **Streaming Outputs**\n",
    "# Stream responses from LLMs for real-time updates, which is useful for interactive applications.\n",
    "from langchain.llms import OpenAI, Anthropic, Ollama, HuggingFaceHub, HuggingFacePipeline\n",
    "from transformers import pipeline\n",
    "\n",
    "llm = OpenAI(streaming=True)\n",
    "llm = HuggingFaceHub(repo_id=\"username/repo\", model_kwargs= {'temperature':0.7, 'max_length':125}, streaming=True)\n",
    "llm = HuggingFacePipeline(pipeline = pipeline(\"text-generation\", model=\"gpt2\", tokenizer=tokenizer, max_length=125), streaming=True)\n",
    "\n",
    "for chunk in llm.generate(\"Generate a step-by-step plan\"):\n",
    "    print(chunk)\n",
    "\n",
    "---\n",
    "### **Core Components**\n",
    "\n",
    "#### **Prompt Templates**\n",
    "# Create customizable prompts to standardize input formatting for LLMs.\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Example 1: Question and Engaging Answer\n",
    "question_prompt = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=\"Question: {question}\\nMake the answer more engaging by incorporating puns.\\nAnswer:\"\n",
    ")\n",
    "formatted_prompt = question_prompt.format(question=\"Why is the sky blue?\")\n",
    "print(formatted_prompt)\n",
    "\n",
    "# Example 2: Translation Task\n",
    "translation_prompt = PromptTemplate(\n",
    "    input_variables=[\"cuisine\", \"text\"],\n",
    "    template=\"Translate this {cuisine} cuisine text to another language: {text}\"\n",
    ")\n",
    "formatted_prompt = translation_prompt.format(cuisine=\"Italian\", text=\"Buon appetito!\")\n",
    "\n",
    "# Example 3: From template (from_template, from_examples, from_dict)\n",
    "prompt = PromptTemplate.from_template(\"Translate to French: {text}\")\n",
    "prompt.format(text=\"Hello, world!\")\n",
    "\n",
    "\n",
    "# download prompts, templates and usecases from LangSmith\n",
    "\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "print(prompt.messages[0].prompt.template)\n",
    "\n",
    "\n",
    "#### **Chains**\n",
    "    # - **Chains:** Sequentially execute tasks involving multiple components like LLMs and retrievers or prompts.\n",
    "    from langchain.chains import LLMChain, RetrievalQA\n",
    "    from langchain.agents import initialize_agent\n",
    "    \n",
    "    # LLM chain\n",
    "    chain = LLMChain(llm=OpenAI(), prompt=prompt)\n",
    "    response = chain.run(\"Translate this text to French.\")\n",
    "    \n",
    "    # Sequential Chain\n",
    "    from langchain.chains import SimpleSequentialChain, SequentialChain, ConversationChain\n",
    "    \n",
    "    key_details_prompt = PromptTemplate(    # Step 1: Extract key details\n",
    "        input_variables=[\"text\"],\n",
    "        template=\"Extract the product name, issue, and customer sentiment from this feedback:\\n{text}\"\n",
    "    )\n",
    "    \n",
    "    response_prompt = PromptTemplate(   # Step 2: Respond to feedback\n",
    "        input_variables=[\"product_name\", \"issue\", \"sentiment\"],\n",
    "        template=\"Respond to the feedback about {product_name}. The issue was: {issue}. Sentiment: {sentiment}.\"\n",
    "    )\n",
    "\n",
    "    chain = SequentialChain(chains=[    # Create the chain\n",
    "        key_details_prompt,\n",
    "        response_prompt\n",
    "    ], llm=llm)\n",
    "\n",
    "    feedback_text = \"I loved the camera quality of the new Phone X, but the battery drains too quickly.\"\n",
    "    result = chain.run(feedback_text)   # Execute the chain\n",
    "    print(result)\n",
    "\n",
    "\n",
    "#### **Agents**\n",
    "    # - **Agents:** Higher-level abstractions that manage chains and tools for specific use cases like chatbots or Q&A systems.\n",
    "    from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "    \n",
    "    # Using predefined tools\n",
    "        agent = initialize_agent(   # Initialize an agent with predefined tools and models\n",
    "            tools=load_tools([\"tool1\", \"tool2\"], llm=OpenAI()),\n",
    "            llm=OpenAI(),\n",
    "            agent=AgentType.CHATBOT, # Choose the agent type (e.g., CHATBOT, QNA, ZERO_SHOT_REACT_DESCRIPTION)\n",
    "            verbose=True\n",
    "        )\n",
    "        agent.run(\"What's the weather like today?\")\n",
    "        \n",
    "    # Dynamic Tool Usage\n",
    "        # Enable agents to call tools dynamically based on the input query.\n",
    "        from langchain.agents import Tool\n",
    "\n",
    "        tool = Tool(\n",
    "            name=\"search_tool\",\n",
    "            func=lambda x: f\"Searching for {x}\",\n",
    "            description=\"A tool to perform searches\"\n",
    "        )\n",
    "\n",
    "    # Tool Creation\n",
    "        # Define custom tools to extend LangChain functionality for specific use cases.\n",
    "        from langchain.tools import BaseTool\n",
    "\n",
    "        def custom_tool_func(input_text):\n",
    "            return f\"Processed: {input_text}\"\n",
    "\n",
    "        class CustomTool(BaseTool):\n",
    "            def run(self, input_text):\n",
    "                return custom_tool_func(input_text)\n",
    "\n",
    "\n",
    "\n",
    "#### **Memory Integration**\n",
    "# ---------------------------- Memory from LangChain Core ----------------------------\n",
    "    from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
    "    from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "    from langchain_core.messages import AIMessage\n",
    "    \n",
    "    store = {}\n",
    "    def get_session_history(session_id:str) -> BaseChatMessageHistory:\n",
    "        if session_id not in store:\n",
    "            store[session_id] = InMemoryChatMessageHistory()\n",
    "        return store[session_id]\n",
    "    \n",
    "    config = {\"configurable\": {\"session_id\": \"first_session\"}}\n",
    "    model_with_memory = RunnableWithMessageHistory(llm, get_session_history)\n",
    "    model_with_memory.invoke((\"Hello, tell me a story\",), config=config).content\n",
    "# ---------------------------------------------------------------------------------------\n",
    "\n",
    "# ---------------------------- Memory from LangChain ----------------------------\n",
    "    # Use memory to maintain context in stateful conversations.\n",
    "    from langchain.memory import ConversationBufferMemory, ConversationBufferWindowMemory\n",
    "    memory = ConversationBufferMemory() # Basic memory\n",
    "    memory_2 = ConversationBufferWindowMemory(window_size=5, return_messages=True, k=4) # Memory with windowing\n",
    "    \n",
    "    conversation_chain = LLMChain(llm=OpenAI(), memory=memory)\n",
    "    response = conversation_chain.run(\"What's my name?\")\n",
    "    \n",
    "    conversation_chain_2 = ConversationChain(llm=OpenAI(temperature=0.7), memory=memory_2)  # Use a different chain type\n",
    "    conversation_chain_2.run(\"What's the weather like today?\")\n",
    "    \n",
    "---\n",
    "\n",
    "### **Document Retrieval and Vector Stores**\n",
    "\n",
    "#### **Document Directory Loaders**\n",
    "# Load a direcotry of documents for processing.\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "loader = DirectoryLoader(\"path/to/documents\", glob = \"./*.txt\", loader_cls = TextLoader())\n",
    "documents = loader.load()\n",
    "\n",
    "#### **Document Loaders**\n",
    "# Load documents from various formats like PDFs for processing.\n",
    "from langchain.document_loaders import PyPDFLoader, PDFLoader, TextLoader, PyPDFDirectoryLoader, DirectoryLoader, UnstructuredURLLoader\n",
    "loader = PDFLoader(\"file.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "#### **Text Splitter**\n",
    "# Split text into chunks for processing.\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, RecursiveJsonSplitter, TokenTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splitter = TokenTextSplitter(model_name=\"gpt-3.5-turbo\", chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "chunks = splitter.split_documents(documents)    # Split a list of documents\n",
    "chunks = splitter.split_text(\"Long text to split\") # Split a long text\n",
    "\n",
    "\n",
    "### **Embedding Models**\n",
    "from langchain.embeddings import SentenceTransformer, OpenAIEmbeddings, OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings()\n",
    "embeddings.embed(\"Hello, world!\")\n",
    "\n",
    "\n",
    "#### **Vector Stores**\n",
    "# Store document embeddings in vector databases for efficient similarity search.\n",
    "from langchain.vectorstores import FAISS, Pinecone, Chroma  # Choose a vector store\n",
    "from langchain.vectorstores.cassandra import CassandraVectorStore, Cassandra \n",
    "\n",
    "vector_store = FAISS.from_documents(documents, embeddings)   # Create a vector store\n",
    "query_result = vector_store.similarity_search(\"example query\")\n",
    "\n",
    "#### **Retrievers**\n",
    "# Use retrievers to fetch relevant documents based on similarity search.\n",
    "retriever = vectorstore.as_retriever()\n",
    "retriever.get_relevant_documents(\"example query\")\n",
    "\n",
    "#### **Advanced Retrieval Techniques**\n",
    "    # - **Hybrid Search:** Combine vector-based and keyword-based searches for more robust retrieval.\n",
    "    # - **Time-Weighted Retrieval:** Assign higher importance to recent documents.\n",
    "    # - **Metadata Filters:** Filter results based on custom metadata like author or date.\n",
    "\n",
    "---\n",
    "\n",
    "### **Advanced Features**\n",
    "\n",
    "#### **Q&A Systems**\n",
    "# Build question-answering systems by combining retrieval mechanisms and LLMs.\n",
    "from langchain.chains import RetrievalQA\n",
    "qa_chain = RetrievalQA(retriever=retriever, llm=OpenAI())\n",
    "qa_chain = RetrievalQA.from_chain_type(llm=OpenAI(), chain_type = \"stuff\", retriever=vector_store.as_retriever())\n",
    "\n",
    "response = qa_chain.run(\"What is the document about?\")\n",
    "\n",
    "#### **Custom Output Parsers**\n",
    "# Define custom parsers to structure model outputs.\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "class CustomOutput(BaseModel):\n",
    "    field: str\n",
    "\n",
    "parser = PydanticOutputParser(pydantic_model=CustomOutput)\n",
    "response = parser.parse(llm.predict(\"{\"field\": \"value\"}\"))\n",
    "\n",
    "#### **Callbacks**\n",
    "# Attach callback handlers to monitor and debug chain execution.\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "handler = StdOutCallbackHandler()\n",
    "response = chain.run(input_data, callbacks=[handler])\n",
    "\n",
    "#### **Evaluation and Monitoring**\n",
    "!pip install -U langsmith langsmith_openai\n",
    "\n",
    "LANGCHAIN_TRACING_V2 = True\n",
    "LANGCHAIN_ENDPOINT = \"https://api.langsmith.com\"\n",
    "LANGCHAIN_API_KEY = \"<API_KEY>\"\n",
    "LANGCHAIN_PROJECT = \"<PROJECT_NAME>\"    # ADD YOUR PROJECT NAME\n",
    "\n",
    "# Use LangSmith for performance evaluation and debugging.\n",
    "from langsmith import LangSmith\n",
    "langsmith = LangSmith()\n",
    "langsmith.trace(chain, input_data)\n",
    "\n",
    "---\n",
    "\n",
    "### **Tools**\n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "from langchain.agents import tool\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "print(f'Length of the word '{get_word_length.invoke(\"hello\")})\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57587e9",
   "metadata": {},
   "source": [
    "> Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999c8261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# Chroma DB\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "    # the Chroma database is a powerful tool for storing and querying vector embeddings. It is available locally\n",
    "    # and can be used to store embeddings generated from text, images, or other data types.\n",
    "\n",
    "!pip install chroma-db\n",
    "\n",
    "from chromadb import ChromaDB\n",
    "\n",
    "# Initialize the Chroma database\n",
    "db = ChromaDB()\n",
    "\n",
    "\n",
    "\n",
    "vector_db = Chroma.from_documents(documents = texts, # List of text documents or chunks\n",
    "                                  embedding=embeddings, persist_directory=\"db\")\n",
    "vector_db.persist() # Save the database to disk\n",
    "vector_db = None # Clear the database from memory\n",
    "vector_db = Chroma(persist_directory=\"db\",  # Load the database from disk\n",
    "                   embedding_function=embedding) \n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# PINECONE\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "!pip install pinecone-client\n",
    "\n",
    "import pinecone\n",
    "\n",
    "# Initialize Pinecone\n",
    "pinecone.init(\n",
    "    api_key=\"YOUR_API_KEY\",\n",
    "    environment=\"us-west1-gcp\"\n",
    ")\n",
    "\n",
    "# Create a Pinecone index\n",
    "index = pinecone.create_index(\"my-index\", metric=\"cosine\")  # Choose the metric based on your use case. You can also do this on the site\n",
    "index_name = \"my-index\" # Name of the index created on Pinecone site\n",
    "vector_store = Pinecone.from_texts([t.page_content for t in text_chunks], embeddings, index_name=index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# WEAVIATE\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "# FAISS\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "!pip install faiss-cpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47cf9cfc",
   "metadata": {},
   "source": [
    "> AI Agents Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105a3f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------------------------------------\n",
    "# Tavily - for web search\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "!pip install tavily-python\n",
    "\n",
    "from tavily import TavilyClient\n",
    "\n",
    "# Step 1. Instantiating your TavilyClient\n",
    "tavily_client = TavilyClient(api_key=\"tvly-YOUR_API_KEY\")\n",
    "\n",
    "# Step 2. Executing a simple search query\n",
    "response = tavily_client.search(\"Who is Leo Messi?\")\n",
    "\n",
    "# Step 2. Executing a context search query\n",
    "context = tavily_client.get_search_context(query=\"What happened during the Burning Man floods?\")\n",
    "\n",
    "# Step 3. That's it! You've done a Tavily Search!\n",
    "print(response)\n",
    "print(context)\n",
    "\n",
    "\n",
    "# -----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd364c02",
   "metadata": {},
   "source": [
    "> LLama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bb7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "my-llama-project/\n",
    " model/\n",
    "    zephyr-7b-beta.Q4_0.gguf\n",
    " data/\n",
    "    training_data.json\n",
    " scripts/\n",
    "    train.py\n",
    "    infer.py\n",
    "    summarize.py\n",
    " README.md\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# Llama.cpp provides an efficient C++ implementation for LLaMA models, optimized for CPU inference and lightweight deployment. \n",
    "# Its focus is on enabling developers to utilize LLaMA models in constrained environments, such as edge devices.\n",
    "\n",
    "\n",
    "# Installation\n",
    "\n",
    "    # Create a new Conda environment for Llama.cpp \n",
    "    conda create --name llama-cpp-env python=3.9\n",
    "    conda activate llama-cpp-env\n",
    "\n",
    "    # Install the Llama.cpp library\n",
    "    pip install llama-cpp-python\n",
    "\n",
    "    # Verify the installation: Create a file verify_llama.py:\n",
    "    from llama_cpp import Llama\n",
    "    print(\"Llama.cpp is successfully installed!\")\n",
    "\n",
    "\n",
    "# Basics\n",
    "    # Load a Pretrained Model\n",
    "    # Download a GGUF model file (e.g., Zephyr-7B from Hugging Face).\n",
    "    \n",
    "        # Load a pre-trained LLaMA model\n",
    "        from llama_cpp import Llama\n",
    "        \n",
    "        model_path = \"./model/zephyr-7b-beta.Q4_0.gguf\"\n",
    "        llama = Llama(model_path=model_path, n_ctx=512)\n",
    "\n",
    "    # Generate Text:\n",
    "    # Use the model to generate a response:\n",
    "\n",
    "        prompt = \"What is the capital of France?\"\n",
    "        output = llama(prompt, max_tokens=50, temperature=0.7, top_p=0.9)\n",
    "        response = output[\"choices\"][0][\"text\"].strip()\n",
    "        print(response)\n",
    "\n",
    "\n",
    "# Advanced Features\n",
    "    # Custom Tokenizer \n",
    "        tokenizer_path = \"./tokenizer.json\"\n",
    "        llama = Llama(model_path=model_path, tokenizer_path=tokenizer_path)\n",
    "\n",
    "    # Stop tokens\n",
    "        stop_tokens = [\" [SEP]\", \" [CLS]\"]\n",
    "        llama = Llama(model_path=model_path, stop_tokens=stop_tokens)\n",
    "    \n",
    "    # Multi-threading\n",
    "        llama = Llama(model_path=model_path, num_threads=4)\n",
    "        \n",
    "    # Custom Post-Processing\n",
    "        def custom_postprocess(output):\n",
    "            return output[\"choices\"][0][\"text\"].strip().capitalize()\n",
    "    \n",
    "        llama = Llama(model_path=model_path, postprocess_fn=custom_postprocess)\n",
    "    \n",
    "    # Batch Inference\n",
    "        batch_prompts = [\"What is the capital of France?\", \"Who wrote the novel 'Pride and Prejudice'?\"]\n",
    "        batch_output = llama(batch_prompts, max_tokens=50, temperature=0.7, top_p=0.9)\n",
    "        for output in batch_output:\n",
    "            response = output[\"choices\"][0][\"text\"].strip()\n",
    "            print(response)\n",
    "        \n",
    "    # Save and Load Model\n",
    "        llama.save(\"path/to/save/model\")\n",
    "        llama = Llama(\"path/to/saved/model\")\n",
    "        \n",
    "\n",
    "# Training\n",
    "    # Train a LLaMA model using the llama-cpp library.\n",
    "    # Create a training script train.py:\n",
    "    \n",
    "        from llama_cpp import Llama, Trainer\n",
    "        \n",
    "        model_path = \"./model/zephyr-7b-beta.Q4_0.gguf\"\n",
    "        dataset = \"./training_data.json\"\n",
    "        \n",
    "        trainer = Trainer(\n",
    "        model_path=model_path,\n",
    "        training_data=dataset,\n",
    "        epochs=5,\n",
    "        learning_rate=0.001\n",
    "        )\n",
    "        \n",
    "        trainer.train()\n",
    "    \n",
    "    \n",
    "# Real World Application\n",
    "    \n",
    "    # Text-to-Speech Integration\n",
    "    # Combine Llama.cpp with OpenAI's Speech API for building conversational agents:\n",
    "    \n",
    "        from llama_cpp import Llama\n",
    "        from openai import OpenAI\n",
    "\n",
    "        llama = Llama(model_path=\"./model/zephyr-7b-beta.Q4_0.gguf\")\n",
    "        client = OpenAI(api_key=\"your_openai_api_key\")\n",
    "\n",
    "        prompt = \"Write a short speech for a wedding toast.\"\n",
    "        text = llama(prompt)[\"choices\"][0][\"text\"]\n",
    "\n",
    "        speech_response = client.audio.speech.create(\n",
    "            model=\"tts-1\", voice=\"alloy\", input=text\n",
    "        )\n",
    "        speech_response.stream_to_file(\"speech.mp3\")\n",
    "\n",
    "    # Knowledge Assistants\n",
    "    # Deploy as a local Q&A system:\n",
    "\n",
    "        prompt = \"What are the benefits of renewable energy?\"\n",
    "        response = llama(prompt, max_tokens=100)\n",
    "        print(response[\"choices\"][0][\"text\"])\n",
    "\n",
    "\n",
    "    # Create a summarization script summarize.py:\n",
    "    \n",
    "        from llama_cpp import Llama\n",
    "        \n",
    "        model_path = \"./model/zephyr-7b-beta.Q4_0.gguf\"\n",
    "        llama = Llama(model_path=model_path)\n",
    "        \n",
    "        text = \"Summarize the following article: 'The impact of climate change on global ecosystems.'\"\n",
    "        output = llama(text, max_tokens=100, temperature=0.7, top_p=0.9)\n",
    "        summary = output[\"choices\"][0][\"text\"].strip()\n",
    "        print(summary)\n",
    "    \n",
    "    # Run the script to generate a summary of the article.\n",
    "    python\n",
    "    \n",
    "    \n",
    "# Deployment\n",
    "\n",
    "    # Dockerize the Application\n",
    "    # Create a Dockerfile to package the Llama.cpp application:\n",
    "    \n",
    "        # Dockerfile\n",
    "        FROM python:3.9\n",
    "        RUN pip install llama-cpp-python\n",
    "        COPY app.py .\n",
    "        CMD [\"python\", \"app.py\"]\n",
    "    \n",
    "    # Build the Docker image:\n",
    "    docker build -t llama-app . \n",
    "    # Run the Docker container:\n",
    "    docker run llama-app\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a4947f",
   "metadata": {},
   "source": [
    "> Finetune LLMs using LoRa and QLoRa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1df9beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# A Quick Cheat Sheet for Fine-Tuning LLMs\n",
    "# Using LoRA and QLoRA (Step-by-Step Walkthrough)\n",
    "# ============================================\n",
    "#\n",
    "# This code cell provides a concise, step-by-step cheat sheet for \n",
    "# fine-tuning Large Language Models (LLMs) using LoRA and QLoRA. \n",
    "# Each step includes sample commands, key parameters, and explanations \n",
    "# in comments. Adapt as needed for your own training scripts!\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 1: INSTALL DEPENDENCIES\n",
    "# --------------------------------------------------\n",
    "# Make sure you have the following libraries installed:\n",
    "#   - transformers (latest)\n",
    "#   - accelerate\n",
    "#   - bitsandbytes\n",
    "#   - peft\n",
    "#   - datasets\n",
    "#   - (optional) wandb for experiment tracking\n",
    "#\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 2: DATA PREPARATION\n",
    "# --------------------------------------------------\n",
    "# 1. Load your dataset(s). In many cases, you'll have train/eval/test splits.\n",
    "# 2. Define a prompt format function to guide how the data is turned into text.\n",
    "# 3. Tokenize the prompts to create input_ids and labels for causal LM.\n",
    "\n",
    "train_data = load_dataset('gem', 'viggo', split='train')\n",
    "eval_data  = load_dataset('gem', 'viggo', split='validation')\n",
    "\n",
    "# Example prompt format function\n",
    "def create_prompt(example):\n",
    "    # Customize your prompt as needed:\n",
    "    prompt = (\n",
    "        f\"Given a target sentence, construct the underlying meaning representation.\\n\\n\"\n",
    "        f\"Target Sentence:\\n{example['target']}\\n\\n\"\n",
    "        f\"Meaning Representation:\\n{example['meaning_representation']}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# We'll create a tokenizer later when we load the model. For demonstration, here's a placeholder.\n",
    "# tokenized_train_data = train_data.map(lambda x: tokenizer(create_prompt(x)), batched=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 3: LOAD THE BASE MODEL (LLAMA 2 EXAMPLE)\n",
    "# --------------------------------------------------\n",
    "# For QLoRA, we use bitsandbytes to load the model in 4-bit precision.\n",
    "# You can adapt for other LLMs as well.\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"  # Example: 7B Llama 2\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                # 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,   # Double quantization for improved memory usage\n",
    "    bnb_4bit_quant_type=\"nf4\",        # Normal Float 4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# Make sure the tokenizer has a pad_token or set it to eos.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 4: PREPARE MODEL FOR K-BIT TRAINING\n",
    "# --------------------------------------------------\n",
    "# We use PEFT's utility function to prepare the model for 8bit/4bit training. \n",
    "# This helps ensure certain layers are frozen appropriately.\n",
    "\n",
    "model.gradient_checkpointing_enable()  # Optional memory-saving trick\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 5: SET UP LoRA OR QLoRA ADAPTERS\n",
    "# --------------------------------------------------\n",
    "# LoRA config parameters:\n",
    "#   - r: Rank of the adapter matrix (larger => more capacity).\n",
    "#   - lora_alpha: Scaling factor.\n",
    "#   - lora_dropout: Dropout for LoRA layers.\n",
    "#   - target_modules: Which model modules to apply LoRA to.\n",
    "# For QLoRA, use the same approach but ensure you loaded the model in 4bit above.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)  # Apply LoRA adapters\n",
    "\n",
    "# Utility to see trainable params\n",
    "def print_trainable_parameters(m):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, p in m.named_parameters():\n",
    "        all_params += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable_params += p.numel()\n",
    "    print(f\"Trainable params: {trainable_params} | All params: {all_params} | Trainable%: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "# --------------------------------------------------\n",
    "# STEP 6: TOKENIZATION PIPELINE (UPDATED)\n",
    "# --------------------------------------------------\n",
    "# We'll tokenize with appropriate padding and truncation for your max_length.\n",
    "\n",
    "def tokenize_function(ex):\n",
    "    prompt = create_prompt(ex)\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,         # Adjust as needed\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # Causal LM training: labels match input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=False)\n",
    "tokenized_eval_data  = eval_data.map(tokenize_function, batched=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 7: TRAINING PREP WITH TRANSFORMERS\n",
    "# --------------------------------------------------\n",
    "# We'll create a Trainer or Accelerate-based training loop.\n",
    "# For demonstration, here's a minimal Trainer approach:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-qlora-output\",\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=2,   # Adjust as needed\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=25,\n",
    "    num_train_epochs=1,            # or use max_steps if you prefer\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,                     # set bf16 or fp16 as your hardware allows\n",
    "    optim=\"paged_adamw_8bit\",      # recommended for 4-bit/8-bit\n",
    "    report_to=\"none\"               # or \"wandb\" if you want to track metrics\n",
    ")\n",
    "\n",
    "# Data collator: ensure we handle LM-style tasks properly\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_eval_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 8: TRAIN THE MODEL\n",
    "# --------------------------------------------------\n",
    "# This will run the LoRA/QLoRA fine-tuning. Watch out for OOM (Out of Memory) errors.\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 9: SAVING AND LOADING THE FINETUNED ADAPTER\n",
    "# --------------------------------------------------\n",
    "# Once training completes, you can save the PEFT LoRA adapter weights with:\n",
    "trainer.save_model(\"./lora-qlora-output\")\n",
    "\n",
    "# Later, to reload, you do:\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./lora-qlora-output\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 10: INFERENCE\n",
    "# --------------------------------------------------\n",
    "# Let's see how the model performs with a test prompt after fine-tuning.\n",
    "\n",
    "sample_prompt = \"Please convert the following text into a meaning representation: ... \"\n",
    "tokens = tokenizer(sample_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output_ids = lora_model.generate(**tokens, max_new_tokens=100)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# NOTE ON QLoRA vs LoRA\n",
    "# --------------------------------------------------\n",
    "# - QLoRA: A memory-efficient form of LoRA that uses 4-bit quantization for base model weights.\n",
    "# - LoRA: Traditional approach that can run at full precision or 8-bit if loaded that way.\n",
    "#\n",
    "# Both rely on the PEFT library's approach of training rank-decomposed adapter matrices \n",
    "# while freezing most of the base model's parameters.\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DONE!\n",
    "# --------------------------------------------------\n",
    "# This cheat sheet has walked you through:\n",
    "#  1) Installing dependencies\n",
    "#  2) Loading data and formatting prompts\n",
    "#  3) Loading a base LLM in 4-bit quantization (QLoRA approach)\n",
    "#  4) Preparing the model for k-bit training\n",
    "#  5) Setting LoRA config for target modules\n",
    "#  6) Tokenizing data for causal LM\n",
    "#  7) Creating a Transformers Trainer\n",
    "#  8) Fine-tuning with LoRA/QLoRA\n",
    "#  9) Saving/loading your LoRA adapters\n",
    "# 10) Running inference with the finetuned model\n",
    "\n",
    "print(\"Cheat Sheet for LoRA & QLoRA Fine-tuning Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ecab48",
   "metadata": {},
   "source": [
    "> Chainlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab03e53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# ADVANCED CHEAT SHEET FOR CHAINLIT\n",
    "# ==============================================================================\n",
    "#\n",
    "# In this code cell, well walk through several advanced Chainlit concepts:\n",
    "#   1. Quick Setup\n",
    "#   2. Building a Basic Chatbot\n",
    "#   3. Integrating with Other Frameworks (LangChain)\n",
    "#   4. Adding File Uploads & Memory Management\n",
    "#   5. Customizing UI and Deployment Tips\n",
    "#\n",
    "# Each section is explained in-line with step-by-step instructions and example\n",
    "# code. Adapt for your own use case!\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. QUICK SETUP\n",
    "# ------------------------------------------------------------------------------\n",
    "# NOTES:\n",
    "#  - Chainlit is installed via `pip install chainlit`\n",
    "#  - Run your Chainlit scripts: `chainlit run your_script.py -w`\n",
    "#  - The `-w` flag auto-reloads on code changes.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "# 1) In your terminal:\n",
    "pip install chainlit\n",
    "\n",
    "# 2) Check installation:\n",
    "chainlit --version\n",
    "\n",
    "# 3) Create a Python script (e.g., quick_start.py) and insert:\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "def main():\n",
    "    cl.title(\"Hello, Chainlit!\")\n",
    "    cl.button(\"Click Me\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# 4) Run the script:\n",
    "chainlit run quick_start.py\n",
    "\n",
    "# Your basic Chainlit app is now live at the printed localhost URL.\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. BUILDING A BASIC CHATBOT\n",
    "# ------------------------------------------------------------------------------\n",
    "# In Chainlit, building a chatbot is straightforward with text_input, buttons,\n",
    "# and other UI components. Below is a minimal chatbot example:\n",
    "# ==============================================================================\n",
    "\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "def on_chat_start():\n",
    "    \"\"\"\n",
    "    Called once when the user session starts.\n",
    "    You can initialize session variables, show a welcome message, etc.\n",
    "    \"\"\"\n",
    "    cl.title(\"My Minimal Chatbot\")\n",
    "    cl.header(\"Welcome to the Chainlit Chatbot Demo!\")\n",
    "\n",
    "@cl.on_message\n",
    "async def on_user_message(message: str):\n",
    "    \"\"\"\n",
    "    Fired each time the user sends a message in the chat UI.\n",
    "    The 'message' parameter is a string containing the user's input.\n",
    "    \"\"\"\n",
    "    # For this simple example, just echo the users message.\n",
    "    reply_text = f\"Chatbot says: You said '{message}'\"\n",
    "    await cl.Message(content=reply_text).send()\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. INTEGRATING WITH OTHER FRAMEWORKS (E.G., LANGCHAIN)\n",
    "# ------------------------------------------------------------------------------\n",
    "# Chainlit can seamlessly integrate with libraries like LangChain for:\n",
    "#   - LLM queries\n",
    "#   - Prompt templates\n",
    "#   - Vector store retrieval\n",
    "#   - Agents & tools\n",
    "#\n",
    "# Here's a quick demonstration of how to integrate LangChain for a Q&A scenario.\n",
    "# Make sure you have `pip install langchain` and any relevant dependencies.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import chainlit as cl\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import AzureChatOpenAI   # or OpenAI, etc.\n",
    "\n",
    "# 1) Define your prompt template:\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant specialized in mathematics.\"),\n",
    "    (\"user\", \"{question}\")\n",
    "])\n",
    "\n",
    "# 2) Create a callback to build the LLMChain and store in user session:\n",
    "@cl.on_chat_start\n",
    "def initialize_chain():\n",
    "    llm = AzureChatOpenAI(\n",
    "        # Ensure environment variables or direct config for your LLM:\n",
    "        deployment_name=os.getenv(\"OPENAI_CHAT_MODEL\"),\n",
    "        temperature=0.0\n",
    "    )\n",
    "    chain = LLMChain(llm=llm, prompt=prompt_template)\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "    cl.title(\"LangChain + Chainlit Math Assistant\")\n",
    "\n",
    "# 3) On each user message, run the chain and display the result:\n",
    "@cl.on_message\n",
    "async def handle_message(message: str):\n",
    "    chain = cl.user_session.get(\"chain\")\n",
    "    result = chain.run(question=message)\n",
    "    await cl.Message(content=f\"Answer: {result}\").send()\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. ADDING FILE UPLOADS & MEMORY MANAGEMENT\n",
    "# ------------------------------------------------------------------------------\n",
    "# Often, we want to let the user upload files (PDF, CSV, etc.) and store the\n",
    "# content in memory or vector stores for retrieval augmentation. Chainlit\n",
    "# includes convenience methods for file uploads, and session-based memory using\n",
    "# `cl.user_session`.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"\n",
    "import os\n",
    "import chainlit as cl\n",
    "from dotenv import load_dotenv\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.document_loaders.pdf import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "load_dotenv()  # If you have .env with your API keys\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
    "\n",
    "@cl.on_chat_start\n",
    "async def rag_init():\n",
    "    # 1. Ask user to upload a PDF\n",
    "    files = None\n",
    "    while not files:\n",
    "        files = await cl.AskFileMessage(\n",
    "            content=\"Please upload a PDF to begin.\",\n",
    "            accept=[\"application/pdf\"],\n",
    "            max_size_mb=25\n",
    "        ).send()\n",
    "\n",
    "    # 2. Save file locally\n",
    "    file = files[0]\n",
    "    os.makedirs(\"tmp\", exist_ok=True)\n",
    "    pdf_path = f\"tmp/{file.name}\"\n",
    "    with open(pdf_path, \"wb\") as f:\n",
    "        f.write(file.content)\n",
    "\n",
    "    # 3. Process the PDF\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load_and_split(text_splitter=text_splitter)\n",
    "    embeddings = OpenAIEmbeddings(\n",
    "        openai_api_key=os.getenv(\"OPENAI_API_KEY\"),\n",
    "        openai_api_base=os.getenv(\"OPENAI_API_BASE\"),\n",
    "        deployment=os.getenv(\"OPENAI_EMBEDDING_MODEL\")\n",
    "    )\n",
    "    vectorstore = Chroma.from_documents(documents, embedding=embeddings)\n",
    "\n",
    "    # 4. Create a ConversationalRetrievalChain\n",
    "    llm = AzureChatOpenAI(deployment_name=os.getenv(\"OPENAI_CHAT_MODEL\"), temperature=0)\n",
    "    memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "    chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm, vectorstore.as_retriever(), memory=memory, return_source_documents=True\n",
    "    )\n",
    "\n",
    "    # 5. Let the user know were ready\n",
    "    cl.user_session.set(\"chain\", chain)\n",
    "    await cl.Message(content=\"PDF processed! Ask a question below.\").send()\n",
    "\n",
    "@cl.on_message\n",
    "async def rag_query(message: str):\n",
    "    chain = cl.user_session.get(\"chain\")\n",
    "    result = chain({\"question\": message})\n",
    "    answer = result[\"answer\"]\n",
    "    sources = result[\"source_documents\"]\n",
    "\n",
    "    # 6. Send back a formatted reply with sources\n",
    "    sources_str = \", \".join([doc.metadata.get(\"source\", \"No source\") for doc in sources])\n",
    "    await cl.Message(content=f\"Answer: {answer}\\n\\nSources: {sources_str}\").send()\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 5. CUSTOMIZING UI AND DEPLOYMENT TIPS\n",
    "# ------------------------------------------------------------------------------\n",
    "# Chainlit provides multiple ways to customize your UI and handle deployment:\n",
    "#   - `chainlit.md`: A Markdown file auto-generated for your welcome screen\n",
    "#   - Themes & styling (dark mode, element styles, etc.)\n",
    "#   - Docker support for containerized deployments\n",
    "#   - Integrations with Google App Engine, AWS, Heroku, etc.\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "# UI CUSTOMIZATION EXAMPLE\n",
    "import chainlit as cl\n",
    "\n",
    "@cl.on_chat_start\n",
    "def custom_ui():\n",
    "    cl.title(\"Custom Themed Chatbot\")\n",
    "    cl.header(\"This is a specialized chatbot UI!\")\n",
    "    cl.theme(\"dark\")  # switch to dark mode\n",
    "\n",
    "@cl.on_message\n",
    "async def handle_user_msg(msg: str):\n",
    "    await cl.Message(content=f\"You said: {msg}\", style=\"italic\").send()\n",
    "\n",
    "# DEPLOYMENT EXAMPLE (Dockerfile):\n",
    "#  FROM python:3.10\n",
    "#  WORKDIR /app\n",
    "#  COPY requirements.txt ./\n",
    "#  RUN pip install --no-cache-dir -r requirements.txt\n",
    "#  COPY . .\n",
    "#  CMD [\"chainlit\", \"run\", \"my_app.py\", \"-p\", \"8000\"]\n",
    "\n",
    "# Then run:\n",
    "#  docker build -t my_chainlit_app .\n",
    "#  docker run -p 8000:8000 my_chainlit_app\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# DONE - ADVANCED CHEAT SHEET COMPLETE\n",
    "# ------------------------------------------------------------------------------\n",
    "# This cheat sheet should provide you with a robust overview of Chainlits\n",
    "# advanced features, including:\n",
    "#   - Basic usage and setup\n",
    "#   - Chatbot creation (on_chat_start & on_message usage)\n",
    "#   - Integration with frameworks like LangChain\n",
    "#   - File uploads, memory management, & vector stores\n",
    "#   - UI customization & deployment tips\n",
    "#\n",
    "# Adapt these techniques to your own environment and enjoy building powerful\n",
    "# LLM-driven applications with Chainlit!\n",
    "# ==============================================================================\n",
    "print(\"Advanced Chainlit Cheat Sheet Loaded Successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1d109",
   "metadata": {},
   "source": [
    "> Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d941c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://supervision.roboflow.com/             # reusable code for computer vision \n",
    "import supervision as sv\n",
    "\n",
    "# Model Detection and Classification\n",
    "    detr = sv.Detections     # class for getting detections from model. params are: bbox (xyxy), confidence scores, labels(class_id).\n",
    "        # attributes\n",
    "        detr.class_id   # Identifier for the detected object class.\n",
    "        detr.confidence # Confidence score for the detection.\n",
    "        detr.box_area   # The area of the detection bounding box.\n",
    "        detr.mask       # The mask of the detected object, if available.\n",
    "        detr.tracker_id     # The ID assigned to the detected object by a tracker.\n",
    "        detr.area           # Alias for box_area.\n",
    "        detr.get_anchors_coordinates # Retrieve anchor coordinates for detections.\n",
    "        detr.with_nms       # Apply Non-Maximum Suppression (NMS) to filter detections\n",
    "        # initialization and conversion\n",
    "            results = model(image)[0]\n",
    "            detections = sv.Detections.from_ultralytics(results)\n",
    "        detr.from_detectron2        # Initialize from Detectron2 library output.\n",
    "        detr.from_ultralytics       # Initialize from Ultralytics output.\n",
    "        detr.from_deepsparse        # Initialize from DeepSparse output.\n",
    "        detr.from_roboflow          # Initialize from Roboflow detection output.\n",
    "        detr.from_sam               # Initialize from Segment Anything Model.\n",
    "        detr.from_transformers      # Initialize from Hugging Face's transformers output.\n",
    "        \n",
    "    clasf = sv.Classifications # function for getting classifications from model.\n",
    "        # attributes\n",
    "        clasf.confidence    # Confidence score for the classification.\n",
    "        clasf.from_clip    # Initialize from OpenAI's CLIP model output.\n",
    "        clasf.from_timm   # Initialize from PyTorch Image Models (timm) output.\n",
    "        clasf.rom_ultralytics   # Initialize from Ultralytics output.\n",
    "        clasf.get_top_k   # Get the top k classifications.\n",
    "\n",
    "# Datasets and annotators\n",
    "    sv.BaseDataset                              # Base class for creating custom datasets.\n",
    "    sv.ClassificationDataset                    # For classification tasks.\n",
    "    sv.DetectionDataset                         # For object detection tasks.\n",
    "    sv.BoundingBoxAnnotator, sv.BoxAnnotator, sv.BoxCornerAnnotator,      # Annotate bounding boxes.\n",
    "    sv.CircleAnnotator, sv.LabelAnnotator, sv.PolygonAnnotator, sv.TraceAnnotator,  # Annotate circles, labels, polygons, and traces.\n",
    "    \n",
    "# Drawing Functions\n",
    "    sv.draw_image | sv.draw_line | sv.draw_polygon | sv.draw_rectangle | sv.draw_text   # Functions to draw various shapes and text on images.\n",
    "    sv.draw_filled_rectangle                                                            # Draw filled rectangles.\n",
    "    sv.plot_image, sv.plot_images_grid                                                  # Plot single or grid of images.\n",
    "\n",
    "# Metrics and Evaluation\n",
    "    sv.metrics # Metrics for evaluation.\n",
    "    sv.ConfusionMatrix,       # Generate a confusion matrix.\n",
    "    sv.MeanAveragePrecision     # Calculate mean average precision for detection tasks.\n",
    "\n",
    "# Geometry and Tracking\n",
    "    sv.Point, sv.Position, sv.Rect       # Basic geometric entities.\n",
    "    sv.ByteTrack                         # Tracking algorithm implementation.\n",
    "    sv.LineZone, sv.PolygonZone          # Define zones as lines or polygons.\n",
    "    \n",
    "# Annotators (More detailed)\n",
    "    sv.BlurAnnotator                         # Apply blur effect to annotations.\n",
    "    sv.BoundingBoxAnnotator, sv.BoxAnnotator, sv.BoxCornerAnnotator     # Annotate bounding boxes.\n",
    "    sv.CircleAnnotator                                      # Annotate circles.\n",
    "    sv.ColorAnnotator                                       # Apply color to annotations.\n",
    "    sv.DotAnnotator                                         # Annotate single points.\n",
    "    sv.EllipseAnnotator                                     # Annotate ellipses.\n",
    "    sv.LabelAnnotator                                       # Add labels to annotations.\n",
    "    sv.LineZoneAnnotator                                    # Annotate line zones.\n",
    "    sv.MaskAnnotator                                        # Annotate with masks.\n",
    "    sv.PixelateAnnotator                                    # Pixelate certain areas.\n",
    "    sv.PolygonAnnotator                                     # Annotate polygons.\n",
    "    sv.PolygonZoneAnnotator                                 # Annotate polygon zones.\n",
    "    sv.TraceAnnotator                                       # Trace annotations.\n",
    "    sv.TriangleAnnotator                                    # Annotate triangles.\n",
    "    \n",
    "# Utilities\n",
    "    sv.Color, sv.ColorLookup, sv.ColorPalette       # Utilities for handling colors.\n",
    "    sv.FPSMonitor      # Monitor frames per second in video processing.\n",
    "    sv.ImageSink, sv.VideoSink      # save an image or video in a target directory.\n",
    "    sv.InferenceSlicer      # Slice images for inference.\n",
    "    \n",
    "    # code for videos\n",
    "    sv.VideoInfo        # Retrieve video information. \n",
    "    sv.process_video        # Process video files.\n",
    "    sv.get_video_frames_generator    # Utility function to get video frames as a generator.\n",
    "\n",
    "    sv.box_iou_batch        # Calculate Intersection over Union (IoU) for batches of boxes.\n",
    "    sv.calculate_dynamic_line_thickness, sv.calculate_dynamic_text_scale        # Calculate dynamic sizes based on image dimensions.\n",
    "    sv.calculate_masks_centroids       # Calculate centroids of masks.\n",
    "    sv.list_files_with_extensions       # List files in a directory with specific extensions.\n",
    "    sv.polygon_to_mask, sv.polygon_to_xyxy      # Convert polygons to masks or bounding box coordinates.\n",
    "     \n",
    "# Data Manipulation and Processing\n",
    "    sv.crop_image       # Crop images.\n",
    "    sv.filter_polygons_by_area      # Filter polygons based on area.\n",
    "    sv.get_polygon_center, sv.get_video_frames_generator        # Utility functions for polygons and video frame processing.\n",
    "    sv.mask_to_polygons, sv.mask_to_xyxy        # Convert masks to polygons or bounding box coordinates.\n",
    "    sv.move_boxes, sv.non_max_suppression, sv.scale_boxes       # Manipulate bounding boxes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc252ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examples:\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "# initiate polygon zone\n",
    "polygon = np.array([\n",
    "    [540,  985],\n",
    "    [1620, 985],\n",
    "    [2160, 1920],\n",
    "    [1620, 2855],\n",
    "    [540,  2855],\n",
    "    [0,    1920]\n",
    "])\n",
    "\n",
    "video_info = sv.VideoInfo.from_video_path(MALL_VIDEO_PATH)   # Get video information from a specific video file path.\n",
    "zone = sv.PolygonZone(polygon=polygon, frame_resolution_wh=video_info.resolution_wh)\n",
    "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "zone_annotator = sv.PolygonZoneAnnotator(zone=zone, color=sv.Color.white(), thickness=6, text_thickness=6, text_scale=4)\n",
    "\n",
    "\n",
    "# extract video frame\n",
    "generator = sv.get_video_frames_generator(MALL_VIDEO_PATH)\n",
    "iterator = iter(generator)\n",
    "frame = next(iterator)\n",
    "\n",
    "# detect\n",
    "results = model(frame, imgsz=1280)[0]\n",
    "detections = sv.Detections(\n",
    "    xyxy=results[\"instances\"].pred_boxes.tensor.cpu().numpy(),\n",
    "    confidence=results[\"instances\"].scores.cpu().numpy(),\n",
    "    class_id=results[\"instances\"].pred_classes.cpu().numpy().astype(int)\n",
    ")\n",
    "mask = zone.trigger(detections=detections)\n",
    "\n",
    "# annotate\n",
    "box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n",
    "frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "frame = zone_annotator.annotate(scene=frame)\n",
    "\n",
    "\n",
    "def process_frame(frame: np.ndarray, _) -> np.ndarray:\n",
    "    # detect\n",
    "    results = model(frame, imgsz=1280)[0]\n",
    "    detections = sv.Detections.from_yolov8(results)\n",
    "    detections = detections[detections.class_id == 0]\n",
    "    zone.trigger(detections=detections)\n",
    "\n",
    "    # annotate\n",
    "    box_annotator = sv.BoxAnnotator(thickness=4, text_thickness=4, text_scale=2)\n",
    "    labels = [f\"{model.names[class_id]} {confidence:0.2f}\" for _, confidence, class_id, _ in detections]\n",
    "    frame = box_annotator.annotate(scene=frame, detections=detections, labels=labels)\n",
    "    frame = zone_annotator.annotate(scene=frame)\n",
    "\n",
    "    return frame\n",
    "\n",
    "sv.process_video(source_path=MALL_VIDEO_PATH, target_path=f\"{HOME}/mall-result.mp4\", callback=process_frame)\n",
    "\n",
    "\n",
    "%matplotlib inline  \n",
    "sv.show_frame_in_notebook(frame, (16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47a2773",
   "metadata": {},
   "source": [
    "> Detectron2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d58f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/facebookresearch/detectron2 \n",
    "\n",
    "import cv2  \n",
    "import detectron2\n",
    "from detectron2.utils.logger import setup_logger\n",
    "setup_logger()\n",
    "\n",
    "# Import necessary modules\n",
    "from detectron2 import model_zoo                    # to access all the models available \n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer, ColorMode \n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "# Step 1: Register the dataset with Detectron2 \n",
    "######################################################################################################################################\n",
    "\n",
    "from detectron2.data import MetadataCatalog, DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances\n",
    "\n",
    "# Step 1: Define the dataset format and paths (do for train, val, and test datasets)\n",
    "dataset_name = \"fruit_dataset\"\n",
    "json_file = \"/path/to/your/annotations/train.json\"  # Update this path\n",
    "image_root = \"/path/to/your/fruit_images\"  # Update this path\n",
    "\n",
    "# Step 2: Register the dataset\n",
    "# This function registers the dataset in COCO format with Detectron2 and makes it accessible via the DatasetCatalog and MetadataCatalog.\n",
    "register_coco_instances(name = dataset_name, \n",
    "                        metadata={}, \n",
    "                        json_file=json_file, \n",
    "                        image_root=image_root)                        # train dataset\n",
    "register_coco_instances(\"dataset_val_name\", {}, \"/path/to/your/annotations/val.json\", \"/path/to/your/fruit_images\")  # val dataset \n",
    "register_coco_instances(\"dataset_test_name\", {}, \"/path/to/your/annotations/test.json\", \"/path/to/your/fruit_images\")  # test dataset \n",
    "\n",
    "# Optional: Verify dataset registration and metadata (e.g., class names)\n",
    "fruit_metadata = MetadataCatalog.get(dataset_name)\n",
    "print(f\"Registered dataset with metadata: {fruit_metadata}\")\n",
    "# or\n",
    "[ data_set for data_set in MetadataCatalog.list() if data_set.startswith(DATA_SET_NAME) ]\n",
    "\n",
    "\n",
    "MetadataCatalog.get(dataset_name).set(thing_classes=[\"apple\", \"orange\", \"banana\"])    # class names\n",
    "DatasetCatalog.register(dataset_name, MetadataCatalog.get(dataset_name))              # dataset metadata\n",
    "\n",
    "######################################################################################################################################\n",
    "# Step 2: Load the model configuration and pre-trained model\n",
    "######################################################################################################################################\n",
    "\n",
    "# The following code snippet loads a pre-trained object detection model from Detectron2's model zoo. The specific model used here is \"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\", which is pre-trained on the COCO dataset. You can change the model configuration and pre-trained weights based on your specific use case and requirements.\n",
    "\n",
    "setup_logger() # Set up Detectron2 logger\n",
    "\n",
    "def load_model(model_name, *args, **kwargs):\n",
    "        # Load model configuration and pre-trained model from the model zoo\n",
    "        cfg = get_cfg()\n",
    "        # finetune from a pretrained model\n",
    "\n",
    "        cfg.SOLVER.IMS_PER_BATCH = 2\n",
    "        cfg.SOLVER.BASE_LR = 0.0025\n",
    "        cfg.SOLVER.MAX_ITER = 1000\n",
    "        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128\n",
    "        cfg.MODEL.DEVICE = 'cuda'   #cpu or cuda \n",
    "        \n",
    "        # Dataset & DataLoader settings\n",
    "        cfg.DATASETS.TRAIN = (\"dataset_train\",)\n",
    "        cfg.DATASETS.TEST = (\"dataset_val\",)\n",
    "        cfg.DATALOADER.NUM_WORKERS = 4  # Number of data loading workers\n",
    "\n",
    "        # Solver (Optimization) settings\n",
    "        cfg.SOLVER.IMS_PER_BATCH = 16  # Images per batch\n",
    "        cfg.SOLVER.BASE_LR = 0.001  # Base Learning Rate\n",
    "        cfg.SOLVER.MAX_ITER = 10000  # Number of iterations (updates to the model)\n",
    "        cfg.SOLVER.STEPS = (7000, 9000)  # Points to decay the learning rate\n",
    "        cfg.SOLVER.GAMMA = 0.1  # Learning rate decay multiplier\n",
    "        cfg.SOLVER.WARMUP_ITERS = 1000  # Warm-up iterations before learning rate scheduler starts\n",
    "        cfg.SOLVER.WARMUP_FACTOR = 1.0 / 1000  # Starting factor for learning rate\n",
    "        cfg.SOLVER.WARMUP_METHOD = \"linear\"  # Method for learning rate warm-up\n",
    "\n",
    "        # Model settings\n",
    "        cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128  # Number of proposals to sample for training\n",
    "        cfg.MODEL.ROI_HEADS.NUM_CLASSES = 20  # Number of classes in your dataset\n",
    "        cfg.MODEL.BACKBONE.FREEZE_AT = 2  # Freeze the first k stages of the backbone\n",
    "        cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # Confidence threshold for predictions during testing\n",
    "\n",
    "        # Output directory\n",
    "        cfg.OUTPUT_DIR = \"./output\"\n",
    "\n",
    "        # Checkpoint settings\n",
    "        cfg.SOLVER.CHECKPOINT_PERIOD = 1000  # Save a checkpoint after every N iterations\n",
    "        \n",
    "        # Load a pre-defined config from Detectron2's model zoo\n",
    "        cfg.merge_from_file(model_zoo.get_config_file(model_name))\n",
    "        cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(model_name)\n",
    "        predictor = DefaultPredictor(cfg)  # Create a predictor from a config file. It requires the model weights to be available on the machine.\n",
    "        return predictor, cfg \n",
    "\n",
    "                # model_zoo.get_config_file(\"COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml\")        # object detection-COCO dataset\n",
    "                # model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")  # instance segmentation-COCO dataset\n",
    "                # model_zoo.get_config_file(\"COCO-PanopticSegmentation/panoptic_fpn_R_50_3x.yaml\")  # panoptic segmentation-COCO dataset\n",
    "                # model_zoo.get_config_file(\"COCO-Keypoints/keypoint_rcnn_R_50_FPN_3x.yaml\")        # keypoint detection-COCO dataset\n",
    "\n",
    "predictor, cfg = load_model(model_name, *args, **kwargs)\n",
    "\n",
    "######################################################################################################################################\n",
    "# Step 3: Train the Detectron2 model\n",
    "######################################################################################################################################\n",
    "import logging\n",
    "from collections import OrderedDict\n",
    "from detectron2.engine import DefaultTrainer, DEFAULT_TIMEOUT, default_argument_parser, default_setup, hooks, launch\n",
    "from detectron2.evaluation import COCOEvaluator, inference_on_dataset\n",
    "from detectron2.data import build_detection_test_loader \n",
    "from detectron2.evaluation import (\n",
    "    DatasetEvaluator,\n",
    "    DatasetEvaluators,\n",
    "    inference_on_dataset,\n",
    "    print_csv_format,\n",
    ")\n",
    "from detectron2.config import CfgNode\n",
    "from detectron2.utils import comm\n",
    "import os \n",
    "from typing import List, Optional, Union\n",
    "from densepose.modeling.cse import Embedder\n",
    "\n",
    "\n",
    "# Extend the DefaultTrainer to use a custom evaluator\n",
    "class Trainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def build_evaluator(cls, cfg, dataset_name, output_folder=None):\n",
    "        if output_folder is None:\n",
    "            os.makedirs(cfg.OUTPUT_DIR + \"/coco_eval\", exist_ok=True)\n",
    "            output_folder = cfg.OUTPUT_DIR + \"/coco_eval\"\n",
    "        return COCOEvaluator(dataset_name, cfg, False, output_folder)\n",
    "\n",
    "# or \n",
    "class Trainer(DefaultTrainer):\n",
    "    @classmethod\n",
    "    def extract_embedder_from_model(cls, model: nn.Module) -> Optional[Embedder]:\n",
    "        if isinstance(model, nn.parallel.DistributedDataParallel):\n",
    "            model = model.module\n",
    "        if hasattr(model, \"roi_heads\") and hasattr(model.roi_heads, \"embedder\"):\n",
    "            return model.roi_heads.embedder\n",
    "        return None\n",
    "\n",
    "    # TODO: the only reason to copy the base class code here is to pass the embedder from\n",
    "    # the model to the evaluator; that should be refactored to avoid unnecessary copy-pasting\n",
    "    @classmethod\n",
    "    def test(\n",
    "        cls,\n",
    "        cfg: CfgNode,\n",
    "        model: nn.Module,\n",
    "        evaluators: Optional[Union[DatasetEvaluator, List[DatasetEvaluator]]] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            cfg (CfgNode):\n",
    "            model (nn.Module):\n",
    "            evaluators (DatasetEvaluator, list[DatasetEvaluator] or None): if None, will call\n",
    "                :meth:`build_evaluator`. Otherwise, must have the same length as\n",
    "                ``cfg.DATASETS.TEST``.\n",
    "\n",
    "        Returns:\n",
    "            dict: a dict of result metrics\n",
    "        \"\"\"\n",
    "        logger = logging.getLogger(__name__)\n",
    "        if isinstance(evaluators, DatasetEvaluator):\n",
    "            evaluators = [evaluators]\n",
    "        if evaluators is not None:\n",
    "            assert len(cfg.DATASETS.TEST) == len(evaluators), \"{} != {}\".format(\n",
    "                len(cfg.DATASETS.TEST), len(evaluators)\n",
    "            )\n",
    "\n",
    "        results = OrderedDict()\n",
    "        for idx, dataset_name in enumerate(cfg.DATASETS.TEST):\n",
    "            data_loader = cls.build_test_loader(cfg, dataset_name)\n",
    "            # When evaluators are passed in as arguments,\n",
    "            # implicitly assume that evaluators can be created before data_loader.\n",
    "            if evaluators is not None:\n",
    "                evaluator = evaluators[idx]\n",
    "            else:\n",
    "                try:\n",
    "                    embedder = cls.extract_embedder_from_model(model)\n",
    "                    evaluator = cls.build_evaluator(cfg, dataset_name, embedder=embedder)\n",
    "                except NotImplementedError:\n",
    "                    logger.warn(\n",
    "                        \"No evaluator found. Use `DefaultTrainer.test(evaluators=)`, \"\n",
    "                        \"or implement its `build_evaluator` method.\"\n",
    "                    )\n",
    "                    results[dataset_name] = {}\n",
    "                    continue\n",
    "            if cfg.DENSEPOSE_EVALUATION.DISTRIBUTED_INFERENCE or comm.is_main_process():\n",
    "                results_i = inference_on_dataset(model, data_loader, evaluator)\n",
    "            else:\n",
    "                results_i = {}\n",
    "            results[dataset_name] = results_i\n",
    "            if comm.is_main_process():\n",
    "                assert isinstance(\n",
    "                    results_i, dict\n",
    "                ), \"Evaluator must return a dict on the main process. Got {} instead.\".format(\n",
    "                    results_i\n",
    "                )\n",
    "                logger.info(\"Evaluation results for {} in csv format:\".format(dataset_name))\n",
    "                print_csv_format(results_i)\n",
    "\n",
    "        if len(results) == 1:\n",
    "            results = list(results.values())[0]\n",
    "        return results\n",
    "    \n",
    "# either write the trainer class or use the default trainer below\n",
    "\n",
    "# Initialize the trainer and start training\n",
    "trainer = Trainer(cfg)\n",
    "trainer.resume_or_load(resume=False)\n",
    "if cfg.TEST.AUG.ENABLED:\n",
    "    trainer.register_hooks(\n",
    "        [hooks.EvalHook(0, lambda: trainer.test_with_TTA(cfg, trainer.model))]\n",
    "    )\n",
    "trainer.train()\n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "# Step 4: Make Predictions an image/video\n",
    "######################################################################################################################################\n",
    "\n",
    "# Load an image for object detection\n",
    "image_path = \"path/to/object_detection_image.jpg\"\n",
    "im = cv2.imread(image_path)\n",
    "\n",
    "metadata = MetadataCatalog.get(TRAIN_DATA_SET_NAME)     # get the metadata for the dataset used during training (should be the same as in step 1)\n",
    "dataset_train = DatasetCatalog.get(TRAIN_DATA_SET_NAME)\n",
    "\n",
    "# OBJECT DETECTION\n",
    "# Make object detection prediction\n",
    "outputs = predictor(im)\n",
    "# Visualize object detection predictions\n",
    "v = Visualizer(im[:, :, ::-1], metadata= MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2, \n",
    "               instance_mode=ColorMode.IMAGE_BW) \n",
    "                    # ColorMode.IMAGE \n",
    "                    # ColorMode.IMAGE_BW\n",
    "                    # ColorMode.SEGMENTATION\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "cv2.imshow(\"Object Detection\", v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "# INSTANCE SEGMENTATION                - change the predictor to a segmentation model \n",
    "# Make image segmentation prediction\n",
    "outputs = predictor(im)\n",
    "# Visualize image segmentation predictions\n",
    "v = Visualizer(im[:, :, ::-1], metadata= MetadataCatalog.get(cfg.DATASETS.TRAIN[0]), scale=1.2, \n",
    "               instance_mode=ColorMode.IMAGE_BW)\n",
    "v = v.draw_sem_seg(outputs[\"sem_seg\"].argmax(dim=0).to(\"cpu\"))\n",
    "cv2.imshow(\"Image Segmentation\", v.get_image()[:, :, ::-1])\n",
    "cv2.waitKey(0)\n",
    "\n",
    "        # Visualizer.draw_instance_predictions()\n",
    "        # Visualizer.draw_panoptic_seg()\n",
    "        # Visualizer.draw_panoptic_seg_predictions()\n",
    "        # Visualizer.draw_sem_seg() \n",
    "        \n",
    "\n",
    "\n",
    "######################################################################################################################################\n",
    "# Structure of your dataset should look like this:\n",
    "######################################################################################################################################\n",
    "\n",
    "'''\n",
    "dataset-directory/\n",
    " README.dataset.txt\n",
    " README.roboflow.txt\n",
    " train\n",
    "   train-image-1.jpg\n",
    "   train-image-1.jpg\n",
    "   ...\n",
    "   _annotations.coco.json\n",
    " test\n",
    "   test-image-1.jpg\n",
    "   test-image-1.jpg\n",
    "   ...\n",
    "   _annotations.coco.json\n",
    " valid\n",
    "    valid-image-1.jpg\n",
    "    valid-image-1.jpg\n",
    "    ...\n",
    "    _annotations.coco.json\n",
    "\n",
    "\n",
    "\n",
    "annotations.coco.json\n",
    "{\n",
    "    \"info\":{\n",
    "        \"year\":\"2024\",\n",
    "        \"version\":\"5\",\n",
    "        \"description\":\"Exported from roboflow.com\",\n",
    "        \"contributor\":\"\",\n",
    "        \"url\":\"https://public.roboflow.com/object-detection/undefined\",\n",
    "        \"date_created\":\"2024-02-06T02:29:57+00:00\"},\n",
    "        \"licenses\":[{\"id\":1,\"url\":\"https://creativecommons.org/licenses/by/4.0/\",\"name\":\"CC BY 4.0\"}],\n",
    "    \"categories\":[\n",
    "        {\"id\":0,\"name\":\"Comodities\",\"supercategory\":\"none\"},\n",
    "        {\"id\":1,\"name\":\"agriculture\",\"supercategory\":\"Comodities\"},\n",
    "        {\"id\":2,\"name\":\"building\",\"supercategory\":\"Comodities\"},\n",
    "        {\"id\":3,\"name\":\"cloud\",\"supercategory\":\"Comodities\"},\n",
    "        {\"id\":4,\"name\":\"forest\",\"supercategory\":\"Comodities\"}],\n",
    "    \"images\":[\n",
    "        {\"id\":0,\"license\":1,\"file_name\":\"PAN_4e82d531-6f00-4f9e-9201-09903f2d0ab2_png.rf.2df4e8b1a5ebcb9eda1b690d6eeef51f.jpg\",\"height\":256,\"width\":256,\"date_captured\":\"2024-02-06T02:29:57+00:00\"},\n",
    "        {\"id\":1,\"license\":1,\"file_name\":\"PAN_3d1811f6-ef40-4899-a745-67978477fc8b_png.rf.430b9a64ae7949049d6511b755928f17.jpg\",\"height\":256,\"width\":256,\"date_captured\":\"2024-02-06T02:29:57+00:00\"},\n",
    "        {\"id\":2,\"license\":1,\"file_name\":\"PAN_13a6b3b2-73e9-4442-a7c3-b7810de0d389_png.rf.76ff23e585fc7cb319e01f42d4e4d728.jpg\",\"height\":256,\"width\":256,\"date_captured\":\"2024-02-06T02:29:57+00:00\"},\n",
    "        {\"id\":3,\"license\":1,\"file_name\":\"PAN_1a45bec3-98b0-4fdd-ac68-a8b7943b792e_png.rf.46993983202dc374d5fd2d596b3e8c1a.jpg\",\"height\":256,\"width\":256,\"date_captured\":\"2024-02-06T02:29:57+00:00\"}],\n",
    "    \"annotations\":[\n",
    "        {\"id\":0,\"image_id\":0,\"category_id\":4,\"bbox\":[0,0,256,255.93],\"area\":65518.08,\"segmentation\":[[0,-0.005,256,-0.005,256,255.925,0,255.925]],\"iscrowd\":0},\n",
    "        {\"id\":1,\"image_id\":0,\"category_id\":7,\"bbox\":[73,0,116.25,49.25],\"area\":5725.313,\"segmentation\":[[96.5,49.25,106.25,49,137.75,33,151.25,32.5,166.25,25,180.75,25,189,14.75,188.5,4.75,184.444,0,72.75,0,84.5,36.75,92,48.25]],\"iscrowd\":0},{\"id\":2,\"image_id\":0,\"category_id\":7,\"bbox\":[0,0,187.778,256],\"area\":48071.117,\"segmentation\":[[187.778,256,164.25,247,158.75,250.5,149.5,230.25,139.5,224.75,131.25,206.5,116.5,230.75,113,230.75,123.5,208.25,116.5,200.25,124,199.75,113,196.75,124,195.25,117,193.75,116.5,183.25,108.5,174.75,107,163.25,97.5,158.25,98.75,152.5,94.25,152,93.25,156,92.25,152,87.5,152.75,90.25,159.5,95.5,160.25,88.75,167.5,60.5,167.25,60,160.75,56,159.25,60,152.25,55.5,150.75,55.5,144.75,59.75,144,60,151.75,64.75,153,73.75,151.5,73.5,144.75,68.75,140,67.75,144,61.5,142.25,59.5,120.25,62.5,119.25,58.25,119,54.75,112,49.75,115,38.25,111,40.5,108.75,35.5,107.25,34.75,99.5,28.25,99.5,25,107.25,27,98.75,32,96.25,30.25,91.5,25,95.25,25,89.75,31.5,88.75,23.75,88,20.5,79.25,27.5,40.25,10.5,14.75,10,3.75,0,0,0,256]],\"iscrowd\":0},\n",
    "        {\"id\":3,\"image_id\":1,\"category_id\":4,\"bbox\":[0,0,256,256],\"area\":65536,\"segmentation\":[[0,0,256,0,256,256,0,256]],\"iscrowd\":0}]\n",
    "}\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f7bf61",
   "metadata": {},
   "source": [
    "> YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd69374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting YOLO Model Architecture and Weights in PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from ultralytics import YOLO\n",
    "from torchsummary import summary\n",
    "\n",
    "\n",
    "def load_yolo_checkpoint(checkpoint_path):\n",
    "    \"\"\"\n",
    "    Load a YOLO checkpoint, extract architecture and weights.\n",
    "\n",
    "    Args:\n",
    "    - checkpoint_path (str): Path to the YOLO checkpoint.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Module: PyTorch model extracted from the YOLO checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load YOLO checkpoint\n",
    "        checkpoint = YOLO(checkpoint_path)\n",
    "        print(\"YOLO checkpoint loaded successfully.\")\n",
    "\n",
    "        yolo_model = checkpoint.model # YOLO model architecture and weights (if available) - For Classification and Detection \n",
    "        print(\"Model extracted from checkpoint.\")\n",
    "        if not isinstance(yolo_model, nn.Module):\n",
    "            raise ValueError(\"Extracted model is not a valid PyTorch nn.Module.\")\n",
    "\n",
    "        # Ensure it is a PyTorch nn.Module\n",
    "        if isinstance(yolo_model, nn.Module):\n",
    "            print(\"Model is a valid PyTorch nn.Module.\")\n",
    "            return yolo_model\n",
    "        else:\n",
    "            raise ValueError(\"The extracted model is not a PyTorch nn.Module.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading YOLO checkpoint: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def save_pytorch_model(model, save_path):\n",
    "    \"\"\"\n",
    "    Save the PyTorch model architecture and weights.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The PyTorch model to save.\n",
    "    - save_path (str): Path to save the model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        torch.save(model, save_path)\n",
    "        print(f\"Model saved to {save_path}.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving PyTorch model: {e}\")\n",
    "\n",
    "\n",
    "def load_pytorch_model(load_path):\n",
    "    \"\"\"\n",
    "    Load a saved PyTorch model.\n",
    "\n",
    "    Args:\n",
    "    - load_path (str): Path to the saved PyTorch model.\n",
    "\n",
    "    Returns:\n",
    "    - nn.Module: Loaded PyTorch model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        model = torch.load(load_path)\n",
    "        print(\"PyTorch model loaded successfully.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading PyTorch model: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def inspect_model(model):\n",
    "    \"\"\"\n",
    "    Inspect the PyTorch model's architecture and parameters.\n",
    "\n",
    "    Args:\n",
    "    - model (nn.Module): The PyTorch model to inspect.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(\"\\n--- Model Architecture ---\")\n",
    "        # print(model)\n",
    "\n",
    "        # Check if model is a valid PyTorch module\n",
    "        if not isinstance(model, nn.Module):\n",
    "            raise TypeError(\"The loaded model is not a valid PyTorch nn.Module.\")\n",
    "\n",
    "        print(\"\\n--- Model Parameters ---\")\n",
    "        for name, param in model.named_parameters():\n",
    "            # print(f\"Parameter: {name}, Shape: {param.shape}\")\n",
    "            pass # Comment out to avoid printing all parameters\n",
    "\n",
    "        print(\"\\n--- Generate a summary if possible\")\n",
    "        try:\n",
    "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "            model = model.to(device)\n",
    "            print(type(model))\n",
    "            if isinstance(model, list):\n",
    "                model = model[0]  # Extract the first item\n",
    "            elif isinstance(model, dict):\n",
    "                model = model.get('model', None)  # Extract the 'model' key\n",
    "\n",
    "            dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
    "            summary(model, input_size=(3, 640, 640), device=device.type)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating model summary: {e}\")\n",
    "\n",
    "    except TypeError as e:\n",
    "        print(f\"Model Type Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error inspecting model: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "# Workflow in Jupyter Notebook\n",
    "checkpoint_path = \"yolo11n-cls.pt\"  # Path to YOLO checkpoint for Classification\n",
    "# checkpoint_path = \"yolo11n.pt\"  # Path to YOLO checkpoint for Detection\n",
    "save_path = \"pytorch_model.pth\"  # Path to save extracted PyTorch model\n",
    "\n",
    "# Step 1: Load YOLO checkpoint and extract PyTorch model\n",
    "extracted_model = load_yolo_checkpoint(checkpoint_path)\n",
    "\n",
    "if extracted_model:\n",
    "    # Step 2: Save as standalone PyTorch model\n",
    "    save_pytorch_model(extracted_model, save_path)\n",
    "\n",
    "    # Step 3: Load saved PyTorch model\n",
    "    loaded_model = load_pytorch_model(save_path)\n",
    "\n",
    "    if loaded_model:\n",
    "        # Step 4: Inspect and validate the loaded model\n",
    "        inspect_model(loaded_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "363a1ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "# ================================\n",
    "# DIFFERENT WAYS TO BUILD A MODEL\n",
    "# ================================\n",
    "\n",
    "model = YOLO('yolov8n.pt', task='detect')  # load a pretrained object detection model (start with this if you are training)\n",
    "    # model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
    "    # model = YOLO('yolov8n-s.yaml')  # build a YOLOv8n segmentation model from YAML\n",
    "    # model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n pose model\n",
    "    # model = YOLO('yolov8n.yaml', task='detect')  # build a YOLOv8n detection model from YAML\n",
    "model = YOLO('yolov8n.yaml').load('yolov8n.pt') # build from YAML and transfer weights from a pretrained model\n",
    "\n",
    "# task: detect # (str) YOLO task, i.e. detect, segment, classify, pose\n",
    "# mode: train # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark\n",
    "\n",
    "# Important model functions \n",
    "results = model.train(data='coco128.yaml', epochs=100, imgsz=640, device=[0, 1]) # train a YOLOv8n model for 100 epochs on 640x640 images in two GPUs\n",
    "model.predict(source='bus.jpg', stream = False, save=True, imgsz=320, conf=0.5, show_boxes=True) # predict on an image\n",
    "    # results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "    # if stream = True, it treats the input source as a continuous video stream for predictions\n",
    "model.val()     # evaluate model performance on the validation set\n",
    "model.export(format = \"onnx\")  # Export a YOLOv8 model to any supported format (other formar: torchscript, paddle, openvino, coreml, tflite, savedmodel, etc.)\n",
    "model.track(source=\"https://youtu.be/LNwODJXcvt4\", tracker=\"bytetrack.yaml\", stream = False, persist=True, conf=0.3, iou=0.5, show=True)   # track objects in a video\n",
    "model.benchmark()   # benchmark the model's speed and accuracy\n",
    "\n",
    "\n",
    "results = model.train(resume=True)  # resume training after interrrupted session\n",
    "model.tune(data=\"/kaggle/working/data.yaml\",          # fine-tune a YOLOv8 model on a custom dataset\n",
    "           task=\"obb\", batch=64, imgsz=640, device=(0, 1),\n",
    "           epochs=65, iterations=50,\n",
    "           plots=True, save=True, val=True, \n",
    "          )\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"coco128.yaml\", epochs=3)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image\n",
    "    # results[0].plot()\n",
    "    # results[0].boxes()\n",
    "    # results[0].names\n",
    "    # results[0].masks()\n",
    "    # results[0].xyxy[0]\n",
    "    \n",
    "path = model.export(format=\"onnx\")  # export the model to ONNX format\n",
    "\n",
    "# Validate the model\n",
    "metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "metrics.box.map    # map50-95\n",
    "metrics.box.map50  # map50\n",
    "metrics.box.map75  # map75\n",
    "metrics.box.maps   # a list contains map50-95 of each category\n",
    "\n",
    "# using YOLO on video file or webcam\n",
    "model = YOLO('yolov8n.pt')\n",
    "\n",
    "video_path = \"path/to/your/video/file.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLOv8 inference on the frame\n",
    "        results = model(frame)\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLOv8 Inference\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "# other functions\n",
    "model.MODE(ARGS)    # Run MODE mode using the custom arguments ARGS \n",
    "\n",
    "\n",
    "# Using CLI\n",
    "!yolo task=detect mode=train model=yolov8s.pt data=data/data.yaml epochs=25 imgsz=800 plots=True\n",
    "!yolo task=segment mode=predict model=yolov8x-seg.pt source='input/video_3.mp4' show=True\n",
    "!yolo task=pose mode=predict source=bus.jpg show = True \n",
    "!yolo task=detect mode=track source=video.mp4 output=output.avi conf=cfg/yolov5s.yaml weights=yolov5s.pt\n",
    "!yolo task=benchmark model=yolov8n.pt data='coco8.yaml' imgsz=640 half=False device=0\n",
    "\n",
    "\n",
    "----------------------------------------------------------------\n",
    "# Move the trained runs into colab folder \n",
    "import shutil\n",
    "\n",
    "# Define the source and destination paths\n",
    "source_path = '/content/runs'\n",
    "destination_path = '/content/gdrive/MyDrive/Colab Notebooks/Licence-plate/runs'\n",
    "\n",
    "shutil.move(source_path, destination_path)  # Move the folder from source to destination path \n",
    "----------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "# YOLO data.yaml format  for training\n",
    "path: \"C:\\Users\\pault\\Documents\\5b. Datasets\\traffic signs detection\"\n",
    "train: data/train/images   # Path to the training images (relative to this file).\n",
    "val: data/valid/images    # Path to the validation images (relative to this file).\n",
    "test: ../test/images    # Path to the test images (relative to this file).\n",
    "nc: 5   # Number of classes\n",
    "names: ['Green Light', 'Red Light', 'Speed Limit 10', 'Speed Limit 100', 'Speed Limit 110']   # Class names. NB: # index must match the order of the class names here.\n",
    "# or\n",
    "names:\n",
    "- '0'\n",
    "- '1'\n",
    "- '2'\n",
    "- '3'\n",
    "- A\n",
    "- B\n",
    "- C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409e25d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics YOLO , AGPL-3.0 license\n",
    "# Default training settings and hyperparameters for medium-augmentation COCO training\n",
    "\n",
    "task: detect # (str) YOLO task, i.e. detect, segment, classify, pose\n",
    "mode: train # (str) YOLO mode, i.e. train, val, predict, export, track, benchmark\n",
    "\n",
    "# Train settings -------------------------------------------------------------------------------------------------------\n",
    "model: # (str, optional) path to model file, i.e. yolov8n.pt, yolov8n.yaml\n",
    "data: # (str, optional) path to data file, i.e. coco128.yaml\n",
    "epochs: 100 # (int) number of epochs to train for\n",
    "time: # (float, optional) number of hours to train for, overrides epochs if supplied\n",
    "patience: 50 # (int) epochs to wait for no observable improvement for early stopping of training\n",
    "batch: 16 # (int) number of images per batch (-1 for AutoBatch)\n",
    "imgsz: 640 # (int | list) input images size as int for train and val modes, or list[w,h] for predict and export modes\n",
    "save: True # (bool) save train checkpoints and predict results\n",
    "save_period: -1 # (int) Save checkpoint every x epochs (disabled if < 1)\n",
    "cache: False # (bool) True/ram, disk or False. Use cache for data loading\n",
    "device: # (int | str | list, optional) device to run on, i.e. cuda device=0 or device=0,1,2,3 or device=cpu\n",
    "workers: 8 # (int) number of worker threads for data loading (per RANK if DDP)\n",
    "project: # (str, optional) project name\n",
    "name: # (str, optional) experiment name, results saved to 'project/name' directory\n",
    "exist_ok: False # (bool) whether to overwrite existing experiment\n",
    "pretrained: True # (bool | str) whether to use a pretrained model (bool) or a model to load weights from (str)\n",
    "optimizer: auto # (str) optimizer to use, choices=[SGD, Adam, Adamax, AdamW, NAdam, RAdam, RMSProp, auto]\n",
    "verbose: True # (bool) whether to print verbose output\n",
    "seed: 0 # (int) random seed for reproducibility\n",
    "deterministic: True # (bool) whether to enable deterministic mode\n",
    "single_cls: False # (bool) train multi-class data as single-class\n",
    "rect: False # (bool) rectangular training if mode='train' or rectangular validation if mode='val'\n",
    "cos_lr: False # (bool) use cosine learning rate scheduler\n",
    "close_mosaic: 10 # (int) disable mosaic augmentation for final epochs (0 to disable)\n",
    "resume: False # (bool) resume training from last checkpoint\n",
    "amp: True # (bool) Automatic Mixed Precision (AMP) training, choices=[True, False], True runs AMP check\n",
    "fraction: 1.0 # (float) dataset fraction to train on (default is 1.0, all images in train set)\n",
    "profile: False # (bool) profile ONNX and TensorRT speeds during training for loggers\n",
    "freeze: None # (int | list, optional) freeze first n layers, or freeze list of layer indices during training\n",
    "multi_scale: False # (bool) Whether to use multi-scale during training\n",
    "# Segmentation\n",
    "overlap_mask: True # (bool) masks should overlap during training (segment train only)\n",
    "mask_ratio: 4 # (int) mask downsample ratio (segment train only)\n",
    "# Classification\n",
    "dropout: 0.0 # (float) use dropout regularization (classify train only)\n",
    "\n",
    "# Val/Test settings ----------------------------------------------------------------------------------------------------\n",
    "val: True # (bool) validate/test during training\n",
    "split: val # (str) dataset split to use for validation, i.e. 'val', 'test' or 'train'\n",
    "save_json: False # (bool) save results to JSON file\n",
    "save_hybrid: False # (bool) save hybrid version of labels (labels + additional predictions)\n",
    "conf: # (float, optional) object confidence threshold for detection (default 0.25 predict, 0.001 val)\n",
    "iou: 0.7 # (float) intersection over union (IoU) threshold for NMS\n",
    "max_det: 300 # (int) maximum number of detections per image\n",
    "half: False # (bool) use half precision (FP16)\n",
    "dnn: False # (bool) use OpenCV DNN for ONNX inference\n",
    "plots: True # (bool) save plots and images during train/val\n",
    "\n",
    "# Predict settings -----------------------------------------------------------------------------------------------------\n",
    "source: # (str, optional) source directory for images or videos\n",
    "vid_stride: 1 # (int) video frame-rate stride\n",
    "stream_buffer: False # (bool) buffer all streaming frames (True) or return the most recent frame (False)\n",
    "visualize: False # (bool) visualize model features\n",
    "augment: False # (bool) apply image augmentation to prediction sources\n",
    "agnostic_nms: False # (bool) class-agnostic NMS\n",
    "classes: # (int | list[int], optional) filter results by class, i.e. classes=0, or classes=[0,2,3]\n",
    "retina_masks: False # (bool) use high-resolution segmentation masks\n",
    "embed: # (list[int], optional) return feature vectors/embeddings from given layers\n",
    "\n",
    "# Visualize settings ---------------------------------------------------------------------------------------------------\n",
    "show: False # (bool) show predicted images and videos if environment allows\n",
    "save_frames: False # (bool) save predicted individual video frames\n",
    "save_txt: False # (bool) save results as .txt file\n",
    "save_conf: False # (bool) save results with confidence scores\n",
    "save_crop: False # (bool) save cropped images with results\n",
    "show_labels: True # (bool) show prediction labels, i.e. 'person'\n",
    "show_conf: True # (bool) show prediction confidence, i.e. '0.99'\n",
    "show_boxes: True # (bool) show prediction boxes\n",
    "line_width: # (int, optional) line width of the bounding boxes. Scaled to image size if None.\n",
    "\n",
    "# Export settings ------------------------------------------------------------------------------------------------------\n",
    "format: torchscript # (str) format to export to, choices at https://docs.ultralytics.com/modes/export/#export-formats\n",
    "keras: False # (bool) use Kera=s\n",
    "optimize: False # (bool) TorchScript: optimize for mobile\n",
    "int8: False # (bool) CoreML/TF INT8 quantization\n",
    "dynamic: False # (bool) ONNX/TF/TensorRT: dynamic axes\n",
    "simplify: False # (bool) ONNX: simplify model\n",
    "opset: # (int, optional) ONNX: opset version\n",
    "workspace: 4 # (int) TensorRT: workspace size (GB)\n",
    "nms: False # (bool) CoreML: add NMS\n",
    "\n",
    "# Hyperparameters ------------------------------------------------------------------------------------------------------\n",
    "lr0: 0.01 # (float) initial learning rate (i.e. SGD=1E-2, Adam=1E-3)\n",
    "lrf: 0.01 # (float) final learning rate (lr0 * lrf)\n",
    "momentum: 0.937 # (float) SGD momentum/Adam beta1\n",
    "weight_decay: 0.0005 # (float) optimizer weight decay 5e-4\n",
    "warmup_epochs: 3.0 # (float) warmup epochs (fractions ok)\n",
    "warmup_momentum: 0.8 # (float) warmup initial momentum\n",
    "warmup_bias_lr: 0.1 # (float) warmup initial bias lr\n",
    "box: 7.5 # (float) box loss gain\n",
    "cls: 0.5 # (float) cls loss gain (scale with pixels)\n",
    "dfl: 1.5 # (float) dfl loss gain\n",
    "pose: 12.0 # (float) pose loss gain\n",
    "kobj: 1.0 # (float) keypoint obj loss gain\n",
    "label_smoothing: 0.0 # (float) label smoothing (fraction)\n",
    "nbs: 64 # (int) nominal batch size\n",
    "hsv_h: 0.015 # (float) image HSV-Hue augmentation (fraction)\n",
    "hsv_s: 0.7 # (float) image HSV-Saturation augmentation (fraction)\n",
    "hsv_v: 0.4 # (float) image HSV-Value augmentation (fraction)\n",
    "degrees: 0.0 # (float) image rotation (+/- deg)\n",
    "translate: 0.1 # (float) image translation (+/- fraction)\n",
    "scale: 0.5 # (float) image scale (+/- gain)\n",
    "shear: 0.0 # (float) image shear (+/- deg)\n",
    "perspective: 0.0 # (float) image perspective (+/- fraction), range 0-0.001\n",
    "flipud: 0.0 # (float) image flip up-down (probability)\n",
    "fliplr: 0.5 # (float) image flip left-right (probability)\n",
    "mosaic: 1.0 # (float) image mosaic (probability)\n",
    "mixup: 0.0 # (float) image mixup (probability)\n",
    "copy_paste: 0.0 # (float) segment copy-paste (probability)\n",
    "auto_augment: randaugment # (str) auto augmentation policy for classification (randaugment, autoaugment, augmix)\n",
    "erasing: 0.4 # (float) probability of random erasing during classification training (0-1)\n",
    "crop_fraction: 1.0 # (float) image crop fraction for classification evaluation/inference (0-1)\n",
    "\n",
    "# Custom config.yaml ---------------------------------------------------------------------------------------------------\n",
    "cfg: # (str, optional) for overriding defaults.yaml\n",
    "\n",
    "# Tracker settings ------------------------------------------------------------------------------------------------------\n",
    "tracker: botsort.yaml # (str) tracker type, choices=[botsort.yaml, bytetrack.yaml]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45679954",
   "metadata": {},
   "source": [
    "> DeepSORT Object Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2d39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single Object Trackers:\n",
    "    # we only track a single object, no matter how many objects are present in the frame. e.g. CSRT, KCF, MOSSE, etc.\n",
    "\n",
    "# Multi Object Trackers:\n",
    "    # if there is more than one person in the frame, we track all of them separately. e.g. SORT, DeepSORT, JDE, CenterTrack etc.\n",
    "\n",
    "\n",
    "# Simple Online Realtime Tracking (SORT):\n",
    "    # SORT is a simple and real-time tracker for multiple objects. It is based on the idea of minimizing a cost function that consists of both the distance between the predicted object location and the new detection, and the size of the predicted object bounding box and the new detection.\n",
    "\n",
    "# https://github.com/nwojke/deep_sort   # clone DeepSORT repository\n",
    "# or use the one by computervisionengineer  https://github.com/computervisioneng/deep_sort \n",
    "\n",
    "##################################################################################\n",
    "# Create a custom tracker \"tracker.py\" file for the tracker settings  you want to use     #\n",
    "# You can copy and modify this template or create a new file from scratch        #\n",
    "##################################################################################\n",
    "\n",
    "from deep_sort.deep_sort.tracker import Tracker as DeepSortTracker\n",
    "from deep_sort.tools import generate_detections as gdet\n",
    "from deep_sort.deep_sort import nn_matching\n",
    "from deep_sort.deep_sort.detection import Detection\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Tracker:\n",
    "    tracker = None\n",
    "    encoder = None\n",
    "    tracks = None\n",
    "\n",
    "    def __init__(self):\n",
    "        max_cosine_distance = 0.4\n",
    "        nn_budget = None\n",
    "\n",
    "        encoder_model_filename = 'model_data/mars-small128.pb'\n",
    "\n",
    "        metric = nn_matching.NearestNeighborDistanceMetric(\"cosine\", max_cosine_distance, nn_budget)\n",
    "        self.tracker = DeepSortTracker(metric)\n",
    "        self.encoder = gdet.create_box_encoder(encoder_model_filename, batch_size=1)\n",
    "\n",
    "    def update(self, frame, detections):\n",
    "\n",
    "        if len(detections) == 0:\n",
    "            self.tracker.predict()\n",
    "            self.tracker.update([])  \n",
    "            self.update_tracks()\n",
    "            return\n",
    "\n",
    "        bboxes = np.asarray([d[:-1] for d in detections])\n",
    "        bboxes[:, 2:] = bboxes[:, 2:] - bboxes[:, 0:2]\n",
    "        scores = [d[-1] for d in detections]\n",
    "\n",
    "        features = self.encoder(frame, bboxes)\n",
    "\n",
    "        dets = []\n",
    "        for bbox_id, bbox in enumerate(bboxes):\n",
    "            dets.append(Detection(bbox, scores[bbox_id], features[bbox_id]))\n",
    "\n",
    "        self.tracker.predict()\n",
    "        self.tracker.update(dets)\n",
    "        self.update_tracks()\n",
    "\n",
    "    def update_tracks(self):\n",
    "        tracks = []\n",
    "        for track in self.tracker.tracks:\n",
    "            if not track.is_confirmed() or track.time_since_update > 1:\n",
    "                continue\n",
    "            bbox = track.to_tlbr()\n",
    "\n",
    "            id = track.track_id\n",
    "\n",
    "            tracks.append(Track(id, bbox))\n",
    "\n",
    "        self.tracks = tracks\n",
    "\n",
    "\n",
    "class Track:\n",
    "    track_id = None\n",
    "    bbox = None\n",
    "\n",
    "    def __init__(self, id, bbox):\n",
    "        self.track_id = id\n",
    "        self.bbox = bbox\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################################################################################\n",
    "# You can then use the custom tracker in the YOLO model by setting the tracker argument # to an instance of your own Tracker class.###############################################\n",
    "########################################################################################\n",
    "#                              EVALUATION CODE                                          #\n",
    "########################################################################################\n",
    "    \n",
    "# Example usage:\n",
    "from tracker import Tracker\n",
    "import cv2\n",
    "\n",
    "def main():\n",
    "    # Create a tracker instance\n",
    "    tracker = Tracker()\n",
    "\n",
    "    colors = [(random.randint(0, 255), random.randint(0, 255), random.randint(0, 255)) for j in range(10)] # create a list of random colors for the bounding boxes\n",
    "    # Create an evaluator instance\n",
    "    # evaluator = TrackerEvaluator(tracker)           # create an instance of the TrackerEvaluator class (This is not important, only use the Tracker class)\n",
    "\n",
    "    # Load the video\n",
    "    cap = cv2.VideoCapture('video.mp4')\n",
    "\n",
    "    detection_threshold = 0.5\n",
    "    \n",
    "    # Loop through the video frames\n",
    "    while cap.isOpened():\n",
    "        # Read a frame from the video\n",
    "        success, frame = cap.read()\n",
    "\n",
    "        if success:\n",
    "            # Run object detection on the frame\n",
    "            results = model(frame)\n",
    "        \n",
    "            # Update the tracking system with the current\n",
    "            for result in results:\n",
    "                detections = []\n",
    "                for r in result.boxes.data.tolist():\n",
    "                    x1, y1, x2, y2, score, class_id = r\n",
    "                    x1 = int(x1)\n",
    "                    x2 = int(x2)\n",
    "                    y1 = int(y1)\n",
    "                    y2 = int(y2)\n",
    "                    class_id = int(class_id)\n",
    "                    if score > detection_threshold:\n",
    "                        detections.append([x1, y1, x2, y2, score])\n",
    "\n",
    "                tracker.update(frame, detections)\n",
    "\n",
    "                for track in tracker.tracks:\n",
    "                    bbox = track.bbox\n",
    "                    x1, y1, x2, y2 = bbox\n",
    "                    track_id = track.track_id\n",
    "\n",
    "                    cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (colors[track_id % len(colors)]), 3)\n",
    "\n",
    "            # Break the loop if 'q' is pressed\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                break\n",
    "        else:\n",
    "            # Break the loop if the end of the video is reached\n",
    "            break\n",
    "\n",
    "    # Release the video capture object and close the display window\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d489a2",
   "metadata": {},
   "source": [
    "> Argparse Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0345c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys \n",
    "\n",
    "parser = argparse.ArgumentParser(description='SORT demo')   # create a parser object to store the command line arguments\n",
    "parser.add_argument('--display', dest='display', help='Display online tracker output (slow) [False]', action='store_true')\n",
    "parser.add_argument('--task', type=str, choices=['detect', 'classify', 'segment', 'pose'], default='detect', help='Task to perform.')\n",
    "parser.add_argument(\"--seq_path\", help=\"Path to detections.\", required=False, type=str, default='data')\n",
    "args = parser.parse_args()  # parse the command line arguments and store them in the args object\n",
    "\n",
    "python main.py --display --task classify --seq_path /your/custom/seq_path\n",
    "\n",
    "\n",
    "# Here's a brief explanation of some of the common parameters:\n",
    "name or flags   # The name of the command-line argument (e.g., '--myoption'). For positional arguments, only the name is specified (e.g., 'myargument').\n",
    "action  # Defines the action to be taken when the argument is encountered. Some common values include:\n",
    "    'store' # Just stores the arguments value (default action).\n",
    "    'store_true'/'store_false'  # Stores True/False when the argument is present/absent.\n",
    "    'append'    # Stores a list, appending each argument value to the list.\n",
    "    'count' # Counts the number of times a keyword argument occurs.\n",
    "nargs   # Indicates the number of command-line arguments that should be consumed. Can be an integer or special characters like '+', '*', or '?'.\n",
    "const   # A constant value required by some action and nargs selections.\n",
    "default # The value produced if the command-line argument is absent from the command line.\n",
    "type    # The type to which the command-line argument should be converted. This can be a Python type like int, float, str, or a function that takes a single string argument and returns the converted value.\n",
    "choices # A container of the allowable values for the argument. If the command-line argument is outside this container, the program will raise an error.\n",
    "required    # Whether or not the command-line option may be omitted (optionals only).\n",
    "help    # A brief description of what the argument does, which is displayed in the help message.\n",
    "metavar # A name for the argument in usage messages.\n",
    "dest    # The name of the attribute to be added to the object returned by parse_args(). By default, argparse derives the value from the command-line argument, removing the initial -- prefix and replacing - with _.\n",
    "\n",
    "\n",
    "# main.py file structure\n",
    "\n",
    "def parse_args():\n",
    "    \"\"\"Parse input arguments.\"\"\"\n",
    "    parser = argparse.ArgumentParser(description='Model training and inference script')\n",
    "    parser.add_argument('--display', dest='display', help='Display online tracker output (slow) [False]', action='store_true')\n",
    "    parser.add_argument(\"--seq_path\", help=\"Path to detections.\", type=str, default='data')\n",
    "    parser.add_argument('--task', type=str, choices=['detect', 'classify', 'segment', 'pose'], default='detect', help='Task to perform.')\n",
    "    parser.add_argument('--mode', type=str, choices=['train', 'test', 'predict'], default='train', help='Mode of operation.')\n",
    "    parser.add_argument('--data', type=str, required=True, help='Path to the data file.')\n",
    "    parser.add_argument(\"--iou_threshold\", help=\"Minimum IOU for match.\", type=float, default=0.3)\n",
    "    parser.add_argument(\"--epochs\", help=\"Number of training epochs.\", type=int, default=10)\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "args = parse_args()\n",
    "# print(f\"Task: {args.task}\")\n",
    "# print(f\"Mode: {args.mode}\")\n",
    "# print(f\"Data Path: {args.data}\")\n",
    "# print(f\"IOU Threshold: {args.iou_threshold}\")\n",
    "# print(f\"Epochs: {args.epochs}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# use this to run the script\n",
    "python main.py --task recognition --mode train --data \"path/to/data/file\" --epochs 25\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64892f41",
   "metadata": {},
   "source": [
    "> Neptune AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de93c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install neptune\n",
    "\n",
    "# Step 1: Sign Up and Create a project\n",
    "    # Go to the Neptune app,\n",
    "    # Sign up for an account if you havent already,\n",
    "    # Click on New project button on the left,\n",
    "    # Give it a name,\n",
    "    # Decide whether you want it to be public or private,\n",
    "    # Done.\n",
    "    \n",
    "# Step 2: Get your API token: You will need a Neptune API token (your personal key) to connect the scripts you run with Neptune.\n",
    "    # Click on your user logo on the right side of the screen,\n",
    "    # Click on Get Your API token ,\n",
    "    # Copy your API token,\n",
    "    # Paste it to the environment variable, config file, or directly to your script.\n",
    "\n",
    "# Step 3: Install Neptune client library:\n",
    "pip install neptune\n",
    "\n",
    "# Step 4: Initialize and Configuring Neptune Run\n",
    "import neptune\n",
    "run = neptune.init_run(\n",
    "  project=\"workspace/project-name\",\n",
    "  api_token=\"your-api-token\",\n",
    ")\n",
    "\n",
    "##################################################################\n",
    "# You can also register and track ML models\n",
    "##################################################################\n",
    "import neptune\n",
    "model = neptune.init_model(\n",
    "    name=\"Prediction model\",\n",
    "    key=\"MOD\", \n",
    "    project=\"gra-research/cytovance-research\", \n",
    "    api_token=\"add-api-token\", # your credentials\n",
    ")\n",
    "\n",
    "########################################################################################################\n",
    "# You can also store project level metadata like datasets, models, python notebooks, images etc.\n",
    "########################################################################################################\n",
    "import neptune\n",
    "project = neptune.init_project(project=\"gra-research/cytovance-research\", \n",
    "                               api_token=\"add-api-token\")\n",
    "\n",
    "project[\"general/brief\"] = URL_TO_PROJECT_BRIEF\n",
    "project[\"general/data_analysis\"].upload(\"data_analysis.ipynb\")\n",
    "project[\"dataset/v0.1\"].track_files(\"s3://datasets/images\")\n",
    "project[\"dataset/latest\"] = project[\"dataset/v0.1\"].fetch() # fetch the latest version of the dataset\n",
    "########################################################################################################\n",
    "\n",
    "\n",
    "# Logging Metadata to Neptune\n",
    "\n",
    "# Log Parameters\n",
    "params = {\n",
    "    \"activation\": \"ReLU\", \n",
    "    \"datasetRandomState\": randomState, \n",
    "    \"lr\": learningRate,\n",
    "    \"trainingEpochs\": trainingEpochs,\n",
    "    \"trainingBatchSize\": 128,\n",
    "    \"inputSize\": 224 * 224 * 3,\n",
    "    \"randomSeed\": randomState,\n",
    "    \"numberOfClasses\": 2,\n",
    "}\n",
    "run[\"model/parameters\"] = params\n",
    "\n",
    "\n",
    "# Log Metrics\n",
    "for epoch in num_epochs:\n",
    "    run[\"train/loss\"].append(epoch, loss)       \n",
    "    run[\"train/accuracy\"].log(epoch, accuracy)\n",
    "    run['training/accuracy'] = 0.95\n",
    "    run[\"sys/tags\"].add([\"colab\", \"simple\"]) # Adds tags or other categorical metadata to the experiment\n",
    "    run[\"validation/accuracy\"].log(epoch, accuracy)\n",
    "    # run[\"validation/accuracy\"].append(validationAccuracy) # appending the loss and accuracy values to the list 'metrics' under 'losses' and 'accuracy'.\n",
    "\n",
    "# track files\n",
    "run[\"data_versions/train\"].track_files(\"sample.csv\")\n",
    "\n",
    "# Log model architecture and summary \n",
    "from neptune.utils import stringify_unsupported\n",
    "run[\"model/model_summary\"] = model.summary()  # model.summary() is the summary of the model architecture\n",
    "run[\"model/archicture\"] = stringify_unsupported(model)\n",
    "run[\"model/checkpoints\"].upload('path/to/your_checkpoint.pth')\n",
    "\n",
    "# Log Images (e.g., heatmaps, images etc.)\n",
    "run[\"validation/heatmap\"].upload(heatmap)\n",
    "\n",
    "# Log text or just anything using the '=' operator\n",
    "run['summary'] = 'This experiment explores the effect of learning rate on accuracy.'\n",
    "run['training/accuracy'] = 0.95\n",
    "\n",
    "# Log Artifacts\n",
    "run[\"model/checkpoints\"].upload(\"path/to/your/model.pth\")\n",
    "run['visualizations'].upload('path/to/your_plot.png')\n",
    "\n",
    "# Log tags\n",
    "run[\"tags\"].add(\"experiment-1\", 'another_tag')\n",
    "\n",
    "# Log Dataframes or track files\n",
    "run['data/sample'].upload(neptune.types.File.as_html(your_dataframe))\n",
    "run[\"validation/metrics\"].upload('path/to/your_metrics.csv')\n",
    "run[\"data_versions/train\"].track_files(\"sample.csv\")\n",
    "\n",
    "# Querying and retriving data from Neptune\n",
    "metrics = run[\"validation/metrics\"].get()   # get the metrics from the run\n",
    "params = run['parameters'].fetch()          # Retrieve Logged Data:\n",
    "losses = run['train/loss'].fetch_values()   # Retrieve Logged Data:\n",
    "run['model/checkpoints'].download('destination/path/checkpoint.pth')    # Downloading Artifacts\n",
    "\n",
    "# Close the run when you are done with it to free up resources\n",
    "run.stop()\n",
    "\n",
    "# Download all runs as a pandas DataFrame\n",
    "project = neptune.init_project(\n",
    "    project=\"YOUR-WORKSPACE/tutorial\",\n",
    "    mode=\"read-only\",\n",
    "        )\n",
    "\n",
    "runs_table_df = project.fetch_runs_table().to_pandas()\n",
    "\n",
    "\n",
    "# helpful tips\n",
    "log     # Usage: run['some/metric'].log(value)\n",
    "        # Purpose: Logs a single data point for a given metric or parameter. It's commonly used inside training loops to log metrics \n",
    "        # like loss or accuracy at each iteration or epoch.\n",
    "    \n",
    "append  # Usage: run[\"train/loss\"].append(1, 0.5)  # Append data point (epoch 1, loss 0.5)\n",
    "        # Purpose: The append method is typically used when you want to track the progression of a metric or parameter over time. \n",
    "        # Each time you call append with new data, Neptune will store that data point along with its associated timestamp.\n",
    "\n",
    "add     # Usage: run['sys/tags'].add(['tag1', 'tag2'])\n",
    "        # Purpose: Adds tags or other categorical metadata to the experiment. Tags help in categorizing, filtering, and organizing \n",
    "        # experiments in Neptune.\n",
    "\n",
    "assign  # Usage: run['parameters'].assign({'lr': 0.01, 'batch_size': 32})\n",
    "        # Purpose: Assigns a dictionary of parameters to the experiment. Useful for logging hyperparameters and other parameters \n",
    "        # at the beginning of an experiment.\n",
    "                        \n",
    "fetch   # Usage: run['some/metric'].fetch() returns a pandas Series object containing all logged values for this metric.\n",
    "\n",
    "fetch_values # Usage: run['train/loss'].fetch_values()\n",
    "             # To retrieve all logged values for a metric. Returns a pandas Series object containing all logged values for this metric.\n",
    "             \n",
    "upload  # Usage: run['artifacts/my_model'].upload('path/to/model')\n",
    "        # Purpose: Uploads files or artifacts like model checkpoints, plots, or data files to Neptune. Useful for versioning and \n",
    "        # sharing artifacts.\n",
    "        \n",
    "download # Usage: run['artifacts/my_model'].download('destination/path')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "##################################################################################################\n",
    "\n",
    "# The project tab is for storing project-level metadata. It can be used to store datasets as artifacts,\n",
    "project = neptune.init_project(\n",
    "  name = \"your_project\",\n",
    "  api_token = \"your_api_token\"\n",
    ")\n",
    "\n",
    "# to store datasets or link to them\n",
    "project['general'] = {\n",
    "        'data_link':'link_to_data', \n",
    "    }\n",
    "# or \n",
    "project[\"datasets/raw\"].upload(\"path/to/your/dataset\")\n",
    "\n",
    "\n",
    "# to save models and create many versions of models\n",
    "model = neptune.init_model(\n",
    "    name=\"Prediction model\",\n",
    "    key=\"MOD\", \n",
    "    project=\"your_project\", \n",
    "    api_token=\"your_api_token\", # your credentials\n",
    ")\n",
    "\n",
    "# after the model is created, to create a version of the model\n",
    "model_version = neptune.init_model_version(\n",
    "        model = 'NEP-MOD',\n",
    "        project = 'your_project',\n",
    "        name = 'first',\n",
    "        api_token=\"your_api_token\",\n",
    ")\n",
    "\n",
    "model_version['model'].upload('model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab847088",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9901f74a",
   "metadata": {},
   "source": [
    "### Computer Vision Frameworks and Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88be2b9",
   "metadata": {},
   "source": [
    "| Framework/Tool | Description | Models/Applications | Common Usage | Performance | Data Format |\n",
    "|----------------|-------------|---------------------|--------------|-------------|-------------|\n",
    "| YOLO           | Real-time object detection system | Various versions (v1-v5) for improved accuracy and speed | Object detection in images/videos | High performance in real-time applications | Requires specific formatting for annotations, often in a separate `.txt` file for each image, and a `.yaml` file for dataset configuration |\n",
    "| Detectron2     | Facebook AI's library for object detection and segmentation | Models like Faster R-CNN, Mask R-CNN for detection and segmentation | Object detection, instance and panoptic segmentation | High accuracy, suitable for research and production | Uses JSON for annotations, often in COCO format, with details on bounding boxes, segmentation masks, etc. |\n",
    "| DeepSORT       | Tracking algorithm | Used in conjunction with detection models like YOLO for tracking objects | Object tracking in video sequences | Good for applications where object tracking over time is crucial | Depends on the detection model used for initial object identification |\n",
    "| TIMM           | PyTorch Image Models | A collection of SOTA deep learning models for image classification and more | Image classification, transfer learning | Offers a wide range of models with various performance levels | Standard image datasets in formats like ImageNet, with labels typically in `.txt` files or within dataset folders |\n",
    "| PaddleOCR      | PaddlePaddle's OCR tool | Lightweight models for text detection and recognition | Text detection and recognition in images | High accuracy with relatively fast inference | Images as input, annotations can vary but often include coordinates in `.txt` files or JSON format |\n",
    "| MediaPipe      | Framework for building multimodal applied ML pipelines | Face, hand, pose detection, holistic tracking | Real-time, interactive applications | Optimized for mobile and edge devices | Uses internal data formats, often processing raw video frames directly |\n",
    "| CVZone         | Simplified computer vision modules | Modules for face detection, color tracking, etc. | Rapid prototyping of CV applications | Designed for ease of use over cutting-edge performance | Works with standard image and video formats, annotations are not typically required |\n",
    "| Supervision (Roboflow) | Platform for creating and managing computer vision datasets | Provides tools for dataset creation, annotation, and preprocessing. Integrates with ML models for detection and segmentation. | Used for building, annotating, and exporting datasets for computer vision models. | Performance depends on the models and frameworks used. | Datasets are managed within the platform, with export options in various formats for compatibility with different models and frameworks. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a960dde1",
   "metadata": {},
   "source": [
    "| Task                                          | SOTA Models                                                   |\n",
    "|-----------------------------------------------|--------------------------------------------------------------|\n",
    "| Image Classification                         | EfficientNet, ViT (Vision Transformer), ResNet, DenseNet, RegNet |\n",
    "| Facial Recognition and Analysis               | ArcFace, FaceNet, VGGFace, DeepFace                           |\n",
    "| Optical Character Recognition (OCR)           | Tesseract OCR, Paddle OCR, Google Cloud Vision OCR, OCRopus, Rosoka |\n",
    "| Object Detection                              | YOLOvx, SSD, Faster R-CNN, Mask R-CNN, Detectron2                          |\n",
    "| Deep Segmentation                             | U-Net, SegNet, DeeplabV3, Mask R-CNN                           |\n",
    "| Segment Anything Model (SAM)                  | SAM                                                          |\n",
    "| Body Pose Estimation                          | OpenPose, HRNet, PoseNet                                      |\n",
    "| Tracking with DeepSORT                        | DeepSORT (Deep Simple Online and Realtime Tracking)          |\n",
    "| Deep Fakes                                    | DeepFakeDetection, XceptionNet, MesoNet                       |\n",
    "| Vision Transformers (VIT)                     | Vision Transformers (VIT)                                    |\n",
    "| Depth Estimation                              | Monodepth2, MiDaS, DeepDepth                                  |\n",
    "| Image Similarity using Metric learning         | SimCLR, Triplet Loss                                          |\n",
    "| Image Captioning                              | Show, Attend, and Tell (SAT), Transformer-based captioning models |\n",
    "| Video Classification (CNN+RNN, Transformers)  | CNN + RNN architectures, Transformer-based video models       |\n",
    "| Point Cloud Classification and Segmentation   | PointNet, PointNet++, KPConv                                  |\n",
    "| Neural Radiance Fields (NeRFs)                | NeRF, PlenOctrees                                             |\n",
    "| 3D Vision and Lidar                           | Volumetric CNNs, RangeNet++, SqueezeSegV2                      |\n",
    "| 3D Reconstruction and Scene Reconstruction    | MVSNet, COLMAP, Bundle Fusion                                 |\n",
    "|     --                                          |                                                               |\n",
    "|Frameworks                                     | Detectron2, YOLO, Pytorch, Opencv, cvzone, DeepSort, Supervision            |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655dc4b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a47f414",
   "metadata": {},
   "source": [
    "### ML Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33677c01",
   "metadata": {},
   "source": [
    "> Docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1cb07975",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDocker is a platform for developing, shipping, and running applications inside lightweight, portable containers. \\nIt enables developers to package an application with all of its dependencies into a standardized unit called a *container*.\\n\\n\\n**Containers**: Containers are isolated units of software that contain everything needed to run an application (code, runtime, \\nsystem tools, libraries, and settings). They are portable and can run consistently across different computing environments.\\n\\n\\n**Images**: An image is a read-only template used to create Docker containers. Images can be built from a set of instructions in a \\n*Dockerfile* or pulled from repositories such as Docker Hub.\\n\\n\\n**Docker Hub**: Docker Hub is a cloud-based registry where Docker users can create, store, and share containers and images. \\nIt hosts official and community-contributed images.\\n\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Docker is a platform for developing, shipping, and running applications inside lightweight, portable containers. \n",
    "It enables developers to package an application with all of its dependencies into a standardized unit called a *container*.\n",
    "\n",
    "\n",
    "**Containers**: Containers are isolated units of software that contain everything needed to run an application (code, runtime, \n",
    "system tools, libraries, and settings). They are portable and can run consistently across different computing environments.\n",
    "\n",
    "\n",
    "**Images**: An image is a read-only template used to create Docker containers. Images can be built from a set of instructions in a \n",
    "*Dockerfile* or pulled from repositories such as Docker Hub.\n",
    "\n",
    "\n",
    "**Docker Hub**: Docker Hub is a cloud-based registry where Docker users can create, store, and share containers and images. \n",
    "It hosts official and community-contributed images.\n",
    "\n",
    "'''\n",
    "\n",
    "# Basic Setup Instructions\n",
    "\n",
    "# Install Docker on Linux\n",
    "curl -sSL https://get.docker.com/ | sh        # Quick install script provided by Docker\n",
    "\n",
    "# Install Docker on macOS (using Homebrew)\n",
    "brew install --cask docker                    # Install Docker Desktop on macOS via Homebrew\n",
    "\n",
    "# Install Docker on Windows 10\n",
    "choco install docker-desktop                  # Install Docker Desktop using Chocolatey package manager\n",
    "\n",
    "# Run a Test Container After Installation\n",
    "docker run hello-world       # Run a test container to confirm Docker is installed correctly\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7b563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "### Step-by-Step Guide to Package and Run an Existing Application Using Docker\n",
    "'''\n",
    "Assuming you have an application (e.g., an AI app) located in a directory called `my_app`. Below are detailed Docker steps to package \n",
    "and run your application in a container, focusing solely on Docker commands and configurations.\n",
    "\n",
    "'''\n",
    "\n",
    "# Step 1: Navigate to Your Application Directory\n",
    "cd /path/to/my_app    # Change directory to where your application code resides\n",
    "\n",
    "# Step 2: Create a Dockerfile in Your Application Directory\n",
    "# This Dockerfile specifies how to build your Docker image\n",
    "# Example Dockerfile content:\n",
    "\"\"\"\n",
    "# Use an official Python runtime as a parent image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set the working directory in the container\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy the current directory contents into the container at /app\n",
    "COPY . /app\n",
    "\n",
    "# Install any needed packages specified in requirements.txt\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Expose port 8000 for the application\n",
    "EXPOSE 8000\n",
    "\n",
    "# Define environment variable (if needed)\n",
    "ENV ENVIRONMENT=production\n",
    "\n",
    "# Run the application\n",
    "CMD [\"python\", \"app.py\"]\n",
    "\"\"\"\n",
    "# Save this content in a file named 'Dockerfile' in your application directory\n",
    "\n",
    "# Step 3: Create a .dockerignore File (Optional)\n",
    "# This file tells Docker which files/directories to ignore when building the image\n",
    "# Example .dockerignore content:\n",
    "\"\"\"\n",
    ".git\n",
    "__pycache__\n",
    "*.pyc\n",
    ".env\n",
    "\"\"\"\n",
    "# Save this content in a file named '.dockerignore' in your application directory\n",
    "\n",
    "# Step 4: Build the Docker Image\n",
    "docker build -t my_app_image:latest .    # Build an image named 'my_app_image' using the Dockerfile in the current directory\n",
    "\n",
    "# Step 5: Verify the Docker Image Was Created\n",
    "docker images                             # List all Docker images to confirm 'my_app_image' is present\n",
    "\n",
    "docker image ls                          # Alternative command to list Docker images\n",
    "# Step 6: Run the Docker Container from the Image\n",
    "docker run -d -p 8000:8000 --name my_app_container my_app_image   # Run the container in detached mode, mapping port 8000\n",
    "\n",
    "# Step 7: Verify the Docker Container Is Running\n",
    "docker ps                                  # List running containers to verify 'my_app_container' is up\n",
    "\n",
    "# Step 8: Access Your Application\n",
    "# If your app is a web service, you can access it via http://localhost:8000 or use curl\n",
    "curl http://localhost:8000                # Test the application endpoint\n",
    "\n",
    "# Step 9: View Container Logs (Optional)\n",
    "docker logs my_app_container              # View logs to ensure the application is running properly\n",
    "\n",
    "# Step 10: Stop the Docker Container When Done\n",
    "docker stop my_app_container              # Stop the running container\n",
    "\n",
    "# Step 11: Remove the Docker Container (Optional)\n",
    "docker rm my_app_container                # Remove the container to free up resources\n",
    "\n",
    "#####\n",
    "### Using Docker Compose for Applications with Multiple Services\n",
    "'''\n",
    "If your application relies on additional services like databases or caches, Docker Compose simplifies the setup by managing multiple containers.\n",
    "\n",
    "'''\n",
    "#####\n",
    "\n",
    "# Step 1: Create a docker-compose.yml File in Your Application Directory\n",
    "# This file defines the services and how they interact\n",
    "# Example docker-compose.yml content:\n",
    "\"\"\"\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  app:\n",
    "    build: .\n",
    "    container_name: my_app_container\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - ENVIRONMENT=production\n",
    "    depends_on:\n",
    "      - redis\n",
    "    volumes:\n",
    "      - .:/app\n",
    "\n",
    "  redis:\n",
    "    image: redis:6\n",
    "    container_name: my_app_redis\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "\"\"\"\n",
    "# Save this content in a file named 'docker-compose.yml' in your application directory\n",
    "\n",
    "# Step 2: Build and Start the Services Using Docker Compose\n",
    "docker-compose up -d    # Build images (if necessary) and start all services in detached mode\n",
    "\n",
    "# Step 3: Verify All Services Are Running\n",
    "docker-compose ps       # List all containers started by Docker Compose\n",
    "\n",
    "# Step 4: Access Your Application\n",
    "curl http://localhost:8000    # Test the application endpoint to ensure it's accessible\n",
    "\n",
    "# Step 5: View Logs for All Services\n",
    "docker-compose logs -f        # Follow logs for all services\n",
    "\n",
    "# Step 6: Stop and Remove All Services\n",
    "docker-compose down           # Stop containers and remove containers, networks, and volumes created by 'up'\n",
    "\n",
    "#####\n",
    "### Notes:\n",
    "\n",
    "- **Dockerfile**: A file named 'Dockerfile' with instructions to build your Docker image.\n",
    "- **docker-compose.yml**: A YAML file that defines services, networks, and volumes for multi-container applications.\n",
    "- **Ports**: Ensure the ports exposed in the Dockerfile and mapped in `docker run` or `docker-compose.yml` match your application's listening port.\n",
    "- **Environment Variables**: Use `ENV` in Dockerfile or `environment` in `docker-compose.yml` to set variables.\n",
    "- **Volumes**: Use volumes to persist data or share code between your host and container.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61382b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#####\n",
    "### Using Docker via Command Line Interface (CLI)\n",
    "'''\n",
    "Docker provides a powerful command-line interface for managing containers, images, volumes, and networks. Below are some common CLI \n",
    "commands to get you started.\n",
    "'''\n",
    "\n",
    "# Check Docker Version\n",
    "docker version               # Display the Docker version installed on the system\n",
    "\n",
    "# List Running Containers\n",
    "docker ps                    # List all running containers (ps: processes)\n",
    "\n",
    "# List All Containers (including stopped ones)\n",
    "docker ps -a                 # List all containers (running and stopped)\n",
    "\n",
    "# Create and Run a Container\n",
    "docker run <image_name>      # Create and start a container from an image\n",
    "\n",
    "# Create and Run a Container in Detached Mode\n",
    "docker run -d <image_name>   # Run a container in the background (detached mode)\n",
    "\n",
    "# Create a Container and Open a Shell Session\n",
    "docker run -it <image_name> /bin/bash  # Run a container interactively with a shell session\n",
    "\n",
    "# Start an Existing Container\n",
    "docker start <container_id>            # Start a stopped container using its ID or name\n",
    "\n",
    "# Stop a Running Container\n",
    "docker stop <container_id>             # Stop a running container\n",
    "\n",
    "# Remove a Container\n",
    "docker rm <container_id>               # Remove a stopped container\n",
    "\n",
    "# Display Logs of a Container\n",
    "docker logs <container_id>             # Show logs of a specific container\n",
    "\n",
    "# Execute a Command in a Running Container\n",
    "docker exec -it <container_id> <command>  # Execute a command inside a running container\n",
    "\n",
    "#####\n",
    "### Using Docker Desktop\n",
    "'''\n",
    "Docker Desktop is a GUI-based application that helps developers manage Docker containers, images, and volumes more conveniently.\n",
    "- Launch Docker Desktop from your system menu.\n",
    "- Click on the \"Containers\" tab to view and manage your containers.\n",
    "- Use the \"Images\" tab to pull, view, or delete images.\n",
    "- Click on a container to access options like starting, stopping, or removing containers.\n",
    "\n",
    "'''\n",
    "\n",
    "# Managing Docker Images\n",
    "\n",
    "# List All Docker Images\n",
    "docker images               # Show all images available on the local system\n",
    "\n",
    "# Pull an Image from Docker Hub\n",
    "docker pull <image_name>:<tag>   # Pull a specific image from Docker Hub with the specified tag\n",
    "\n",
    "# Build an Image from a Dockerfile\n",
    "docker build -t <image_name> .   # Build an image from a Dockerfile in the current directory\n",
    "\n",
    "# Remove an Image\n",
    "docker rmi <image_id>            # Remove a specific image using its ID\n",
    "\n",
    "#####\n",
    "### Networking in Docker\n",
    "Docker networking allows containers to communicate with each other and with the outside world. Docker automatically sets up several networks by default, but you can create custom networks for isolated container communication.\n",
    "#####\n",
    "\n",
    "# Create a New Network\n",
    "docker network create <network_name>    # Create a custom bridge network\n",
    "\n",
    "# List All Docker Networks\n",
    "docker network ls                       # List all networks available in Docker\n",
    "\n",
    "# Connect a Container to a Network\n",
    "docker network connect <network_name> <container_id>  # Connect a container to a specified network\n",
    "\n",
    "# Disconnect a Container from a Network\n",
    "docker network disconnect <network_name> <container_id> # Disconnect a container from a network\n",
    "\n",
    "#####\n",
    "### Working with Docker Volumes\n",
    "Docker volumes are used to persist data and share it between containers or with the host machine. Volumes help separate container storage from container lifecycle, making them ideal for storing and sharing persistent data.\n",
    "#####\n",
    "\n",
    "# Create a Docker Volume\n",
    "docker volume create <volume_name>      # Create a named volume\n",
    "\n",
    "# List All Docker Volumes\n",
    "docker volume ls                        # List all volumes in Docker\n",
    "\n",
    "# Mount a Volume to a Container\n",
    "docker run -v <volume_name>:<container_path> <image_name>  # Mount a volume to a container\n",
    "\n",
    "# Remove a Docker Volume\n",
    "docker volume rm <volume_name>          # Remove a specific volume\n",
    "\n",
    "#####\n",
    "### Docker Compose\n",
    "'''\n",
    "Docker Compose is a tool for defining and running multi-container applications using a YAML configuration file. It allows developers to manage multiple containers with a single command.\n",
    "\n",
    "'''\n",
    "\n",
    "# Start All Services Defined in docker-compose.yml\n",
    "docker-compose up              # Start all services in detached mode\n",
    "\n",
    "# Stop All Services Defined in docker-compose.yml\n",
    "docker-compose down            # Stop all services and remove containers\n",
    "\n",
    "# View the Logs of All Services\n",
    "docker-compose logs            # Display logs for all services in the docker-compose.yml\n",
    "\n",
    "#####\n",
    "### Security Best Practices\n",
    "'''\n",
    "To secure Docker containers, limit their privileges and ensure they are configured correctly. Follow these commands to enhance security.\n",
    "'''\n",
    "\n",
    "# Run a Container with Limited Privileges\n",
    "docker run --cap-drop=ALL <image_name>   # Run a container with all Linux capabilities dropped\n",
    "\n",
    "# Set Container to Read-Only Mode\n",
    "docker run --read-only <image_name>      # Run a container in read-only mode for added security\n",
    "     \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba828607",
   "metadata": {},
   "source": [
    "> Gradio Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bb0cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import joblib # for loading the model\n",
    "\n",
    "\n",
    "# Steps to create a Gradio App\n",
    "    # 1. Define the input and output components.\n",
    "    # 2. Load the model.\n",
    "    # 3. Define the function to make predictions.\n",
    "    # 4. Create the Gradio interface.\n",
    "    # 5. Launch the interface.\n",
    "\n",
    "\n",
    "# Components\n",
    "    # Input Components\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter text here\", label=\"Your Text\")                                 # Text\n",
    "        gr.Number(label=\"Your Number\", default=0, step=1)                                                     # Number\n",
    "        gr.Slider(minimum=0, maximum=100, step=1, label=\"Your Slider\")                                        # Slider\n",
    "        gr.Checkbox(label=\"Check if True\", default=False)                                                     # Checkbox\n",
    "        gr.Radio(choices=[\"Option 1\", \"Option 2\"], label=\"Select One\", default=\"Option 1\")                    # Radio\n",
    "        gr.Dropdown(choices=[\"Option 1\", \"Option 2\"], label=\"Select One\", default=\"Option 1\")                 # Dropdown\n",
    "        gr.Image(tool=\"select\", shape=(224, 224), label=\"Upload Image\")                                       # Image\n",
    "        gr.File(label=\"Upload File\", type=[\".pdf\", \".docx\"])                                                  # File\n",
    "        gr.Dataframe(headers=[\"Column 1\", \"Column 2\"], value = [] ,datatype=[\"str\", \"number\"], label=\"Your Dataframe\")    # Dataframe\n",
    "        gr.Audio(label=\"Upload Audio\", type=\"file\", source=\"upload\")                                          # Audio\n",
    "        gr.Video(label=\"Upload Video\")                                                                        # Video\n",
    "        gr.Series(gr.Number(), label=\"Number Series\")                                                         # Series: for inputting a series of numbers.\n",
    "        \n",
    "    # Output Components\n",
    "        gr.Label(label=\"Output Label\")   # Label\n",
    "        gr.Textbox(label=\"Output Text\")   # Text\n",
    "        gr.Image(label=\"Output Image\")   # Image\n",
    "        gr.Plot(label=\"Output Plot\")  # Plot\n",
    "        gr.Dataframe(label=\"Output Dataframe\")   # Dataframe\n",
    "        gr.Audio(label=\"Output Audio\")   # Audio\n",
    "        gr.Video(label=\"Output Video\")   # Video\n",
    "        gr.JSON(label=\"Output JSON\")  # JSON\n",
    "\n",
    "# setting the input and output variables\n",
    "input1 = gr.Textbox(label=\"Input 1\")\n",
    "input2 = gr.Dropdown(choices=[\"Male\", \"Female\"], label='Gender')\n",
    "output = gr.Number(label=\"Prediction\")\n",
    "\n",
    "# Interface Functions\n",
    "model = joblib.load('file path of model')  # Load your model here\n",
    "\n",
    "def predict(var1, var2, var3, var4, var5, var6, var7, var8):\n",
    "    inputs = [var1, var2, var3, var4, var5, var6, var7, var8] \n",
    "    prediction = model.predict([inputs])[0]  # Make prediction (adjust according to your model's prediction method)\n",
    "    \"\"\"Return the prediction (you might want to format or round the prediction)\"\"\"\n",
    "    return prediction  # Return the prediction (you might want to format or round the prediction)\n",
    "\n",
    "# Building the Interface\n",
    "interface = gr.Interface(\n",
    "    fn=predict,\n",
    "    inputs = [input1, input2],\n",
    "    outputs=output,\n",
    "    examples = [['example_1', data], ['example_2' data]]    # Example inputs to show in the interface\n",
    "    title=\"Model Prediction App\",\n",
    "    description=\"Enter the values for the 8 variables to get the numeric prediction from the model.\",\n",
    "    theme=\"abidlabs/pakistan\",  # You can choose from various themes like 'default', 'huggingface', 'dark', 'grass', 'peach', etc.\n",
    "                    # 'ParityError/LimeFace', 'abidlabs/pakistan', 'HaleyCH/HaleyCH_Theme', 'freddyaboulton/dracula_revamped'\n",
    "                    # https://huggingface.co/spaces/gradio/theme-gallery for more themes\n",
    "    css=\"\"\"\n",
    "        body { font-family: Arial, sans-serif; }\n",
    "        .gr-interface { max-width: 800px; margin: auto; }\n",
    "        .gr-title, .gr-description { text-align: center; }\n",
    "        .gr-inputs, .gr-output { border-radius: 10px; }\n",
    "        .gr-group { margin-bottom: 20px; }\n",
    "        .gr-output-label { margin-top: 20px; }\n",
    "    \"\"\",\n",
    "    live=True, # Set to False to disable live updates and use a submit button for triggering predictions,\n",
    "    share = True, # Share the interface on social media sites like Twitter and LinkedIn\n",
    ")\n",
    "\n",
    "# Starting the Server (blocking call)\n",
    "interface.launch(share=True, auth=(\"username\", \"password\"), # Share the interface and set authentication (auth is optional)\n",
    "                 debug = True,   # Set debug to True to see error messages in the console when things go wrong\n",
    "                 inline = False # Set inline to True to display the interface in the Jupyter notebook instead of opening a new browser window\n",
    "                 )        \n",
    "\n",
    "\n",
    "\n",
    "# Function Decorators\n",
    "@gr.update                                                                  # Dynamically update component values based on other inputs.\n",
    "@gr.capture                                                                 # Capture and log inputs and outputs for debugging.\n",
    "\n",
    "# Advanced Components\n",
    "gr.Series(gr.Number(), label=\"Number Series\")                               # Series: for inputting a series of numbers.\n",
    "gr.JSON(label=\"Your JSON\")                                                  # JSON: for complex data structures.\n",
    "gr.Plot(plot_type=\"matplotlib\")                                             # Interactive Plots: for interactive Matplotlib or Plotly plots.\n",
    "\n",
    "# State and Session Management\n",
    "gr.State()                                                                  # Stateful Components: to maintain state across interactions.\n",
    "gr.session_state                                                            # Session State: to store and manage session-specific data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56985cd8",
   "metadata": {},
   "source": [
    "> Streamlit App Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f93f9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622723c6",
   "metadata": {},
   "source": [
    "> Flask App Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851709ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3e5b188",
   "metadata": {},
   "source": [
    "> Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859f95a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fc2492d8",
   "metadata": {},
   "source": [
    "> Heroku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a7e46b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "43b959a2",
   "metadata": {},
   "source": [
    "> YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -U yt-dlp   # Install or upgrade yt-dlp\n",
    "\n",
    "\n",
    "yt-dlp https://www.youtube.com/watch?v=example_video_id  # Download a YouTube video using yt-dlp\n",
    "\n",
    "yt-dlp -o \"%(title)s.%(ext)s\" <video_url>   # Download a video with a custom filename\n",
    "\n",
    "yt-dlp --extract-audio --audio-format mp3 https://www.youtube.com/watch?v=example_video_id  # Download audio only\n",
    "\n",
    "yt-dlp --extract-audio --audio-format mp3 --playlist-items 1-5 https://www.youtube.com/playlist?list=example_playlist_id  # Download audio from a playlist\n",
    "\n",
    "yt-dlp --get-title https://www.youtube.com/watch?v=example_video_id  # Get the title of a YouTube video\n",
    "\n",
    "yt-dlp --get-description https://www.youtube.com/watch?v=example_video_id  # Get the description of a YouTube video\n",
    "\n",
    "yt-dlp --get-thumbnail https://www.youtube.com/watch?v=example_video_id  # Get the thumbnail of a YouTube video\n",
    "\n",
    "yt-dlp --get-duration https://www.youtube.com/watch?v=example_video_id  # Get the duration of a YouTube video\n",
    "\n",
    "yt-dlp --get-filename -o \"%(title)s.%(ext)s\" https://www.youtube.com/watch?v=example_video_id  # Get the filename of a YouTube video\n",
    "\n",
    "yt-dlp --get-url https://www.youtube.com/watch?v=example_video_id  # Get the direct download URL of a YouTube video\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c662fabb",
   "metadata": {},
   "source": [
    "### OU Supercomputer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c39e60",
   "metadata": {},
   "source": [
    "> Supercomputer intro with code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b50a486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (B) PuTTY - Login to Putty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce972f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with a Supercomputer\n",
    "\n",
    "# Accessing the Supercomputer\n",
    "    # Access is typically granted through SSH (Secure Shell), using a command like:\n",
    "        ssh username@schooner.oscer.ou.edu\n",
    "    # Windows users might need an SSH client like PuTTY or Git Bash to use SSH.\n",
    "\n",
    "# File Transfer\n",
    "    # Downloading files: Use the scp command to securely copy files from the supercomputer to your local machine.\n",
    "        scp username@dtn2.oscer.ou.edu:/path/to/remote/file /path/to/local/destination\n",
    "        scp -r obinopaul@dtn2.oscer.ou.edu:/home/obinopaul/LIBOL-python_CS_2/plot_metrics_PA C:/Users/pault/Downloads   #add -r to extract the directory and run on local terminal\n",
    "        scp -r obinopaul@dtn2.oscer.ou.edu:/home/obinopaul/LIBOL-python_CS_2/results \"C:\\Users\\pault\\OneDrive - University of Oklahoma\\GRA - Bio-Manufacturing\\1. ML-Cytovance-OU-Research\\titer\\summer\\LIBOL-python_CS_2\"\n",
    "        scp -r obinopaul@dtn2.oscer.ou.edu:/home/obinopaul/LIBOL-python_CS_2 \"C:\\Users\\pault\\OneDrive - University of Oklahoma\\GRA - Bio-Manufacturing\\1. ML-Cytovance-OU-Research\\titer\\summer\\LIBOL-python_CS_2\\Supercomputer\"\n",
    "        scp -r \"C:\\Users\\pault\\OneDrive - University of Oklahoma\\GRA - Bio-Manufacturing\\1. ML-Cytovance-OU-Research\\titer\\summer\\LIBOL-python_CS_2\\Supercomputer\\LIBOL-python_sample\\.\" obinopaul@dtn2.oscer.ou.edu:/home/obinopaul/LIBOL-python_sample\n",
    "        cp -r obinopaul@dtn2.oscer.ou.edu:/home/obinopaul/LIBOL-python_sample /ourdisk/hpc/disc/obinopaul/auto_archive_notyet/tape_2copies/         #copy from supercomputer to tape in the supercomputer \n",
    "        \n",
    "    # Uploading files: Reverse the scp command to upload files, or use Git to clone and pull your repository on the supercomputer.\n",
    "\n",
    "# Running Code\n",
    "    # Bash Scripts: You typically run code on a supercomputer by writing a bash script with a .sh extension that specifies the resources needed and the commands to execute.\n",
    "        #!/bin/bash\n",
    "        #SBATCH --job-name=test_job\n",
    "        #SBATCH --output=result.txt\n",
    "        #SBATCH --time=01:00:00\n",
    "        #SBATCH --ntasks=1\n",
    "        #SBATCH --mem=4GB\n",
    "\n",
    "        module load python3\n",
    "        python3 my_script.py\n",
    "    # Submit the job using:\n",
    "        sbatch your_script.sh.\n",
    "    # Slurm: Supercomputers often use Slurm for job scheduling. Slurm commands include \n",
    "        # sbatch to submit jobs, \n",
    "        # squeue to view job status, and \n",
    "        # scancel to cancel jobs.\n",
    "\n",
    "# Monitoring Jobs\n",
    "    # Use squeue to check job status and top or htop on the assigned node to monitor resource usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b918880",
   "metadata": {},
   "source": [
    "> VScode on OSCER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607dffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caution with VS Code on OSCER:\n",
    "    # Important: Do not use VS Code's Remote SSH feature to connect to Schooner, as it may consume a significant amount of RAM \n",
    "    # and potentially crash the login nodes.\n",
    "    \n",
    "\n",
    "# Setting Up VS Code for Supercomputing:\n",
    "    # Downloading VS Code\n",
    "    # Installing Remote Tunnels Extension: (Search for \"Remote - Tunnels\" and install the extension.)\n",
    "    # Creating a Github Account\n",
    "    # Logging into Schooner:\n",
    "        # Use SSH to log into Schooner. The command format is \n",
    "        ssh username@schooner.oscer.ou.edu\n",
    "    # Setting Up VS Code CLI on Schooner: Depending on the VS Code version, \n",
    "        # download the compatible CLI version using curl and extract it. You'll end up with an executable named \"code\".\n",
    "        curl -Lk 'URL' --output vscode_cli.tar.gz\n",
    "        tar -xf vscode_cli.tar.gz\n",
    "        rm vscode_cli.tar.gz\n",
    "    # Preparing VS Code Server Batch Script: Create a batch script to run VS Code Server on Schooner\n",
    "        #!/bin/bash\n",
    "        #SBATCH --partition=partition_name\n",
    "        #SBATCH --output=vscode_%J_stdout.txt\n",
    "        #SBATCH --error=vscode_%J_stderr.txt\n",
    "        #SBATCH --ntasks=1\n",
    "        #SBATCH --mem=2G\n",
    "        #SBATCH --time=1:00:00\n",
    "\n",
    "        module load Python/3.10.8-GCCcore-12.2.0\n",
    "        # Load other modules or activate environments as needed\n",
    "        $HOME/code tunnel --accept-server-license-terms --name=$HOSTNAME\n",
    "\n",
    "    # Submitting the Batch Job:\n",
    "        sbatch your_script_name.sbatch\n",
    "        tail -f vscode_jobID_stdout.txt # to monitor the job's output\n",
    "    # Monitoring Job Output: Use \n",
    "        tail -f job_output_file_name # to monitor the job's output\n",
    "    # Connecting to the Tunnel in VS Code:  In VS Code, use the command palette (Ctrl+Shift+P) to connect to the tunnel created by the job on Schooner.\n",
    "    # Camcel a batch job:\n",
    "        scancel job_ID\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe59642",
   "metadata": {},
   "source": [
    "> Helpful step-by-step guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62665d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging into Schooner:\n",
    "    # Using Bash: Open your terminal and log in to the supercomputer using SSH (Secure Shell). \n",
    "    # Replace your_username with your actual username on Schooner:\n",
    "    ssh your_username@schooner.oscer.ou.edu\n",
    "    # Enter Password: You'll be prompted to enter your password. Type it in to establish the connection.\n",
    "\n",
    "# Activating or Starting a Job:\n",
    "    # SLURM Job Scheduler: Schooner utilizes SLURM for job management. You'll interact with SLURM to submit, monitor, and manage your jobs.\n",
    "\n",
    "# Compress Your Project Directory (if you need to)\n",
    "    # tar -czvf project.tar.gz /path/to/your/project\n",
    "    # tar -czvf ChestXRay.tar.gz \"data/Chest XRay\"      #this compresses the Chest XRay dataset into the .tar.gz\n",
    "\n",
    "# Transferring Files:\n",
    "    # Use SCP (Secure Copy Protocol) to transfer files between your local machine and Schooner. Use Bash to run the command:\n",
    "    scp \"data/Chest XRay.zip\" obinopaul@schooner.oscer.ou.edu:~/Healthcare-Project/ \n",
    "    scp ChestXRay.tar.gz obinopaul@schooner.oscer.ou.edu:~/Healthcare-Project/\n",
    "\n",
    "\n",
    "# or Load only one Script:\n",
    "    # Transfer Script: Use scp (Secure Copy Protocol) to transfer your Python scripts or entire project directory from your local \n",
    "    # machine to Schooner.\n",
    "    scp your_script.py your_username@schooner.oscer.ou.edu:~/path_to_your_directory/\n",
    "\n",
    "# Decompress Your Project Directory once logged in: \n",
    "    ssh obinopaul@schooner.oscer.ou.edu\n",
    "    cd ~/Healthcare-Project\n",
    "    tar -xzvf ChestXRay.tar.gz\n",
    "\n",
    "    rm ChestXRay.tar.gz # remove the compressed file after decompressing it\n",
    "\n",
    "\n",
    "## If you are running multiple scripts, you can use the following command to run them in the background:\n",
    "    nohup python script1.py & nohup python script2.py & nohup python script3.py \n",
    "    \n",
    "# Set Up Your Environment\n",
    "    module load Python/3.11.3-GCCcore-12.3.0    # Load the Python environment\n",
    "    module load cuDNN/8.6.0.163-CUDA-11.8.0     # Load CUDA\n",
    "\n",
    "    source /home/obinopaul/Healthcare-Project/yolo/bin/activate # activate the YOLO environment\n",
    "\n",
    "    # OR Create a virtual environment\n",
    "    module load Python/3.11.3-GCCcore-12.3.0\n",
    "    python3.11 -m venv /home/obinopaul/LIBOL-python/libol_env   # create a python environment with python 3.11\n",
    "    source /home/obinopaul/LIBOL-python/libol_env/bin/activate  # Activate the virtual environment\n",
    "    pip install --upgrade pip\n",
    "    pip install pandas numpy scikit-learn optuna matplotlib\n",
    "    pip install -r requirements.txt \n",
    "\n",
    "\n",
    "    \n",
    "# Running a Job:\n",
    "    # Prepare Job Script: Create a job script (your_job_script.sbatch) specifying resource requests and commands to run your script. \n",
    "    # Here's an example for a non-parallel job:\n",
    "    \n",
    "    #!/bin/bash\n",
    "    #SBATCH --job-name=compare_all        # Job name\n",
    "    #SBATCH --output=compare_all.out      # Standard output and error log\n",
    "    #SBATCH --error=compare_all.err       # Standard error log\n",
    "    #SBATCH --ntasks=1                      # Number of tasks (jobs)\n",
    "    #SBATCH --cpus-per-task=4               # Number of CPU cores per task\n",
    "    #SBATCH --mem=10G                       # Memory limit\n",
    "    #SBATCH --time=05:00:00                 # Time limit hrs:min:sec\n",
    "    #SBATCH --partition=normal              # Partition name\n",
    "\n",
    "    # Load the necessary Python module\n",
    "    module load Python/3.11.3-GCCcore-12.3.0 \n",
    "\n",
    "    source /home/obinopaul/LIBOL-python/libol_env/bin/activate \n",
    "\n",
    "    # Run the Python script\n",
    "    python compare_all.py \n",
    "   \n",
    "    # Save this as run_project.sh and submit it:\n",
    "\n",
    "# Submit Job: Use sbatch to submit your job script to SLURM.\n",
    "    sbatch your_job_script.sbatch   # Submit batch jobs\n",
    "\n",
    "# Monitoring Jobs: Use squeue to check the status of your job. You can filter jobs by your username:\n",
    "    squeue -u your_username         # to check the status of your job(s) (add -t at the end to check all pending/running jobs)\n",
    "    tail -f job_output_file_name    # to monitor the job's output\n",
    "\n",
    "# Cancelling Jobs: If needed, cancel a job using scancel followed by the job ID.\n",
    "    scancel job_id\n",
    "    scancel <job number>                    # cancels the specified job\n",
    "    scancel -u <username>                   # cancels all jobs for the specified user\n",
    "    scancel -t PENDING -u <username>        # cancels all pending jobs for the specified user\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "###################### LOADING A MODULE ON SCHOONER ############################\n",
    "    \n",
    "    module avail    # This will list all the available modules on Schooner\n",
    "    module list         # This will list all the modules that are currently loaded in your environment\n",
    "    module load Python      # Load the default Python module\n",
    "    module load <application name>  # Load a specific module\n",
    "    module unload <application name>    # Unload a module from your environment\n",
    "    module purge                        # Unload all modules from your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e454351",
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p ~/Healthcare-Project   #create a proejct directory \n",
    "module load Python/3.11.3-GCCcore-12.3.0    # set up your python environment\n",
    "python -m venv ~/Healthcare-Project/yolo # create a virtual environment\n",
    "source ~/Healthcare-Project/yolo/bin/activate   # activate the environment\n",
    "\n",
    "# Install cuda\n",
    "module avail cuda   # check the CUDA available \n",
    "module load cuDNN/8.6.0.163-CUDA-11.8.0\n",
    "module load CUDA/12.3.0\n",
    "\n",
    "\n",
    "# Install Libraries \n",
    "scp path/to/requirements.txt obinopaul@schooner.oscer.ou.edu:~/Healthcare-Project/  #transfer the requirements.txt file to the supercomputer\n",
    "pip install -r ~/Healthcare-Project/requirements.txt    # load the requiremennts.txt file\n",
    "\n",
    "# Transfer additional files from your PC to the Supercomputer \n",
    "scp ChestXRay.tar.gz obinopaul@schooner.oscer.ou.edu:~/Healthcare-Project/\n",
    "\n",
    "# Installing Pytorch\n",
    "\n",
    "pip install wheel   # For the latest pytorch built with CUDA 11.8, first, you need to install wheel package:\n",
    "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 #Then, according to the official pytorch installation instruction, type:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "########################### NOTE ############################\n",
    "deactivate  # deactivate a virtual environment\n",
    "rm -rf ~/Healthcare-Project/yolo    # delete a virtual environment\n",
    "du -h --max-depth=1 ~/Healthcare-Project/   # check the contents and size of a directory\n",
    "du -ah /home/obinopaul/Healthcare-Project/ | sort -rh | head -20    # or this \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e41911",
   "metadata": {},
   "source": [
    "> OnDemand App (An interactive app for OSCER SCHOONER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b52d75c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open OnDemand provides a user-friendly web interface for accessing and managing jobs on OSCER's Schooner supercomputer. \n",
    "# Here's a concise guide to get started:\n",
    "\n",
    "\n",
    "# Logging into OSCER via Open OnDemand:\n",
    "    # Account Requirement: Ensure you have an OSCER account. If not, request one at the OSCER account request page.\n",
    "    # Accessing Open OnDemand: Navigate to the Open OnDemand portal at https://ondemand.oscer.ou.edu and log in using your OSCER credentials.\n",
    "\n",
    "# Basic Features of Open OnDemand:\n",
    "    # Dashboard: Upon logging in, you'll encounter the Dashboard, which gives you access to various functionalities through pulldown menus.\n",
    "\n",
    "# Key Functionalities:\n",
    "    # Files Menu: \n",
    "        # Access and manage your files directly through a File Explorer-like interface. You can upload, download, edit, move, or \n",
    "        # rename files between your local device and Schooner.\n",
    "    # Jobs Menu:\n",
    "        # Active Jobs: View the status of your queued, running, or recently completed jobs. You can see detailed information or \n",
    "            # sort jobs based on different criteria.\n",
    "        # Job Composer: Create, edit, and submit Slurm batch job scripts to run applications on Schooner. Templates for various job types \n",
    "            # and applications will be provided for easy editing.\n",
    "    # Clusters Menu: \n",
    "        # Currently featuring Schooner, this menu offers \"Schooner Shell Access\" for command-line interaction via Secure Shell \n",
    "        # within the web interface. It's crucial to avoid running compute- or data-intensive applications through this shell.\n",
    "    # Interactive Apps:\n",
    "        # GUIs: Launch interactive sessions with applications like MATLAB, VMD, and ANSYS Workbench. These sessions provide \n",
    "            # interactive front ends for the respective software.\n",
    "        # Servers: Initiate Jupyter Notebook sessions interactively. You'll need to set some job parameters before launching, \n",
    "            # which can be adjusted as per your requirement or left to default.\n",
    "    # My Interactive Sessions: Manage and interact with your active sessions initiated through the \"Interactive Apps\" menu.\n",
    "\n",
    "# Ending Your Session:\n",
    "    # Logging Out: Ensure to log out after completing your tasks to secure your session and free up resources.\n",
    "\n",
    "# Additional Notes:\n",
    "    # Open OnDemand is an evolving platform, and feedback or requests for additional features are encouraged. \n",
    "    # If you need more resources or have suggestions, contact OSCER support.\n",
    "    \n",
    "    # For comprehensive details, refer to the official SLURM documentation or use the man command on Schooner (e.g., man sbatch) \n",
    "    # to learn more about specific commands.\n",
    "    \n",
    "# This guide provides an overview of starting and managing your machine learning tasks on Schooner using Open OnDemand, emphasizing user-friendly access and management of computational jobs and resources."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e30715",
   "metadata": {},
   "source": [
    "### Data Analytics  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8fa02d",
   "metadata": {},
   "source": [
    "#### Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Preprocessing\n",
    "    importing the required libraries\n",
    "    importing the dataset\n",
    "    handling missing data\n",
    "    encoding the categoical data\n",
    "    feature engineering\n",
    "    spliting the dataset into test set and training set\n",
    "    feature scaling \n",
    "    *webscraping with beautifulsoup\n",
    "    \n",
    "Machine Learning\n",
    "    Simple linear regression\n",
    "    multiple linear regression\n",
    "    logistic regression\n",
    "    K nearest neighbours\n",
    "    support vector machines\n",
    "    Naive Bayes Classifier\n",
    "    Decision trees\n",
    "    random forest\n",
    "    neural networks\n",
    "    k-means clustering\n",
    "    Hierarchical Clustering\n",
    "\n",
    "Deep Learning\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a303da5",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b460abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyforest import * #automatically imports all the necessary libraries for data science/analytics\n",
    "\n",
    "# Data Analysis\n",
    "import pandas as pd          # data analysis library for handling structured data\n",
    "from typing import List\n",
    "from pandas import DataFrame \n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "from pandas.plotting import parallel_coordinates \n",
    "from pivottablejs import pivot_ui\n",
    "import statsmodels.api as sm # library for performing statistical analysis\n",
    "import scipy.stats as stats\n",
    "import pandas_profiling\n",
    "\n",
    "\n",
    "import os       # import the os module for accessing operating system functionalities\n",
    "import sqlite3  # import the sqlite3 module for working with SQLite databases\n",
    "import math     # import the math module for mathematical operations and functions\n",
    "from collections import Counter  # import the Counter class from the collections module for counting items in an iterable\n",
    "from pathlib import Path  # import the Path class from the pathlib module for working with file paths and directories\n",
    "from tqdm import tqdm  # import the tqdm module for displaying progress bars during loops and iterations\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px   # interactive data visualization library\n",
    "import plotly.graph_objects as go   # library for creating interactive graphs and charts\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno \n",
    "import kaleido \n",
    "\n",
    "# Machine Learning \n",
    "from scipy.stats import skew, norm\n",
    "import yellowbrick\n",
    "import sklearn # machine learning library containing many algorithms for building models\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bb1e655",
   "metadata": {},
   "source": [
    "#### Preprocessing Structured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8446dd7",
   "metadata": {},
   "source": [
    "##### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f69cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('fileName.csv', sheet_name = \"Sheet 1\", parse_dates=True, columns=['product', 'price'], skiprows = [2], header = None,  )\n",
    "pd.read_html(\"https://developers.turing.com/dashboard/turing_test\")\n",
    "pd.read_excel\n",
    "pd.read_spss\n",
    "import pyreadstat\n",
    "pyreadstat.read_spss(\"filename.sav\", ) \n",
    "1. # parse_dates=True or [\"Date\", \"DOB\"] - if you want dates to be recognized as dates, add this argument:\n",
    "    #infer_datetime_format = True (used for only csv and others, but not excel)\n",
    "2. #columns=['product', 'price'] - if you want to include the names of the columns\n",
    "3. #sheet_name = \"Sheet 1\" - to reat sheet 1 only. You could also just write \"Sheet 1\" instead of \"sheet_name\"\n",
    "4. #skiprows = [2] - if you want to skip three rows\n",
    "5. #usecols = [2, 5] - if you want to import only columns three and 6\n",
    "6. #header = None/1/2 - if there are no headers in the dataset (None), or use header 2\n",
    "7. #skip_blank_lines=True - Skip blank lines in the dataset. \n",
    "8. #names = [\"Age\", \"DOB\"] - names of the hearders assuming they were not provided\n",
    "9. #nrows = 2 - number of rows to import\n",
    "10 #na_values = [\"not available\", \"n.a\", -1] - identify n.a values and assign them as n.a\n",
    "   #na_values = {\"eps\" = [\"not available\", \"n.a\", -1],\n",
    "    #            \"DOB\" = [\"not available\", 0.5]\n",
    "11. #index_col = False/\"Date\" - False for no index column, or use any column of choice.\n",
    "12. #dtype = { \"DOB\" : float, - used to specify the datatype of the columns while reading the file. To change the datatype, use .astype function. \n",
    " #           \"Name\" : int}\n",
    "\n",
    "#Read from SQL Query\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "pd.read_sql(\"SELECT * FROM my_table;\", engine)\n",
    "pd.read_sql_table('my_table', engine)\n",
    "pd.read_sql_query(\"SELECT * FROM my_table;\", engine)\n",
    "\n",
    "#import a .tsv file from a url\n",
    "chipo = pd.read_csv(\"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\", sep = '\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bcdffa4",
   "metadata": {},
   "source": [
    "##### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef04b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('dir/myDataFrame.xlsx',  sheet_name='Sheet2', index = False) #write to excel\n",
    "df.to_sql('myDf', engine) #write to sql\n",
    "\n",
    "with pd.ExcelWriter (\"courses.xlsx\") as writer: #writing to multiple sheets\n",
    "    df1.to_excel (writer, sheet_name = \"sheet_1\")\n",
    "    df2.to_excel (writer, sheet_name = \"sheet_2\")\n",
    "    \n",
    "with pd.ExcelWriter (\"courses.xlsx\", mode = \"a\") as writer: #writing to multiple sheets\n",
    "    df.to_excel (writer, sheet_name = \"sheet_1\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61829839",
   "metadata": {},
   "source": [
    "##### DataFrame Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',800) #column width\n",
    "pd.set_option(\"display.max_columns\",200) #Number of columns:\n",
    "bp_data.iloc[:, 1:10] #inner slicing\n",
    "bp_data.loc[:, \"Year\": \"Home Runs\"] #outer slicing \n",
    "df.iloc[np.r_[0:5, -5:0]] #selects the first 5 and last 5 rows. very important. \n",
    "numeric_cols = smart_home.select_dtypes(include=[\"float64\", \"int64\"]) # Select only numeric columns\n",
    "\n",
    "#duplicate detection\n",
    "df.groupby('item_name').item_name.nunique() #Find duplicates #count unique values of one column:\n",
    "pd.concat(i for _, i in df.groupby('my_user_id') if len(i) > 1)  #Find duplicates #Show duplicate values for one column:\n",
    "df = df.drop_duplicates(subset='my_user_id', keep=False)    #Remove duplicates\n",
    "df['n'].replace({'a': 'x', 'b': 'y', 'c': 'w', 'd': 'z'}) #replace the values in a column\n",
    "df['n'].rename(columns = ['a', 'b']) #rename columns\n",
    "df[df.duplicated(subset = \"patient_id\", keep = False)].sort_values(\"patient_id\")#A. To isolate duplicate values and view the results.\n",
    "repeat_patients = df.groupby(by = \"patient_id\").size().sort_values(ascending = False)#B Dsiplat all duplicate values in order\n",
    "\n",
    "#other basic data manipulation\n",
    "df = df.drop(['col1', 'col2'], axis=1)  #Drop one or more column(s): axis = 1 (column) or axis=0 (row)\n",
    "del df['COL1'] #remove columns\n",
    "df_train.rename(columns={\"trans_date_trans_time\":\"transaction_time\", \"cc_num\":\"credit_card_number\"},\n",
    "                inplace=True) #rename columns\n",
    "df.sort_values(by = ['COL1', 'COL2'], ascending = (0, 1)) # sorting\n",
    "df[(df['colCOL11'] >= 1) & (df['COL2'] <=1 )] #filter multiple conditions \"and\"\n",
    "df[(df['colCOL11'] >= 1) | (df['COL2'] <=1 )] #filter multiple conditions \"or\"\n",
    "df = pd.merge(df1, df2, how = 'left', left_on = ['my_user_id'], right_index = True) #combine dataframe\n",
    "df = pd.concat([df1, df2], axis = 0) #combine dataframe #concatenate\n",
    "df_train.credit_card_number = df_train.credit_card_number.astype('category') # Change dtypes\n",
    "df_train.credit_card_number = df_train.credit_card_number.astype('object') # Change dtypes (other dtypes = float64, int64)\n",
    "df.info() #check basic information of dataframe\n",
    "df.unique() #check unique variables of a dataframe\n",
    "df.describe() #describe basic details like count, mean, sd, percentile etc.\n",
    "np.round(df_train.describe(),2) #round up to the nearest 2 significant figures\n",
    "df.columns.value_counts().sum() or chipo.shape[1] #count number of columns in a dataset\n",
    "\n",
    "#check datatypes of features\n",
    "sns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize = (8,6))\n",
    "ax = df_train.dtypes.value_counts().plot(kind='bar',grid = False,fontsize=20,color='grey')\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+ p.get_width() / 2., height + 0.2, height, ha = 'center', size = 25)\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26ef4c5e",
   "metadata": {},
   "source": [
    "##### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23 datascience techniques you should know (https://ai.plainenglish.io/23-data-science-techniques-you-should-know-61bc2c9d1b3a)\n",
    "\n",
    "#Data profiling - very usedul for statistical analysis\n",
    "import pandas_profiling\n",
    "df.profile_report() \n",
    "\n",
    "#Pivottables - Smart tables\n",
    "from pivottablejs import pivot_ui \t#for smart tables in Jupyter notebook\n",
    "pivot_ui(df)\n",
    "\n",
    "#or (use for google colab)\n",
    "from google.colab import data_table\t\t#for smart tables in Google Colab\n",
    "data_table.enable_datafrane_formatter()\n",
    "\n",
    "#filtering data\n",
    "weather_per_day = weather_data.resample('D') #filters out the daily data from a dataframe with time as index\n",
    "weather_per_month = weather_data.resample('M') #filters out monthly data from a dataframe with time as index\n",
    "weather_per_hour = weather_data.resample('H') #filters out hourly data from a dataframe with time as index\n",
    "weather_data = smart_home.filter(items=['temperature',\n",
    "                                      'humidity', 'visibility', ])    #filters out the following columns from the dataframe\n",
    "africa_deaths_cause = df_deaths_cause[df_deaths_cause[\"Code\"].isin(Africa[\"Three_Letter_Country_Code\"])]\n",
    "                                                                #filters the rows in df_deaths_cause for Africa['three_letter_country_code..']\n",
    "numeric_cols = smart_home.select_dtypes(include=[\"float64\", \"int64\"]) # Select only numeric columns\n",
    "\n",
    "\n",
    "\n",
    "#grouping variables in a dataset\n",
    "grouped = df.groupby(\"Year\", as_index=False, dropna=False) #as_index = True/False\n",
    "print(grouped.get_group(2010))\n",
    "    sales.groupby(\"store\", \"product_group\", as_index=False)[\"stock_qty\"].mean()  |  sales.groupby(\"store\")[[\"stock_qty\",\"price\"]].mean()  \n",
    "    sales.groupby(\"store\", as_index=False)[\"stock_qty\"].agg([\"mean\", \"max\", \"unique\"])\n",
    "    sales.groupby(\"store\", as_index=False)[\"last_week_sales\"].nlargest(2) #returns largest values for each column (or .nsmallest(4))\n",
    "    sales_sorted.groupby(\"store\", as_index=False).nth(4) #returns the nth value (or nth(-2) to return the second row from the end)\n",
    "    sales.groupby([\"store\",\"product_group\"], as_index=False).agg(avg_sales = (\"last_week_sales\", \"mean\"))    \n",
    "    sales.groupby(\"store\", as_index=False).agg(total_sales_in_thousands = (\"last_month_sales\", lambda x: round(x.sum() / 1000, 1))) #lambda function\n",
    "    daisy_pg1 = sales.groupby([\"store\", \"product_group\"], as_index=False).get_group((\"Daisy\",\"PG1\")) #multiple grouping, and getting multiple grouped output\n",
    "    sales.groupby(\"continent\").agg({\n",
    "                        \"sprit_servings\": \"mean\",\n",
    "                        \"beer_servings\": \"max\",\n",
    "                        \"wine_servings\": lambda x: x**3\n",
    "                        })\n",
    "\n",
    "#Multi-indexing\n",
    "        # if your indices are ['index1', 'index2'] and you want to select the row where index1 is 'value1' and index2 is 'value2', \n",
    "df.loc['value1']\n",
    "df.loc[('value1', 'value2')]\n",
    "df.loc['value1', 'column_label']\n",
    "df.xs('value2', level='index2') #cross-section method - very helpful for multi-index dataframes\n",
    "df.loc[df.index.get_level_values('index2') == 'value2', ['col1', \"col2\"]]\n",
    "                                          \n",
    "#Merging                                                                           \n",
    "result = pd.merge(df1, df2, on='Product', how='inner')              \n",
    "result = pd.merge(df1, df2, on='Employee ID', how='left')\n",
    "result = pd.merge(df1, df2, on='Product', how='right')\n",
    "train = train.merge(riders, how = 'left', left_on='rider_id', right_on='Rider ID')\n",
    "test = test.merge(riders, how = 'left', left_on='rider_id', right_on='Rider ID')\n",
    "                                               \n",
    "If the column names in both DataFrames are the same, we can use the \"on\" parameter instead of \"left_on\" and \"right_on\".                                        \n",
    "\n",
    "\"left_on\"; \"right_on\":\n",
    "The resulting DataFrame result will contain only the rows where the values in the 'key1' column of df1 \n",
    "match the values in the 'key2' column of df2. In this case, the resulting DataFrame will contain two rows, \n",
    "where the values in the 'key1' column are 2 and 3.\n",
    "\n",
    "                                               \n",
    "\"left\": This performs a left join, \"keeping all the rows from the left\" (left_on) dataframe \n",
    "        and merging with the matching rows from the right (right_on) dataframe. \n",
    "        \"If there is no match, the result will have NaN values in the columns from the right dataframe\".\n",
    "\n",
    "\"right\": This performs a right join, \"keeping all the rows from the right\" (right_on) dataframe \n",
    "        and merging with the matching rows from the left (left_on) dataframe. \n",
    "        \"If there is no match, the result will have NaN values in the columns from the left dataframe\".\n",
    "\n",
    "\"outer\": This performs an outer join, \"keeping all the rows from both dataframes\". \n",
    "        If there is no match between the rows from the left and right dataframes, \n",
    "        \"the result will have NaN values in the columns from the non-matching dataframe\".\n",
    "\n",
    "\"inner\": This performs an inner join, \"keeping only the rows that have matching values\" \n",
    "        in both the left (left_on) and right (right_on) dataframes.                                 \n",
    "                                               \n",
    "                                               \n",
    "                                               \n",
    "#Pandas Apply and Map functions (use .apply() for multiple columns and .map() for single column)\n",
    "    df['colA'] = df['colA'].map(lambda x: x + 1)\n",
    "    df[['colA', 'colD']] = df[['colA', 'colD']].apply(lambda x: x + 1)\n",
    "    \n",
    "\n",
    "#Regular expression\n",
    "import re\n",
    "re.findall(pattern, text). #This function takes two arguments in the form of re.findall(pattern, string). \n",
    "              #Here, pattern represents the substring we want to find, and string represents the main string we want to find it in                               \n",
    "              #returns the matches as a list of strings or tuples    \n",
    "    match = re.findall(r\"From:.*\", fh)\n",
    "re.search(pattern, text) #matches the first instance of a pattern in a string, and returns it as a re match object.\n",
    "        match = re.search(r\"From:.*\", fh)\n",
    "re.split(pattern, text) \n",
    "        address = re.findall(r\"From:.*\", fh)\n",
    "        for item in address:\n",
    "            for line in re.findall(r\"\\w\\S*@.*\\w\", item):\n",
    "                username, domain_name = re.split(r\"@\", line)\n",
    "                print(\"{}, {}\".format(username, domain_name))\n",
    "re.sub(pattern, repl, text) #it substitutes parts of a string - this replaces the matched substring(s) with the repl string.\n",
    "re.finditer(pattern, text) #returns the matches with extra functionality\n",
    "re.match(pattern, text) #returns the matches at the beginning of strings\n",
    "\n",
    "\n",
    "#get duplicated and non-duplicated values\n",
    "duplicated = df[df.duplicated(\"Roll_no\")]\n",
    "non_duplicated = df[~df.duplicated(\"Roll_no\")]\n",
    "\n",
    "transactions.rename(columns={'Quantity' :'Quant'}) #rename Column \"Quantity\" to \"Quant\"\n",
    "transactions.sort_values('TransactionID', ascending=False) #Sort rows by TransactionId decending\n",
    "transactions.sort_values(['Quantity','TransactionDate'],ascending=[True,False]) #Sort multiple rows\n",
    "transactions[pd.unique(['UserID'] + transactions.columns.values.tolist()).tolist()] # Make UserID the first column\n",
    "transactions[foo | (bar <0)] # Subset rows where foo is TRUE or bar is negative\n",
    "transactions[(transactions.Quantity >0) & (transactions.UserID == 2)] # Subset rows where Quantity > 1 and UserID = 2\n",
    "chipo.groupby([\"item_name\"]).max().sort_values(by=[\"quantity\"] , ascending= False) #group items and find the most sales\n",
    "\n",
    "#filter out (extract) data from a column in pandas (use regular expression)\n",
    "df['col1'] = df['original'].str.extract(r\"\\(([A-Za-z0-9 _]+)\\)\") #using regular expression (#The '.str.extract()' method extracts substrings from the Series that match a given regular expression.)\n",
    "emails_df[emails_df[\"sender_email\"].str.contains(\"epatra|spinfinder\")] #using regular expression\n",
    "df_most[\"price_float\"] = list(map((lambda x: (x[1:])), df_most[\"item_price\"])) #another method using lambda and map functions\n",
    "df[\"FName\"] = [i.split(\" \")[0].split(\",\")[-1].strip() for i in first_name] #to extract features from a dataset\n",
    "\n",
    "\n",
    "\n",
    "#How to add an empty column to DataFrame\n",
    "df[\"Blank_Column\"] = \" \"\n",
    "df[\"NaN_Column\"] = np.nan\n",
    "df[\"None_Column\"] = None\n",
    "df2 = df.assign(Blank_Column=\" \", NaN_Column = np.nan, None_Column=None) # Add an empty columns using the assign() method\n",
    "df2 = df.reindex(columns = df.columns.tolist() + [\"None_Column\", \"None_Column_2\"]) # Add multiple columns with NaN , uses columns param\n",
    "df2 = df.reindex(df.columns.tolist() + [\"None_Column\", \"None_Column_2\"],axis=1) # Add multiple columns with NaN, , uses axis param \n",
    "df2 = df.reindex(columns=[\"None_Column\", \"None_Column_2\"]+df.columns.tolist()) # Add multiple columns to the Beginning\n",
    "df2 = df.reindex(columns=[\"Courses\",\"None_Column\", \"None_Column_2\",\"Fee\"]) # Add multiple columns with NaN, , uses axis param \n",
    "df.insert(0,\"Blank_Column\", \" \") # Using insert(), add empty column at first position\n",
    "df[\"Blank_Column\"] = df.apply(lambda _: ' ', axis=1) # Using apply() & lambda function\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38f66320",
   "metadata": {},
   "source": [
    "##### Speed up EDA in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('my_data.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "#Pandas Profiling\n",
    "    \"\"\"a library for creating comprehensive reports on dataframes\n",
    "    \"\"\"\n",
    "from pandas_profiling import ProfileReport    \n",
    "profile = ProfileReport(df) # Create the report\n",
    "profile.to_file('my_data_report.html') # Save the report\n",
    "\n",
    "#SweetViz\n",
    "    \"\"\"a library for comparing datasets and generating report on the comparison\n",
    "    \"\"\"\n",
    "import sweetviz as sv\n",
    "my_report = sv.compare([train, 'Train'], [test, 'Test'], 'Survived') # Create the report\n",
    "my_report.show_html('my_report.html') # Save the report\n",
    "\n",
    "#AutoViz\n",
    "    \"\"\"a tool that automatically generates visualizations for a given dataset\n",
    "    \"\"\"\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "# Create the visualizations\n",
    "AV = AutoViz_Class()\n",
    "viz = AV.AutoViz('my_data.csv')\n",
    "viz.view_plots()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1174649d",
   "metadata": {},
   "source": [
    "##### WebScraping with ChatGPT Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc11d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use beta.openai.com/playground\n",
    "\n",
    "BeautifulSoup Prompts\n",
    "#Scrape this website: https://stackoverflow.com/questions/ with Python and BeautifulSoup\n",
    "#Locate the element with tag \"aa\" and class \"asa\". Scrape all the \"a\" elements inside.\n",
    "#Get the text attribute and print it.\n",
    "\n",
    "Selenium Prompts\n",
    "#Scrape this website: https://stackoverflow.com/questions/ with Python, Selenium and Chromedriver\n",
    "#Maximize the window, wait 5 seconds and locate all the elements with following xpath: \"span\" tag, \"class\" attribute name, and \"dsd\" attribute value\n",
    "#Get the text attribute and print them. \n",
    "\n",
    "NB:\n",
    "    tag --> attribute\n",
    "    wait 5 seconds or 15 seconds\n",
    "\n",
    "\n",
    "\n",
    "#Interate ChatGPT with python, and ineract on the command line.\n",
    "To run the code below on Windows, follow these steps:\n",
    "\n",
    "1. Open a text editor and paste the modified code into a new file.\n",
    "2. Save the file with the name HeyChatGPT.py in a directory of your choice.\n",
    "3. Open a command prompt or PowerShell window and navigate to the directory \n",
    "    where the HeyChatGPT.py file is located.\n",
    "4. Type set api_key=xxxxxxxxxx to set the value of the api_key environment variable \n",
    "    to your OpenAI API key. Replace xxxxxxxxxx with your actual API key.\n",
    "    \n",
    "5. Type python HeyChatGPT.py \"How to reach Mars?\" to run the HeyChatGPT.py script and generate \n",
    "    a response to the prompt \"How to reach Mars?\". Replace the prompt text with any other \n",
    "    prompt you want to generate a response to.\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Import openai, os, and sys modules\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the prompt to the first command-line argument\n",
    "prompt = sys.argv[1]\n",
    "\n",
    "# Set the OpenAI API key to the value of the 'api_key' environment variable\n",
    "openai.api_key = os.environ['api_key']\n",
    "\n",
    "# Call the OpenAI API to generate a response to the prompt\n",
    "completions = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "# Extract the generated text from the API response\n",
    "message = completions.choices[0].text\n",
    "\n",
    "# Print the generated text to the console\n",
    "print(message)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56dbd2a9",
   "metadata": {},
   "source": [
    "##### WebScraping - BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f899b22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3920982088.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 77\u001b[1;36m\u001b[0m\n\u001b[1;33m    for item1, item2, item3 in zip(company_name, skills, published_date) # Write the scraped data to the CSV file\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "\n",
    "html_text = requests.get(\"https://www.sofascore.com/tournament/basketball/usa/nba/132\").text\n",
    "soup = BeautifulSoup(html_text, \"lxml\") #create an instance of beautifulsoup\n",
    "jobs = soup.find_all(\"li\", class_ = \"clearfix job-bx wh-shd-bx\" ) #\"li\" = tag, class = \"clearfix ....\" #Main tag\n",
    "#or\n",
    "jobs = soup.find(name = 'table', attrs = {'id' : 'per_game'}) # BeautifulSoup's .find() method searches for a tag and specified attributes, \n",
    "#or\n",
    "jobs = soup.find_all('tbody', {'class': 'Crom_body__UYOcU'})\n",
    "\n",
    "#having searched the main tag \"li\", next step would be to search and extract some particular information of choice\n",
    "# Creating a list of dictionaries to then convert into a Pandas Dataframe\n",
    "wiz_stats = []\n",
    "for row in wiz_per_game.find_all('tr')[1:]:  # Excluding the first 'tr', since that's the table's title head\n",
    "\n",
    "    player = {}\n",
    "    player['Name'] = row.find('a').text.strip()\n",
    "    player['Age'] = row.find('td', {'data-stat' : 'age'}).text\n",
    "    player['Min PG'] = row.find('td', {'data-stat' : 'mp_per_g'}).text\n",
    "    player['Field Goal %'] = row.find('td', {'data-stat' : 'fg_pct'}).text\n",
    "    player['Rebounds PG'] = row.find('td', {'data-stat' : 'trb_per_g'}).text\n",
    "    player['Assists PG'] = row.find('td', {'data-stat' : 'ast_per_g'}).text\n",
    "    player['Steals PG'] = row.find('td', {'data-stat' : 'stl_per_g'}).text\n",
    "    player['Blocks PG'] = row.find('td', {'data-stat' : 'blk_per_g'}).text\n",
    "    player['Turnovers PG'] = row.find('td', {'data-stat' : 'tov_per_g'}).text\n",
    "    player['Points PG'] = row.find('td', {'data-stat' : 'pts_per_g'}).text\n",
    "    wiz_stats.append(player)\n",
    "\n",
    "pd.DataFrame(wiz_stats)\n",
    "\n",
    "\n",
    "#OR\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#specify the url\n",
    "url = 'https://www.nba.com/stats/alltime-leaders'\n",
    "\n",
    "#query the website and return the html to the variable 'page'\n",
    "page = requests.get(url)\n",
    "\n",
    "#parse the html using beautiful soup and store in variable 'soup'\n",
    "soup = BeautifulSoup(page.text, 'html.parser') #or use lxml to parse\n",
    "\n",
    "#find all elements with class \"Block_blockContent__6iJ_n\"\n",
    "divs = soup.find_all('tbody', {'class': 'Crom_body__UYOcU'})\n",
    "\n",
    "#loop through the elements and print the text inside\n",
    "for div in divs:\n",
    "    print(div.text)\n",
    "    \n",
    "    \n",
    "\n",
    "#A better way to write it\n",
    "html_text = requests.get(\"https://www.sofascore.com/tournament/basketball/usa/nba/132\").text\n",
    "soup = BeautifulSoup(html_text, \"lxml\") #create an instance of beautifulsoup\n",
    "jobs = soup.find_all(\"li\", class_ = \"clearfix job-bx wh-shd-bx\" ) #\"li\" = tag, class = \"clearfix ....\"\n",
    "\n",
    "#having searched the main tag \"li\", next step would be to search and extract some particular information of choice\n",
    "#some particular information include \"company name\", \"skills\", and whether the job was posted recently\n",
    "#\".find\" - would extract a single instance. \".find_all\" would extract all instances\n",
    "for job in enumerate(jobs): #you can remove the enumerate\n",
    "    published_date = job.find(\"span\", class_ = \"sim-posted\").text \n",
    "    if date in published_date: \n",
    "        company_name = job.find(\"h3\", class_ = \"job_list-comp\").text #add .replace(\" \", \"\") to remove whitespaces\n",
    "        skills = job.find(\"span\", class_ = \"srp-skills\").text #add .replace(\" \", \"\") to remove whitespaces\n",
    "        more_info = job.header.h2.a['href'] #used to extract the link for additional info.\n",
    "                                            #header is the first tag, h2 is second. \"a\" is the third indicating a link\n",
    "                                            #\"href\" is not a tag but a class name where the link is at.\n",
    "        \n",
    "#         print(f\"Company Name: {company_name.strip()} \\n\") #you can decide not to use \".strip() function\"\n",
    "#         print(f\"Skills: {skills.strip()} \\n\") \n",
    "#         print(f\"Date Posted: {published_date} \\n\")\n",
    "#         print (\" \") \n",
    "        \n",
    "#         with open('data.csv', 'w', newline='') as csv_file: #open a new csv file in write mode\n",
    "#             writer = csv.writer(csv_file) #create an instance of csv\n",
    "#             writer.writerow([\"Company Name\", \"Skills\", \"Date Posted\"])\n",
    "#             for item1, item2, item3 in zip(company_name, skills, published_date) # Write the scraped data to the CSV file\n",
    "#                 writer.writerow([item1, item2, item3])\n",
    "#             csv_file.close()\n",
    "\n",
    "with open('data.csv', 'w', newline='') as csv_file: #open a new csv file in write mode\n",
    "        writer = csv.writer(csv_file) #create an instance of csv\n",
    "        writer.writerow([\"Company Name\", \"Skills\", \"Date Posted\"])\n",
    "        for item1, item2, item3 in zip(company_name, skills, published_date) # Write the scraped data to the CSV file\n",
    "            writer.writerow([item1, item2, item3])\n",
    "csv_file.close() # Close the CSV file\n",
    "print(f\"Scraped data saved in {csv_file_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fc8136f",
   "metadata": {},
   "source": [
    "##### WebScraping with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2416a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bacff3c2",
   "metadata": {},
   "source": [
    "##### Git and Github Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting & Creating Projects\n",
    "git init #Initialize a local Git repository\n",
    "git clone ssh://git@github.com/[username]/[repository-name].git #Create a local copy of a remote repository\n",
    "\n",
    "#Basic Snapshotting\n",
    "git status #Check status\n",
    "git add [file-name.txt] #Add a file to the staging area\n",
    "git add -A #Add all new and changed files to the staging area\n",
    "git commit -m \"[commit message]\" #Commit changes\n",
    "git rm -r [file-name.txt] #Remove a file (or folder)\n",
    "\n",
    "#Branching & Merging\n",
    "git branch #List branches (the asterisk denotes the current branch)\n",
    "git branch -a #List all branches (local and remote)\n",
    "git branch [branch name] #Create a new branch\n",
    "git branch -d [branch name] #Delete a branch\n",
    "git push origin --delete [branch name] #Delete a remote branch\n",
    "git checkout -b [branch name] #Create a new branch and switch to it\n",
    "git checkout -b [branch name] origin/[branch name] #Clone a remote branch and switch to it\n",
    "git branch -m [old branch name] [new branch name] #Rename a local branch\n",
    "git checkout [branch name] #Switch to a branch\n",
    "git checkout - #Switch to the branch last checked out\n",
    "git checkout -- [file-name.txt] #Discard changes to a file\n",
    "git merge [branch name] #Merge a branch into the active branch\n",
    "git merge [source branch] [target branch] #Merge a branch into a target branch\n",
    "git stash #Stash changes in a dirty working directory\n",
    "git stash clear #Remove all stashed entries\n",
    "\n",
    "#Sharing & Updating Projects\n",
    "git push origin [branch name] #Push a branch to your remote repository\n",
    "git push -u origin [branch name] #Push changes to remote repository (and remember the branch)\n",
    "git push #Push changes to remote repository (remembered branch)\n",
    "git push origin --delete [branch name] #Delete a remote branch\n",
    "git pull #Update local repository to the newest commit\n",
    "git pull origin [branch name] #Pull changes from remote repository\n",
    "git remote add origin ssh://git@github.com/[username]/[repository-name].git #Add a remote repository\n",
    "git remote set-url origin ssh://git@github.com/[username]/[repository-name].git \n",
    "                                            #Set a repository's origin branch to SSH\n",
    "    \n",
    "#Inspection & Comparison\n",
    "git log #View changes\n",
    "git log --summary #View changes (detailed)\n",
    "git log --oneline #View changes (briefly)\n",
    "git diff [source branch] [target branch] #Preview changes before merging   \n",
    "\n",
    "#Git LFS \n",
    "cd my-sample-project\n",
    "git lfs install                       # initialize the Git LFS project\n",
    "git lfs track \"*.csv\"                 # select the file extensions that you want to treat as large files\n",
    "git add .                             # add the large file to the project (make sure that .gitattributes is tracked \"git add .gitattributes\")\n",
    "git commit -am \"Added Debian iso\"     # commit the file meta data\n",
    "git push origin main                  # sync the git repo and large file to the GitLab server\n",
    "\n",
    "#fast way to do it \n",
    "navigate to the directory\n",
    "run this \"git config --global core.autocrlf input\"\n",
    "then copy and paste the code from Github. shown below: \n",
    "    echo \"# Tackling-the-Health-Crises-in-Africa\" >> README.md\n",
    "    git init\n",
    "    git add README.md\n",
    "    git commit -m \"first commit\"\n",
    "    git branch -M main\n",
    "    git remote add origin https://github.com/obinopaul/Tackling-the-Health-Crises-in-Africa.git\n",
    "    git push -u origin main\n",
    "\n",
    "#or  \n",
    "#Creating a repository and pushing files\n",
    "Open the command line on your local machine.\n",
    "Navigate to the local directory where your code files are located.\n",
    "Initialize a new git repository by running the command \"git init\".\n",
    "Add your code files to the repository by running the command \"git add .\". (use git add . to add all files in the directory)\n",
    "    If a warning tells you that your files will be replaced by CRLF the newxt time git touchs it. use git \"config --global core.autocrlf input\"\n",
    "    use this \"git\"\n",
    "Commit your changes by running the command \"git commit -m 'Initial commit'\".\n",
    "create the main branch \"git branch -M main\"\n",
    "Connect your local repository to the remote repository by running the command \"git remote add origin https://github.com/[username]/[repository_name].git\"\n",
    "Push your code to the remote repository by running the command \"git push -u origin main\" \n",
    "You will be prompted to enter your Github username and password. \n",
    "Once the files are pushed, they should now appear on your Github repository.\n",
    "\n",
    "\n",
    "git status  #To check if a folder contains an existing Git repository\n",
    "Remove-Item -Recurse -Force -Path \".git\"    #remove the Git repository\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "406ef646",
   "metadata": {},
   "source": [
    "##### README (MarkDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd07e7",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "Here are some of the most common and useful markdown syntax to beautify your README file on GitHub:\n",
    "\n",
    "Headings: Use # to denote headings, with one # for the largest heading and six # for the smallest.\n",
    "Lists: Use - or * to create unordered lists, and 1. to create ordered lists.\n",
    "Bold & Italic: Use **bold text** or __bold text__ for bold and *italic text* or _italic text_ for italic.\n",
    "Code blocks: Use ``` to create code blocks, for example:\n",
    "        ```bash \n",
    "        ```\n",
    "        #or\n",
    "        ```python \n",
    "        ```\n",
    "        #or just ```\n",
    "Links: Use [Link text](URL) to create clickable links.\n",
    "Images: Use ![Alt text](image URL) to add images to your README.\n",
    "    #or use the code below to center the image \n",
    "    <p align=\"center\">\n",
    "        <img src=\"image_url\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "    </p>\n",
    "\n",
    "Tables: You can create tables using the following syntax:\n",
    "        | Column 1 | Column 2 |\n",
    "        |----------|----------|\n",
    "        | Row 1, Column 1 | Row 1, Column 2 |\n",
    "        | Row 2, Column 1 | Row 2, Column 2 |\n",
    "Emoji: You can add emoji to your README by using the colon symbol (:) followed by the name of the emoji. \n",
    "    For example, :smile: will show a smiling face emoji.\n",
    "Task lists: You can create a list of tasks that can be checked off using the following syntax:\n",
    "    - [x] Task 1\n",
    "    - [ ] Task 2\n",
    "Blockquotes: You can add quotes or highlight important text by using the > symbol\n",
    "    > Quote or highlighted text\n",
    "Horizontal lines: To separate sections in your README, you can use the following syntax:\n",
    "    ---\n",
    "Strikethrough: To strike through text, use the following syntax:\n",
    "    ~~Strikethrough text~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e13bd6d",
   "metadata": {},
   "source": [
    "##### Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze | findstr /i pandas >> requirements.txt #copy all the libraries one by one. It is case insensitive.\n",
    "\n",
    "#another way to do it\n",
    "Open a terminal or command prompt.\n",
    "Activate your virtual environment: Run the following command:\n",
    "        conda activate myenv\n",
    "Install the packages you need for your project: Run the following command for each package that your project requires:\n",
    "        pip install package_name\n",
    "Freeze the packages and versions: Run the following command:\n",
    "        pip freeze > requirements.txt\n",
    "Deactivate the virtual environment: Run the following command:\n",
    "        conda deactivate\n",
    "        conda deactivate envy #or\n",
    "        CALL conda.bat deactivate #or\n",
    "\n",
    "#Creating a new environment \n",
    "\tconda env export > environment.yml      (from a previous environment)\n",
    "\tconda env create -f environment.yml --name myenv\t(conda env create -f /path/to/environment.yml --name myenv)\n",
    "\tconda activate <environment_name>\n",
    "        conda env export > environment.yml (do this to export the environment after completing the project)\n",
    "\t#OR \t\n",
    "        conda create --name project1 --clone base\t(use this to clone the base environment with all its libraries into the new environment)\n",
    "\n",
    "        #if you wish to use the requiremnts.txt file instead of environment.yaml file\n",
    "        conda list -e | findstr /V \"^#\" > requirements.txt      (from a previous environment)\n",
    "        conda create --name myenv --file C:\\Users\\Cornel\\requirements.txt\t \n",
    "\n",
    "\n",
    "# Use the conda env update --file environment.yml command to update the new environment with the packages and dependencies \n",
    "# from the exported YAML file\n",
    "\n",
    "#Google Colab - Creating a new environment and installing libraries into it.\n",
    "        # Start by installing the virtualenv package using the !pip command:\n",
    "                !pip3 install virtualenv\n",
    "        # Create a new virtual environment by running the virtualenv command with the desired environment name:\n",
    "                !virtualenv myenv\n",
    "        # Activate the virtual environment using the source command:\n",
    "                !source /content/drive/MyDrive/colab_env/bin/activate; pip3 list \n",
    "        #if permission is denied, then run below\n",
    "                !chmod +x /content/drive/MyDrive/colab_env/bin/*\n",
    "\n",
    "        # Install the required libraries using !pip within the virtual environment:\n",
    "                !pip3 install library1 library2 library3\n",
    "                pip install -r requirements.txt\n",
    "\n",
    "# Having done this, to load the environment with its libraries:\n",
    "        # Mount Google Drive in your Colab notebook\n",
    "                from google.colab import drive\n",
    "                drive.mount('/content/drive')\n",
    "        # Navigate to the directory where your virtual environment is located on Google Drive\n",
    "                %cd /content/drive/MyDrive/myenv\n",
    "        # Activate the virtual environment using the source command:\n",
    "                !source bin/activate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42465628",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install virtualenv\n",
    "!virtualenv theanoEnv\n",
    "\n",
    "!source /content/theanoEnv/bin/activate; pip3 install theano\n",
    "\n",
    "!source /content/theanoEnv/bin/activate; pip3 list\n",
    "\n",
    "!source /content/theanoEnv/bin/activate; pip3 install robotframework; pip3 list; python3 -m robot --help\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb197176",
   "metadata": {},
   "source": [
    "##### Basic Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naina Chaturvedi on Medium for all courses on Data Science,ML, Deep learning, etc. (https://medium.com/@naina0412)\n",
    "\n",
    "# Object: used for text, numeric, and non-numeric values. If the given data does not fit any of the below dtype, then object type is assigned to it. It is both the blessing and curse in a package. There is a reason why objects are not the way to go for the performance-tuned codebases\n",
    "# int64: Integer numbers  covers both signed and unsigned integers along with the varying variable-length  8, 16, 32 and 64\n",
    "# float64: Floating point numbers  covers the lengths 16, 32 and 64\n",
    "# bool: True and False values\n",
    "# datetime64: covers the date and time values\n",
    "# timedelta[ns]: used to capture the difference between two DateTime values. this dtype is helpful when the user is working with time-series data\n",
    "# category: used to cover a finite list of text values\n",
    "\n",
    "\n",
    "# In a correlation test, the results will include a correlation coefficient and a p-value.\n",
    "# The correlation coefficient is a number that tells us how strong the relationship is between the two things \n",
    "#we are testing. It can range from -1 to 1. A value of -1 means that there is a strong negative relationship \n",
    "#between the two things (when one thing increases, the other thing decreases). A value of 1 means that \n",
    "#there is a strong positive relationship between the two things (when one thing increases, the other thing increases). \n",
    "#A value of 0 means that there is no relationship between the two things.\n",
    "# The p-value is a number that tells us the probability that the relationship between the two things occurred by chance. \n",
    "#If the p-value is less than 0.05, we can say that the relationship is statistically significant, which means that \n",
    "#it is unlikely to have occurred by chance. If the p-value is greater than 0.05, we cannot say that the relationship \n",
    "#is statistically significant.\n",
    "# For example, let's say that we perform a correlation test and the results show a correlation coefficient of 0.6 \n",
    "#and a p-value of 0.01. This means that there is a moderate positive relationship between the two things we are testing \n",
    "#(the coefficient is close to 1) and that this relationship is statistically significant (the p-value is less than 0.05). \n",
    "#This means that we can be confident that the relationship between the two things is real and not just a coincidence.\n",
    "\n",
    "\n",
    "#Cookiecutter Data Science Project format (check https://drivendata.github.io/cookiecutter-data-science/)\n",
    "To clone a cookiecutter template \n",
    "$ cookiecutter cookiecutter https://github.com/drivendata/cookiecutter-data-science \n",
    "#or\n",
    "$ cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science\n",
    "\n",
    "xcopy C:\\Users\\Cornel\\Documents\\5. Projects\\cookiecutter\\cookiecutter_main - what he used\\* C:\\backup /s /y\n",
    "This switch tells xcopy to overwrite files in the destination folder if they already exist.\n",
    "    \n",
    " LICENSE\n",
    " Makefile           <- Makefile with commands like `make data` or `make train`\n",
    " README.md          <- The top-level README for developers using this project.\n",
    " data\n",
    "    external       <- Data from third party sources.\n",
    "    interim        <- Intermediate data that has been transformed.\n",
    "    processed      <- The final, canonical data sets for modeling.\n",
    "    raw            <- The original, immutable data dump.\n",
    "\n",
    " docs               <- A default Sphinx project; see sphinx-doc.org for details\n",
    "\n",
    " models             <- Trained and serialized models, model predictions, or model summaries\n",
    "\n",
    " notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "                         the creator's initials, and a short `-` delimited description, e.g.\n",
    "                         `1.0-jqp-initial-data-exploration`.\n",
    "\n",
    " references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "\n",
    " reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    figures        <- Generated graphics and figures to be used in reporting\n",
    "\n",
    " requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "                         generated with `pip freeze > requirements.txt`\n",
    "\n",
    " setup.py           <- Make this project pip installable with `pip install -e`\n",
    " src                <- Source code for use in this project.\n",
    "    __init__.py    <- Makes src a Python module\n",
    "   \n",
    "    data           <- Scripts to download or generate data\n",
    "       make_dataset.py\n",
    "   \n",
    "    features       <- Scripts to turn raw data into features for modeling\n",
    "       build_features.py\n",
    "   \n",
    "    models         <- Scripts to train models and then use trained models to make\n",
    "                       predictions\n",
    "       predict_model.py\n",
    "       train_model.py\n",
    "   \n",
    "    visualization  <- Scripts to create exploratory and results oriented visualizations\n",
    "        visualize.py\n",
    "\n",
    " tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\n",
    "\n",
    "\n",
    "#Windows CMD\n",
    "dir: This command is used to display a list of files and folders in a directory.\n",
    "cd: This command is used to change the current working directory.\n",
    "copy: This command is used to copy files from one location to another.\n",
    "mkdir: This command is used to create a new directory.\n",
    "del: This command is used to delete files.\n",
    "ren: This command is used to rename files or directories.\n",
    "cls: This command is used to clear the command prompt screen.\n",
    "ipconfig: This command is used to display information about the network configuration of the computer.\n",
    "ping: This command is used to test the connectivity to a network.\n",
    "shutdown: This command is used to shut down the computer.\n",
    "type: This command is used to display the contents of a text file on the screen.\n",
    "xcopy: This command is used to copy entire directory trees, including subdirectories and files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7462286d",
   "metadata": {},
   "source": [
    "##### Convert Pandas Categorical Data For Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca04188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(raw_data, columns = ['patient', 'obs', 'treatment', 'score']) #create dataframe\n",
    "le = preprocessing.LabelEncoder() # Create a label (category) encoder object\n",
    "le.fit(df['score']) # Fit the encoder to the pandas column\n",
    "list(le.classes_) # View the labels (if you want)\n",
    "le.transform(df['score']) # Apply the fitted encoder to the pandas column\n",
    "list(le.inverse_transform([2, 2, 1])) # Convert some integers into their category names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e13e72f",
   "metadata": {},
   "source": [
    "##### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Missing Values' Class With Most Frequent Class\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='most_frequent', axis=0) # Create Imputer object\n",
    "imputer.fit_transform(X) # Fill missing values with most frequent class. assuming X = DataFrame variable\n",
    "\n",
    "#fill missing values \n",
    "df = pd.fillna({\"DOB\": 0, \"Age\": \"no-event\"}, method = \"ffill\", axis = 1) #method = \"ffil\" or \"bfill\" \n",
    "df.interpolate (method = \"linear\") #method = \"linear\", \"time\", \"index\", \"quadratic\" etc. \n",
    "  #fill missing values with average values\n",
    "    c = avg = 0 #compute average\n",
    "    for ele in df[\"Marks\"]:\n",
    "        if str(ele).isnumeric():\n",
    "            c += 1\n",
    "            avg += ele\n",
    "    avg /= c\n",
    "    #Replace missing values\n",
    "    df = df.replace(to_replace=\"NaN\", value = avg)\n",
    "    #Display data\n",
    "    df\n",
    "\n",
    "#percentage of missing valuess in dataset\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "m_data = pd.concat([total, percent],axis=1 )\n",
    "m_data.head(10)\n",
    "    \n",
    "#Summary of missing values\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      df.isnull().sum()\n",
    "df.shape #total number of rows and columns in the dataset. Then compare with the number of missing values.\n",
    "\n",
    "#visualize missing data\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "df = pd.read_csv('dataset.csv') # Load the dataset\n",
    "msno.matrix(df) # Visualize the missing values\n",
    "#This will create a matrix plot that shows the missing values as white lines.\n",
    "\"\"\"You can also use the 'msno.bar' function to visualize the missing values as a bar chart, \n",
    "or the \"msno.heatmap\" function to visualize the missing values as a heatmap.\"\"\"\n",
    "\n",
    "#or\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "mask = df.isnull() # Create a Boolean mask indicating which values are missing\n",
    "plt.figure(figsize=(10,10)) # Use the mask to create a heatmap\n",
    "plt.title(\"Heatmap of Missing Values\")\n",
    "sns.heatmap(mask, cbar=False, annot=True, cmap='PuBu')\n",
    "plt.show()\n",
    "#This will create a heatmap with white squares indicating missing values, and black squares indicating non-missing values.\n",
    "\n",
    "# Remove observations with missing values\n",
    "X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "#Imputing Missing Class Labels Using k-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = # Create feature matrix with categorical feature \n",
    "X_with_nan = # Create feature matrix with missing values in the categorical feature\n",
    "clf = KNeighborsClassifier(3, weights='distance') # Train KNN learner\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:]) # Predict missing values' class\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:])) # Join column of predicted class with their other features\n",
    "np.vstack((X_with_imputed, X)) # Join two feature matrices\n",
    "\n",
    "#Deleting Missing Values\n",
    "df = pd.DataFrame(X, columns=['feature_1', 'feature_2']) # Load data as a data frame\n",
    "df.dropna(axis=0, how=\"any\", thresh=2) # Remove observations with missing values (how = \"any\" or \"all\"; thresh = 1,2,3 - if you have atleast 1 or 2 non-na values)\n",
    "\n",
    "#Impute Missing Values With Means\n",
    "from sklearn.preprocessing import Imputer\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0) # Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0)\n",
    "mean_imputer = mean_imputer.fit(df) # Train the imputor on the df dataset\n",
    "imputed_df = mean_imputer.transform(df.values) # Apply the imputer to the df dataset\n",
    "imputed_df # View the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5f0df4",
   "metadata": {},
   "source": [
    "##### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df_train[\"dob\"] = pd.to_datetime(df_train[\"dob\"], infer_datetime_format=True) # convert to datetime format \n",
    "df_train['time'] = df_train['unix_time'].apply(datetime.utcfromtimestamp) # Apply function utcfromtimestamp and drop column unix_time\n",
    "df_train['hour_of_day'] = df_train[\"time\"].dt.hour # Add column hour of day\n",
    "\n",
    "#convert date of birth to current age (It subtracts the years of birth from the current year to calculate the age of each person represented in the 'dob' column)\n",
    "import datetime as dt\n",
    "df_train['age']= dt.date.today().year-pd.to_datetime(df_train['dob']).dt.year\n",
    "\n",
    "dt.date.today().year #computes the current year while performing the datetime functionality\n",
    "pd.to_datetime #converts to a datetime object\n",
    ".dt.year #extracts the year from each datetime object\n",
    ".dt.month #extracts the month from each datetime object\n",
    ".dt.day #extracts the day from each datetime object\n",
    ".dt.hour #extracts the hour from each datetime object\n",
    "\n",
    "#If it shows that unicode nonesense of 1970, use this\n",
    "# df_no_of_deaths[\"Year\"].astype(\"datetime64[ns]\")\n",
    "df_deaths_cause[\"Year\"] = pd.to_datetime(df_deaths_cause[\"Year\"].astype(int).astype(str) + '-01-01')\n",
    "\n",
    "df_deaths_cause['Year'] = df_deaths_cause[\"Year\"].dt.year\n",
    "\n",
    "#create a datetime time range, and making it a dataframe index\n",
    "time_index = pd.date_range('2016-01-01 05:00', periods=503911,  freq='min')  \n",
    "time_index = pd.DatetimeIndex(time_index)\n",
    "df = df.set_index(time_index)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ff9e30",
   "metadata": {},
   "source": [
    "##### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff713a",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# A. Outlier Detection\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "# Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "# Select only numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])  #you may or may not include this step. \n",
    "# Fit detector\n",
    "outlier_detector.fit(numeric_cols)\n",
    "# Predict outliers\n",
    "outlier_predictions = outlier_detector.predict(numeric_cols) \n",
    "# Add outlier predictions to original dataframe\n",
    "smart_home[\"outlier\"] = outlier_predictions         #you may or may not add outlier predictions to the original dataframe\n",
    "\n",
    "#OR \n",
    "#Boxplot \n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "''''\n",
    "In a box plot, the central rectangle spans the first quartile to the third quartile (the 25th to 75th percentiles),\n",
    "and a line is also drawn along the median (50th percentile). The \"whiskers\" extend from the box to show the range of the data, \n",
    "and any points that lie outside the whiskers are plotted as individual points. This will create a box plot for each numerical \n",
    "column in the DataFrame. Outliers are typically represented as points outside the \"whiskers\" of the box plot.'''\n",
    "\n",
    "#B. Tukey's test for extreme values (you can replace 1.5 with 3)\n",
    "def find_outliers_tukey(x): # Define function using 1.5x interquartile range deviations from quartile 1/3 as floor/ceiling\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "    iqr = q3-q1 \n",
    "    floor = q1 - 1.5 * iqr\n",
    "    ceiling = q3 + 1.5 * iqr\n",
    "    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n",
    "    outlier_values = list(x[outlier_indices])\n",
    "    return outlier_indices, outlier_values\n",
    "\n",
    "for x in range(1, 7): # Modify to select numeric columns # Print outliers for each numeric variable\n",
    "    tukey_indices, tukey_values = find_outliers_tukey(data.ix[:, x])\n",
    "    print(list(data[[x]]), np.sort(tukey_values))\n",
    "    \n",
    "#C. Kernel density estimation\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "def find_outliers_kde(x):  # Define outlier function\n",
    "    x_scaled = scale(list(map(float, x)))\n",
    "    kde = KDEUnivariate(x_scaled)\n",
    "    kde.fit(bw = \"scott\", fft = True)\n",
    "    pred = kde.evaluate(x_scaled)    \n",
    "    n = sum(pred < 0.05)\n",
    "    outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "    outlier_value = np.asarray(x)[outlier_ind]\n",
    "    return outlier_ind, outlier_value\n",
    "\n",
    "for x in range(1, 7): # Modify to select numeric columns   # Print outlier values\n",
    "    kde_indices, kde_values = find_outliers_kde(data.ix[:, x])\n",
    "    print(list(data[[x]]), np.sort(kde_values))\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "#outlier handling (by removal)\n",
    "\n",
    "# Z-score method:  \n",
    "    This method involves calculating the z-score for each data point and removing those that fall outside \n",
    "a certain threshold. Typically, data points with a z-score of more than 3 or less than -3 are considered outliers.\n",
    "\n",
    "def outlier_z-score(df: pd.DataFrame):\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Calculate z-score for each data point\n",
    "    z_scores = np.abs((df - df.mean()) / df.std())\n",
    "\n",
    "    # Remove data points with z-score greater than 3\n",
    "    df_cleaned = df[(z_scores < 3).all(axis=1)]\n",
    "    return df_cleaned \n",
    "\n",
    "# 2. Mahalanobis distance method: (multivariate outlier detection)\n",
    "\n",
    "def outlier_Mahalanobis_distance (df: pd.DataFrame):\n",
    "    from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Calculate the Mahalanobis distance for each data point\n",
    "    cov = df.cov()\n",
    "    inv_cov = pd.DataFrame(np.linalg.pinv(cov.values), index=cov.index, columns=cov.columns)\n",
    "    mean = df.mean()\n",
    "    dist = []\n",
    "    for i in range(len(df)):\n",
    "        x = df.iloc[i]\n",
    "        dist.append(mahalanobis(x, mean, inv_cov))\n",
    "\n",
    "    # Set threshold for Mahalanobis distance\n",
    "    threshold = 3\n",
    "\n",
    "    # Remove data points with Mahalanobis distance greater than threshold\n",
    "    df_cleaned = df[dist < threshold]\n",
    "    return df_cleaned\n",
    "\n",
    "# 3. Isolation Forest model for outlier detection and removal\n",
    "# The `contamination` parameter is again set to 0.01, indicating that we expect 1% of the data to be outliers.\n",
    "def outlier_isolation_forest (df: pd.DataFrame): \n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Fit IsolationForest model to data\n",
    "    model = IsolationForest(contamination=0.01)\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict outliers\n",
    "    outliers = model.predict(df) == -1\n",
    "\n",
    "    # Remove outliers from data\n",
    "    df_cleaned = df[~outliers]\n",
    "    return df_cleaned\n",
    "\n",
    "#4 EllipticEnvelope\n",
    "\n",
    "def outlier_EllipticEnvelope (df: pd.DataFrame): \n",
    "    from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Fit EllipticEnvelope model to data\n",
    "    model = EllipticEnvelope(contamination=0.01)\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict outliers\n",
    "    outliers = model.predict(df) == -1\n",
    "\n",
    "    # Remove outliers from data\n",
    "    df_cleaned = df[~outliers]\n",
    "    return df_cleaned \n",
    "\n",
    "\n",
    "#oulier handling (with removal)\n",
    "\n",
    "#IQR Method\n",
    "import numpy as np\n",
    "\n",
    "def handle_outliers_iqr(data):\n",
    "    \"\"\"\n",
    "    This function uses the interquartile range (IQR) method to handle outliers in the data. \n",
    "    Any data points that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are replaced with the nearest bound.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the outliers replaced by the nearest bound.\n",
    "    \"\"\"\n",
    "    Q1 = np.percentile(data, 25, interpolation='midpoint')\n",
    "    Q3 = np.percentile(data, 75, interpolation='midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data = np.where(data < lower_bound, lower_bound, data)\n",
    "    data = np.where(data > upper_bound, upper_bound, data)\n",
    "    return data\n",
    "\n",
    "#Trimming Method\n",
    "\n",
    "def handle_outliers_trimming(data, percentage):\n",
    "    \"\"\"\n",
    "    This function uses the trimming method to handle outliers in the data. \n",
    "    The percentage of data points to be trimmed from the lower and upper ends is specified.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "    percentage (float): A value between 0 and 100 specifying the percentage of data points to be trimmed from the lower and upper ends.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the trimmed outliers.\n",
    "    \"\"\"\n",
    "    lower_percentage = percentage / 2\n",
    "    upper_percentage = 100 - lower_percentage\n",
    "    lower_bound = np.percentile(data, lower_percentage, interpolation='midpoint')\n",
    "    upper_bound = np.percentile(data, upper_percentage, interpolation='midpoint')\n",
    "    data = np.where(data < lower_bound, lower_bound, data)\n",
    "    data = np.where(data > upper_bound, upper_bound, data)\n",
    "    return data\n",
    "\n",
    "#Arbitrary Capping\n",
    "def handle_outliers_arbitrary(data, lower_cap, upper_cap):\n",
    "    \"\"\"\n",
    "    This function uses the arbitrary capping method to handle outliers in the data. \n",
    "    Any data points that fall below the lower_cap or above the upper_cap are replaced with the nearest bound.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "    lower_cap (float): The lower bound for capping outliers.\n",
    "    upper_cap (float): The upper bound for capping outliers.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the capped outliers.\n",
    "    \"\"\"\n",
    "    data = np.where(data < lower_cap, lower_cap, data)\n",
    "    data = np.where(data > upper_cap, upper_cap, data)\n",
    "    return data\n",
    "\n",
    "#Outlier Capping with Quantiles\n",
    "\n",
    "def handle_outliers_quantile(data, lower_quantile, upper_quantile):\n",
    "    \"\"\"\n",
    "    This function uses the outlier capping method with quantiles to handle outliers in the data. \n",
    "    Any data points that fall below the lower_quantile or above the upper_quantile are replaced with the nearest bound.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "    lower_quantile (float): The lower quantile value for capping outliers.\n",
    "    upper_quantile (float): The upper quantile value for capping outliers.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the capped outliers\n",
    "    \"\"\"\n",
    "    lower_bound = np.quantile(data, lower_quantile, interpolation='midpoint')\n",
    "    upper_bound = np.quantile(data, upper_quantile, interpolation='midpoint')\n",
    "    data = np.where(data < lower_bound, lower_bound, data)\n",
    "    data = np.where(data > upper_bound, upper_bound, data)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3ba6f41",
   "metadata": {},
   "source": [
    "##### Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf76da22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#A. For a non-binary categorical variable:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dummy \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mget_dummies(dta1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m], prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      3\u001b[0m dummy\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      4\u001b[0m df1\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#A. For a non-binary categorical variable:\n",
    "dummy = pd.get_dummies(dta1['country'], prefix='ct').astype(int)\n",
    "dummy.head()\n",
    "df1.astype(\"string\")\n",
    "\n",
    "#B. For all categorical variables of a dataframe:\n",
    "# Create a list of features to dummy\n",
    "dummy_vars = ['COL1', 'COL2', 'COL3'] \n",
    "\n",
    "# Create dummies for all categorical variables\n",
    "def dummy_data(df, dummy_vars): \n",
    "    for x in dummy_vars:\n",
    "        dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False)\n",
    "        df = df.drop(x, 1)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "df = dummy_df(df, dummy_data)\n",
    "print(df.head())\n",
    "\n",
    "# Dummies \n",
    "dummy_ct = pd.get_dummies(df1['country'], prefix='ct').astype(int)\n",
    "dummy_pf = pd.get_dummies(df1['platform'], prefix='pf').astype(int)\n",
    "merged = pd.concat([df1, dummy_ct], axis = \"columns\") #after creating dummies, you merge the dummy with the original dataframe \n",
    "            # and drop df1['country'] and one dummy variable\n",
    "\n",
    "# Define columns to keep\n",
    "colstokeep = ['ABC', 'DEF']\n",
    "\n",
    "# Join dummies to columns to be kept from original dataframe using an identifier that is not in the list of columns to keep\n",
    "df1 = df1[colstokeep].join(dummy_ct.ix[:, 'ct_BR':]).join(dummy_pf.ix[:, 'pf_amazon':])\n",
    "df1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02c23e2d",
   "metadata": {},
   "source": [
    "##### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26720070",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#Encoding Ordinal Categorical Features\n",
    "df = pd.DataFrame({'Score': ['Low', 'Low', 'Medium', 'Medium', 'High']}) # Create features\n",
    "scale_mapper = {'Low':1, 'Medium':2, 'High':3} # Create mapper (Scale Map)\n",
    "df['Scale'] = df['Score'].replace(scale_mapper) # Map feature values to scale\n",
    "df # View data frame\n",
    "\n",
    "#Reshaping and catgorising data\n",
    "df[\"Gender\"] = df[\"Gender\"].map({\"M\":0, \"F\": 1}).astype(float)\n",
    "\n",
    "#One-Hot Encode Features With Multiple Labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Load libraries\n",
    "y = [('Texas', 'Florida')]  # Create NumPy array\n",
    "one_hot = MultiLabelBinarizer() # Create MultiLabelBinarizer object\n",
    "one_hot.fit_transform(y) # One-hot encode data\n",
    "one_hot.classes_ # View classes\n",
    "\n",
    "#One-Hot Encode Nominal Categorical Features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = np.array([['Texas'], ['California'], ['Texas'], ['Delaware'], ['Texas']]) # Create NumPy array\n",
    "one_hot = OneHotEncoder() # Create LabelBinzarizer object (Method 1)\n",
    "one_hot.fit_transform(x) # One-hot encode data\n",
    "one_hot.categories_ # View classes\n",
    "pd.get_dummies(x[:,0]) # Dummy feature (Method 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13e982c7",
   "metadata": {},
   "source": [
    "##### Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20176d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. Handling Imbalanced Classes With Downsampling\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() # Load iris data\n",
    "X = iris.data # Create feature matrix\n",
    "y = iris.target # Create target vector\n",
    "X = X[40:,:] # Remove first 40 observations\n",
    "y = y[40:] \n",
    "y = np.where((y == 0), 0, 1) # Create binary target vector indicating if class 0\n",
    "y # Look at the imbalanced target vector\n",
    "i_class0 = np.where(y == 0)[0] # Indicies of each class' observations\n",
    "i_class1 = np.where(y == 1)[0]\n",
    "n_class0 = len(i_class0) # Number of observations in each class\n",
    "n_class1 = len(i_class1)\n",
    "i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False) # For every observation of class 0, randomly sample from class 1 without replacement\n",
    "np.hstack((y[i_class0], y[i_class1_downsampled])) # Join together class 0's target vector with the downsampled class 1's target vector\n",
    "\n",
    "\n",
    "#B. Handling Imbalanced Classes With Upsampling\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() # Load iris data\n",
    "X = iris.data # Create feature matrix\n",
    "y = iris.target # Create target vector\n",
    "X = X[40:,:] # Remove first 40 observations\n",
    "y = y[40:] \n",
    "y = np.where((y == 0), 0, 1) # Create binary target vector indicating if class 0\n",
    "y # Look at the imbalanced target vector\n",
    "i_class0 = np.where(y == 0)[0] # Indicies of each class' observations\n",
    "i_class1 = np.where(y == 1)[0]\n",
    "n_class0 = len(i_class0) # Number of observations in each class\n",
    "n_class1 = len(i_class1)\n",
    "i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True) # For every observation in class 1, randomly sample from class 0 with replacement\n",
    "np.concatenate((y[i_class0_upsampled], y[i_class1])) # Join together class 0's upsampled target vector with class 1's target vector\n",
    "\n",
    "\n",
    "\n",
    "def check_imbalance(dataset):\n",
    "    \"\"\"\n",
    "    This function takes a dataset as input and returns True if the dataset is imbalanced, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the counts of each class in the dataset\n",
    "    class_counts = dataset['class'].value_counts()\n",
    "    \n",
    "    # Calculate the percentage of each class in the dataset\n",
    "    class_percentages = class_counts / len(dataset) * 100\n",
    "    \n",
    "    # Plot the class percentages\n",
    "    plt.bar(class_counts.index, class_percentages)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if the dataset is imbalanced\n",
    "    if len(class_counts) == 2:\n",
    "        # Binary classification\n",
    "        minority_class = class_counts.index[1]\n",
    "        minority_class_percentage = class_percentages[1]\n",
    "        if minority_class_percentage < 10 or minority_class_percentage > 90:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        class_imbalance = False\n",
    "        for percentage in class_percentages:\n",
    "            if percentage < 10 or percentage > 90:\n",
    "                class_imbalance = True\n",
    "                break\n",
    "        return class_imbalance\n",
    "\n",
    "def check_imbalance(dataset, columns=None, threshold=10):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and one or more columns as input and returns True if any of the specified columns\n",
    "    are imbalanced, False otherwise. A column is considered imbalanced if the percentage of the minority class is less\n",
    "    than the specified threshold.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, use all columns except for the last one as the features\n",
    "    if columns is None:\n",
    "        features = dataset.iloc[:, :-1]\n",
    "        columns = features.columns\n",
    "    \n",
    "    # Check the imbalance of each specified column\n",
    "    for col in columns:\n",
    "        # Get the counts of each class in the column\n",
    "        class_counts = dataset[col].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each class in the column\n",
    "        class_percentages = class_counts / len(dataset) * 100\n",
    "\n",
    "        # Plot the class percentages\n",
    "        plt.bar(class_counts.index, class_percentages)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the column is imbalanced\n",
    "        minority_class = class_counts.index[-1]\n",
    "        minority_class_percentage = class_percentages.iloc[-1]\n",
    "        if minority_class_percentage < threshold:\n",
    "            print(f'{col} is imbalanced. Minority class: {minority_class}, Percentage: {minority_class_percentage:.2f}%')\n",
    "            return True\n",
    "\n",
    "    # If none of the specified columns are imbalanced, return False\n",
    "    print('No imbalance found.')\n",
    "    return False\n",
    "\n",
    "\n",
    "# Oversampling involves increasing the number of instances in the minority class by generating new samples.\n",
    "# This can be done by randomly duplicating existing instances, or by generating synthetic instances using techniques \n",
    "# such as SMOTE (Synthetic Minority Over-sampling Technique). \n",
    "# Undersampling, on the other hand, involves reducing the number of instances in the majority class. This can be done by \n",
    "# randomly removing instances from the majority class, or by selecting a subset of instances based on some criteria, \n",
    "# such as their distance to the minority class.  \n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def handle_imbalanced_data(X, y, strategy='over-sampling'):\n",
    "    \"\"\"\n",
    "    Handle imbalanced data using imblearn library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array-like of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y: array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    strategy: str, default='over-sampling'\n",
    "        The strategy to use for handling imbalanced data. Possible values are\n",
    "        'over-sampling' and 'under-sampling'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled: array-like of shape (n_samples_new, n_features)\n",
    "        The resampled input data.\n",
    "    y_resampled: array-like of shape (n_samples_new,)\n",
    "        The resampled target values.\n",
    "    \"\"\"\n",
    "    if strategy == 'over-sampling':\n",
    "        # Initialize the RandomOverSampler object\n",
    "        ros = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    elif strategy == 'under-sampling':\n",
    "        # Initialize the RandomUnderSampler object\n",
    "        rus = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Possible values are 'over-sampling' and 'under-sampling'.\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def apply_smote(X, y, sampling_strategy='auto', random_state=None):\n",
    "    \"\"\"\n",
    "    Applies Synthetic Minority Over-sampling Technique (SMOTE) to address data imbalance in a binary classification problem.\n",
    "    \n",
    "    SMOTE is a technique for oversampling the minority class in a dataset by generating synthetic examples from the\n",
    "    minority class to balance the class distribution. This function takes the input features (X) and corresponding\n",
    "    labels (y), and applies SMOTE to oversample the minority class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input features.\n",
    "    y : array-like, shape (n_samples,)\n",
    "        The corresponding labels.\n",
    "    sampling_strategy : str or float or dict or callable, optional (default='auto')\n",
    "        The sampling strategy to be applied by SMOTE. This parameter is passed to the `sampling_strategy` argument of\n",
    "        the SMOTE class from the imbalanced-learn library. Possible values:\n",
    "            - 'auto': Resamples the minority class to have the same number of samples as the majority class.\n",
    "            - 'minority': Resamples the minority class to have the same number of samples as the majority class.\n",
    "            - 'not minority': Resamples all classes except the minority class to have the same number of samples as the\n",
    "              majority class.\n",
    "            - 'all': Resamples all classes to have the same number of samples as the majority class.\n",
    "            - float: Resamples the minority class to have the specified ratio of samples compared to the majority class.\n",
    "            - dict: Resamples each class to have the specified number of samples.\n",
    "            - callable: A custom function that defines the sampling strategy.\n",
    "    random_state : int or RandomState or None, optional (default=None)\n",
    "        Seed or random number generator for reproducibility of results.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_resampled : array-like, shape (n_samples_new, n_features)\n",
    "        The oversampled feature matrix.\n",
    "    y_resampled : array-like, shape (n_samples_new,)\n",
    "        The corresponding oversampled labels.\n",
    "    \"\"\"\n",
    "    # Create an instance of SMOTE\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state) #sampling_strategy={1:48050}\n",
    "    \n",
    "    # Apply SMOTE to oversample the minority class\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fee641d0",
   "metadata": {},
   "source": [
    "##### Tips for Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006632cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib Plots\n",
    "\n",
    "#First Chart (single or multiple charts)\n",
    "fig = plt.figure(figsize=(3,2), dpi=100) #figsize(width,height) in inches; dpi\n",
    "\n",
    "axes_1 = fig.add_axes([0.1,0.1,0.9, 0.8]) #left, bottom, width, and height. If you want more than one chart, keep adding new axis\n",
    "axes_1.plot(x, y**2, label = \"First Chart\", color = \"r\", linewidth = 3, linestyle = \":\",\n",
    "            markers = \"*\" , markersize = 10, markerfacecolor = \"yellow\", markeredgewidth = 3, \n",
    "            markeredgewidthecolor = \"green\" , alpha = 0.5) #alpha shows the level of transparency of the line\n",
    "            #you can also use the following: lw = linewidth; ls = linestyle\n",
    "            #for color and customization, you can combine it together: \"r--\"            \n",
    "axes_1.plot(x,y**3, label = \"Second Chart\", \"r\")\n",
    "axes_1.set_title(\"This is my title\")\n",
    "axes_1.set_xlabel(\"This is xlabel\")\n",
    "axes_1.set_ylabel(\"This is ylabel\")\n",
    "axes_1.legend(loc= 0) #adding legends #for location: 0 = best; you can also use tupe (left, bottom) to specify exact location. \n",
    "axes.set_xlim([0, 1]) #plot range for x-axis  #[lower bound, upper bound]\n",
    "axes.set_ylim([0, 2]) #plot range for y-axis  #[lower bound, upper bound]\n",
    "axes_1.tight_layout()\n",
    "fig.savefig(\"my_first_chart.png\") #save figure\n",
    "\n",
    "\n",
    "\n",
    "#Second Chart (single or multiple charts)\n",
    "fig,axes = plt.subplots(nrows = 2, ncols = 2, figsize=(3,2), dpi=100) #figsize(width,height) in inches; dpi. #You then specify the number of plots based on the rows and columns\n",
    "\n",
    "plt.tight_layout() #to fix the issue of overlapping\n",
    "axes[o,1].plot(x,y, label = \"First Chart\", \"r\") \n",
    "axes[0,1].set_title(\"This is the title for the first chart\")\n",
    "axes[0,1].set_xlabel(\"This is xlabel\")\n",
    "axes[0,1].set_ylabel(\"This is ylabel\")\n",
    "axes[0,1].legend(loc= 0) #adding legends #for location: 0 = best; you can also use tuple (left, bottom) to specify exact location.\n",
    "axes.set_xlim([0, 1]) #plot range for x-axis  #[lower bound, upper bound]\n",
    "axes.set_ylim([0, 2]) #plot range for y-axis  #[lower bound, upper bound]\n",
    "\n",
    "axes[1].plot(x,y, label = \"Second Chart\")\n",
    "axes[1].set_title(\"xxxxxxx\")\n",
    "axes[1].set_xlabel(\"This the label for the x-axis\")\n",
    "axes[1].set_ylabel(\"This is ylabel\")\n",
    "axes[1].legend(loc= 0) #adding legends #for location: 0 = best; you can also use tuple (left, bottom) to specify exact location.\n",
    "axes.set_xlim([0, 1]) #plot range for x-axis  #[lower bound, upper bound]\n",
    "axes.set_ylim([0, 2]) #plot range for y-axis  #[lower bound, upper bound]\n",
    "fig.savefig(\"my_first_chart.png\") #save figure\n",
    "\n",
    "#NB: other important things you can use in your plots\n",
    "sns.boxplot(x=\"Year\", y=\"Deaths 70+ years\", hue='Entity', data=focus_countries, palette='mako')\n",
    "plt.title('Deaths caused by Cardiovascular deaths for 70+ Years')\n",
    "ax = plt.gca() # Get the Axes object\n",
    "ax.set_xlim(0, 50) # Set the x-axis range\n",
    "ax.xaxis.set_ticks(range(0, 55, 5)) # Set the number of x-axis values to display or ax.xaxis.set_ticks([1990, 1995, 2000, 2005, 2010])\n",
    "ax.set_xticks([1990, 1995, 2000, 2005, 2010, 2015, 2020])\n",
    "ax.set_xticklabels([\"1990\", \"1995\", \"2000\", \"2005\", \"2010\", \"2015\", \"2020\"])\n",
    "plt.savefig(\"C:\\\\myDrive\\xx_image.png\")\n",
    "\n",
    "#OR\n",
    "fig,ax = plt.subplots(nrows = 2, ncols = 2, figsize=(3,2), dpi=100)\n",
    "pivot.plot(kind='line', ax = ax)\n",
    "plt.xlabel(\"Year\") # Add labels and title to the plot\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.title(\"Deaths by Cardiovascular diseases\")\n",
    "plt.show() # Show the plot\n",
    "fig.savefig(\"C:\\\\myDrive\\xx_image.png\")\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#seaborn inbuilt dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "#Dist plot\n",
    "Dist plot allows us to show the distribution of a univariate set of observations \n",
    "KDE (kernel density estimates) A kernel density estimate (KDE) is a non-parametric method for estimating the \n",
    "    probability density function (PDF) of a random variable. It is the curve on the histogram. \n",
    "#bins are each rectangles in the histogram. \n",
    "\n",
    "sns.distplot(tips[\"total_bill\"]) #or\n",
    "sns.histplot(tips[\"total_bill\"], kde=True, stat=\"density\", linewidth=0, bins = 30)\n",
    "\n",
    "#joint plots\n",
    "allows you to basically match up two distplots for bivariate data\n",
    "#kind allows you to affect what is going on inside of the joint plot. default is scattered. \n",
    "    #hex allows you to make a hexagon distribution representation\n",
    "    #reg - regression \n",
    "    #kde - kernel density estimates\n",
    "sns.jointplot(x= \"total_bill\",y= \"tip\",data = tips, kind= \"hex\")\n",
    "\n",
    "#pairplot (very useful)\n",
    "this plots pairwise relationships across the entire dataframe (at least for the numerical columns). Very important\n",
    "    This is like a jointplot but for the entire dataframe\n",
    "#hue - use hue to access and explore the categorical columns. Simply input the categorical column name\n",
    "sns.pairplot(tips, hue=\"sex\", palette=\"coolwarm\")\n",
    "\n",
    "#rugplot\n",
    "sns.rugplot(tips)\n",
    "\n",
    "#kde plots\n",
    "sns.kde(tips[\"total_bill\"])\n",
    "\n",
    "#Line Plots\n",
    "import plotly.express as px\n",
    "fig = px.line(data_frame=energy_per_day.filter(items=['Dishwasher [kW]', 'Kitchen 14 [kW]', 'Kitchen 38 [kW]',\n",
    "                                                       'Microwave [kW]', 'Living room [kW]', 'Solar [kW]']),\n",
    "              line_dash_sequence=['solid']*16, width=900, height=600)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#CATEGORICAL PLOTS\n",
    "\n",
    "#bar plots\n",
    "allows you to aggregate the categorical data based off of a function (by default it is the mean)\n",
    "    to change the aggregate, use the estimator and specify the new aggregate. \n",
    "sns.barplot(x= \"sex\", y = \"total_bill\", data = tips, estimator=np.std)\n",
    "\n",
    "#countplot\n",
    "essentially the same as a bar plot, except the estimator is explicitly counting the number of ocurences. \n",
    "    Hence we only set the x value\n",
    "sns.countplot(x = \"sex\", data = tips)\n",
    "\n",
    "#box plots and violin plots\n",
    "useed to show the distribution of categorical data. \n",
    "    A box plot is aka box and wisker plot. it shows the dist of quantitative data in a way that \n",
    "        facilitates comparison btw variableables\n",
    "    x = categorical data; y = numerical data \n",
    "    you can also use \"hue\" for both plots \n",
    "sns.boxplot(x = \"day\", y = \"total_bill\", data=tips) \n",
    "sns.violinplot(x = \"day\", y = \"total_bill\", data=tips) \n",
    "sns.boxplot(x = \"day\", y = \"total_bill\", data=tips, hue=\"sex\", split = True) #if you decide to use \"hue\" \n",
    "\n",
    "#strip plots\n",
    "it will draw a scatter plot where one variable is categorical. \n",
    "sns.stripplot(x = \"day\", y = \"total_bill\", data=tips, jitter=True )\n",
    "sns.stripplot(x = \"day\", y = \"total_bill\", data=tips, jitter=True, hue=\"sex\", split = True) #if you decide to use \"hue\"\n",
    "\n",
    "#swarm plots (you really don't need this type of plot)\n",
    "often used as a combinantion of strip plots and violin plots\n",
    "the points of a swam plot are adjusted so that it doesn not overlap\n",
    "the drawback of swarm plots are that they do not scale well to very large numbers. \n",
    "sns.swarmplot(x = \"day\", y = \"total_bill\", data=tips)\n",
    "\n",
    "#factor plot (or catplot)\n",
    "this is the most general form of all the above plots\n",
    "you can change the kind to \"bar\", 'strip','swarm','box','violin', \"hex\", \"reg\" etc. for additional plots. \n",
    "sns.catplot(x = \"day\", y = \"total_bill\", data=tips, kind = \"bar\")\n",
    "\n",
    "\n",
    "\n",
    "#MATRIX PLOTS\n",
    "you must first convert a datset into a matrix form. To do that use a pivot table, or try to get correlation data\n",
    "    corelation data method: tips_corr = tips.corr()\n",
    "    pivot table method: flights.pivot_table(index = \"month\", columns = \"year\", values= \"passengers\")\n",
    "cmap: color map. a lot of options from \"coolwarm\", \"magma\" etc. \n",
    "sns.heatmap(tips_corr, annot=True, cmap=\"coolwarm\", linecolor=\"white\",linewidths=0.5)\n",
    "sns.clustermap(tips_corr, standard_scale=1) #you can also add clustermap to the heatmap\n",
    "#NB: A heat map will display things in the order you put them in, \n",
    "    #a cluster map will cluster things in an order so that imilar groups are close to each other\n",
    "\n",
    "\n",
    "\n",
    "#GRIDS ()\n",
    "1. grid_m = sns.PairGrid(tips) #similar to pair plots for plotting multiple plots\n",
    "grid_m.map(plt.scatter)\n",
    "\n",
    "#for additional functionalities\n",
    "grid_m.map_diag(sns.distplot) #map a distribution plot on the diagonal grids \n",
    "grid_m.map_upper(plt.scatter) #map a scatter plot on the upper part of the pair grids created\n",
    "grid_m.map_lower(sns.kdeplot) #map a kde plot on the lower part of the pair grids created\n",
    "\n",
    "2. grid_p = sns.FacetGrid(data=tips, col=\"time\", row=\"smoker\")\n",
    "grid_p.map(sns.distplot, \"total_bill\")\n",
    "#or use the other additional functionalities above. \n",
    "\n",
    "\n",
    "\n",
    "#CHLOROPLETH MAPS\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data = {'Country': ['Algeria', 'Egypt', 'South Africa', 'Kenya', 'Morocco', 'Tunisia'],\n",
    "        'Year': [2020, 2020, 2020, 2020, 2020, 2020],\n",
    "        'Deaths': [1000, 800, 1500, 700, 500, 600]}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot a choropleth map of Africa showing the number of deaths by country and year\n",
    "fig = px.choropleth(df, locations=\"Country\", locationmode='country names', color=\"Deaths\",\n",
    "                    title='Deaths in Africa by Country and Year', color_continuous_scale=\"Viridis\",\n",
    "                    animation_frame=\"Year\", animation_group=\"Country\", hover_name=\"Country\",\n",
    "                    height=500, width=1000)\n",
    "\n",
    "fig.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.844px",
    "left": "996px",
    "right": "20px",
    "top": "118px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
