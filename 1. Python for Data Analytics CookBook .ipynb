{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0637b7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/prashant111/code \n",
    "\n",
    "#for all the notebook tutorials on Machine Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79c2c80",
   "metadata": {},
   "source": [
    "#### Python Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3b5cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Operators \n",
    "    # - Arithmetic operators: +, -, *, **, /, // (floor division), % (modulus) \n",
    "    # - Comparison operators: ==, !=, >, <, >=, <= (returns True or False) \n",
    "    # - Logical operators: and, or, not \n",
    "    # - Identity Operators: is, is not\n",
    "    # - Assignment operators: =, +=, -=, *=, /=, //=, %=, &=, |=, \n",
    "    # - Membership operators: in, not in \n",
    "    # - Bitwise shift operators: <<, >> \n",
    "    # - Unary operators: +, -, ~, not \n",
    "    # - String concatenation operator: \"+\" \n",
    "\n",
    "\n",
    "# # Lambda Function\n",
    "    # - The lambda function can be used to create small anonymous functions.\n",
    "    #   Syntax: lambda arguments : expression\n",
    "        # Example: f = lambda x : 2*x+3\n",
    "        #          print(f(5)) -> prints 16\n",
    "\n",
    "\n",
    "# Data types\n",
    "    myInt = int() # 1. Integer: A whole number like 4203987654.\n",
    "    myFloat = float() # 2. Floats/Doubles: Decimal numbers with a decimal point such as 3.14159.\n",
    "    myString = \"\" # 3. String: Text enclosed by quotes \"example\" or 'example'.\n",
    "    myBool = False # 4. Boolean: True of False.\n",
    "    myList = [] # 5. Lists: ordered collection of items inside square brackets [].\n",
    "    myTuple = () # 6. Tuples: ordered collection of immutable items inside parentheses ().\n",
    "    mySet = set() # 7. Sets: unordered collection of unique elements {}.\n",
    "    myDict = {} # 8. Dictionaries: key : value pairs {}\n",
    "\n",
    "\n",
    "# Control Flow Statements\n",
    "    # 1. if statement: checks condition and executes code block if true.\n",
    "    # 2. elif statement: else if clause to an if statement.\n",
    "    # 3. else statement: executes when no other conditions are met.\n",
    "    # 4. while loop: repeats until specified condition is false.\n",
    "    # 5. break statement: breaks out of current closest looping structure.\n",
    "    # 6. continue statement: skips rest of current iteration and goes back to beginning of next iteration.\n",
    "    # 7. for loops: iterate over each item in something iterable(list, tuple etc.).\n",
    "    \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterable:\n",
    "# Iterable: An object is called iterable if we can get an iterator from it. Examples of iterables include all sequence types \n",
    "# (such as list, str, tuple) and some non-sequence types like dict, file objects, and objects of any classes you define with an __iter__() \n",
    "# or __getitem__() method.\n",
    "\n",
    "# Iterator:\n",
    "# Iterator: An iterator is an object that can be iterated upon and which returns data, one element at a time when next() is called on it. \n",
    "# In Python, an iterator must implement two methods:  \n",
    "    # __iter__() which returns the iterator object itself. This is used in for and in statements.\n",
    "    # __next__() which returns the next value from the iterator. If there is no more items to return, it should raise StopIteration exception.\n",
    "    my_list = [\"go\", \"hello\", 23, 10, 12, 19]\n",
    "    my_iter = iter(my_list)  # Create an iterator\n",
    "\n",
    "    for _ in range(4):\n",
    "        print(next(my_iter))\n",
    "    \n",
    "    \n",
    "    \n",
    "#basic Python Functions and Operations\n",
    "# built in functions\n",
    "\n",
    "int, bool, tuple, str, chr, list, set, dict, \n",
    "all, any, dir, type, help,\n",
    "enumerate, filter, zip, zip(*zipped)-> unzip, input, isinstance, iter or __iter__(), next or __next__(),\n",
    "len, map, min, max, pow, print, range, reversed, round, sorted, sum, super, \n",
    "\n",
    "#functions of list, strings, dictionary, tuples etc\n",
    "dir(list)\n",
    "dir(str)\n",
    "dir(dict)\n",
    "dir(tuple)\n",
    "\n",
    "# python built-in functions\n",
    "import builtins\n",
    "dir(builtins)\n",
    "\n",
    "# other python libraries\n",
    "import pkgutil\n",
    "standard_lib = sorted([module for _, module, _ in pkgutil.iter_modules()])  # sorted list of all standard library module names.\n",
    "\n",
    "for module in standard_lib:\n",
    "    print(module)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cb913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#List Manipulation\n",
    "val = [10, 12, 23, 24, 22]  # Sample List for reference\n",
    "\n",
    "\" \".join(my_list)   # convert a list to string\n",
    "my_string.split(\" \") # convert a string to list | or list(my_string)\n",
    "del val[1]  # Removing an item by index (delete)\n",
    "first_item = val[0]  # Accessing an item\n",
    "first_two = val[:2]  # Slicing the list\n",
    "list('Hello') # Creates a list from the string.\n",
    "list(range(5)) # Creates a list from the range object.\n",
    "list.append('Hello') # Adds an item to the end of the list.\n",
    "list.count('Hello') # Counts the occurrences of an item in the list.\n",
    "list.extend('Hello') # Adds all items of an iterable to the end of the list.\n",
    "list.index('Hello') # Returns the index of an item in the list.\n",
    "list.insert(0, 'Hello') # Inserts an item at a specified position in the list.\n",
    "list.pop() # Removes an item from the list.\n",
    "list.remove('Hello') # Removes the first occurrence of an item from the list.\n",
    "list.reverse() # Reverses the order of the items in the list.\n",
    "list.sort() # Sorts the items in the list.\n",
    "val.sort(key=lambda x: x[0])  # Sorting the list by the first element\n",
    "for i in val:\n",
    "    if isinstance(i, list):\n",
    "        print(\"List\")\n",
    "print(\",\".join(sorted(list(set(words)))))\n",
    "\n",
    "\n",
    "# List Operations\n",
    "val.clear()  # Clearing the list\n",
    "copied = val.copy()  # Copying the list\n",
    "repeated = val * 2  # Multiplying the list (repeating it)\n",
    "\n",
    "\n",
    "#List Comprehensions (with Lambda Functions)\n",
    "        # Expression: [expression for item in iterable if condition]\n",
    "filtered = [item for item in val if item[0] == 20]  # Filtering the list\n",
    "[num for sublist in val for num in sublist]  # Flattening the list\n",
    "[[x+1, y+1] for x, y in val]  # Increase each element\n",
    "[i**3 if i %2 else i**2 for i in numbers]\n",
    "are_all_greater_than_20 = all(x[0] > 20 for x in val)  # Check if all items meet a condition\n",
    "is_any_greater_than_50 = any(x[1] > 50 for x in val)  # Check if any item meets a condition\n",
    "flattened_even = [y for x in val for y in x if y % 2 == 0]  # Nested list comprehension\n",
    "\n",
    "\n",
    "# Advanced List Processing\n",
    "another_list = [[30, 40], [40, 50]]  # Combining with another list using zip\n",
    "combined = list(zip(val, another_list))\n",
    "first_elements, second_elements = zip(*val)  # Unzipping a list of pairs\n",
    "as_dict = dict(val)  # Converting list of pairs to a dictionary\n",
    "\n",
    "\n",
    "# List Statistics and Queries\n",
    "count = val.count([20, 45])  # Counting occurrences\n",
    "length = len(val)  # Length of the list\n",
    "exists = [20, 45] in val  # Checking if an item exists in the list\n",
    "maximum = max(val, key=lambda x: x[0])  # Finding the maximum based on the first element\n",
    "minimum = min(val, key=lambda x: x[0])  # Finding the minimum based on the first element\n",
    "total = sum(item[0] for item in val)  # Summing the first elements\n",
    "unique_set = set(tuple(item) for item in val)  # Converting list to a set (for unique values) - set is an unordered collection of unique items\n",
    "\n",
    "\n",
    "# Functional Programming with Lambda\n",
    "        # Expression: lambda arguments: expression\n",
    "            add = lambda x, y: x + y\n",
    "filtered_lambda = list(filter(lambda x: x[0] > 20, val))  # Filtering using a lambda function\n",
    "mapped_lambda = list(map(lambda x: [x[0]*2, x[1]*2], val))  # Mapping using a lambda function\n",
    "max_num = lambda x, y: x if x > y else y  # Using lambda to find the max value\n",
    "\n",
    "\n",
    "# String Manipulation\n",
    "my_string = \"Hello World\"\n",
    "reversed_joined_string = \" \".join(reversed(string.split()))  # Split, reverse, and join to a string\n",
    "new_string = string[:4] + string[4+1:]    # using string concatenation to delete an item\n",
    "new_string = string.replace('0', \"\") # using replace to delete an item\n",
    "new_string = reversed(string)  # Reverse order strings\n",
    "\n",
    "# Advanced List and String Manipulation\n",
    "joined = ', '.join(map(str, val))  # Joining the list into a string\n",
    "str_val = str(val)  # Converting to string and back to list\n",
    "back_to_list = eval(str_val)\n",
    "k = 2  # Rotate by 2 positions\n",
    "rotated = val[k:] + val[:k]  # Rotate the list\n",
    "half = len(val)//2  # Split the list into two halves\n",
    "first_half, second_half = val[:half], val[half:]\n",
    "\n",
    "\n",
    "my_string.lower() # Converts all characters in the string to lowercase. Frequently used for case-insensitive comparisons.\n",
    "my_string.upper() # Converts all characters in the string to uppercase. Often used for standardizing text input.\n",
    "my_string.replace() # Replaces occurrences of a specified substring with another substring.\n",
    "my_string.split() # Divides a string into a list based on a specified delimiter.\n",
    "my_string.join() # Combines a list of strings into a single string, using a specified separator.\n",
    "my_string.find() # Searches for a substring within a string and returns the lowest index where the substring is found.\n",
    "my_string.strip() # Removes leading and trailing characters (spaces by default) from a string.\n",
    "my_string.startswith() # Checks if the string starts with the specified substring.\n",
    "my_string.endswith() # Checks if the string ends with the specified substring.\n",
    "my_string.count() #  Counts occurrences of a substring within the string.\n",
    "my_string.format() # Formats the string in a specified format.\n",
    "my_string.capitalize() # Capitalizes the first character of the string.\n",
    "my_string.title() # Converts the first character of each word to uppercase and the rest to lowercase.\n",
    "\n",
    "\n",
    "my_string.isalnum()   # Checks if all characters in the text are alphanumeric. \n",
    "my_string.isalpha()   # Checks if all characters in the text are letters.\n",
    "my_string.isnumeric() # Checks if all characters in the text are numeric.\n",
    "my_string.isdigit()   # Checks if all characters in the text are digits.\n",
    "my_string.is_integer() # Checks if a floating-point number is an integer.\n",
    "my_string.isdecimal() # Checks if all characters in the text are decimals.\n",
    "my_string.isascii()   # Checks if all characters in the text are ASCII. (ord('A) or chr(65))\n",
    "my_string.isupper()   # Checks if all characters in the text are uppercase.\n",
    "my_string.islower()   # Checks if all characters in the text are lowercase.\n",
    "my_string.isinstance()    # Checks if an object is an instance of a specified type.\n",
    "my_string.issubclass()    # Checks if a class is a subclass of a specified class.\n",
    "my_string.isidentifier()  # Checks if the string is a valid identifier.\n",
    "my_string.isprintable()   # Checks if all characters in the text are printable.\n",
    "my_string.isspace()   # Checks if all characters in the text are whitespaces.\n",
    "my_string.istitle()   # Checks if the string follows the title case style.\n",
    "\n",
    "\n",
    "# Dictionary \n",
    "    my_dict = {'key1': 'value1', 'key2': 'value2', 'key3': 'value3'}\n",
    "    new_dict = {'a': 1, 'b': 2}\n",
    "    my_dict_2 = {'b': 3, 'c': 4}\n",
    "\n",
    "new_dict.update(dict2) # merge dictionaries (similar to append or extend for lists)\n",
    "my_dict.items()    # provides a view on the key-value pairs.\n",
    "my_dict.keys() # provides a view on the keys.\n",
    "my_dict.values()   # provides a view on the values.\n",
    "my_dict[key]    # accessing values\n",
    "my_dict[key] = value    # adding or updating\n",
    "dict1.copy()   # copying a dictionary\n",
    "dict1.clear() # clearing all items\n",
    "dict1.pop('a', 'default value if key not found')   # removes a key-value pair and returns the value.\n",
    "dict1.get('b', 'default value')    # access the value associated with a key, while providing a default value if the key is not found.\n",
    "del dict1['b'] #delete a key\n",
    "{value : key for key, value in dict_one.items() if condition}    #swap keys and values in a dictionary\n",
    "\n",
    "\n",
    "# Using external moduls\n",
    "import itertools\n",
    "chained = list(itertools.chain.from_iterable(val))  # Using itertools.chain to flatten\n",
    "combinations = list(itertools.combinations(val, 2))  # Using itertools.combinations\n",
    "permutations = list(itertools.permutations(val, 2))  # Using itertools.permutations\n",
    "\n",
    "sorted_val = sorted(val, key=lambda x: x[0])  # Using sorted and itertools.groupby\n",
    "grouped = {k: list(g) for k, g in itertools.groupby(sorted_val, key=lambda x: x[0])}\n",
    "\n",
    "import random\n",
    "random.shuffle(val)  # Shuffling the list\n",
    "sample = random.sample(val, 2)  # Taking random samples\n",
    "\n",
    "\n",
    "#Others\n",
    "from functools import reduce\n",
    "total_sum = reduce(lambda acc, x: acc + x[0] + x[1], val, 0)  # Using reduce to sum elements\n",
    "\n",
    "for idx, item in enumerate(val):  # Using enumerate to get index and item\n",
    "    print(idx, item)\n",
    "\n",
    "index_of_first_greater_than_20 = next((i for i, x in enumerate(val) if x[0] > 20), None)  # Finding index with a condition\n",
    "\n",
    "import copy\n",
    "deep_copied = copy.deepcopy(val)  # Deep copy of the list\n",
    "\n",
    "\n",
    "\n",
    "# working with time\n",
    "import time\n",
    "from datetime import datetime, time\n",
    "import pytz #this is much easier to work with time\n",
    "\n",
    "nigeria_timezone = pytz.timezone('Africa/Lagos')    # Set the timezone to Nigeria\n",
    "current_time = datetime.now(nigeria_timezone)   # Get the current time\n",
    "current_time = datetime.datetime.now(nigeria_timezone) # Get the current time in Nigeria\n",
    "formatted_time = current_time.strftime(\"%H:%M:%S\")  # Format the time as desired\n",
    "start_time = time.time()\n",
    "end_time = = time.time()\n",
    "print(\"Time taken by function is %s seconds\" % (end_time - start_time))\n",
    "\n",
    "\n",
    "\n",
    "#Getting Help and Documentation\n",
    "?list.pop  # Get help on the pop method of a list\n",
    "?max # or max? - if it is a baseline function\n",
    "??list.pop  # View the source code of the pop method if possible\n",
    "help(list.pop)  # Get help on the pop method of a list\n",
    "print(list.pop.__doc__)  # Print the documentation string of the pop method\n",
    "print(dir(list))  # List all attributes and methods of the list class\n",
    "\n",
    "import numpy as np  #checking documentation from external libraries\n",
    "help(np.some_function)\n",
    "\n",
    "# Each datatype has its own unique methods, hence its own way of finding documentation\n",
    "my_string = \"Hello, World!\"     # for string\n",
    "help(str.upper)\n",
    "help(my_list.append)    # for list\n",
    "help(my_dict.get)   # for dictionary\n",
    "help(my_tuple.index)   # for tuple\n",
    "help(my_float.is_integer)  # for floating point number\n",
    "\n",
    "\n",
    "import builtins\n",
    "\n",
    "# List all built-in functions and classes\n",
    "builtins_list = dir(builtins)\n",
    "print(builtins_list)\n",
    "print(dir(list))\n",
    "print(dir(str))\n",
    "print(dir(dict))\n",
    "print(dir(tuple))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89347b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lambda Functions: Anonymous functions defined using the lambda keyword.\n",
    "    # Expression: lambda arguments: expression\n",
    "add = lambda x, y: x + y\n",
    "result = add(3, 5)  # result is 8\n",
    "\n",
    "\n",
    "# List Comprehensions: Concise syntax for creating lists.\n",
    "    # Expression: [expression for item in iterable if condition]\n",
    "numbers = [1, 2, 3, 4, 5]\n",
    "squares = [x ** 2 for x in numbers if x % 2 == 0]\n",
    "# squares will be [4, 16]\n",
    "\n",
    "\n",
    "# Generators: Functions that yield values one at a time using the yield keyword.\n",
    "    # Expression: def generator_function(): yield value\n",
    "def count_up_to(n):\n",
    "    i = 1\n",
    "    while i <= n:\n",
    "        yield i\n",
    "        i += 1\n",
    "\n",
    "gen = count_up_to(5)\n",
    "for num in gen:\n",
    "    print(num)  # Prints 1, 2, 3, 4, 5\n",
    "\n",
    "# Classes:\n",
    "class Person:\n",
    "    def __init__(self, height, shoe_size, shirt_size, pant_size):\n",
    "        self.height = height\n",
    "        self.shoe_size = shoe_size\n",
    "        self.shirt_size = shirt_size\n",
    "        self.pant_size = pant_size\n",
    "\n",
    "    def school(self):   # object functions (methods)\n",
    "        if (self.height > 6) and (self.pant_size) > 35:\n",
    "            return \"University of Oklahoma'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def work(self, name): # object function (methods)\n",
    "        print(\"{name} is going to work.\")\n",
    "\n",
    "Paul = Person(6.5, 13, \"XXL\", 38)  #instantiating the class (an object)\n",
    "Paul.height # assessing an attribute in the object\n",
    "Paul.school()       # calling a method of the class\n",
    "dir(class_name) # this gives you a list of all the methods of the class.\n",
    "\n",
    "\n",
    "# Decorators: Functions that modify other functions or methods.\n",
    "    # Expression: @decorator_function\n",
    "def my_decorator(func):\n",
    "    def wrapper():\n",
    "        print(\"Something is happening before the function is called.\")\n",
    "        func()\n",
    "        print(\"Something is happening after the function is called.\")\n",
    "    return wrapper\n",
    "\n",
    "@my_decorator\n",
    "def say_hello():\n",
    "    print(\"Hello!\")\n",
    "\n",
    "say_hello()\n",
    "\n",
    "\n",
    "def fibonacci (numb):\n",
    "    if numb < 1:\n",
    "        return \"enter a valid integer > 0\"\n",
    "    elif numb == 1:\n",
    "        return [0]\n",
    "    elif numb == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        fibonacci = [0, 1]\n",
    "        for i in range(2, numb):\n",
    "            fibonacci.append(fibonacci[-1] + fibonacci[-2])\n",
    "        return fibonacci\n",
    "        \n",
    "\n",
    "print(fibonacci(9))\n",
    "# Context Managers: Used with the with statement to manage resources.\n",
    "    # Expression: with context_manager as resource:\n",
    "with open(\"file.txt\", \"r\") as file:\n",
    "    data = file.read()\n",
    "# File is automatically closed when exiting the 'with' block.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a723f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_library (module)\n",
    "|\n",
    "|--- Animal (class)\n",
    "|    |\n",
    "|    |--- __init__(self, name) (method)\n",
    "|    |    |--- self.name (attribute)\n",
    "|    |\n",
    "|    |--- speak(self) (method)\n",
    "|\n",
    "|--- Dog (class, inherits from Animal)\n",
    "|    |\n",
    "|    |--- __init__(self, name, breed) (method)\n",
    "|    |    |--- self.breed (attribute)\n",
    "|    |\n",
    "|    |--- speak(self) (method)\n",
    "|    |--- fetch(self, item) (method)\n",
    "|\n",
    "|--- Cat (class, inherits from Animal)\n",
    "     |\n",
    "     |--- __init__(self, name, color) (method)\n",
    "     |    |--- self.color (attribute)\n",
    "     |\n",
    "     |--- speak(self) (method)\n",
    "     |--- purr(self) (method)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the Animal base class\n",
    "class Animal:\n",
    "    def __init__(self, name):\n",
    "        self.name = name  # All animals will have a name\n",
    "\n",
    "    def introduce(self):\n",
    "        return f\"I am an animal and my name is {self.name}.\"\n",
    "\n",
    "# Define the Dog class which inherits from Animal\n",
    "class Dog(Animal):\n",
    "    def __init__(self, name, breed):\n",
    "        super().__init__(name)  # Call the superclass constructor to set the name\n",
    "        self.breed = breed  # Dog-specific attribute\n",
    "\n",
    "    def introduce(self):\n",
    "        # Override the method to include breed specific introduction\n",
    "        return f\"I am a {self.breed} dog and my name is {self.name}.\"\n",
    "\n",
    "# Create an instance of Dog\n",
    "my_dog = Dog(\"Buddy\", \"Golden Retriever\")\n",
    "\n",
    "# Call the introduce method\n",
    "print(my_dog.introduce())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numpy Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Numpy \n",
    "#1D - Vectors\n",
    "#2D - Matrices\n",
    "import numpy as np\n",
    "\n",
    "arr = np.array([[1,2,3], [1,2,3]])\n",
    "arr\n",
    "arr[~np.isnan(arr)] #filter array 'arr' where there are no missing values.\n",
    "\n",
    "\n",
    "#Numpy built-in functions\n",
    "    #creating arrays\n",
    "np.arange(0,11,2) #(start, stop, step)\n",
    "np.linspace(1,5,10) #(start, stop, number of items)\n",
    "np.zeros((3,3)) #(row, col)\n",
    "np.zeros_like(arr)\n",
    "np.ones((2,3)) #(row, col)\n",
    "np.ones_like(arr)\n",
    "np.eye(4) #identity matrix\n",
    "np.empty_like()\n",
    "\n",
    "\n",
    "#creating arrays of random numbers\n",
    "np.random.rand(5) #random samples of a uniform distribution from 0-1\n",
    "    np.random.rand (5,5) #2D random samples from 0-1\n",
    "np.random.randn(2) #random samples from a standard normal distribution centered around zero (0)\n",
    "np.random.randint(5) #returns random integers from a low to a high number\n",
    "    np.random.randint(1,100,10) #(low, high, size) #inclusive on the low end, exclusive on the high end\n",
    "np.random.permutation(np.arange(10)) #to shuffle\n",
    "np.random.choice(a=(3, 10), size=(3,5), replace=False) # returns a random sample from a range of 'a' values for the inputted size. \n",
    "np.random.sample(size=(1, 5), k = 5) #returns k random samples of the inputted size \n",
    "\n",
    "\n",
    "#attributes and methods of Array\n",
    "arr = np.array(range(10))\n",
    "arr.reshape(5,5) #reshape an array (row, col)\n",
    "arr.shape #find the shape of an array\n",
    "\n",
    "\n",
    "#Numpy Indexing and Slicing\n",
    "arr[1:3] = array([1,2]) #for 1D #inclusive on the low end and exclusive on the high end\n",
    "arr[1:2, 0:1] #for 2-D (row, col)\n",
    "arr[0][2] #[row][col]\n",
    "\n",
    "arr[1:2] = 10 #Broadcasting an array #its always best to make a copy before broadcasting an array\n",
    "    arr.copy() #to copy an array\n",
    "\n",
    "\n",
    "#Conditional selection\n",
    "arr[arr>3]\n",
    "arr[(arr<8) & (arr > 4)] #and\n",
    "arr[(arr>5) | (arr < 3)] #or\n",
    "\n",
    "\n",
    "#Useful functions in Numpy\n",
    "np.where(arr==3), np.argwhere(arr==3) or np.unravel_index(3, arr) #find index of 3 in array \"arr\"\n",
    "np.repeat(3,5) or np.tile(arr,5) #repeat number 3, 5 times. #repeat - 1D; tile - 2D\n",
    "    np.tile(arr, (2,2)) #repeat array \"arr\" in 2 rows and 2 cols\n",
    "# we can also have: np.where(condition, x, y), which means \"Replace elements in x with y where the condition is False.\"\n",
    "\n",
    "#plot\n",
    "np.polyfit(x,y)\n",
    "np.polyval(p,x)\n",
    "np.histogram(a, bins=10, range=None, normed=None)\n",
    "\n",
    "np.full((2,4), fill_value=2) #fill an array of 2 rows, 4 cols with the value 2\n",
    "np.unique(arr, return_counts=True) #return_counts = If True, also return the number of times each unique item appears\n",
    "np.T(arr) or arr.T (transpose)\n",
    "\n",
    "#maths\n",
    "np.mean()\n",
    "np.describe()\n",
    "np.sum()\n",
    "np.sort()\n",
    "np.flatten(arr) #flatten from x-dim to 1D\n",
    "np.add(arr_1, arr_2)\n",
    "np.subtract(x,y)\n",
    "np.multiply(x,y)\n",
    "np.divide(x,y)\n",
    "np.min(arr)\n",
    "np.max(arr)\n",
    "np.power(arr)\n",
    "np.sqrt(arr)\n",
    "np.sin(arr)\n",
    "np.floor(arr)\n",
    "np.round(arr)\n",
    "np.std(arr)\n",
    "np.abs(arr)\n",
    "np.var(arr)\n",
    "\n",
    "np.digitize(x, bins, right=False) #to xxxx\n",
    "np.shape or np.reshape()\n",
    "np.expand_dims(ar, axis=0) or np.flatten(arr, axis=0) or np.squeeze(arr, axis=0)\n",
    "np.count_nonzero(arr) #count all non-zero elements\n",
    "np.argwhere(arr)\n",
    "np.argmin(arr) or np.argmax(arr)\n",
    "arr.clip(0,5) #to keep all values of an array within a range\n",
    "np.put(arr, [1,2], [6,7]) #(arr, (indices = [where to replace]), (value = [what to replace it with]))\n",
    "np.astype(int) #change the datatype to integer\n",
    "np.astype(float) #change the datatype to float\n",
    "np.transpose(image, (1, 2, 0)) \n",
    "\n",
    "#set operations\n",
    "np.intersect1d(arr1,arr2) #(arr1 n arr2 ) returns all unique values from the two arrays\n",
    "np.setdiff1d(a,b) #returns al unique elements of a not in b\n",
    "np.setxor1d(a,b) #same with above but in sorted order\n",
    "np.union1d(a,b) (a U b)\n",
    "\n",
    "#splitting\n",
    "np.hsplit(arr, 2) #split the data horizontally (rows) into two equal parts\n",
    "np.vsplit (arr, 3) #split the data vertically (cols) into three equal parts\n",
    "\n",
    "#stacking\n",
    "np.hstack((a, b)) #stack/appends b unto a horizontally (rows)\n",
    "np.vstack((a,b)) #stack/appends b unto a vertically (cols)\n",
    "\n",
    "#comparing\n",
    "np.allclose(a,b, tolerance) #compares two array base in tolerance level\n",
    "np.equal(a,b) #compares elements of both arrays\n",
    "\n",
    "#print options\n",
    "np.set_printoptions(precision=2) #show floats within two decimal points\n",
    "np.set_printoptions(threshold=np.inf) #print array to its max (np.inf - positive infinity)\n",
    "np.printoptions(linewidth = 100) #increase the number of elements in a line\n",
    "\n",
    "#save and load array\n",
    "np.savetxt(\"array.txt\", arr)\n",
    "np.loadtxt(\"array.txt\")\n",
    "\n",
    "#multiply\n",
    "np.multiply(a,b) #multiply two arrays of the same shape\n",
    "np.dot(a,b) #multiply two arrays of different shapes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.Dataframe()\n",
    "\n",
    "# Most used Pandas DataFrame methods - dir(pd.DataFrame)\n",
    "\n",
    "df.head() / tail() # Display the first or last few rows of the DataFrame.\n",
    "df.loc[] / iloc[] # Access a group of rows and columns by label(s) or integer index.\n",
    "df.columns()\n",
    "df.index() / df.reset_index() / df.set_index()  # Set or Reset the index of the DataFrame, and use the default one.\n",
    "& | # and, or df[(df[\"col_4\"] > 0) & (df[\"col_7\" < 7])]\n",
    "\n",
    "\n",
    "df.groupby() / df.groupby().get_group()  # Group DataFrame using a mapper or by a series of columns.\n",
    "pd.merge(df_1, df_2, how=\"inner\", on = \" \") # Merge DataFrame objects with a database-style join. - (merges on columns)\n",
    "df.concat() \n",
    "df.join(df_2, on=\"inner\") # Join columns with other DataFrame either on index or on a key column.   - (merges on index)\n",
    "df['Name'].str.contains('Sum')  // df.filter(like='Sum', axis=1) // df['Name'].str.startswith('Sum')   # filter a specific column/row (df.filter) or its contents (str.contains)\n",
    "df.sort_values() # Sort by the values along either axis.\n",
    "df.value_counts() # Return a Series containing counts of unique values.\n",
    "df.describe() # Generate descriptive statistics.\n",
    "df.info() # Provide a concise summary of the DataFrame.\n",
    "df.unique(), nunique() # Find unique values / count of unique values.\n",
    "df.sum(), mean(), median(), std(), count() # Compute the sum / mean / median / standard deviation of the values.\n",
    "df.dtypes / df.astype(float) / df.select_dtypes(include=[\"float64\", \"int64\"]) # check the datatype / Cast a pandas object to a specified dtype.\n",
    "pd.to_datetime(df[\"dob\"], infer_datetime_format=True)\n",
    "df.duplicated()\n",
    "df.isin()\n",
    "df.drop() / df.pop() / dropna() / df.drop_duplicates() # Remove rows or columns / rows with missing values.\n",
    "df.fillna() / df.ffill# Fill NA/NaN values.\n",
    "df.isna(), isnull() # Detect missing values.\n",
    "df.shape() / df.size()\n",
    "df.copy()\n",
    "df.rename() / df.replace() # Rename or Replace values given in 'to_replace' with 'value'.\n",
    "df.filter(items=[\"lab1\", \"lab2\"], regex='e$', axis=1) # Subset rows or columns of DataFrame according to labels in the specified index.\n",
    "df.apply() / df.applymap(lambda x:x[1:]) # Apply a function along an axis of the DataFrame. use applymap if you want to apply to only an element.\n",
    "df.map() #\n",
    "df.to_csv(), to_excel(), to_json(), to_sql() # Methods to save DataFrame in different formats.\n",
    "df.query('col1 > 30 and col2 == \"Male\"') / df.query('col1.str.startswith(\"J\")')  # pay attention to the single, double quotes and backsticks\n",
    "df.query('index2 == \"Hotel room\" and `col3` <= 300') # Query the columns of a DataFrame with a boolean expression\n",
    "\n",
    "\n",
    "df.corr()\n",
    "df.pivot_table() # Create a spreadsheet-style pivot table as a DataFrame.\n",
    "df.plot() # Make plots of DataFrame using matplotlib / plotly.\n",
    "df.copy() # Make a copy of this object's indices and data.\n",
    "df.sample() # Return a random sample of items from an axis of the object.\n",
    "df.notna() / notnull() # Detect existing (non-missing) values.\n",
    "df.idxmax(), idxmin() # Return index of first occurrence of maximum / minimum over requested axis.\n",
    "df.melt() # Unpivot a DataFrame from wide format to long format.\n",
    "df.transpose() # Transpose index and columns of the DataFrame.\n",
    "df.clip() # Trim values at input threshold(s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Visualization (Matplotlib, Searborn etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "markdown_table = \"\"\"\n",
    "| Plot Type             | Complexity Level | Data Type               | Variable Type | Description |\n",
    "|-----------------------|------------------|-------------------------|---------------|-------------|\n",
    "| Histogram             | Basic            | Numeric                 | Univariate    | Shows the distribution of a single numeric variable. |\n",
    "| Bar Chart             | Basic/Intermediate | Categorical           | Univariate/Bivariate | Shows numeric comparison across categories; can be extended to show multiple categories (stacked bar). |\n",
    "| Box Plot              | Intermediate     | Numeric vs Categorical  | Bivariate     | Displays the distribution of a numeric variable across different categories. |\n",
    "| Violin Plot           | Intermediate     | Numeric vs Categorical  | Bivariate     | Similar to box plots but with a kernel density estimation for better distribution visualization. |\n",
    "| Scatter Plot          | Basic/Intermediate | Numeric               | Bivariate     | Visualizes the relationship between two numeric variables. |\n",
    "| Line Plot             | Basic/Intermediate | Numeric               | Bivariate/Multivariate | Used for numeric x and y data; often for time series. |\n",
    "| Heatmap               | Intermediate     | Categorical/Numeric    | Bivariate/Multivariate | Visualizes data in matrix format; effective for correlation matrices, cross-tabulations. |\n",
    "| Pair Plot             | Intermediate     | Numeric                | Multivariate  | Showcases pairwise relationships in a dataset. |\n",
    "| KDE Plot              | Intermediate     | Numeric                | Bivariate/Multivariate | Kernel Density Estimation for visualizing the distribution of one or more variables. |\n",
    "| Joint Plot            | Intermediate     | Numeric                | Bivariate     | Combines scatter and histogram plots for joint distributions. |\n",
    "| Hexbin Plot           | Intermediate     | Numeric                | Bivariate     | Represents bivariate data using hexagons. |\n",
    "| Bubble Chart          | Intermediate     | Numeric                | Bivariate/Multivariate | Scatter plot variation where bubble size adds a third dimension. |\n",
    "| Swarm Plot            | Intermediate     | Numeric vs Categorical | Bivariate     | Similar to strip plot, but adjusts points to avoid overlap. |\n",
    "| Strip Plot            | Intermediate     | Numeric vs Categorical | Bivariate     | Displays individual data points as strips, providing a clear visualization of distribution. |\n",
    "| Facet Grid            | Intermediate     | Any                    | Multivariate  | Enables plotting of multiple plots on a grid for complex data comparisons. |\n",
    "| Ridge Plot            | Intermediate/Advanced | Numeric vs Categorical | Multivariate | Overlapping KDE plots for visualizing distributions across different categories. |\n",
    "| Parallel Coordinates  | Advanced         | Numeric/Categorical    | Multivariate  | Visualizes data points in terms of their features; good for comparing many variables. |\n",
    "| Contour Plot          | Advanced         | Numeric                | Bivariate     | Represents three-dimensional data in two dimensions using contours. |\n",
    "| Radar Chart             | Advanced           | Numeric/Categorical     | Multivariate        | Displays multivariate data in terms of a two-dimensional chart with one axis for each variable, typically used for performance analysis. |\n",
    "| Treemap                 | Intermediate       | Categorical/Numeric     | Multivariate        | Visualizes hierarchical data using nested rectangles, where size and color can represent different dimensions. |\n",
    "| Sunburst Chart          | Intermediate       | Categorical/Numeric     | Multivariate        | Visual representation of a hierarchy in a radial layout, useful for showing levels of a tree diagram. |\n",
    "| Sankey Diagram          | Advanced           | Categorical/Numeric     | Multivariate        | Visualizes the flow from one set of values to another, commonly used in data flow and financial transaction mapping. |\n",
    "| Choropleth Map          | Intermediate/Advanced | Geospatial Data      | Multivariate        | Maps where areas are shaded in proportion to a statistical variable; useful in geographical data visualization. |\n",
    "| 3D Scatter Plot         | Advanced           | Numeric                 | Multivariate        | A three-dimensional scatter plot, useful for visualizing multivariate data in 3D space. |\n",
    "| 3D Surface Plot         | Advanced           | Numeric                 | Multivariate        | Represents three-dimensional data as a surface in 3D space. |\n",
    "| Streamgraph             | Advanced           | Numeric/Categorical     | Multivariate        | A type of stacked area graph which is displaced around a central axis, resulting in a flowing, organic shape. |\n",
    "| Word Cloud              | Intermediate       | Textual Data            | Univariate          | Visual representation of text data where size of each word indicates its frequency or importance. |\n",
    "| Polar Chart             | Intermediate       | Numeric/Categorical     | Multivariate        | Displays data in a circular graph, where each variable is represented along a separate axis. |\n",
    "| Network/Graph Diagram   | Advanced           | Categorical/Numeric     | Multivariate        | Visualizes relationships and flows between nodes and connections in a network. |\n",
    "| Candlestick Chart       | Advanced           | Numeric                 | Bivariate/Multivariate | Used in financial analysis to represent the price movements of stocks, derivatives, etc. |\n",
    "| Gantt Chart             | Intermediate       | Categorical/Numeric     | Multivariate        | A type of bar chart that illustrates a project schedule or timelines. |\n",
    "| Density Plot            | Intermediate       | Numeric                 | Bivariate/Multivariate | Similar to KDE Plot, visualizes the distribution of a continuous variable. |\n",
    "| Dot Plot                | Intermediate       | Numeric/Categorical     | Bivariate/Multivariate | Represents data points as dots along an axis; useful for small to medium-sized datasets. |\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important Plots\n",
    "    # Bar Chart\n",
    "    # Line Chart\n",
    "    # Pie Chart\n",
    "    # Scatter Plot\n",
    "    # Histogram\n",
    "    # Heat Map\n",
    "    # Box Plot and Violin Plot\n",
    "    # Density Plot and KDE plots\n",
    "\n",
    "\n",
    "# Bar Chart:\n",
    "    # Column Chart: Vertical bar chart.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(df_bar['Category'], df_bar['Value1'], color='b')\n",
    "        # sns.barplot(data=df, x='category', y='value', hue='sub_category') # to add hue\n",
    "        # sns.pairplot(data=df, hue='sub_category')\n",
    "        plt.title('Horizontal Bar Chart')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "    # Stacked Bar Chart: Bars divided into sub-parts to show cumulative effect.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(df_bar['Category'], df_bar['Value1'], color='b', label='Value1')\n",
    "        plt.bar(df_bar['Category'], df_bar['Value2'], color='r', bottom=df_bar['Value1'], label='Value2')\n",
    "\n",
    "    # Grouped Bar Chart: Bars for different groups placed next to each other for comparison.\n",
    "        x = np.arange(len(df_bar['Category']))\n",
    "        width = 0.35\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.bar(x - width/2, df_bar['Value1'], width, label='Value1')\n",
    "        plt.bar(x + width/2, df_bar['Value2'], width, label='Value2')\n",
    "        plt.xticks(x, df_bar['Category'])\n",
    "        \n",
    "    # Horizontal Bar Chart: Bars are displayed horizontally instead of vertically.\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.barh(df_bar['Category'], df_bar['Value1'], color='b')\n",
    "        plt.title('Horizontal Bar Chart')\n",
    "        plt.xlabel('Value')\n",
    "        plt.ylabel('Category')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "# Line Chart:\n",
    "    # Basic Line Chart: Single line showing a trend over time.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(df_line['A'], marker='o', color='b')\n",
    "    # sns.lineplot(data=df, x='time', y='value', hue='category') # to add hue\n",
    "    \n",
    "    # Multi-Line Chart: Multiple lines representing different categories for comparison.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    for column in df_line.columns:\n",
    "        plt.plot(df_line[column], marker='o', label=column)\n",
    "        plt.legend()\n",
    "    \n",
    "    # Area Chart: Similar to a line chart but with the area below the line filled in.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.fill_between(df_line.index, df_line['A'], color='skyblue', alpha=0.5)\n",
    "    plt.plot(df_line['A'], color='Slateblue', alpha=0.6)\n",
    "\n",
    "    # Stacked Area Chart: Multiple area charts stacked on top of one another.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.stackplot(df_line.index, df_line['A'], df_line['B'], df_line['C'], labels=['A', 'B', 'C'])\n",
    "\n",
    "    \n",
    "# Pie Chart:\n",
    "    sizes = df['gender'].value_counts() # Count the occurrences of each gender\n",
    "\n",
    "    # Basic Pie Chart: Standard pie with different slices.\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    \n",
    "    # Doughnut Chart: Similar to a pie chart but with a round hole in the center.\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, wedgeprops=dict(width=0.3))\n",
    "\n",
    "    # Exploded Pie Chart: Pie slices slightly separated from each other.\n",
    "    explode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'B')\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', startangle=140)\n",
    "    \n",
    "    \n",
    "# Scatter Plot:\n",
    "    # Basic Scatter Plot: Plots individual data points on a two-dimensional graph.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df_scatter['X'], df_scatter['Y'])   #hue = df_scatter['Z']\n",
    "    # sns.scatterplot(data=df, x='variable_x', y='variable_y', hue='category') # to add hue \n",
    "    \n",
    "    # Bubble Chart: Similar to scatter plots but uses bubble size as an additional variable.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.scatter(df_scatter['X'], df_scatter['Y'], s=df_scatter['Size'], sizes=(20, 200), alpha=0.5, edgecolors='w', linewidth=0.5)   # hue = df_scatter['Z']\n",
    "    \n",
    "    # 3D Scatter Plot: Uses three dimensions to plot data points.\n",
    "    \n",
    "    \n",
    "# Histogram:\n",
    "    # Frequency Histogram: Shows the frequency of each data bin.\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(hist_data, bins=30, color='skyblue', edgecolor='black')\n",
    "    # sns.histplot(df[df['fire_occured'] == 'Yes']['RH'], color=\"red\", label='Fire Days', kde=True)\n",
    "    plt.title('Distribution of Relative Humidity on Days With and Without Forest Fires')\n",
    "    plt.xlabel('Relative Humidity (RH)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Density Histogram: Similar to a frequency histogram but shows the probability density.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(hist_data, bins=30, density=True, color='green', alpha=0.6, edgecolor='black')\n",
    "\n",
    "    # Cumulative Histogram: Displays the cumulative count of data points.\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.hist(hist_data, bins=30, cumulative=True, color='orange', alpha=0.6, edgecolor='black')\n",
    "\n",
    "    \n",
    "# Heat Map:\n",
    "    # Geographical Heat Map: Displays data density on maps.\n",
    "    # Matrix Heat Map: Represents data in a matrix format with colors indicating magnitude.\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(df.select_dtypes(exclude='object').corr(), annot=True, cmap='viridis')\n",
    "\n",
    "    # Tree Map: Uses nested rectangles to represent hierarchical data with color and size.\n",
    "    import squarify\n",
    "    colors = ['red', 'green', 'blue', 'yellow']\n",
    "\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    squarify.plot(sizes=df['Value'], label=df['Category'], color=colors, alpha=0.6)\n",
    "    plt.title('Tree Map')\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Box Plots and Violin Plots\n",
    "    np.random.seed(0)\n",
    "    box_violin_data = np.random.randn(100, 3)\n",
    "    df_box_violin = pd.DataFrame(box_violin_data, columns=['A', 'B', 'C'])\n",
    "    \n",
    "    # Box Plots: used to show the distribution of quantitative data and effectively highlights the median, quartiles, and outlier\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.boxplot(data=df_box_violin)\n",
    "    # sns.boxplot(data=df, x='category', y='value', hue='sub_category') # to add hue\n",
    "\n",
    "    # Violin Plot: combines features of the box plot with a kernel density estimation\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.violinplot(data=df_box_violin)\n",
    "    # sns.violinplot(data=df, x='category', y='value', hue='sub_category', split=True) # to add hue\n",
    "\n",
    "\n",
    "# Density/KDE Plots: visualizes the distribution of data over a continuous interval\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.histplot(data, kde=True, color='blue', bins=30)\n",
    "    # or\n",
    "    df_density['Value'].plot(kind='density', color='blue')\n",
    "    # or\n",
    "    sns.kdeplot(df_density['Value'], shade=True, color='green') # use this\n",
    "    \n",
    "\n",
    "\n",
    "# WordCloud\n",
    "    from wordcloud import WordCloud\n",
    "    \n",
    "    # Join all the strings in the column to create one large string\n",
    "    text = ' '.join(df['Text'])\n",
    "\n",
    "    # Create a word cloud object\n",
    "    wordcloud = WordCloud(width = 800, height = 800, \n",
    "                    background_color ='white', \n",
    "                    stopwords = None, \n",
    "                    min_font_size = 10).generate(text)\n",
    "\n",
    "    # Plot the WordCloud image                        \n",
    "    plt.figure(figsize = (8, 8), facecolor = None) \n",
    "    plt.imshow(wordcloud) \n",
    "    plt.axis(\"off\") \n",
    "    plt.tight_layout(pad = 0) \n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scikit-Learn Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here are some of the popular modules within scikit-learn (sklearn) grouped by functionality:\n",
    "\n",
    "dir(sklearn)\n",
    "# Data Processing and Preprocessing:\n",
    "    sklearn.preprocessing # Contains functions and classes for data preprocessing and feature scaling.\n",
    "    sklearn.impute #contains functions to input missing values (e.g. IterativeImputer(estimator = RandomForest())\n",
    "\n",
    "# Machine Learning Models:\n",
    "    sklearn.linear_model # Includes linear regression, logistic regression, and other linear models.\n",
    "    sklearn.cluster # Provides clustering algorithms such as K-Means and hierarchical clustering.\n",
    "    sklearn.tree # Contains decision tree and ExtraTree.\n",
    "    sklearn.svm # Support Vector Machines for classification and regression.\n",
    "    sklearn.ensemble # Ensemble methods like AdaBoost, Bagging, and Gradient Boosting, RandomForest.\n",
    "    sklearn.neighbors # Nearest Neighbors algorithms.\n",
    "    sklearn.naive_bayes # Naive Bayes classifiers.\n",
    "    sklearn.discriminant_analysis # Linear and Quadratic Discriminant Analysis.\n",
    "    sklearn.neural_network # Neural network-based models.\n",
    "    sklearn.decomposition # Matrix decomposition techniques like PCA and NMF.\n",
    "    sklearn.kernel_approximation # Approximation techniques for kernel methods.\n",
    "    sklearn.isotonic # Isotonic regression.\n",
    "\n",
    "# Model Evaluation and Selection:\n",
    "    sklearn.model_selection # Tools for model selection, hyperparameter tuning, and cross-validation.\n",
    "    sklearn.metrics # Metrics for evaluating model performance (e.g., accuracy, ROC-AUC, etc.).\n",
    "    sklearn.feature_selection #used to select the most relevant features from a dataset for training machine learning models\n",
    "    sklearn.feature_extraction #contains functions for feature extraction from raw data, such as text data, including Bag of Words, CountVectorizer, and TfidfVectorizer\n",
    "\n",
    "# Data Manipulation and Utility:\n",
    "    sklearn.base # Base classes and utility functions.\n",
    "    sklearn.utils # Various utility functions and classes.\n",
    "\n",
    "# Other:\n",
    "    sklearn.pipeline # provides tools for building machine learning pipelines, which allows you to chain together multiple steps.\n",
    "    sklearn.datasets    # datasets\n",
    "    sklearn.exceptions # Custom exceptions.\n",
    "    sklearn.externals # External dependencies.\n",
    "    sklearn.manifold # Dimensionality reduction techniques.\n",
    "    sklearn.logger and logging # Logging utilities.\n",
    "    sklearn.config_context # Context for configuration settings.\n",
    "    sklearn.get_config # Function to get global configuration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ed569b",
   "metadata": {},
   "source": [
    "> How to find any function in SKlearn, Pytorch Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9303e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pkgutil\n",
    "import importlib\n",
    "\n",
    "def find_all_instances_in_sklearn(search_term):\n",
    "    search_term_lower = search_term.lower()\n",
    "    locations = []\n",
    "    for importer, modname, ispkg in pkgutil.walk_packages(path=sklearn.__path__, prefix=sklearn.__name__ + '.'):\n",
    "        try:\n",
    "            module = importlib.import_module(modname)\n",
    "            for attribute_name in dir(module):\n",
    "                if search_term_lower in attribute_name.lower():\n",
    "                    locations.append(f'{modname}.{attribute_name}')\n",
    "        except (ImportError, SystemError):\n",
    "            pass  # Handle cases where a module can't be imported\n",
    "    return locations\n",
    "\n",
    "# Example usage\n",
    "search_term = 'logistic'  # This can be 'tsne', 'randomforestregressor', 'random', etc.\n",
    "locations = find_all_instances_in_sklearn(search_term)\n",
    "if locations:\n",
    "    print(f'Instances matching \"{search_term}\" found in:')\n",
    "    for location in locations:\n",
    "        print(f'  - {location}')\n",
    "else:\n",
    "    print(f'No instances matching \"{search_term}\" found in sklearn.')\n",
    "\n",
    "# you can replace sklearn with your preferred library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5d7e71",
   "metadata": {},
   "source": [
    "#### OpenCV Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d92031",
   "metadata": {},
   "source": [
    "> os  | zipfile   | PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767c070d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Operating System\n",
    "import os\n",
    "\n",
    "os.listdir(path='.') # Lists files and directories in a given directory. Useful for accessing image datasets.\n",
    "os.path.join(path, *paths) # Combines paths intelligently; key for building file paths in data processing.\n",
    "os.getcwd() # Gets the current working directory, often used in setting up paths for data input/output.\n",
    "os.chdir(path) # Changes the current working directory, useful in scripts that require relative paths.\n",
    "os.path.exists(path) # Checks if a given path exists, important for verifying data paths or model checkpoint existence.\n",
    "os.makedirs(path, mode=0o777, exist_ok=False) # Creates directories recursively, useful for creating structured directories to store \n",
    "    # processed images or models.\n",
    "os.rename(src, dst) # Renames files, can be used in organizing or re-labeling datasets.\n",
    "os.path.isdir(path) / os.path.isfile(path) # Checks if a path is a directory or a file, respectively. These are crucial in handling \n",
    "    # data files and directories.\n",
    "os.path.getsize(path) # Gets the size of a file, which can be important when handling large datasets or models.\n",
    "os.walk(top, topdown=True, onerror=None, followlinks=False) # Traverses directory trees, essential for processing datasets stored in \n",
    "    # nested directory structures. \n",
    "    for dirpath, dirnames, filenames in os.walk(file_path):\n",
    "        pass \n",
    "os.path.split(path) # Splits a pathname into head and tail components, useful for parsing file paths.\n",
    "os.remove(path) / os.unlink(path) # Removes files, which can be useful for cleaning up temporary files or outputs.\n",
    "\n",
    "\n",
    "=====================================================================================================================================\n",
    "# when working with datasets that have been zipped \n",
    "import zipfile \n",
    "from zipfile import ZipFile, is_zipfile \n",
    "\n",
    "data_path = 'lung-and-colon-cancer-histopathological-images.zip'\n",
    "\n",
    "zipfile.is_zipfile(data_path)    # check if the file is a zip filed.\n",
    "with ZipFile(data_path,'r') as zip:\n",
    "  zip.extractall()\n",
    "  print('The data set has been extracted.')\n",
    "  \n",
    "  \n",
    "===================================================================================================================================\n",
    "# using the PIL library (PIL - Python Imaging Library)  \n",
    "from PIL import Image\n",
    "\n",
    "Image.open(filepath) # Opens and identifies the given image file.\n",
    "Image.save(filename, format) # Saves the image under the given filename and format.\n",
    "Image.resize((width, height)) # Resizes the image to the specified width and height.\n",
    "Image.crop((left, top, right, bottom)) # Crops the image using the given coordinates.\n",
    "Image.rotate(angle) # Rotates the image by the given angle.\n",
    "Image.convert(mode) # Converts the image to the specified mode (e.g., \"RGB\", \"L\").\n",
    "Image.filter(filter) # Applies a specified filter to the image (e.g., ImageFilter.BLUR).\n",
    "Image.transpose(method) # Transposes the image (e.g., flipping)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6565be77",
   "metadata": {},
   "source": [
    "> opencv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273243f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import PIL \n",
    "\n",
    "# cv2 image shape (387, 620, 3) \n",
    "# torch tensor shape (3, 387, 620); channel in RGB\n",
    "# transposed_image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "# PIL.Image.open(str(img))       to view an image           img = image path\n",
    "# plt.imshow(img)                to view an image            img = np array, expects RGB not BGR\n",
    "# cv2.imread(str(img))           to read an image            img = image path, output = np array, channel format in BGR\n",
    "# T.ToTensor()                   to transform an image       expects a PIL Image or ndarray \n",
    "\n",
    "\n",
    "img = np.zeros(shape=(480, 640)) \n",
    "\n",
    "# Core Operations\n",
    "\n",
    "    img = cv2.imread(img, flag) # Reading images (flag: cv2.IMREAD_COLOR or 1, cv2.IMREAD_GRAYSCALE or 0, cv2.IMREAD_UNCHANGED or -1)\n",
    "        img.shape   # returns a tuple of rows, columns, and channels\n",
    "        img.size    # returns the total number of pixels (row*col*channels)\n",
    "        img.copy()  # copy an image into a new variable. or use (np.copy(img))\n",
    "        img.dtype   # returns the image datatype \n",
    "    cv2.imshow(window_name, img)  # Showing Images\n",
    "    cropped_img = img[100:300, 100:300] # Image Cropping\n",
    "    cv2.imwrite(filename, img) # saving an image\n",
    "    cv2.waitKey(0) & 0xFF  # Wait for a key press  \n",
    "    cv2.destroyAllWindows() / cv2.destroyWindow() # Close all OpenCV windows \n",
    "    gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  # Convert Color Spaces - Convert to grayscale\n",
    "    resized_img = cv2.resize(img, (width, height))  # Resize to a specific size\n",
    "    cv2.namedWindow('image')    #set the window to be 'image'\n",
    "    b, g, r = cv2.split(img)\n",
    "    img = cv2.merge((b,g,r)) \n",
    "    cv2.inRange(image, lower_bound, upper_bound)    # used for color segmentation. It filters an image to include only pixels within a specified color range, creating a binary mask\n",
    "    cv2.createTrackbar('trackbar_name', 'window_name', value, count, onChange)  # Create Trackbar\n",
    "    pos = cv2.getTrackbarPos('trackbar_name', 'window_name')    # Get Trackbar Position\n",
    "    hist = cv2.calcHist([gray_image], [0], None, [256], [0, 256])   # calculate histogram for a gray-scale image\n",
    "    hist_blue = cv2.calcHist([color_image], [0], None, [256], [0, 256])     # calculate histogram for the blue channel of a colored image\n",
    "        plt.plot(hist_blue)     # plot the histogram\n",
    "\n",
    "\n",
    "# Working with Images\n",
    "    \n",
    "    # Drawing Shapes and Text on Images\n",
    "\n",
    "        cv2.line(img, (start_coordinates), (end_coordinates), (color_in_bgr), line_thickness)                     # Draw Line\n",
    "        cv2.rectangle(img, (top-left corner coordinates), (bottom-right corner coordinates), color, thickness)    # Draw Rectangle\n",
    "        cvzone.putTextRect(img, text, (top-left corner coordinates), scale, thickness, color, font,..)  # put text in a rectangle \n",
    "        cv2.circle(img, (center_coordinates), (circle_radius), (color_in_bgr), stroke_thickness)     # Draw Circle\n",
    "        cv2.polylines(img, [pts], isClosed, color, thickness)         # Draw Polygon\n",
    "        cv2.putText(img, text, orgin_coordinate, font, font_scale, (color, thickness, line_type))         # Write Text\n",
    "        cvzone.Utils.putTextRect(img, text, pos, scale=3, thickness=3, colorT=(255, 255, 255), colorR=(255, 0, 255), font=1,)    #text with rectangle background\n",
    "        cv2.arrowedLine(img, (start_coordinates), (end_coordinates), (color_in_bgr), line_thickness)      # arrowed line \n",
    "                            # thickness = any digit or use [cv2.FILLED or -1] to fill the shape\n",
    "        cv2.fillPoly(img, [pts], color=(200, 245, 0)) # pts is a numpy array of type np.int32 with all the x,y points in the shape ~ pts.reshape((-1, 1, 2))\n",
    "        cv2.fillConvexPoly(mask, points, color)     # Fill Concave polygon using convex hull algorithm\n",
    "\n",
    "    # Arithmetic Operations on Images\n",
    "\n",
    "        cv2.add(image1, image2) # Image Addition\n",
    "        cv2.addWeighted(image1, weight1, image2, weight2, gammaValue)    #Image Alpha Blending\n",
    "        cv2.subtract(image1, image2)    #Image Subtraction\n",
    "        cv2.bitwise_and(image1, image2, destination, mask)   # Bitwise And\n",
    "        cv2.bitwise_or(image1, image2, destination, mask)   # Bitwise Or\n",
    "        cv2.bitwise_not(image, destination, mask)   # Bitwise Not\n",
    "        cv2.bitwise_xor(image1, image2, destination, mask)  # Bitwise Xor\n",
    "        \n",
    "    # Morphological Operations on Images:   (Manipulates image shape/structure using kernels; includes erosion, dilation, opening, and closing)\n",
    "\n",
    "        cv2.erode(img, kernel, iterations=1)  # Erosion:      Shrinks bright regions and enlarges dark regions by removing pixels at boundaries.\n",
    "        cv2.dilate(img, kernel, iterations=1) # Dilation:     Expands bright areas and shrinks dark regions by adding pixels to boundaries.\n",
    "        cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel, iterations=1)   #Opening:       Removes small bright spots (noise) by erosion followed by dilation\n",
    "        cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel, iterations=1)   # Closing:      Fills small dark spots and small holes by dilation followed by erosion.\n",
    "        cv2.morphologyEx(img, cv2.MORPH_GRADIENT, kernel) #Morphological Gradient:    Highlights edges and boundaries by subtracting an eroded image from a dilated image\n",
    "        cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)   # Top Hat:      Isolates small elements and details brighter than their surroundings.\n",
    "        cv2.morphologyEx(img, cv2.MORPH_BLACKHAT, kernel) # Black Hat:    Isolates dark elements and details in bright surroundings.\n",
    "\n",
    "    # Geometric Transformations on Image:  \n",
    "\n",
    "        res = cv2.resize(img,(2width, 2height), interpolation)  # Scaling (scaling types: cv2.INTER_AREA, cv2.INTER_CUBIC, cv2.INTER_LINEAR)\n",
    "        \n",
    "        T = np.float32([[1, 0, new_width], [0, 1, new_height]])     #  Translation\n",
    "        cv2.warpAffine(img, T, (original_width, original_height)) \n",
    "        \n",
    "        M = cv2.getRotationMatrix2D(center, angle, scale)   # Rotation\n",
    "        cv2.warpAffine(img, M, (width, height))\n",
    "        \n",
    "        M = cv2.getAffineTransform(pts1, pts2)      # Affine Transformation\n",
    "        dst = cv2.warpAffine(img, M, (cols, rows))\n",
    "        \n",
    "        matrix = cv2.getPerspectiveTransform(src, dst)      # Perspective Transformation\n",
    "        cv2.warpPerspective(img, matrix, dsize)\n",
    "        \n",
    "    # Image Thresholding:   (Converts GRAYSCALE images to binary using a set threshold, useful for separating foreground from background)\n",
    "\n",
    "        retval, thresholded_image = cv2.threshold(img, thresh, maxvalue, thresholdingTechnique)    # Simple Threshold\n",
    "        retval, thresholded_image = cv2.adaptiveThreshold(img, maxvalue, adaptivemethod, thresholdingTechnique, blocksize, constant)   # Adaptive Threshold\n",
    "        retval, bw_image = cv2.threshold(img, thresh, maxvalue, thresholdingTechnique)    # Otsu Thresholding\n",
    "        \n",
    "            # Thresholding technique\n",
    "            cv2.THRESH_BINARY   # If pixel intensity is greater than the set threshold, the value is set to 255, else set to 0\n",
    "            cv2.THRESH_BINARY_INV   # Inverted or Opposite case of cv2.THRESH_BINARY\n",
    "            cv2.THRESH_TRUNC        # If the pixel intensity value > threshold, it is truncated to the threshold. set pixel values == threshold\n",
    "            cv2.THRESH_TOZERO       # Pixel intensity is set to 0, for all the pixels intensity, less than the threshold value\n",
    "            cv2.THRESH_TOZERO_INV   # Inverted or Opposite case of cv2.THRESH_TOZERO\n",
    "            cv2.THRESH_OTSU\n",
    "            cv2.THRESH_TRIANGLE \n",
    "            \n",
    "            # Adaptive Threshold Methods\n",
    "            cv2.ADAPTIVE_THRESH_MEAN_C      # Calculates the threshold as the mean of neighboring area minus a constant. Useful for varying lighting conditions.\n",
    "            cv2.ADAPTIVE_THRESH_GAUSSIAN_C  # Uses a Gaussian-weighted sum of the neighborhood values for thresholding, ideal for finer, localized changes.\n",
    "                                                    \n",
    "    # Edge/Line and Object Detection (Features):   (use GRAYSCALE - Identifies sharp changes in intensity to detect object boundaries and lines in images.)\n",
    "\n",
    "        cv2.Canny(img, thresh_lower, thresh_upper, aperture_size, L2Gradient)   # Canny Edge Detection\n",
    "        cv2.HoughLines(img, rho, theta, threshold)    # Houghline Method for Line Detection using the standard Hough transform   (rho: Distance resolution in pixels., theta: Angle resolution in radians., threshold: Minimum votes to consider a line)\n",
    "        cv2.HoughLinesP(img, rho, theta, threshold, ..)    # Finds line segments in a binary image using the probabilistic Hough transform\n",
    "        cv2.HoughCircles(img, cv2.HOUGH_GRADIENT, 1, 20, param1 = 50, param2 = 30, minRadius = 1, maxRadius = 40) #Houghline Method for Circle Detection\n",
    "        cv2.cornerHarris(img, blockSize, kSize, k, borderType)    # Harris Corner Method for Corner Detection\n",
    "        cv2.goodFeaturesToTrack(img, max_corner, quality_level, min_distance)   # Shi-Tomasi Method for Corner Detection\n",
    "        cv2.drawKeypoints(img, key_points, output_image, colour, flag)  # Keypoints Detection\n",
    "        \n",
    "        # Image gradients:      Computes image gradients to emphasize texture and edges\n",
    "        cv2.Sobel(img, ddepth, dx, dy, dst=None, ksize=None, scale=None)       # Sobel Operator, useful for edge detection in both X and Y directions.         (ddepth: Depth of the destination image. dx and dy: Order of the derivative x and y)\n",
    "        cv2.Laplacian(img, ddepth=-1, dst=None, ksize=None)          # Calculates the Laplacian of the image, highlighting regions of rapid intensity change\n",
    "        cv2.Scharr(img, ddepth=-1, dx, dy)    # A variation of Sobel, more sensitive to edges than the standard Sobel operator.\n",
    "\n",
    "        #  Contour Detection:           Used to detect and extract contours from binary images. Essential for shape analysis, object detection, and recognition tasks.\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)  #  Finds contours in a binary image.    Parameters: source image, contour retrieval mode, contour approximation method.\n",
    "        contours, Area, BoundingBox, Center = cvzone.Utils.findContours(img, imgPre, minArea=1000, maxArea=inf,)    # Finds Contours in an image.\n",
    "        cv2.drawContours(img, [contours], contourIdx, color, thickness) # Draw contour      contourIdx = -1\n",
    "        area = cv2.contourArea(contours) # contour area\n",
    "        x, y, w, h = cv2.boundingRect(contours)  # Bounding Rectangle: \n",
    "        length = cv2.arcLength(contours, True)   # Contour Perimeter:\n",
    "        approx = cv2.approxPolyDP(contours, epsilon, True)   # Approximating Contours\n",
    "        \n",
    "        # Template Matching:        used for finding the location of a template image within a larger image. It's mainly used in applications where you need to find specific objects or features in an image\n",
    "        result = cv2.matchTemplate(img, template, method)     # Perform Template Matching:    \n",
    "        h, w = template.shape \n",
    "        min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)  # locate the match \n",
    "        if method in [cv2.TM_SQDIFF, cv2.TM_SQDIFF_NORMED]: \n",
    "            top_left = min_loc \n",
    "        else: \n",
    "            top_left = max_loc\n",
    "        bottom_right = (top_left[0] + w, top_left[1] + h)\n",
    "        cv2.rectangle(img, top_left, bottom_right, color, thickness)  # draw a rectangle around the match \n",
    "\n",
    "        \n",
    "    # Image Pyramids:   (Creates a multi-scale representation of an image, useful for scaling images up or down)\n",
    "\n",
    "        cv2.pyrDown(layer)  # Lower Gaussian Pyramid \n",
    "        cv2.pyrUp(layer)    # Higher Gaussian Pyramid\n",
    "        \n",
    "    # Changing the Colorspace of Images\n",
    "\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # BGR to RGB: - often used when switching from cv2 to pytorch, PIL or Sklearn.\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) # BGR to Grayscale: \n",
    "        cv2.cvtColor(img, cv2.COLOR_RGB2GRAY) # RGB to Grayscale: \n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2HSV)  # BGR to HSV (H-Hue , S-Saturation, V-Value represents intensity)\n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2LAB)  # BGR to Lab (L-Lightness. A-color from Green to Magenta. B colors from Blue to Yellow) \n",
    "        cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb) # BGR to YCrCb Color (Y-Luminance or Luma component, Cb and Cr are Chroma components)\n",
    "        hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)    # Track Blue (color) Object\n",
    "            lower_blue = np.array([110,50,50])\n",
    "            upper_blue = np.array([130,255,255])\n",
    "            cv2.inRange(hsv, lower_blue, upper_blue)                                \n",
    "                                        \n",
    "    # Smoothing Images: (Reduces noise and details by applying filters, like Gaussian blur or median blur, to soften images.)\n",
    "\n",
    "        cv2.filter2D(img, depth, kernel) # Convolve an Image:                                 Applies a custom kernel for convolution, allowing for diverse linear filtering effects.     \n",
    "        cv2.blur(img, shapeOfTheKernel)   # Averaging Filtering:                              Smoothens image using a simple average of neighboring pixels within the kernel.\n",
    "        cv2.getGaussianKernel(ksize, sigma[, ktype])    # Create Gaussian Kernel:               Generates a Gaussian kernel, used for more advanced blurring techniques.\n",
    "        cv2.GaussianBlur(img, shapeOfTheKernel, sigmaX )  # Gaussian Blur:                    Reduces noise using a Gaussian filter, effective for Gaussian noise.\n",
    "        cv2.medianBlur(img, kernel size)  # Median Blur:                                      useful for dealing with salt and pepper noise). Removes salt-and-pepper noise, replacing each pixel with the median of neighboring pixels\n",
    "        cv2.bilateralFilter(img, diameter, sigmaColor, sigmaSpace)    # Bilateral Blur:       Preserves edges while reducing noise, using a non-linear, edge-preserving approach.\n",
    "    \n",
    "# Working With Videos\n",
    "\n",
    "    cv2.VideoCapture('file_name.mp4')       # Playing a Video \n",
    "    PIL.Image.open(filename, mode)  # Create a Video from Multiple Images\n",
    "    cap = cv2.VideoCapture(File_path)     # Extracting Images from Video\n",
    "        cap.read()\n",
    "        cap.imwrite('filename.png', img)    # write the image to a file \n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(\"file_name.mp4\")  # Replace with your video file path\n",
    "    cap = cv2.VideoCapture(0)  # '0' is the default camera. Use '1', '2', etc. for other cameras\n",
    "        ret, frame = cap.read()  # Read Frames from the Video - 'ret' is a boolean indicating success, 'frame' is the current frame\n",
    "        cv2.flip(frame, 1)  # Flip the frame horizontally (1), vertical (0), both (-1) \n",
    "        cv2.imshow('Frame Title', frame)  # Display the current frame\n",
    "        cap.release()  # Release the video file or capturing device\n",
    "        if cap.isOpened():  # Check if VideoCapture Object is Opened\n",
    "        width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)   # Get Video Properties - width\n",
    "        height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT) # Get Video Properties - height\n",
    "        fps = cap.get(cv2.CAP_PROP_FPS)             # Get Video Properties - fps\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)      # set Video Properties\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)     # set Video Properties\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')  # Define the codec (like XVID, MP4V, etc.)\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))  # Filename, codec, FPS, resolution\n",
    "            out.write(frame)  # Write a frame to the output video\n",
    "            out.release()  # Release the VideoWriter object\n",
    "    cv2.namedWindow('image')    #set the window to be 'image'\n",
    "\n",
    "            # Video Properties\n",
    "            3 - cv2.CAP_PROP_FRAME_WIDTH    #  Set the width of the video frames\n",
    "            4 - cv2.CAP_PROP_FRAME_HEIGHT   #  Set the height of the video frames\n",
    "            5 - cv2.CAP_PROP_FPS            # Set the frame rate of the video capture.\n",
    "            10 - cv2.CAP_PROP_BRIGHTNESS    # Adjust the brightness of the video (if the camera supports this setting).\n",
    "            11 - cv2.CAP_PROP_CONTRAST      # Adjust the contrast of the video (if the camera supports this setting).\n",
    "            12 - cv2.CAP_PROP_SATURATION    # Adjust the saturation of the video (if the camera supports this setting).\n",
    "            15 - cv2.CAP_PROP_EXPOSURE      # Adjust the exposure of the camera (if the camera supports this setting).\n",
    "            39 - cv2.CAP_PROP_AUTOFOCUS     # Enable or disable autofocus, if supported (0 or 1).\n",
    "            \n",
    "            # Mouse Events\n",
    "            cv2.EVENT_LBUTTONDOWN   # Triggered when the left mouse button is pressed\n",
    "            cv2.EVENT_LBUTTONUP     # Occurs when the left mouse button is released. Often used in conjunction with EVENT_LBUTTONDOWN for actions like drag-and-drop.\n",
    "            cv2.EVENT_RBUTTONDOWN   # Triggered when the right mouse button is pressed\n",
    "            cv2.EVENT_MOUSEMOVE     # Occurs when the mouse is moved over the window. This is frequently used for tracking mouse movement, real-time drawing, or interactive applications\n",
    "            cv2.EVENT_LBUTTONDBLCLK # Triggered on a double-click of the left mouse button. Commonly used for more complex interactions like object selection or opening properties\n",
    "            \n",
    "            \n",
    "# Machine Learning and Deep Learning with OpenCV\n",
    "\n",
    "    net = cv2.dnn.readNet(model, config, framework) \n",
    "    net.setInput(blob)\n",
    "    output = net.forward()\n",
    "\n",
    "    knn = cv2.ml.KNearest_create()\n",
    "    knn.train(trainData, responses)\n",
    "\n",
    "    svm = cv2.ml.SVM_create()\n",
    "    svm.train(data, cv2.ml.ROW_SAMPLE, labels)\n",
    "\n",
    "# Camera Calibration and 3D Reconstruction\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (width, height)) # Finding Chessboard Corners:\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)  # Calibrate Camera:\n",
    "\n",
    "\n",
    "# Object Detection and Tracking\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')     # Face Detection: Using Haar Cascades. download from opencv github \n",
    "    faces = face_cascade.detectMultiScale(image, scaleFactor, minNeighbors)\n",
    "\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)       # Feature matching \n",
    "    matches = bf.match(descriptors1, descriptors2)\n",
    "    \n",
    "    sift = cv2.SIFT_create()            # Advanced feature detection \n",
    "    kp, des = sift.detectAndCompute(img, None)\n",
    "  \n",
    "    \n",
    "    \n",
    "# Others \n",
    "    cv2.setMouseCallback('image', mouse_callback)   # Mouse Callback Function: For capturing mouse events\n",
    "            def mouse_callback(event, x, y, flags, param):\n",
    "                    \"\"\"\n",
    "                    Mouse event callback.\n",
    "                    Parameters: event (int), x (int), y (int), flags (int), param (int)\n",
    "                    \"\"\"\n",
    "                    # Left mouse button down event\n",
    "                    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "                        # Draw a blue rectangle around the mouse click position\n",
    "                        cv2.rectangle(img, (x, y), (x + 20, y + 20), (255, 0, 0), cv2.FILLED)\n",
    "                        cv2.imshow('image', img)\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834b568",
   "metadata": {},
   "source": [
    "> CVZone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cf0f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install cvzone\n",
    "pip install mediapipe\n",
    "\n",
    "import cvzone \n",
    "\n",
    "# Others:\n",
    "    cvzone.cornerRect(img, bbox, l=30, t=5, rt=1, colorR=(255, 0, 255), colorC=(0, 255, 0))\n",
    "    cvzone.downloadImageFromUrl(url, keepTransparency=True)         # download image from url \n",
    "    cvzone.findContours(img, imgPre, minArea=1000, maxArea=inf,)    # Finds Contours in an image.\n",
    "    cvzone.overlayPNG(img, imgPNG, pos=[-30, 50])                   # overlay image\n",
    "    cvzone.putTextRect(img, text, pos, scale=3, thickness=3, colorT=(255, 255, 255), colorR=(255, 0, 255), font=1,)    #text with rectangle background\n",
    "    cvzone.rotateImage(img, angle, scale=1, keepSize=False)         # rotate an image 60 deg.\n",
    "    cvzone.stackImages(imgList, cols, scale)                        # Stack Images together to display in a single window\n",
    "    \n",
    "# Face Detection: Easily detect faces in an image or video stream.\n",
    "    from cvzone.FaceDetectionModule import FaceDetector\n",
    "    detector = FaceDetector(minDetectionCon=0.5, modelSelection=0)\n",
    "            img, bboxs = detector.findFaces(img, draw=False)        # # bbox contains 'id', 'bbox', 'score', 'center'\n",
    "\n",
    "# Hand Tracking: Track hand landmarks in real-time.\n",
    "    from cvzone.HandTrackingModule import HandDetector\n",
    "    detector = HandDetector(detectionCon=0.8, maxHands=2)\n",
    "            hands, img = detector.findHands(img, draw = False)      # Finds hands in a BGR image    (hand1[\"lmList\"]  # List of 21 landmarks for the first hand)\n",
    "            length, info, img = detector.findDistance()         # Find the distance between two landmarks input should be (x1,y1) (x2,y2)\n",
    "            fingers = fingersUp()   # Finds how many fingers are open and returns in a list \n",
    "\n",
    "# Pose Estimation: Identify human body positions.\n",
    "    from cvzone.PoseModule import PoseDetector\n",
    "    detector = PoseDetector(staticMode=False, modelComplexity=1, smoothLandmarks=True, enableSegmentation=False, smoothSegmentation=True,\n",
    "                        detectionCon=0.5,\n",
    "                        trackCon=0.5)\n",
    "            img = detector.findPose(img)            # Find the human pose in the frame\n",
    "            lmList, bboxInfo = detector.findPosition(img, draw=True, bboxWithHands=False)   # Find the landmarks, bounding box, and center of the body in the frame\n",
    "            length, img, info = detector.findDistance(lmList[11][0:2], lmList[15][0:2], img, color, scale)     # Calculate the distance between landmarks\n",
    "            angle, img = detector.findAngle(lmList[11][0:2], lmList[13][0:2], lmList[15][0:2], img, color, scale)   # Calculate the angle between landmarks\n",
    "            isCloseAngle50 = detector.angleCheck(myAngle=angle, targetAngle=50, offset=10)      # Check if the angle is close to 50 degrees with an offset of 10\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PyTorch Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8335f04b",
   "metadata": {},
   "source": [
    "> Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn             # neural networks\n",
    "import torch.optim as optim       # optimizers e.g. gradient descent, ADAM, etc.\n",
    "import torch.nn.functional as F   # layers, activations and more\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms   # composable transforms\n",
    "from torchvision import datasets, models, transforms     # vision datasets,\n",
    "                                                         # architectures &\n",
    "                                                         # transforms\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## torch\n",
    "  # **Tensor Creation**\n",
    "    torch.zeros # Tensor filled with zeros.\n",
    "    torch.ones # Tensor filled with ones.\n",
    "    torch.randn #Tensor with random numbers from a normal distribution.   torch.Tensor` - Basic tensor object.\n",
    "    torch.arange # Tensor containing a sequence of numbers.\n",
    "    torch.tensor # Create tensor from data.\n",
    "    \n",
    "  # **Operations**\n",
    "    torch.matmul # Matrix multiplication.\n",
    "    torch.cat # Concatenates tensors along a specified dimension.\n",
    "    torch.stack # Stacks tensors in a new dimension.\n",
    "    torch.split # Splits a tensor into chunks.\n",
    "    \n",
    "  # **Conversion**\n",
    "    torch.from_numpy # Convert numpy array to tensor.\n",
    "    torch.Tensor.numpy # Convert tensor to numpy array.\n",
    "    \n",
    "  # Device Management**   torch.device` - Device interface (CPU/GPU).\n",
    "    torch.cuda.is_available # Check if CUDA is available.\n",
    "    tensor.to('cuda') # Move tensor to GPU.\n",
    "\n",
    "## torch.nn\n",
    "  # Layers**\n",
    "    nn.Linear(m,n) # Fully connected layer from m to n units\n",
    "    nn.Conv2d # 2D convolutional layer.\n",
    "    nn.MaxPool2d # 2D max pooling layer.\n",
    "    \n",
    "  # Activation Functions**\n",
    "    nn.ReLU # Rectified Linear Unit activation function.\n",
    "    nn.Sigmoid  # Sigmoid activation function.    nn.Tanh` - Hyperbolic tangent activation function.\n",
    "      # others: ELU, SELU, PReLU, LeakyReLU, RReLu, CELU, GELU, Threshold, Hardshrink, HardTanh, LogSigmoid, Softplus, Tanh, Softmin, \n",
    "      \n",
    "  # Loss Functions**\n",
    "    nn.CrossEntropyLoss # Loss function for classification.\n",
    "    nn.MSELoss  # Mean Squared Error loss for regression.\n",
    "    nn.BCELoss  # Binary Cross-Entropy Loss.\n",
    "      # others: L1Loss, MSELoss, CrossEntropyLoss, CTCLoss, NLLLoss, PoissonNLLLoss, KLDivLoss, BCELoss, BCEWithLogitsLoss etc.\n",
    "      \n",
    "  # Utilities**\n",
    "    nn.Module # Base class for all neural network modules.\n",
    "\n",
    "## torch.optim\n",
    "  # Optimizers**\n",
    "    optim.SGD # Stochastic Gradient Descent optimizer.\n",
    "    optim.Adam  # Adam optimizer.\n",
    "    optim.Adagrad # Adagrad optimizer.\n",
    "    optim.RMSprop # RMSprop optimizer.\n",
    "    opt.step() # update weights\n",
    "\n",
    "## Learning Rate Scheduling\n",
    "    scheduler = optim.X(optimizer,...)      # create lr scheduler\n",
    "    scheduler.step()                        # update lr after optimizer updates weights\n",
    "    optim.lr_scheduler.X                    # where X is LambdaLR, MultiplicativeLR,\n",
    "                                            # StepLR, MultiStepLR, ExponentialLR,\n",
    "                                            # CosineAnnealingLR, ReduceLROnPlateau, CyclicLR,\n",
    "                                            # OneCycleLR, CosineAnnealingWarmRestarts,\n",
    "\n",
    "## Datasets and Data Utilities \n",
    "    torch.utils.data.DataLoader(data, batch_size, shuffle, drop_last)  # loads data batches agnostic of structure of individual data points\n",
    "    torch.utils.data.TensorDataset(X, y)    # labelled dataset in the form of tensors\n",
    "    torch.utils.data.DatasetFolder()\n",
    "    torch.utils.data.Dataset()    # abstract class representing dataset\n",
    "    torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "## torch.nn.functional\n",
    "  # Activation Functions**\n",
    "    F.relu  # Rectified Linear Unit activation function.\n",
    "    F.sigmoid # Sigmoid activation function.\n",
    "    F.tanh  # Hyperbolic tangent activation function.\n",
    "    \n",
    "  # Loss Functions**\n",
    "    F.cross_entropy # Loss function for classification.\n",
    "    F.mse_loss  # Mean Squared Error loss for regression.\n",
    "    F.binary_cross_entropy  # Binary Cross-Entropy Loss.\n",
    "    \n",
    "  # Convolutional Operations**\n",
    "    F.conv2d  # Apply a 2D convolution.\n",
    "    F.max_pool2d  # Apply 2D max pooling.\n",
    "\n",
    "\n",
    "\n",
    "## torchvision\n",
    "  # Datasets**\n",
    "    torchvision.datasets.MNIST  # Dataset of handwritten digits.\n",
    "    torchvision.datasets.CIFAR10  # Dataset of 32x32 images in 10 classes.\n",
    "    torchvision.datasets.ImageFolder  # Generic data loader where images are arranged in a specific format.\n",
    "  # Models**\n",
    "    torchvision.models.resnet50 # Pretrained ResNet-50 model.\n",
    "    torchvision.models.vgg16  # Pretrained VGG-16 model.\n",
    "    torchvision.models.alexnet  # Pretrained AlexNet model.\n",
    "  # Transforms**\n",
    "    torchvision.transforms.Resize # Resize an image.\n",
    "    torchvision.transforms.CenterCrop # Crop the image at the center.\n",
    "    torchvision.transforms.ToTensor # Convert image to tensor.\n",
    "\n",
    "## torchvision.transforms\n",
    "  # Common Transforms**\n",
    "    transforms.Compose  # Compose several transforms together.\n",
    "    transforms.ToTensor  # Convert PIL image or numpy array to tensor.\n",
    "    transforms.Normalize  # Normalize a tensor image with mean and standard deviation.\n",
    "    transforms.RandomCrop # Crop the image randomly.\n",
    "    transforms.RandomHorizontalFlip # Horizontally flip the image randomly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246134b",
   "metadata": {},
   "source": [
    "> Pytorch Image Models (TIMM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e569b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "from timm.scheduler import CosineLRScheduler\n",
    "from timm.utils import model_parameters\n",
    "\n",
    "# List all available models\n",
    "all_models = timm.list_models()\n",
    "\n",
    "# List models pre-trained on ImageNet\n",
    "pretrained_models = timm.list_models(pretrained=True)\n",
    "all_densenet_models = timm.list_models('*densenet*')    # search for model architectures using Wildcard. this will list all densenet models\n",
    "\n",
    "# Create a model with pretrained weights\n",
    "model = timm.create_model('resnet50', pretrained=True)\n",
    "\n",
    "# Create a model with a specific number of output classes (e.g., 10)\n",
    "model = timm.create_model('resnet50', pretrained=True, num_classes=10)\n",
    "model.eval()\n",
    "\n",
    "lr_scheduler = CosineLRScheduler(optimizer, t_initial=len(train_loader)*num_epochs)\n",
    "\n",
    "# Counting the number of parameters\n",
    "num_params = model_parameters(model, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b572ca46",
   "metadata": {},
   "source": [
    "> Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e497328",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Core CV tasks in Hugging Face:\n",
    "    # Image classification\n",
    "    # Image segmentation\n",
    "    # (Zero-shot) object detection\n",
    "    # Video classification\n",
    "    # Depth estimation\n",
    "    # Image-to-image synthesis\n",
    "    # Unconditional image generation\n",
    "    # Zero-shot image classification\n",
    "    \n",
    "# Tasks that lie at the intersection of vision and language:\n",
    "    # Image-to-text (image captioning, OCR)\n",
    "    # Text-to-image\n",
    "    # Document question-answering\n",
    "    # Visual question-answering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44b68c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline, AutoImageProcessor, AutoModelForObjectDetection, DetrImageProcessor, DetrForObjectDetection\n",
    "\n",
    "pipe = pipeline(\"object-detection\", model=\"facebook/detr-resnet-50\") \n",
    "output = pipe(\"http://images.cocodataset.org/val2017/000000039769.jpg\")\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a1d109",
   "metadata": {},
   "source": [
    "> Supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d941c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://supervision.roboflow.com/             # reusable code for computer vision \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6e30715",
   "metadata": {},
   "source": [
    "### Data Analytics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0f8fa02d",
   "metadata": {},
   "source": [
    "#### Documentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd06f218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data Preprocessing\n",
    "    importing the required libraries\n",
    "    importing the dataset\n",
    "    handling missing data\n",
    "    encoding the categoical data\n",
    "    feature engineering\n",
    "    spliting the dataset into test set and training set\n",
    "    feature scaling \n",
    "    *webscraping with beautifulsoup\n",
    "    \n",
    "Machine Learning\n",
    "    Simple linear regression\n",
    "    multiple linear regression\n",
    "    logistic regression\n",
    "    K nearest neighbours\n",
    "    support vector machines\n",
    "    Naive Bayes Classifier\n",
    "    Decision trees\n",
    "    random forest\n",
    "    neural networks\n",
    "    k-means clustering\n",
    "    Hierarchical Clustering\n",
    "\n",
    "Deep Learning\n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a303da5",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b460abc",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from pyforest import * #automatically imports all the necessary libraries for data science/analytics\n",
    "\n",
    "# Data Analysis\n",
    "import pandas as pd          # data analysis library for handling structured data\n",
    "from typing import List\n",
    "from pandas import DataFrame \n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "from pandas.plotting import parallel_coordinates \n",
    "from pivottablejs import pivot_ui\n",
    "import statsmodels.api as sm # library for performing statistical analysis\n",
    "import scipy.stats as stats\n",
    "import pandas_profiling\n",
    "\n",
    "\n",
    "import os       # import the os module for accessing operating system functionalities\n",
    "import sqlite3  # import the sqlite3 module for working with SQLite databases\n",
    "import math     # import the math module for mathematical operations and functions\n",
    "from collections import Counter  # import the Counter class from the collections module for counting items in an iterable\n",
    "from pathlib import Path  # import the Path class from the pathlib module for working with file paths and directories\n",
    "from tqdm import tqdm  # import the tqdm module for displaying progress bars during loops and iterations\n",
    "\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "import plotly.express as px   # interactive data visualization library\n",
    "import plotly.graph_objects as go   # library for creating interactive graphs and charts\n",
    "from plotly.subplots import make_subplots\n",
    "import missingno as msno \n",
    "import kaleido \n",
    "\n",
    "# Machine Learning \n",
    "from scipy.stats import skew, norm\n",
    "import yellowbrick\n",
    "import sklearn # machine learning library containing many algorithms for building models\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score, classification_report \n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5bb1e655",
   "metadata": {},
   "source": [
    "#### Preprocessing Structured Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e8446dd7",
   "metadata": {},
   "source": [
    "##### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f69cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv('fileName.csv', sheet_name = \"Sheet 1\", parse_dates=True, columns=['product', 'price'], skiprows = [2], header = None,  )\n",
    "pd.read_html(\"https://developers.turing.com/dashboard/turing_test\")\n",
    "pd.read_excel\n",
    "pd.read_spss\n",
    "import pyreadstat\n",
    "pyreadstat.read_spss(\"filename.sav\", ) \n",
    "1. # parse_dates=True or [\"Date\", \"DOB\"] - if you want dates to be recognized as dates, add this argument:\n",
    "    #infer_datetime_format = True (used for only csv and others, but not excel)\n",
    "2. #columns=['product', 'price'] - if you want to include the names of the columns\n",
    "3. #sheet_name = \"Sheet 1\" - to reat sheet 1 only. You could also just write \"Sheet 1\" instead of \"sheet_name\"\n",
    "4. #skiprows = [2] - if you want to skip three rows\n",
    "5. #usecols = [2, 5] - if you want to import only columns three and 6\n",
    "6. #header = None/1/2 - if there are no headers in the dataset (None), or use header 2\n",
    "7. #skip_blank_lines=True - Skip blank lines in the dataset. \n",
    "8. #names = [\"Age\", \"DOB\"] - names of the hearders assuming they were not provided\n",
    "9. #nrows = 2 - number of rows to import\n",
    "10 #na_values = [\"not available\", \"n.a\", -1] - identify n.a values and assign them as n.a\n",
    "   #na_values = {\"eps\" = [\"not available\", \"n.a\", -1],\n",
    "    #            \"DOB\" = [\"not available\", 0.5]\n",
    "11. #index_col = False/\"Date\" - False for no index column, or use any column of choice.\n",
    "12. #dtype = { \"DOB\" : float, - used to specify the datatype of the columns while reading the file. To change the datatype, use .astype function. \n",
    " #           \"Name\" : int}\n",
    "\n",
    "#Read from SQL Query\n",
    "from sqlalchemy import create_engine\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "pd.read_sql(\"SELECT * FROM my_table;\", engine)\n",
    "pd.read_sql_table('my_table', engine)\n",
    "pd.read_sql_query(\"SELECT * FROM my_table;\", engine)\n",
    "\n",
    "#import a .tsv file from a url\n",
    "chipo = pd.read_csv(\"https://raw.githubusercontent.com/justmarkham/DAT8/master/data/chipotle.tsv\", sep = '\\t')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3bcdffa4",
   "metadata": {},
   "source": [
    "##### Export Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef04b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('dir/myDataFrame.xlsx',  sheet_name='Sheet2', index = False) #write to excel\n",
    "df.to_sql('myDf', engine) #write to sql\n",
    "\n",
    "with pd.ExcelWriter (\"courses.xlsx\") as writer: #writing to multiple sheets\n",
    "    df1.to_excel (writer, sheet_name = \"sheet_1\")\n",
    "    df2.to_excel (writer, sheet_name = \"sheet_2\")\n",
    "    \n",
    "with pd.ExcelWriter (\"courses.xlsx\", mode = \"a\") as writer: #writing to multiple sheets\n",
    "    df.to_excel (writer, sheet_name = \"sheet_1\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61829839",
   "metadata": {},
   "source": [
    "##### DataFrame Manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ec3c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('max_colwidth',800) #column width\n",
    "pd.set_option(\"display.max_columns\",200) #Number of columns:\n",
    "bp_data.iloc[:, 1:10] #inner slicing\n",
    "bp_data.loc[:, \"Year\": \"Home Runs\"] #outer slicing \n",
    "df.iloc[np.r_[0:5, -5:0]] #selects the first 5 and last 5 rows. very important. \n",
    "numeric_cols = smart_home.select_dtypes(include=[\"float64\", \"int64\"]) # Select only numeric columns\n",
    "\n",
    "#duplicate detection\n",
    "df.groupby('item_name').item_name.nunique() #Find duplicates #count unique values of one column:\n",
    "pd.concat(i for _, i in df.groupby('my_user_id') if len(i) > 1)  #Find duplicates #Show duplicate values for one column:\n",
    "df = df.drop_duplicates(subset='my_user_id', keep=False)    #Remove duplicates\n",
    "df['n'].replace({'a': 'x', 'b': 'y', 'c': 'w', 'd': 'z'}) #replace the values in a column\n",
    "df['n'].rename(columns = ['a', 'b']) #rename columns\n",
    "df[df.duplicated(subset = \"patient_id\", keep = False)].sort_values(\"patient_id\")#A. To isolate duplicate values and view the results.\n",
    "repeat_patients = df.groupby(by = \"patient_id\").size().sort_values(ascending = False)#B Dsiplat all duplicate values in order\n",
    "\n",
    "#other basic data manipulation\n",
    "df = df.drop(['col1', 'col2'], axis=1)  #Drop one or more column(s): axis = 1 (column) or axis=0 (row)\n",
    "del df['COL1'] #remove columns\n",
    "df_train.rename(columns={\"trans_date_trans_time\":\"transaction_time\", \"cc_num\":\"credit_card_number\"},\n",
    "                inplace=True) #rename columns\n",
    "df.sort_values(by = ['COL1', 'COL2'], ascending = (0, 1)) # sorting\n",
    "df[(df['colCOL11'] >= 1) & (df['COL2'] <=1 )] #filter multiple conditions \"and\"\n",
    "df[(df['colCOL11'] >= 1) | (df['COL2'] <=1 )] #filter multiple conditions \"or\"\n",
    "df = pd.merge(df1, df2, how = 'left', left_on = ['my_user_id'], right_index = True) #combine dataframe\n",
    "df = pd.concat([df1, df2], axis = 0) #combine dataframe #concatenate\n",
    "df_train.credit_card_number = df_train.credit_card_number.astype('category') # Change dtypes\n",
    "df_train.credit_card_number = df_train.credit_card_number.astype('object') # Change dtypes (other dtypes = float64, int64)\n",
    "df.info() #check basic information of dataframe\n",
    "df.unique() #check unique variables of a dataframe\n",
    "df.describe() #describe basic details like count, mean, sd, percentile etc.\n",
    "np.round(df_train.describe(),2) #round up to the nearest 2 significant figures\n",
    "df.columns.value_counts().sum() or chipo.shape[1] #count number of columns in a dataset\n",
    "\n",
    "#check datatypes of features\n",
    "sns.set(style=\"ticks\", context=\"talk\",font_scale = 1)\n",
    "plt.style.use(\"dark_background\")\n",
    "plt.figure(figsize = (8,6))\n",
    "ax = df_train.dtypes.value_counts().plot(kind='bar',grid = False,fontsize=20,color='grey')\n",
    "for p in ax.patches:\n",
    "    height = p.get_height()\n",
    "    ax.text(p.get_x()+ p.get_width() / 2., height + 0.2, height, ha = 'center', size = 25)\n",
    "sns.despine()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "26ef4c5e",
   "metadata": {},
   "source": [
    "##### Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a35aa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#23 datascience techniques you should know (https://ai.plainenglish.io/23-data-science-techniques-you-should-know-61bc2c9d1b3a)\n",
    "\n",
    "#Data profiling - very usedul for statistical analysis\n",
    "import pandas_profiling\n",
    "df.profile_report() \n",
    "\n",
    "#Pivottables - Smart tables\n",
    "from pivottablejs import pivot_ui \t#for smart tables in Jupyter notebook\n",
    "pivot_ui(df)\n",
    "\n",
    "#or (use for google colab)\n",
    "from google.colab import data_table\t\t#for smart tables in Google Colab\n",
    "data_table.enable_datafrane_formatter()\n",
    "\n",
    "#filtering data\n",
    "weather_per_day = weather_data.resample('D') #filters out the daily data from a dataframe with time as index\n",
    "weather_per_month = weather_data.resample('M') #filters out monthly data from a dataframe with time as index\n",
    "weather_per_hour = weather_data.resample('H') #filters out hourly data from a dataframe with time as index\n",
    "weather_data = smart_home.filter(items=['temperature',\n",
    "                                      'humidity', 'visibility', ])    #filters out the following columns from the dataframe\n",
    "africa_deaths_cause = df_deaths_cause[df_deaths_cause[\"Code\"].isin(Africa[\"Three_Letter_Country_Code\"])]\n",
    "                                                                #filters the rows in df_deaths_cause for Africa['three_letter_country_code..']\n",
    "numeric_cols = smart_home.select_dtypes(include=[\"float64\", \"int64\"]) # Select only numeric columns\n",
    "\n",
    "\n",
    "\n",
    "#grouping variables in a dataset\n",
    "grouped = df.groupby(\"Year\", as_index=False, dropna=False) #as_index = True/False\n",
    "print(grouped.get_group(2010))\n",
    "    sales.groupby(\"store\", \"product_group\", as_index=False)[\"stock_qty\"].mean()  |  sales.groupby(\"store\")[[\"stock_qty\",\"price\"]].mean()  \n",
    "    sales.groupby(\"store\", as_index=False)[\"stock_qty\"].agg([\"mean\", \"max\", \"unique\"])\n",
    "    sales.groupby(\"store\", as_index=False)[\"last_week_sales\"].nlargest(2) #returns largest values for each column (or .nsmallest(4))\n",
    "    sales_sorted.groupby(\"store\", as_index=False).nth(4) #returns the nth value (or nth(-2) to return the second row from the end)\n",
    "    sales.groupby([\"store\",\"product_group\"], as_index=False).agg(avg_sales = (\"last_week_sales\", \"mean\"))    \n",
    "    sales.groupby(\"store\", as_index=False).agg(total_sales_in_thousands = (\"last_month_sales\", lambda x: round(x.sum() / 1000, 1))) #lambda function\n",
    "    daisy_pg1 = sales.groupby([\"store\", \"product_group\"], as_index=False).get_group((\"Daisy\",\"PG1\")) #multiple grouping, and getting multiple grouped output\n",
    "    sales.groupby(\"continent\").agg({\n",
    "                        \"sprit_servings\": \"mean\",\n",
    "                        \"beer_servings\": \"max\",\n",
    "                        \"wine_servings\": lambda x: x**3\n",
    "                        })\n",
    "\n",
    "#Multi-indexing\n",
    "        # if your indices are ['index1', 'index2'] and you want to select the row where index1 is 'value1' and index2 is 'value2', \n",
    "df.loc['value1']\n",
    "df.loc[('value1', 'value2')]\n",
    "df.loc['value1', 'column_label']\n",
    "df.xs('value2', level='index2') #cross-section method - very helpful\n",
    "df.loc[df.index.get_level_values('index2') == 'value2', ['col1', \"col2\"]]\n",
    "                                          \n",
    "#Merging                                                                           \n",
    "result = pd.merge(df1, df2, on='Product', how='inner')              \n",
    "result = pd.merge(df1, df2, on='Employee ID', how='left')\n",
    "result = pd.merge(df1, df2, on='Product', how='right')\n",
    "train = train.merge(riders, how = 'left', left_on='rider_id', right_on='Rider ID')\n",
    "test = test.merge(riders, how = 'left', left_on='rider_id', right_on='Rider ID')\n",
    "                                               \n",
    "If the column names in both DataFrames are the same, we can use the \"on\" parameter instead of \"left_on\" and \"right_on\".                                        \n",
    "\n",
    "\"left_on\"; \"right_on\":\n",
    "The resulting DataFrame result will contain only the rows where the values in the 'key1' column of df1 \n",
    "match the values in the 'key2' column of df2. In this case, the resulting DataFrame will contain two rows, \n",
    "where the values in the 'key1' column are 2 and 3.\n",
    "\n",
    "                                               \n",
    "\"left\": This performs a left join, \"keeping all the rows from the left\" (left_on) dataframe \n",
    "        and merging with the matching rows from the right (right_on) dataframe. \n",
    "        \"If there is no match, the result will have NaN values in the columns from the right dataframe\".\n",
    "\n",
    "\"right\": This performs a right join, \"keeping all the rows from the right\" (right_on) dataframe \n",
    "        and merging with the matching rows from the left (left_on) dataframe. \n",
    "        \"If there is no match, the result will have NaN values in the columns from the left dataframe\".\n",
    "\n",
    "\"outer\": This performs an outer join, \"keeping all the rows from both dataframes\". \n",
    "        If there is no match between the rows from the left and right dataframes, \n",
    "        \"the result will have NaN values in the columns from the non-matching dataframe\".\n",
    "\n",
    "\"inner\": This performs an inner join, \"keeping only the rows that have matching values\" \n",
    "        in both the left (left_on) and right (right_on) dataframes.                                 \n",
    "                                               \n",
    "                                               \n",
    "                                               \n",
    "#Pandas Apply and Map functions (use .apply() for multiple columns and .map() for single column)\n",
    "    df['colA'] = df['colA'].map(lambda x: x + 1)\n",
    "    df[['colA', 'colD']] = df[['colA', 'colD']].apply(lambda x: x + 1)\n",
    "    \n",
    "\n",
    "#Regular expression\n",
    "import re\n",
    "re.findall(pattern, text). #This function takes two arguments in the form of re.findall(pattern, string). \n",
    "              #Here, pattern represents the substring we want to find, and string represents the main string we want to find it in                               \n",
    "              #returns the matches as a list of strings or tuples    \n",
    "    match = re.findall(r\"From:.*\", fh)\n",
    "re.search(pattern, text) #matches the first instance of a pattern in a string, and returns it as a re match object.\n",
    "        match = re.search(r\"From:.*\", fh)\n",
    "re.split(pattern, text) \n",
    "        address = re.findall(r\"From:.*\", fh)\n",
    "        for item in address:\n",
    "            for line in re.findall(r\"\\w\\S*@.*\\w\", item):\n",
    "                username, domain_name = re.split(r\"@\", line)\n",
    "                print(\"{}, {}\".format(username, domain_name))\n",
    "re.sub(pattern, repl, text) #it substitutes parts of a string - this replaces the matched substring(s) with the repl string.\n",
    "re.finditer(pattern, text) #returns the matches with extra functionality\n",
    "re.match(pattern, text) #returns the matches at the beginning of strings\n",
    "\n",
    "\n",
    "#get duplicated and non-duplicated values\n",
    "duplicated = df[df.duplicated(\"Roll_no\")]\n",
    "non_duplicated = df[~df.duplicated(\"Roll_no\")]\n",
    "\n",
    "transactions.rename(columns={'Quantity' :'Quant'}) #rename Column \"Quantity\" to \"Quant\"\n",
    "transactions.sort_values('TransactionID', ascending=False) #Sort rows by TransactionId decending\n",
    "transactions.sort_values(['Quantity','TransactionDate'],ascending=[True,False]) #Sort multiple rows\n",
    "transactions[pd.unique(['UserID'] + transactions.columns.values.tolist()).tolist()] # Make UserID the first column\n",
    "transactions[foo | (bar <0)] # Subset rows where foo is TRUE or bar is negative\n",
    "transactions[(transactions.Quantity >0) & (transactions.UserID == 2)] # Subset rows where Quantity > 1 and UserID = 2\n",
    "chipo.groupby([\"item_name\"]).max().sort_values(by=[\"quantity\"] , ascending= False) #group items and find the most sales\n",
    "\n",
    "#filter out (extract) data from a column in pandas (use regular expression)\n",
    "df['col1'] = df['original'].str.extract(r\"\\(([A-Za-z0-9 _]+)\\)\") #using regular expression (#The '.str.extract()' method extracts substrings from the Series that match a given regular expression.)\n",
    "emails_df[emails_df[\"sender_email\"].str.contains(\"epatra|spinfinder\")] #using regular expression\n",
    "df_most[\"price_float\"] = list(map((lambda x: (x[1:])), df_most[\"item_price\"])) #another method using lambda and map functions\n",
    "df[\"FName\"] = [i.split(\" \")[0].split(\",\")[-1].strip() for i in first_name] #to extract features from a dataset\n",
    "\n",
    "\n",
    "\n",
    "#How to add an empty column to DataFrame\n",
    "df[\"Blank_Column\"] = \" \"\n",
    "df[\"NaN_Column\"] = np.nan\n",
    "df[\"None_Column\"] = None\n",
    "df2 = df.assign(Blank_Column=\" \", NaN_Column = np.nan, None_Column=None) # Add an empty columns using the assign() method\n",
    "df2 = df.reindex(columns = df.columns.tolist() + [\"None_Column\", \"None_Column_2\"]) # Add multiple columns with NaN , uses columns param\n",
    "df2 = df.reindex(df.columns.tolist() + [\"None_Column\", \"None_Column_2\"],axis=1) # Add multiple columns with NaN, , uses axis param \n",
    "df2 = df.reindex(columns=[\"None_Column\", \"None_Column_2\"]+df.columns.tolist()) # Add multiple columns to the Beginning\n",
    "df2 = df.reindex(columns=[\"Courses\",\"None_Column\", \"None_Column_2\",\"Fee\"]) # Add multiple columns with NaN, , uses axis param \n",
    "df.insert(0,\"Blank_Column\", \" \") # Using insert(), add empty column at first position\n",
    "df[\"Blank_Column\"] = df.apply(lambda _: ' ', axis=1) # Using apply() & lambda function\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "38f66320",
   "metadata": {},
   "source": [
    "##### Speed up EDA in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2b0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('my_data.csv')\n",
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "#Pandas Profiling\n",
    "    \"\"\"a library for creating comprehensive reports on dataframes\n",
    "    \"\"\"\n",
    "from pandas_profiling import ProfileReport    \n",
    "profile = ProfileReport(df) # Create the report\n",
    "profile.to_file('my_data_report.html') # Save the report\n",
    "\n",
    "#SweetViz\n",
    "    \"\"\"a library for comparing datasets and generating report on the comparison\n",
    "    \"\"\"\n",
    "import sweetviz as sv\n",
    "my_report = sv.compare([train, 'Train'], [test, 'Test'], 'Survived') # Create the report\n",
    "my_report.show_html('my_report.html') # Save the report\n",
    "\n",
    "#AutoViz\n",
    "    \"\"\"a tool that automatically generates visualizations for a given dataset\n",
    "    \"\"\"\n",
    "from autoviz.AutoViz_Class import AutoViz_Class\n",
    "# Create the visualizations\n",
    "AV = AutoViz_Class()\n",
    "viz = AV.AutoViz('my_data.csv')\n",
    "viz.view_plots()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1174649d",
   "metadata": {},
   "source": [
    "##### WebScraping with ChatGPT Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc11d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use beta.openai.com/playground\n",
    "\n",
    "BeautifulSoup Prompts\n",
    "#Scrape this website: https://stackoverflow.com/questions/ with Python and BeautifulSoup\n",
    "#Locate the element with tag \"aa\" and class \"asa\". Scrape all the \"a\" elements inside.\n",
    "#Get the text attribute and print it.\n",
    "\n",
    "Selenium Prompts\n",
    "#Scrape this website: https://stackoverflow.com/questions/ with Python, Selenium and Chromedriver\n",
    "#Maximize the window, wait 5 seconds and locate all the elements with following xpath: \"span\" tag, \"class\" attribute name, and \"dsd\" attribute value\n",
    "#Get the text attribute and print them. \n",
    "\n",
    "NB:\n",
    "    tag --> attribute\n",
    "    wait 5 seconds or 15 seconds\n",
    "\n",
    "\n",
    "\n",
    "#Interate ChatGPT with python, and ineract on the command line.\n",
    "To run the code below on Windows, follow these steps:\n",
    "\n",
    "1. Open a text editor and paste the modified code into a new file.\n",
    "2. Save the file with the name HeyChatGPT.py in a directory of your choice.\n",
    "3. Open a command prompt or PowerShell window and navigate to the directory \n",
    "    where the HeyChatGPT.py file is located.\n",
    "4. Type set api_key=xxxxxxxxxx to set the value of the api_key environment variable \n",
    "    to your OpenAI API key. Replace xxxxxxxxxx with your actual API key.\n",
    "    #api_key=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 (my api_key)\n",
    "5. Type python HeyChatGPT.py \"How to reach Mars?\" to run the HeyChatGPT.py script and generate \n",
    "    a response to the prompt \"How to reach Mars?\". Replace the prompt text with any other \n",
    "    prompt you want to generate a response to.\n",
    "\n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Import openai, os, and sys modules\n",
    "import openai\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Set the prompt to the first command-line argument\n",
    "prompt = sys.argv[1]\n",
    "\n",
    "# Set the OpenAI API key to the value of the 'api_key' environment variable\n",
    "openai.api_key = os.environ['api_key']\n",
    "\n",
    "# Call the OpenAI API to generate a response to the prompt\n",
    "completions = openai.Completion.create(\n",
    "    engine=\"text-davinci-003\",\n",
    "    prompt=prompt,\n",
    "    max_tokens=1024,\n",
    "    n=1,\n",
    "    stop=None,\n",
    "    temperature=0.5,\n",
    ")\n",
    "\n",
    "# Extract the generated text from the API response\n",
    "message = completions.choices[0].text\n",
    "\n",
    "# Print the generated text to the console\n",
    "print(message)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "56dbd2a9",
   "metadata": {},
   "source": [
    "##### WebScraping - BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f899b22f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3920982088.py, line 77)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 77\u001b[1;36m\u001b[0m\n\u001b[1;33m    for item1, item2, item3 in zip(company_name, skills, published_date) # Write the scraped data to the CSV file\u001b[0m\n\u001b[1;37m                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import csv \n",
    "\n",
    "html_text = requests.get(\"https://www.sofascore.com/tournament/basketball/usa/nba/132\").text\n",
    "soup = BeautifulSoup(html_text, \"lxml\") #create an instance of beautifulsoup\n",
    "jobs = soup.find_all(\"li\", class_ = \"clearfix job-bx wh-shd-bx\" ) #\"li\" = tag, class = \"clearfix ....\" #Main tag\n",
    "#or\n",
    "jobs = soup.find(name = 'table', attrs = {'id' : 'per_game'}) # BeautifulSoup's .find() method searches for a tag and specified attributes, \n",
    "#or\n",
    "jobs = soup.find_all('tbody', {'class': 'Crom_body__UYOcU'})\n",
    "\n",
    "#having searched the main tag \"li\", next step would be to search and extract some particular information of choice\n",
    "# Creating a list of dictionaries to then convert into a Pandas Dataframe\n",
    "wiz_stats = []\n",
    "for row in wiz_per_game.find_all('tr')[1:]:  # Excluding the first 'tr', since that's the table's title head\n",
    "\n",
    "    player = {}\n",
    "    player['Name'] = row.find('a').text.strip()\n",
    "    player['Age'] = row.find('td', {'data-stat' : 'age'}).text\n",
    "    player['Min PG'] = row.find('td', {'data-stat' : 'mp_per_g'}).text\n",
    "    player['Field Goal %'] = row.find('td', {'data-stat' : 'fg_pct'}).text\n",
    "    player['Rebounds PG'] = row.find('td', {'data-stat' : 'trb_per_g'}).text\n",
    "    player['Assists PG'] = row.find('td', {'data-stat' : 'ast_per_g'}).text\n",
    "    player['Steals PG'] = row.find('td', {'data-stat' : 'stl_per_g'}).text\n",
    "    player['Blocks PG'] = row.find('td', {'data-stat' : 'blk_per_g'}).text\n",
    "    player['Turnovers PG'] = row.find('td', {'data-stat' : 'tov_per_g'}).text\n",
    "    player['Points PG'] = row.find('td', {'data-stat' : 'pts_per_g'}).text\n",
    "    wiz_stats.append(player)\n",
    "\n",
    "pd.DataFrame(wiz_stats)\n",
    "\n",
    "\n",
    "#OR\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#specify the url\n",
    "url = 'https://www.nba.com/stats/alltime-leaders'\n",
    "\n",
    "#query the website and return the html to the variable 'page'\n",
    "page = requests.get(url)\n",
    "\n",
    "#parse the html using beautiful soup and store in variable 'soup'\n",
    "soup = BeautifulSoup(page.text, 'html.parser') #or use lxml to parse\n",
    "\n",
    "#find all elements with class \"Block_blockContent__6iJ_n\"\n",
    "divs = soup.find_all('tbody', {'class': 'Crom_body__UYOcU'})\n",
    "\n",
    "#loop through the elements and print the text inside\n",
    "for div in divs:\n",
    "    print(div.text)\n",
    "    \n",
    "    \n",
    "\n",
    "#A better way to write it\n",
    "html_text = requests.get(\"https://www.sofascore.com/tournament/basketball/usa/nba/132\").text\n",
    "soup = BeautifulSoup(html_text, \"lxml\") #create an instance of beautifulsoup\n",
    "jobs = soup.find_all(\"li\", class_ = \"clearfix job-bx wh-shd-bx\" ) #\"li\" = tag, class = \"clearfix ....\"\n",
    "\n",
    "#having searched the main tag \"li\", next step would be to search and extract some particular information of choice\n",
    "#some particular information include \"company name\", \"skills\", and whether the job was posted recently\n",
    "#\".find\" - would extract a single instance. \".find_all\" would extract all instances\n",
    "for job in enumerate(jobs): #you can remove the enumerate\n",
    "    published_date = job.find(\"span\", class_ = \"sim-posted\").text \n",
    "    if date in published_date: \n",
    "        company_name = job.find(\"h3\", class_ = \"job_list-comp\").text #add .replace(\" \", \"\") to remove whitespaces\n",
    "        skills = job.find(\"span\", class_ = \"srp-skills\").text #add .replace(\" \", \"\") to remove whitespaces\n",
    "        more_info = job.header.h2.a['href'] #used to extract the link for additional info.\n",
    "                                            #header is the first tag, h2 is second. \"a\" is the third indicating a link\n",
    "                                            #\"href\" is not a tag but a class name where the link is at.\n",
    "        \n",
    "#         print(f\"Company Name: {company_name.strip()} \\n\") #you can decide not to use \".strip() function\"\n",
    "#         print(f\"Skills: {skills.strip()} \\n\") \n",
    "#         print(f\"Date Posted: {published_date} \\n\")\n",
    "#         print (\" \") \n",
    "        \n",
    "#         with open('data.csv', 'w', newline='') as csv_file: #open a new csv file in write mode\n",
    "#             writer = csv.writer(csv_file) #create an instance of csv\n",
    "#             writer.writerow([\"Company Name\", \"Skills\", \"Date Posted\"])\n",
    "#             for item1, item2, item3 in zip(company_name, skills, published_date) # Write the scraped data to the CSV file\n",
    "#                 writer.writerow([item1, item2, item3])\n",
    "#             csv_file.close()\n",
    "\n",
    "with open('data.csv', 'w', newline='') as csv_file: #open a new csv file in write mode\n",
    "        writer = csv.writer(csv_file) #create an instance of csv\n",
    "        writer.writerow([\"Company Name\", \"Skills\", \"Date Posted\"])\n",
    "        for item1, item2, item3 in zip(company_name, skills, published_date) # Write the scraped data to the CSV file\n",
    "            writer.writerow([item1, item2, item3])\n",
    "csv_file.close() # Close the CSV file\n",
    "print(f\"Scraped data saved in {csv_file_name}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fc8136f",
   "metadata": {},
   "source": [
    "##### WebScraping with Selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2416a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bacff3c2",
   "metadata": {},
   "source": [
    "##### Git and Github Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9c5da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting & Creating Projects\n",
    "git init #Initialize a local Git repository\n",
    "git clone ssh://git@github.com/[username]/[repository-name].git #Create a local copy of a remote repository\n",
    "\n",
    "#Basic Snapshotting\n",
    "git status #Check status\n",
    "git add [file-name.txt] #Add a file to the staging area\n",
    "git add -A #Add all new and changed files to the staging area\n",
    "git commit -m \"[commit message]\" #Commit changes\n",
    "git rm -r [file-name.txt] #Remove a file (or folder)\n",
    "\n",
    "#Branching & Merging\n",
    "git branch #List branches (the asterisk denotes the current branch)\n",
    "git branch -a #List all branches (local and remote)\n",
    "git branch [branch name] #Create a new branch\n",
    "git branch -d [branch name] #Delete a branch\n",
    "git push origin --delete [branch name] #Delete a remote branch\n",
    "git checkout -b [branch name] #Create a new branch and switch to it\n",
    "git checkout -b [branch name] origin/[branch name] #Clone a remote branch and switch to it\n",
    "git branch -m [old branch name] [new branch name] #Rename a local branch\n",
    "git checkout [branch name] #Switch to a branch\n",
    "git checkout - #Switch to the branch last checked out\n",
    "git checkout -- [file-name.txt] #Discard changes to a file\n",
    "git merge [branch name] #Merge a branch into the active branch\n",
    "git merge [source branch] [target branch] #Merge a branch into a target branch\n",
    "git stash #Stash changes in a dirty working directory\n",
    "git stash clear #Remove all stashed entries\n",
    "\n",
    "#Sharing & Updating Projects\n",
    "git push origin [branch name] #Push a branch to your remote repository\n",
    "git push -u origin [branch name] #Push changes to remote repository (and remember the branch)\n",
    "git push #Push changes to remote repository (remembered branch)\n",
    "git push origin --delete [branch name] #Delete a remote branch\n",
    "git pull #Update local repository to the newest commit\n",
    "git pull origin [branch name] #Pull changes from remote repository\n",
    "git remote add origin ssh://git@github.com/[username]/[repository-name].git #Add a remote repository\n",
    "git remote set-url origin ssh://git@github.com/[username]/[repository-name].git \n",
    "                                            #Set a repository's origin branch to SSH\n",
    "    \n",
    "#Inspection & Comparison\n",
    "git log #View changes\n",
    "git log --summary #View changes (detailed)\n",
    "git log --oneline #View changes (briefly)\n",
    "git diff [source branch] [target branch] #Preview changes before merging   \n",
    "\n",
    "#Git LFS \n",
    "cd my-sample-project\n",
    "git lfs install                       # initialize the Git LFS project\n",
    "git lfs track \"*.csv\"                 # select the file extensions that you want to treat as large files\n",
    "git add .                             # add the large file to the project (make sure that .gitattributes is tracked \"git add .gitattributes\")\n",
    "git commit -am \"Added Debian iso\"     # commit the file meta data\n",
    "git push origin main                  # sync the git repo and large file to the GitLab server\n",
    "\n",
    "#fast way to do it \n",
    "navigate to the directory\n",
    "run this \"git config --global core.autocrlf input\"\n",
    "then copy and paste the code from Github. shown below: \n",
    "    echo \"# Tackling-the-Health-Crises-in-Africa\" >> README.md\n",
    "    git init\n",
    "    git add README.md\n",
    "    git commit -m \"first commit\"\n",
    "    git branch -M main\n",
    "    git remote add origin https://github.com/obinopaul/Tackling-the-Health-Crises-in-Africa.git\n",
    "    git push -u origin main\n",
    "\n",
    "#or  \n",
    "#Creating a repository and pushing files\n",
    "Open the command line on your local machine.\n",
    "Navigate to the local directory where your code files are located.\n",
    "Initialize a new git repository by running the command \"git init\".\n",
    "Add your code files to the repository by running the command \"git add .\". (use git add . to add all files in the directory)\n",
    "    If a warning tells you that your files will be replaced by CRLF the newxt time git touchs it. use git \"config --global core.autocrlf input\"\n",
    "    use this \"git\"\n",
    "Commit your changes by running the command \"git commit -m 'Initial commit'\".\n",
    "create the main branch \"git branch -M main\"\n",
    "Connect your local repository to the remote repository by running the command \"git remote add origin https://github.com/[username]/[repository_name].git\"\n",
    "Push your code to the remote repository by running the command \"git push -u origin main\" \n",
    "You will be prompted to enter your Github username and password. \n",
    "Once the files are pushed, they should now appear on your Github repository.\n",
    "\n",
    "\n",
    "git status  #To check if a folder contains an existing Git repository\n",
    "Remove-Item -Recurse -Force -Path \".git\"    #remove the Git repository\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "406ef646",
   "metadata": {},
   "source": [
    "##### README (MarkDown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fd07e7",
   "metadata": {
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "Here are some of the most common and useful markdown syntax to beautify your README file on GitHub:\n",
    "\n",
    "Headings: Use # to denote headings, with one # for the largest heading and six # for the smallest.\n",
    "Lists: Use - or * to create unordered lists, and 1. to create ordered lists.\n",
    "Bold & Italic: Use **bold text** or __bold text__ for bold and *italic text* or _italic text_ for italic.\n",
    "Code blocks: Use ``` to create code blocks, for example:\n",
    "        ```bash \n",
    "        ```\n",
    "        #or\n",
    "        ```python \n",
    "        ```\n",
    "        #or just ```\n",
    "Links: Use [Link text](URL) to create clickable links.\n",
    "Images: Use ![Alt text](image URL) to add images to your README.\n",
    "    #or use the code below to center the image \n",
    "    <p align=\"center\">\n",
    "        <img src=\"image_url\" alt=\"Alt text\" width=\"500\" height=\"300\">\n",
    "    </p>\n",
    "\n",
    "Tables: You can create tables using the following syntax:\n",
    "        | Column 1 | Column 2 |\n",
    "        |----------|----------|\n",
    "        | Row 1, Column 1 | Row 1, Column 2 |\n",
    "        | Row 2, Column 1 | Row 2, Column 2 |\n",
    "Emoji: You can add emoji to your README by using the colon symbol (:) followed by the name of the emoji. \n",
    "    For example, :smile: will show a smiling face emoji.\n",
    "Task lists: You can create a list of tasks that can be checked off using the following syntax:\n",
    "    - [x] Task 1\n",
    "    - [ ] Task 2\n",
    "Blockquotes: You can add quotes or highlight important text by using the > symbol\n",
    "    > Quote or highlighted text\n",
    "Horizontal lines: To separate sections in your README, you can use the following syntax:\n",
    "    ---\n",
    "Strikethrough: To strike through text, use the following syntax:\n",
    "    ~~Strikethrough text~~"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0e13bd6d",
   "metadata": {},
   "source": [
    "##### Requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e3f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip freeze | findstr /i pandas >> requirements.txt #copy all the libraries one by one. It is case insensitive.\n",
    "\n",
    "#another way to do it\n",
    "Open a terminal or command prompt.\n",
    "Activate your virtual environment: Run the following command:\n",
    "        conda activate myenv\n",
    "Install the packages you need for your project: Run the following command for each package that your project requires:\n",
    "        pip install package_name\n",
    "Freeze the packages and versions: Run the following command:\n",
    "        pip freeze > requirements.txt\n",
    "Deactivate the virtual environment: Run the following command:\n",
    "        conda deactivate\n",
    "        conda deactivate envy #or\n",
    "        CALL conda.bat deactivate #or\n",
    "\n",
    "#Creating a new environment \n",
    "\tconda env export > environment.yml      (from a previous environment)\n",
    "\tconda env create -f environment.yml --name myenv\t(conda env create -f /path/to/environment.yml --name myenv)\n",
    "\tconda activate <environment_name>\n",
    "        conda env export > environment.yml (do this to export the environment after completing the project)\n",
    "\t#OR \t\n",
    "        conda create --name project1 --clone base\t(use this to clone the base environment with all its libraries into the new environment)\n",
    "\n",
    "        #if you wish to use the requiremnts.txt file instead of environment.yaml file\n",
    "        conda list -e | findstr /V \"^#\" > requirements.txt      (from a previous environment)\n",
    "        conda create --name myenv --file C:\\Users\\Cornel\\requirements.txt\t \n",
    "\n",
    "\n",
    "# Use the conda env update --file environment.yml command to update the new environment with the packages and dependencies \n",
    "# from the exported YAML file\n",
    "\n",
    "#Google Colab - Creating a new environment and installing libraries into it.\n",
    "        # Start by installing the virtualenv package using the !pip command:\n",
    "                !pip3 install virtualenv\n",
    "        # Create a new virtual environment by running the virtualenv command with the desired environment name:\n",
    "                !virtualenv myenv\n",
    "        # Activate the virtual environment using the source command:\n",
    "                !source /content/drive/MyDrive/colab_env/bin/activate; pip3 list \n",
    "        #if permission is denied, then run below\n",
    "                !chmod +x /content/drive/MyDrive/colab_env/bin/*\n",
    "\n",
    "        # Install the required libraries using !pip within the virtual environment:\n",
    "                !pip3 install library1 library2 library3\n",
    "                pip install -r requirements.txt\n",
    "\n",
    "# Having done this, to load the environment with its libraries:\n",
    "        # Mount Google Drive in your Colab notebook\n",
    "                from google.colab import drive\n",
    "                drive.mount('/content/drive')\n",
    "        # Navigate to the directory where your virtual environment is located on Google Drive\n",
    "                %cd /content/drive/MyDrive/myenv\n",
    "        # Activate the virtual environment using the source command:\n",
    "                !source bin/activate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42465628",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install virtualenv\n",
    "!virtualenv theanoEnv\n",
    "\n",
    "!source /content/theanoEnv/bin/activate; pip3 install theano\n",
    "\n",
    "!source /content/theanoEnv/bin/activate; pip3 list\n",
    "\n",
    "!source /content/theanoEnv/bin/activate; pip3 install robotframework; pip3 list; python3 -m robot --help\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb197176",
   "metadata": {},
   "source": [
    "##### Basic Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed02e624",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naina Chaturvedi on Medium for all courses on Data Science,ML, Deep learning, etc. (https://medium.com/@naina0412)\n",
    "\n",
    "# Object: used for text, numeric, and non-numeric values. If the given data does not fit any of the below dtype, then object type is assigned to it. It is both the blessing and curse in a package. There is a reason why objects are not the way to go for the performance-tuned codebases\n",
    "# int64: Integer numbers  covers both signed and unsigned integers along with the varying variable-length  8, 16, 32 and 64\n",
    "# float64: Floating point numbers  covers the lengths 16, 32 and 64\n",
    "# bool: True and False values\n",
    "# datetime64: covers the date and time values\n",
    "# timedelta[ns]: used to capture the difference between two DateTime values. this dtype is helpful when the user is working with time-series data\n",
    "# category: used to cover a finite list of text values\n",
    "\n",
    "\n",
    "# In a correlation test, the results will include a correlation coefficient and a p-value.\n",
    "# The correlation coefficient is a number that tells us how strong the relationship is between the two things \n",
    "#we are testing. It can range from -1 to 1. A value of -1 means that there is a strong negative relationship \n",
    "#between the two things (when one thing increases, the other thing decreases). A value of 1 means that \n",
    "#there is a strong positive relationship between the two things (when one thing increases, the other thing increases). \n",
    "#A value of 0 means that there is no relationship between the two things.\n",
    "# The p-value is a number that tells us the probability that the relationship between the two things occurred by chance. \n",
    "#If the p-value is less than 0.05, we can say that the relationship is statistically significant, which means that \n",
    "#it is unlikely to have occurred by chance. If the p-value is greater than 0.05, we cannot say that the relationship \n",
    "#is statistically significant.\n",
    "# For example, let's say that we perform a correlation test and the results show a correlation coefficient of 0.6 \n",
    "#and a p-value of 0.01. This means that there is a moderate positive relationship between the two things we are testing \n",
    "#(the coefficient is close to 1) and that this relationship is statistically significant (the p-value is less than 0.05). \n",
    "#This means that we can be confident that the relationship between the two things is real and not just a coincidence.\n",
    "\n",
    "\n",
    "#Cookiecutter Data Science Project format (check https://drivendata.github.io/cookiecutter-data-science/)\n",
    "To clone a cookiecutter template \n",
    "$ cookiecutter cookiecutter https://github.com/drivendata/cookiecutter-data-science \n",
    "#or\n",
    "$ cookiecutter -c v1 https://github.com/drivendata/cookiecutter-data-science\n",
    "\n",
    "xcopy C:\\Users\\Cornel\\Documents\\5. Projects\\cookiecutter\\cookiecutter_main - what he used\\* C:\\backup /s /y\n",
    "This switch tells xcopy to overwrite files in the destination folder if they already exist.\n",
    "    \n",
    " LICENSE\n",
    " Makefile           <- Makefile with commands like `make data` or `make train`\n",
    " README.md          <- The top-level README for developers using this project.\n",
    " data\n",
    "    external       <- Data from third party sources.\n",
    "    interim        <- Intermediate data that has been transformed.\n",
    "    processed      <- The final, canonical data sets for modeling.\n",
    "    raw            <- The original, immutable data dump.\n",
    "\n",
    " docs               <- A default Sphinx project; see sphinx-doc.org for details\n",
    "\n",
    " models             <- Trained and serialized models, model predictions, or model summaries\n",
    "\n",
    " notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n",
    "                         the creator's initials, and a short `-` delimited description, e.g.\n",
    "                         `1.0-jqp-initial-data-exploration`.\n",
    "\n",
    " references         <- Data dictionaries, manuals, and all other explanatory materials.\n",
    "\n",
    " reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n",
    "    figures        <- Generated graphics and figures to be used in reporting\n",
    "\n",
    " requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n",
    "                         generated with `pip freeze > requirements.txt`\n",
    "\n",
    " setup.py           <- Make this project pip installable with `pip install -e`\n",
    " src                <- Source code for use in this project.\n",
    "    __init__.py    <- Makes src a Python module\n",
    "   \n",
    "    data           <- Scripts to download or generate data\n",
    "       make_dataset.py\n",
    "   \n",
    "    features       <- Scripts to turn raw data into features for modeling\n",
    "       build_features.py\n",
    "   \n",
    "    models         <- Scripts to train models and then use trained models to make\n",
    "                       predictions\n",
    "       predict_model.py\n",
    "       train_model.py\n",
    "   \n",
    "    visualization  <- Scripts to create exploratory and results oriented visualizations\n",
    "        visualize.py\n",
    "\n",
    " tox.ini            <- tox file with settings for running tox; see tox.readthedocs.io\n",
    "\n",
    "\n",
    "#Windows CMD\n",
    "dir: This command is used to display a list of files and folders in a directory.\n",
    "cd: This command is used to change the current working directory.\n",
    "copy: This command is used to copy files from one location to another.\n",
    "mkdir: This command is used to create a new directory.\n",
    "del: This command is used to delete files.\n",
    "ren: This command is used to rename files or directories.\n",
    "cls: This command is used to clear the command prompt screen.\n",
    "ipconfig: This command is used to display information about the network configuration of the computer.\n",
    "ping: This command is used to test the connectivity to a network.\n",
    "shutdown: This command is used to shut down the computer.\n",
    "type: This command is used to display the contents of a text file on the screen.\n",
    "xcopy: This command is used to copy entire directory trees, including subdirectories and files."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7462286d",
   "metadata": {},
   "source": [
    "##### Convert Pandas Categorical Data For Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca04188",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(raw_data, columns = ['patient', 'obs', 'treatment', 'score']) #create dataframe\n",
    "le = preprocessing.LabelEncoder() # Create a label (category) encoder object\n",
    "le.fit(df['score']) # Fit the encoder to the pandas column\n",
    "list(le.classes_) # View the labels (if you want)\n",
    "le.transform(df['score']) # Apply the fitted encoder to the pandas column\n",
    "list(le.inverse_transform([2, 2, 1])) # Convert some integers into their category names"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e13e72f",
   "metadata": {},
   "source": [
    "##### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dafc141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill Missing Values' Class With Most Frequent Class\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "imputer = Imputer(strategy='most_frequent', axis=0) # Create Imputer object\n",
    "imputer.fit_transform(X) # Fill missing values with most frequent class. assuming X = DataFrame variable\n",
    "\n",
    "#fill missing values \n",
    "df = pd.fillna({\"DOB\": 0, \"Age\": \"no-event\"}, method = \"ffill\", axis = 1) #method = \"ffil\" or \"bfill\" \n",
    "df.interpolate (method = \"linear\") #method = \"linear\", \"time\", \"index\", \"quadratic\" etc. \n",
    "  #fill missing values with average values\n",
    "    c = avg = 0 #compute average\n",
    "    for ele in df[\"Marks\"]:\n",
    "        if str(ele).isnumeric():\n",
    "            c += 1\n",
    "            avg += ele\n",
    "    avg /= c\n",
    "    #Replace missing values\n",
    "    df = df.replace(to_replace=\"NaN\", value = avg)\n",
    "    #Display data\n",
    "    df\n",
    "\n",
    "#percentage of missing valuess in dataset\n",
    "percent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\n",
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "m_data = pd.concat([total, percent],axis=1 )\n",
    "m_data.head(10)\n",
    "    \n",
    "#Summary of missing values\n",
    "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      df.isnull().sum()\n",
    "df.shape #total number of rows and columns in the dataset. Then compare with the number of missing values.\n",
    "\n",
    "#visualize missing data\n",
    "import missingno as msno\n",
    "import pandas as pd\n",
    "df = pd.read_csv('dataset.csv') # Load the dataset\n",
    "msno.matrix(df) # Visualize the missing values\n",
    "#This will create a matrix plot that shows the missing values as white lines.\n",
    "\"\"\"You can also use the 'msno.bar' function to visualize the missing values as a bar chart, \n",
    "or the \"msno.heatmap\" function to visualize the missing values as a heatmap.\"\"\"\n",
    "\n",
    "#or\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "mask = df.isnull() # Create a Boolean mask indicating which values are missing\n",
    "plt.figure(figsize=(10,10)) # Use the mask to create a heatmap\n",
    "plt.title(\"Heatmap of Missing Values\")\n",
    "sns.heatmap(mask, cbar=False, annot=True, cmap='PuBu')\n",
    "plt.show()\n",
    "#This will create a heatmap with white squares indicating missing values, and black squares indicating non-missing values.\n",
    "\n",
    "# Remove observations with missing values\n",
    "X[~np.isnan(X).any(axis=1)]\n",
    "\n",
    "#Imputing Missing Class Labels Using k-Nearest Neighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "X = # Create feature matrix with categorical feature \n",
    "X_with_nan = # Create feature matrix with missing values in the categorical feature\n",
    "clf = KNeighborsClassifier(3, weights='distance') # Train KNN learner\n",
    "trained_model = clf.fit(X[:,1:], X[:,0])\n",
    "imputed_values = trained_model.predict(X_with_nan[:,1:]) # Predict missing values' class\n",
    "X_with_imputed = np.hstack((imputed_values.reshape(-1,1), X_with_nan[:,1:])) # Join column of predicted class with their other features\n",
    "np.vstack((X_with_imputed, X)) # Join two feature matrices\n",
    "\n",
    "#Deleting Missing Values\n",
    "df = pd.DataFrame(X, columns=['feature_1', 'feature_2']) # Load data as a data frame\n",
    "df.dropna(axis=0, how=\"any\", thresh=2) # Remove observations with missing values (how = \"any\" or \"all\"; thresh = 1,2,3 - if you have atleast 1 or 2 non-na values)\n",
    "\n",
    "#Impute Missing Values With Means\n",
    "from sklearn.preprocessing import Imputer\n",
    "mean_imputer = Imputer(missing_values='NaN', strategy='mean', axis=0) # Create an imputer object that looks for 'Nan' values, then replaces them with the mean value of the feature by columns (axis=0)\n",
    "mean_imputer = mean_imputer.fit(df) # Train the imputor on the df dataset\n",
    "imputed_df = mean_imputer.transform(df.values) # Apply the imputer to the df dataset\n",
    "imputed_df # View the data\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1e5f0df4",
   "metadata": {},
   "source": [
    "##### Datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39d202f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "df_train[\"dob\"] = pd.to_datetime(df_train[\"dob\"], infer_datetime_format=True) # convert to datetime format \n",
    "df_train['time'] = df_train['unix_time'].apply(datetime.utcfromtimestamp) # Apply function utcfromtimestamp and drop column unix_time\n",
    "df_train['hour_of_day'] = df_train[\"time\"].dt.hour # Add column hour of day\n",
    "\n",
    "#convert date of birth to current age (It subtracts the years of birth from the current year to calculate the age of each person represented in the 'dob' column)\n",
    "import datetime as dt\n",
    "df_train['age']= dt.date.today().year-pd.to_datetime(df_train['dob']).dt.year\n",
    "\n",
    "dt.date.today().year #computes the current year while performing the datetime functionality\n",
    "pd.to_datetime #converts to a datetime object\n",
    ".dt.year #extracts the year from each datetime object\n",
    ".dt.month #extracts the month from each datetime object\n",
    ".dt.day #extracts the day from each datetime object\n",
    ".dt.hour #extracts the hour from each datetime object\n",
    "\n",
    "#If it shows that unicode nonesense of 1970, use this\n",
    "# df_no_of_deaths[\"Year\"].astype(\"datetime64[ns]\")\n",
    "df_deaths_cause[\"Year\"] = pd.to_datetime(df_deaths_cause[\"Year\"].astype(int).astype(str) + '-01-01')\n",
    "\n",
    "df_deaths_cause['Year'] = df_deaths_cause[\"Year\"].dt.year\n",
    "\n",
    "#create a datetime time range, and making it a dataframe index\n",
    "time_index = pd.date_range('2016-01-01 05:00', periods=503911,  freq='min')  \n",
    "time_index = pd.DatetimeIndex(time_index)\n",
    "df = df.set_index(time_index)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b7ff9e30",
   "metadata": {},
   "source": [
    "##### Outlier Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccff713a",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# A. Outlier Detection\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "# Create detector\n",
    "outlier_detector = EllipticEnvelope(contamination=.1)\n",
    "# Select only numeric columns\n",
    "numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])  #you may or may not include this step. \n",
    "# Fit detector\n",
    "outlier_detector.fit(numeric_cols)\n",
    "# Predict outliers\n",
    "outlier_predictions = outlier_detector.predict(numeric_cols) \n",
    "# Add outlier predictions to original dataframe\n",
    "smart_home[\"outlier\"] = outlier_predictions         #you may or may not add outlier predictions to the original dataframe\n",
    "\n",
    "#OR \n",
    "#Boxplot \n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "''''\n",
    "In a box plot, the central rectangle spans the first quartile to the third quartile (the 25th to 75th percentiles),\n",
    "and a line is also drawn along the median (50th percentile). The \"whiskers\" extend from the box to show the range of the data, \n",
    "and any points that lie outside the whiskers are plotted as individual points. This will create a box plot for each numerical \n",
    "column in the DataFrame. Outliers are typically represented as points outside the \"whiskers\" of the box plot.'''\n",
    "\n",
    "#B. Tukey's test for extreme values (you can replace 1.5 with 3)\n",
    "def find_outliers_tukey(x): # Define function using 1.5x interquartile range deviations from quartile 1/3 as floor/ceiling\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "    iqr = q3-q1 \n",
    "    floor = q1 - 1.5 * iqr\n",
    "    ceiling = q3 + 1.5 * iqr\n",
    "    outlier_indices = list(x.index[(x < floor)|(x > ceiling)])\n",
    "    outlier_values = list(x[outlier_indices])\n",
    "    return outlier_indices, outlier_values\n",
    "\n",
    "for x in range(1, 7): # Modify to select numeric columns # Print outliers for each numeric variable\n",
    "    tukey_indices, tukey_values = find_outliers_tukey(data.ix[:, x])\n",
    "    print(list(data[[x]]), np.sort(tukey_values))\n",
    "    \n",
    "#C. Kernel density estimation\n",
    "from statsmodels.nonparametric.kde import KDEUnivariate\n",
    "def find_outliers_kde(x):  # Define outlier function\n",
    "    x_scaled = scale(list(map(float, x)))\n",
    "    kde = KDEUnivariate(x_scaled)\n",
    "    kde.fit(bw = \"scott\", fft = True)\n",
    "    pred = kde.evaluate(x_scaled)    \n",
    "    n = sum(pred < 0.05)\n",
    "    outlier_ind = np.asarray(pred).argsort()[:n]\n",
    "    outlier_value = np.asarray(x)[outlier_ind]\n",
    "    return outlier_ind, outlier_value\n",
    "\n",
    "for x in range(1, 7): # Modify to select numeric columns   # Print outlier values\n",
    "    kde_indices, kde_values = find_outliers_kde(data.ix[:, x])\n",
    "    print(list(data[[x]]), np.sort(kde_values))\n",
    "\n",
    "-------------------------------------------------------------------------\n",
    "#outlier handling (by removal)\n",
    "\n",
    "# Z-score method:  \n",
    "    This method involves calculating the z-score for each data point and removing those that fall outside \n",
    "a certain threshold. Typically, data points with a z-score of more than 3 or less than -3 are considered outliers.\n",
    "\n",
    "def outlier_z-score(df: pd.DataFrame):\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Calculate z-score for each data point\n",
    "    z_scores = np.abs((df - df.mean()) / df.std())\n",
    "\n",
    "    # Remove data points with z-score greater than 3\n",
    "    df_cleaned = df[(z_scores < 3).all(axis=1)]\n",
    "    return df_cleaned \n",
    "\n",
    "# 2. Mahalanobis distance method: (multivariate outlier detection)\n",
    "\n",
    "def outlier_Mahalanobis_distance (df: pd.DataFrame):\n",
    "    from scipy.spatial.distance import mahalanobis\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Calculate the Mahalanobis distance for each data point\n",
    "    cov = df.cov()\n",
    "    inv_cov = pd.DataFrame(np.linalg.pinv(cov.values), index=cov.index, columns=cov.columns)\n",
    "    mean = df.mean()\n",
    "    dist = []\n",
    "    for i in range(len(df)):\n",
    "        x = df.iloc[i]\n",
    "        dist.append(mahalanobis(x, mean, inv_cov))\n",
    "\n",
    "    # Set threshold for Mahalanobis distance\n",
    "    threshold = 3\n",
    "\n",
    "    # Remove data points with Mahalanobis distance greater than threshold\n",
    "    df_cleaned = df[dist < threshold]\n",
    "    return df_cleaned\n",
    "\n",
    "# 3. Isolation Forest model for outlier detection and removal\n",
    "# The `contamination` parameter is again set to 0.01, indicating that we expect 1% of the data to be outliers.\n",
    "def outlier_isolation_forest (df: pd.DataFrame): \n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    import pandas as pd\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Fit IsolationForest model to data\n",
    "    model = IsolationForest(contamination=0.01)\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict outliers\n",
    "    outliers = model.predict(df) == -1\n",
    "\n",
    "    # Remove outliers from data\n",
    "    df_cleaned = df[~outliers]\n",
    "    return df_cleaned\n",
    "\n",
    "#4 EllipticEnvelope\n",
    "\n",
    "def outlier_EllipticEnvelope (df: pd.DataFrame): \n",
    "    from sklearn.covariance import EllipticEnvelope\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv('data.csv')\n",
    "\n",
    "    # Fit EllipticEnvelope model to data\n",
    "    model = EllipticEnvelope(contamination=0.01)\n",
    "    model.fit(df)\n",
    "\n",
    "    # Predict outliers\n",
    "    outliers = model.predict(df) == -1\n",
    "\n",
    "    # Remove outliers from data\n",
    "    df_cleaned = df[~outliers]\n",
    "    return df_cleaned \n",
    "\n",
    "\n",
    "#oulier handling (with removal)\n",
    "\n",
    "#IQR Method\n",
    "import numpy as np\n",
    "\n",
    "def handle_outliers_iqr(data):\n",
    "    \"\"\"\n",
    "    This function uses the interquartile range (IQR) method to handle outliers in the data. \n",
    "    Any data points that fall below Q1 - 1.5*IQR or above Q3 + 1.5*IQR are replaced with the nearest bound.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the outliers replaced by the nearest bound.\n",
    "    \"\"\"\n",
    "    Q1 = np.percentile(data, 25, interpolation='midpoint')\n",
    "    Q3 = np.percentile(data, 75, interpolation='midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    data = np.where(data < lower_bound, lower_bound, data)\n",
    "    data = np.where(data > upper_bound, upper_bound, data)\n",
    "    return data\n",
    "\n",
    "#Trimming Method\n",
    "\n",
    "def handle_outliers_trimming(data, percentage):\n",
    "    \"\"\"\n",
    "    This function uses the trimming method to handle outliers in the data. \n",
    "    The percentage of data points to be trimmed from the lower and upper ends is specified.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "    percentage (float): A value between 0 and 100 specifying the percentage of data points to be trimmed from the lower and upper ends.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the trimmed outliers.\n",
    "    \"\"\"\n",
    "    lower_percentage = percentage / 2\n",
    "    upper_percentage = 100 - lower_percentage\n",
    "    lower_bound = np.percentile(data, lower_percentage, interpolation='midpoint')\n",
    "    upper_bound = np.percentile(data, upper_percentage, interpolation='midpoint')\n",
    "    data = np.where(data < lower_bound, lower_bound, data)\n",
    "    data = np.where(data > upper_bound, upper_bound, data)\n",
    "    return data\n",
    "\n",
    "#Arbitrary Capping\n",
    "def handle_outliers_arbitrary(data, lower_cap, upper_cap):\n",
    "    \"\"\"\n",
    "    This function uses the arbitrary capping method to handle outliers in the data. \n",
    "    Any data points that fall below the lower_cap or above the upper_cap are replaced with the nearest bound.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "    lower_cap (float): The lower bound for capping outliers.\n",
    "    upper_cap (float): The upper bound for capping outliers.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the capped outliers.\n",
    "    \"\"\"\n",
    "    data = np.where(data < lower_cap, lower_cap, data)\n",
    "    data = np.where(data > upper_cap, upper_cap, data)\n",
    "    return data\n",
    "\n",
    "#Outlier Capping with Quantiles\n",
    "\n",
    "def handle_outliers_quantile(data, lower_quantile, upper_quantile):\n",
    "    \"\"\"\n",
    "    This function uses the outlier capping method with quantiles to handle outliers in the data. \n",
    "    Any data points that fall below the lower_quantile or above the upper_quantile are replaced with the nearest bound.\n",
    "\n",
    "    Parameters:\n",
    "    data (numpy.ndarray): A 1-dimensional numpy array containing the data.\n",
    "    lower_quantile (float): The lower quantile value for capping outliers.\n",
    "    upper_quantile (float): The upper quantile value for capping outliers.\n",
    "\n",
    "    Returns:\n",
    "    numpy.ndarray: A 1-dimensional numpy array with the capped outliers\n",
    "    \"\"\"\n",
    "    lower_bound = np.quantile(data, lower_quantile, interpolation='midpoint')\n",
    "    upper_bound = np.quantile(data, upper_quantile, interpolation='midpoint')\n",
    "    data = np.where(data < lower_bound, lower_bound, data)\n",
    "    data = np.where(data > upper_bound, upper_bound, data)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3ba6f41",
   "metadata": {},
   "source": [
    "##### Dummies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf76da22",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#A. For a non-binary categorical variable:\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dummy \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mget_dummies(dta1[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcountry\u001b[39m\u001b[38;5;124m'\u001b[39m], prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mct\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m      3\u001b[0m dummy\u001b[38;5;241m.\u001b[39mhead()\n\u001b[0;32m      4\u001b[0m df1\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#A. For a non-binary categorical variable:\n",
    "dummy = pd.get_dummies(dta1['country'], prefix='ct').astype(int)\n",
    "dummy.head()\n",
    "df1.astype(\"string\")\n",
    "\n",
    "#B. For all categorical variables of a dataframe:\n",
    "# Create a list of features to dummy\n",
    "dummy_vars = ['COL1', 'COL2', 'COL3'] \n",
    "\n",
    "# Create dummies for all categorical variables\n",
    "def dummy_data(df, dummy_vars): \n",
    "    for x in dummy_vars:\n",
    "        dummies = pd.get_dummies(df[x], prefix=x, dummy_na=False)\n",
    "        df = df.drop(x, 1)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df\n",
    "\n",
    "df = dummy_df(df, dummy_data)\n",
    "print(df.head())\n",
    "\n",
    "# Dummies \n",
    "dummy_ct = pd.get_dummies(df1['country'], prefix='ct').astype(int)\n",
    "dummy_pf = pd.get_dummies(df1['platform'], prefix='pf').astype(int)\n",
    "merged = pd.concat([df1, dummy_ct], axis = \"columns\") #after creating dummies, you merge the dummy with the original dataframe \n",
    "            # and drop df1['country'] and one dummy variable\n",
    "\n",
    "# Define columns to keep\n",
    "colstokeep = ['ABC', 'DEF']\n",
    "\n",
    "# Join dummies to columns to be kept from original dataframe using an identifier that is not in the list of columns to keep\n",
    "df1 = df1[colstokeep].join(dummy_ct.ix[:, 'ct_BR':]).join(dummy_pf.ix[:, 'pf_amazon':])\n",
    "df1.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "02c23e2d",
   "metadata": {},
   "source": [
    "##### Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26720070",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "#Encoding Ordinal Categorical Features\n",
    "df = pd.DataFrame({'Score': ['Low', 'Low', 'Medium', 'Medium', 'High']}) # Create features\n",
    "scale_mapper = {'Low':1, 'Medium':2, 'High':3} # Create mapper (Scale Map)\n",
    "df['Scale'] = df['Score'].replace(scale_mapper) # Map feature values to scale\n",
    "df # View data frame\n",
    "\n",
    "#Reshaping and catgorising data\n",
    "df[\"Gender\"] = df[\"Gender\"].map({\"M\":0, \"F\": 1}).astype(float)\n",
    "\n",
    "#One-Hot Encode Features With Multiple Labels\n",
    "from sklearn.preprocessing import MultiLabelBinarizer # Load libraries\n",
    "y = [('Texas', 'Florida')]  # Create NumPy array\n",
    "one_hot = MultiLabelBinarizer() # Create MultiLabelBinarizer object\n",
    "one_hot.fit_transform(y) # One-hot encode data\n",
    "one_hot.classes_ # View classes\n",
    "\n",
    "#One-Hot Encode Nominal Categorical Features\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "x = np.array([['Texas'], ['California'], ['Texas'], ['Delaware'], ['Texas']]) # Create NumPy array\n",
    "one_hot = OneHotEncoder() # Create LabelBinzarizer object (Method 1)\n",
    "one_hot.fit_transform(x) # One-hot encode data\n",
    "one_hot.categories_ # View classes\n",
    "pd.get_dummies(x[:,0]) # Dummy feature (Method 2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13e982c7",
   "metadata": {},
   "source": [
    "##### Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20176d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. Handling Imbalanced Classes With Downsampling\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() # Load iris data\n",
    "X = iris.data # Create feature matrix\n",
    "y = iris.target # Create target vector\n",
    "X = X[40:,:] # Remove first 40 observations\n",
    "y = y[40:] \n",
    "y = np.where((y == 0), 0, 1) # Create binary target vector indicating if class 0\n",
    "y # Look at the imbalanced target vector\n",
    "i_class0 = np.where(y == 0)[0] # Indicies of each class' observations\n",
    "i_class1 = np.where(y == 1)[0]\n",
    "n_class0 = len(i_class0) # Number of observations in each class\n",
    "n_class1 = len(i_class1)\n",
    "i_class1_downsampled = np.random.choice(i_class1, size=n_class0, replace=False) # For every observation of class 0, randomly sample from class 1 without replacement\n",
    "np.hstack((y[i_class0], y[i_class1_downsampled])) # Join together class 0's target vector with the downsampled class 1's target vector\n",
    "\n",
    "\n",
    "#B. Handling Imbalanced Classes With Upsampling\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris() # Load iris data\n",
    "X = iris.data # Create feature matrix\n",
    "y = iris.target # Create target vector\n",
    "X = X[40:,:] # Remove first 40 observations\n",
    "y = y[40:] \n",
    "y = np.where((y == 0), 0, 1) # Create binary target vector indicating if class 0\n",
    "y # Look at the imbalanced target vector\n",
    "i_class0 = np.where(y == 0)[0] # Indicies of each class' observations\n",
    "i_class1 = np.where(y == 1)[0]\n",
    "n_class0 = len(i_class0) # Number of observations in each class\n",
    "n_class1 = len(i_class1)\n",
    "i_class0_upsampled = np.random.choice(i_class0, size=n_class1, replace=True) # For every observation in class 1, randomly sample from class 0 with replacement\n",
    "np.concatenate((y[i_class0_upsampled], y[i_class1])) # Join together class 0's upsampled target vector with class 1's target vector\n",
    "\n",
    "\n",
    "\n",
    "def check_imbalance(dataset):\n",
    "    \"\"\"\n",
    "    This function takes a dataset as input and returns True if the dataset is imbalanced, False otherwise.\n",
    "    \"\"\"\n",
    "    # Get the counts of each class in the dataset\n",
    "    class_counts = dataset['class'].value_counts()\n",
    "    \n",
    "    # Calculate the percentage of each class in the dataset\n",
    "    class_percentages = class_counts / len(dataset) * 100\n",
    "    \n",
    "    # Plot the class percentages\n",
    "    plt.bar(class_counts.index, class_percentages)\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Class Distribution')\n",
    "    plt.show()\n",
    "    \n",
    "    # Check if the dataset is imbalanced\n",
    "    if len(class_counts) == 2:\n",
    "        # Binary classification\n",
    "        minority_class = class_counts.index[1]\n",
    "        minority_class_percentage = class_percentages[1]\n",
    "        if minority_class_percentage < 10 or minority_class_percentage > 90:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        # Multi-class classification\n",
    "        class_imbalance = False\n",
    "        for percentage in class_percentages:\n",
    "            if percentage < 10 or percentage > 90:\n",
    "                class_imbalance = True\n",
    "                break\n",
    "        return class_imbalance\n",
    "\n",
    "def check_imbalance(dataset, columns=None, threshold=10):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and one or more columns as input and returns True if any of the specified columns\n",
    "    are imbalanced, False otherwise. A column is considered imbalanced if the percentage of the minority class is less\n",
    "    than the specified threshold.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, use all columns except for the last one as the features\n",
    "    if columns is None:\n",
    "        features = dataset.iloc[:, :-1]\n",
    "        columns = features.columns\n",
    "    \n",
    "    # Check the imbalance of each specified column\n",
    "    for col in columns:\n",
    "        # Get the counts of each class in the column\n",
    "        class_counts = dataset[col].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each class in the column\n",
    "        class_percentages = class_counts / len(dataset) * 100\n",
    "\n",
    "        # Plot the class percentages\n",
    "        plt.bar(class_counts.index, class_percentages)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the column is imbalanced\n",
    "        minority_class = class_counts.index[-1]\n",
    "        minority_class_percentage = class_percentages.iloc[-1]\n",
    "        if minority_class_percentage < threshold:\n",
    "            print(f'{col} is imbalanced. Minority class: {minority_class}, Percentage: {minority_class_percentage:.2f}%')\n",
    "            return True\n",
    "\n",
    "    # If none of the specified columns are imbalanced, return False\n",
    "    print('No imbalance found.')\n",
    "    return False\n",
    "\n",
    "\n",
    "# Oversampling involves increasing the number of instances in the minority class by generating new samples.\n",
    "# This can be done by randomly duplicating existing instances, or by generating synthetic instances using techniques \n",
    "# such as SMOTE (Synthetic Minority Over-sampling Technique). \n",
    "# Undersampling, on the other hand, involves reducing the number of instances in the majority class. This can be done by \n",
    "# randomly removing instances from the majority class, or by selecting a subset of instances based on some criteria, \n",
    "# such as their distance to the minority class.  \n",
    "\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def handle_imbalanced_data(X, y, strategy='over-sampling'):\n",
    "    \"\"\"\n",
    "    Handle imbalanced data using imblearn library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array-like of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y: array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    strategy: str, default='over-sampling'\n",
    "        The strategy to use for handling imbalanced data. Possible values are\n",
    "        'over-sampling' and 'under-sampling'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled: array-like of shape (n_samples_new, n_features)\n",
    "        The resampled input data.\n",
    "    y_resampled: array-like of shape (n_samples_new,)\n",
    "        The resampled target values.\n",
    "    \"\"\"\n",
    "    if strategy == 'over-sampling':\n",
    "        # Initialize the RandomOverSampler object\n",
    "        ros = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    elif strategy == 'under-sampling':\n",
    "        # Initialize the RandomUnderSampler object\n",
    "        rus = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Possible values are 'over-sampling' and 'under-sampling'.\")\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "def apply_smote(X, y, sampling_strategy='auto', random_state=None):\n",
    "    \"\"\"\n",
    "    Applies Synthetic Minority Over-sampling Technique (SMOTE) to address data imbalance in a binary classification problem.\n",
    "    \n",
    "    SMOTE is a technique for oversampling the minority class in a dataset by generating synthetic examples from the\n",
    "    minority class to balance the class distribution. This function takes the input features (X) and corresponding\n",
    "    labels (y), and applies SMOTE to oversample the minority class.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        The input features.\n",
    "    y : array-like, shape (n_samples,)\n",
    "        The corresponding labels.\n",
    "    sampling_strategy : str or float or dict or callable, optional (default='auto')\n",
    "        The sampling strategy to be applied by SMOTE. This parameter is passed to the `sampling_strategy` argument of\n",
    "        the SMOTE class from the imbalanced-learn library. Possible values:\n",
    "            - 'auto': Resamples the minority class to have the same number of samples as the majority class.\n",
    "            - 'minority': Resamples the minority class to have the same number of samples as the majority class.\n",
    "            - 'not minority': Resamples all classes except the minority class to have the same number of samples as the\n",
    "              majority class.\n",
    "            - 'all': Resamples all classes to have the same number of samples as the majority class.\n",
    "            - float: Resamples the minority class to have the specified ratio of samples compared to the majority class.\n",
    "            - dict: Resamples each class to have the specified number of samples.\n",
    "            - callable: A custom function that defines the sampling strategy.\n",
    "    random_state : int or RandomState or None, optional (default=None)\n",
    "        Seed or random number generator for reproducibility of results.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_resampled : array-like, shape (n_samples_new, n_features)\n",
    "        The oversampled feature matrix.\n",
    "    y_resampled : array-like, shape (n_samples_new,)\n",
    "        The corresponding oversampled labels.\n",
    "    \"\"\"\n",
    "    # Create an instance of SMOTE\n",
    "    smote = SMOTE(sampling_strategy=sampling_strategy, random_state=random_state) #sampling_strategy={1:48050}\n",
    "    \n",
    "    # Apply SMOTE to oversample the minority class\n",
    "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
    "    \n",
    "    return X_resampled, y_resampled\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fee641d0",
   "metadata": {},
   "source": [
    "##### Tips for Plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006632cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Matplotlib Plots\n",
    "\n",
    "#First Chart (single or multiple charts)\n",
    "fig = plt.figure(figsize=(3,2), dpi=100) #figsize(width,height) in inches; dpi\n",
    "\n",
    "axes_1 = fig.add_axes([0.1,0.1,0.9, 0.8]) #left, bottom, width, and height. If you want more than one chart, keep adding new axis\n",
    "axes_1.plot(x, y**2, label = \"First Chart\", color = \"r\", linewidth = 3, linestyle = \":\",\n",
    "            markers = \"*\" , markersize = 10, markerfacecolor = \"yellow\", markeredgewidth = 3, \n",
    "            markeredgewidthecolor = \"green\" , alpha = 0.5) #alpha shows the level of transparency of the line\n",
    "            #you can also use the following: lw = linewidth; ls = linestyle\n",
    "            #for color and customization, you can combine it together: \"r--\"            \n",
    "axes_1.plot(x,y**3, label = \"Second Chart\", \"r\")\n",
    "axes_1.set_title(\"This is my title\")\n",
    "axes_1.set_xlabel(\"This is xlabel\")\n",
    "axes_1.set_ylabel(\"This is ylabel\")\n",
    "axes_1.legend(loc= 0) #adding legends #for location: 0 = best; you can also use tupe (left, bottom) to specify exact location. \n",
    "axes.set_xlim([0, 1]) #plot range for x-axis  #[lower bound, upper bound]\n",
    "axes.set_ylim([0, 2]) #plot range for y-axis  #[lower bound, upper bound]\n",
    "axes_1.tight_layout()\n",
    "fig.savefig(\"my_first_chart.png\") #save figure\n",
    "\n",
    "\n",
    "\n",
    "#Second Chart (single or multiple charts)\n",
    "fig,axes = plt.subplots(nrows = 2, ncols = 2, figsize=(3,2), dpi=100) #figsize(width,height) in inches; dpi. #You then specify the number of plots based on the rows and columns\n",
    "\n",
    "plt.tight_layout() #to fix the issue of overlapping\n",
    "axes[o,1].plot(x,y, label = \"First Chart\", \"r\") \n",
    "axes[0,1].set_title(\"This is the title for the first chart\")\n",
    "axes[0,1].set_xlabel(\"This is xlabel\")\n",
    "axes[0,1].set_ylabel(\"This is ylabel\")\n",
    "axes[0,1].legend(loc= 0) #adding legends #for location: 0 = best; you can also use tuple (left, bottom) to specify exact location.\n",
    "axes.set_xlim([0, 1]) #plot range for x-axis  #[lower bound, upper bound]\n",
    "axes.set_ylim([0, 2]) #plot range for y-axis  #[lower bound, upper bound]\n",
    "\n",
    "axes[1].plot(x,y, label = \"Second Chart\")\n",
    "axes[1].set_title(\"xxxxxxx\")\n",
    "axes[1].set_xlabel(\"This the label for the x-axis\")\n",
    "axes[1].set_ylabel(\"This is ylabel\")\n",
    "axes[1].legend(loc= 0) #adding legends #for location: 0 = best; you can also use tuple (left, bottom) to specify exact location.\n",
    "axes.set_xlim([0, 1]) #plot range for x-axis  #[lower bound, upper bound]\n",
    "axes.set_ylim([0, 2]) #plot range for y-axis  #[lower bound, upper bound]\n",
    "fig.savefig(\"my_first_chart.png\") #save figure\n",
    "\n",
    "#NB: other important things you can use in your plots\n",
    "sns.boxplot(x=\"Year\", y=\"Deaths 70+ years\", hue='Entity', data=focus_countries, palette='mako')\n",
    "plt.title('Deaths caused by Cardiovascular deaths for 70+ Years')\n",
    "ax = plt.gca() # Get the Axes object\n",
    "ax.set_xlim(0, 50) # Set the x-axis range\n",
    "ax.xaxis.set_ticks(range(0, 55, 5)) # Set the number of x-axis values to display or ax.xaxis.set_ticks([1990, 1995, 2000, 2005, 2010])\n",
    "ax.set_xticks([1990, 1995, 2000, 2005, 2010, 2015, 2020])\n",
    "ax.set_xticklabels([\"1990\", \"1995\", \"2000\", \"2005\", \"2010\", \"2015\", \"2020\"])\n",
    "plt.savefig(\"C:\\\\myDrive\\xx_image.png\")\n",
    "\n",
    "#OR\n",
    "fig,ax = plt.subplots(nrows = 2, ncols = 2, figsize=(3,2), dpi=100)\n",
    "pivot.plot(kind='line', ax = ax)\n",
    "plt.xlabel(\"Year\") # Add labels and title to the plot\n",
    "plt.ylabel(\"Deaths\")\n",
    "plt.title(\"Deaths by Cardiovascular diseases\")\n",
    "plt.show() # Show the plot\n",
    "fig.savefig(\"C:\\\\myDrive\\xx_image.png\")\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#seaborn inbuilt dataset\n",
    "tips = sns.load_dataset('tips')\n",
    "\n",
    "#Dist plot\n",
    "Dist plot allows us to show the distribution of a univariate set of observations \n",
    "KDE (kernel density estimates) A kernel density estimate (KDE) is a non-parametric method for estimating the \n",
    "    probability density function (PDF) of a random variable. It is the curve on the histogram. \n",
    "#bins are each rectangles in the histogram. \n",
    "\n",
    "sns.distplot(tips[\"total_bill\"]) #or\n",
    "sns.histplot(tips[\"total_bill\"], kde=True, stat=\"density\", linewidth=0, bins = 30)\n",
    "\n",
    "#joint plots\n",
    "allows you to basically match up two distplots for bivariate data\n",
    "#kind allows you to affect what is going on inside of the joint plot. default is scattered. \n",
    "    #hex allows you to make a hexagon distribution representation\n",
    "    #reg - regression \n",
    "    #kde - kernel density estimates\n",
    "sns.jointplot(x= \"total_bill\",y= \"tip\",data = tips, kind= \"hex\")\n",
    "\n",
    "#pairplot (very useful)\n",
    "this plots pairwise relationships across the entire dataframe (at least for the numerical columns). Very important\n",
    "    This is like a jointplot but for the entire dataframe\n",
    "#hue - use hue to access and explore the categorical columns. Simply input the categorical column name\n",
    "sns.pairplot(tips, hue=\"sex\", palette=\"coolwarm\")\n",
    "\n",
    "#rugplot\n",
    "sns.rugplot(tips)\n",
    "\n",
    "#kde plots\n",
    "sns.kde(tips[\"total_bill\"])\n",
    "\n",
    "#Line Plots\n",
    "import plotly.express as px\n",
    "fig = px.line(data_frame=energy_per_day.filter(items=['Dishwasher [kW]', 'Kitchen 14 [kW]', 'Kitchen 38 [kW]',\n",
    "                                                       'Microwave [kW]', 'Living room [kW]', 'Solar [kW]']),\n",
    "              line_dash_sequence=['solid']*16, width=900, height=600)\n",
    "fig.show()\n",
    "\n",
    "\n",
    "#CATEGORICAL PLOTS\n",
    "\n",
    "#bar plots\n",
    "allows you to aggregate the categorical data based off of a function (by default it is the mean)\n",
    "    to change the aggregate, use the estimator and specify the new aggregate. \n",
    "sns.barplot(x= \"sex\", y = \"total_bill\", data = tips, estimator=np.std)\n",
    "\n",
    "#countplot\n",
    "essentially the same as a bar plot, except the estimator is explicitly counting the number of ocurences. \n",
    "    Hence we only set the x value\n",
    "sns.countplot(x = \"sex\", data = tips)\n",
    "\n",
    "#box plots and violin plots\n",
    "useed to show the distribution of categorical data. \n",
    "    A box plot is aka box and wisker plot. it shows the dist of quantitative data in a way that \n",
    "        facilitates comparison btw variableables\n",
    "    x = categorical data; y = numerical data \n",
    "    you can also use \"hue\" for both plots \n",
    "sns.boxplot(x = \"day\", y = \"total_bill\", data=tips) \n",
    "sns.violinplot(x = \"day\", y = \"total_bill\", data=tips) \n",
    "sns.boxplot(x = \"day\", y = \"total_bill\", data=tips, hue=\"sex\", split = True) #if you decide to use \"hue\" \n",
    "\n",
    "#strip plots\n",
    "it will draw a scatter plot where one variable is categorical. \n",
    "sns.stripplot(x = \"day\", y = \"total_bill\", data=tips, jitter=True )\n",
    "sns.stripplot(x = \"day\", y = \"total_bill\", data=tips, jitter=True, hue=\"sex\", split = True) #if you decide to use \"hue\"\n",
    "\n",
    "#swarm plots (you really don't need this type of plot)\n",
    "often used as a combinantion of strip plots and violin plots\n",
    "the points of a swam plot are adjusted so that it doesn not overlap\n",
    "the drawback of swarm plots are that they do not scale well to very large numbers. \n",
    "sns.swarmplot(x = \"day\", y = \"total_bill\", data=tips)\n",
    "\n",
    "#factor plot (or catplot)\n",
    "this is the most general form of all the above plots\n",
    "you can change the kind to \"bar\", 'strip','swarm','box','violin', \"hex\", \"reg\" etc. for additional plots. \n",
    "sns.catplot(x = \"day\", y = \"total_bill\", data=tips, kind = \"bar\")\n",
    "\n",
    "\n",
    "\n",
    "#MATRIX PLOTS\n",
    "you must first convert a datset into a matrix form. To do that use a pivot table, or try to get correlation data\n",
    "    corelation data method: tips_corr = tips.corr()\n",
    "    pivot table method: flights.pivot_table(index = \"month\", columns = \"year\", values= \"passengers\")\n",
    "cmap: color map. a lot of options from \"coolwarm\", \"magma\" etc. \n",
    "sns.heatmap(tips_corr, annot=True, cmap=\"coolwarm\", linecolor=\"white\",linewidths=0.5)\n",
    "sns.clustermap(tips_corr, standard_scale=1) #you can also add clustermap to the heatmap\n",
    "#NB: A heat map will display things in the order you put them in, \n",
    "    #a cluster map will cluster things in an order so that imilar groups are close to each other\n",
    "\n",
    "\n",
    "\n",
    "#GRIDS ()\n",
    "1. grid_m = sns.PairGrid(tips) #similar to pair plots for plotting multiple plots\n",
    "grid_m.map(plt.scatter)\n",
    "\n",
    "#for additional functionalities\n",
    "grid_m.map_diag(sns.distplot) #map a distribution plot on the diagonal grids \n",
    "grid_m.map_upper(plt.scatter) #map a scatter plot on the upper part of the pair grids created\n",
    "grid_m.map_lower(sns.kdeplot) #map a kde plot on the lower part of the pair grids created\n",
    "\n",
    "2. grid_p = sns.FacetGrid(data=tips, col=\"time\", row=\"smoker\")\n",
    "grid_p.map(sns.distplot, \"total_bill\")\n",
    "#or use the other additional functionalities above. \n",
    "\n",
    "\n",
    "\n",
    "#CHLOROPLETH MAPS\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Data\n",
    "data = {'Country': ['Algeria', 'Egypt', 'South Africa', 'Kenya', 'Morocco', 'Tunisia'],\n",
    "        'Year': [2020, 2020, 2020, 2020, 2020, 2020],\n",
    "        'Deaths': [1000, 800, 1500, 700, 500, 600]}\n",
    "\n",
    "# Create a DataFrame from the data\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Plot a choropleth map of Africa showing the number of deaths by country and year\n",
    "fig = px.choropleth(df, locations=\"Country\", locationmode='country names', color=\"Deaths\",\n",
    "                    title='Deaths in Africa by Country and Year', color_continuous_scale=\"Viridis\",\n",
    "                    animation_frame=\"Year\", animation_group=\"Country\", hover_name=\"Country\",\n",
    "                    height=500, width=1000)\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31dd5f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a5f69661",
   "metadata": {},
   "source": [
    "##### Preprocessing Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edfca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.pipeline import Pipeline\n",
    "pd.get_dummies(df[\"city\"]) # Create dummy variables for every unique category in df.city\n",
    "integerized_data = preprocessing.LabelEncoder().fit_transform(df[\"city\"]) # Convert strings categorical names to integers\n",
    "integerized_data # View data\n",
    "preprocessing.OneHotEncoder().fit_transform(integerized_data.reshape(-1,1)).toarray() # Convert integer categorical representations to OneHot encodings\n",
    "\n",
    "#know your plots (https://medium.com/coders-mojo/project-9-day-23-of-30-days-of-data-analytics-with-projects-series-6747f695d570)\n",
    "\n",
    "#Univariate analysis using graphical representation (analysing single categorical features one at a time)\n",
    "# Bar Chart\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(x='Survived',data=df_train,palette='mako',order = df_train['Survived'].value_counts().index)\n",
    "plt.xlabel('Survived Passengers')\n",
    "plt.xticks(rotation = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "#plt.title('Survived Passengers')\n",
    "plt.show()\n",
    "\n",
    "# Pie Chart (for representing in Percentage)\n",
    "plt.figure(figsize=(18,12))\n",
    "p_r = df_train['Embarked'].value_counts().head(10)\n",
    "plt.pie(x=p_r,labels=p_r.index,colors=colors1,autopct='%.0f%%',explode=[0.07 for i in p_r.index],startangle=90,wedgeprops={'linewidth':1,'edgecolor':'black'},shadow=True)\n",
    "plt.title('Embarked Port percentage ')\n",
    "plt.legend(loc='upper right',title='Embarked Port (C = Cherbourg, Q = Queenstown, S = Southampton)')\n",
    "plt.show()\n",
    "\n",
    "#Bar Chart (with variables on the y-axis) - Numerical analysis\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.countplot(y='Age',data=df_train,palette='mako',order=df_train['Age'].value_counts().index[0:15],orient= 'h')\n",
    "plt.title('Passengers Age Count')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Passengers Age')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "#Histogram (for reprsenting frequncy distribution of a variable)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.distplot(x=df_train['Fare'],bins=40,color='darkcyan',kde=True,hist=True)\n",
    "plt.title('Fare Distribution')\n",
    "plt.xlabel('Fare')\n",
    "plt.ylabel('Frequency')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#Bivariate analysis (for analyzing two variables/features together, and studying their relationship)\n",
    "#kernel density estimation (KDE) to show the distribution of the data (Passenger Fare distribution by Survived  passengers) \n",
    "plt.figure(figsize=(25,12))\n",
    "sns.kdeplot(df_train[\"Fare\"], hue=df_train[\"Survived\"], fill=True, linewidth=1.5, palette='mako')\n",
    "plt.axvline(df_train['Fare'].mean(), c='black',ls='--')\n",
    "plt.title(\"Passenger Fare distribution by Survived  passengers \")\n",
    "plt.show()\n",
    "\n",
    "#Bar Plot (that shows the counts of observations in each categorical bin using bars.)  - Survived Passengers by Gender or Sex\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.countplot(x='Survived',data=df_train,palette='mako',order = df_train['Survived'].value_counts().index, hue = 'Sex')\n",
    "plt.xlabel('Survived Passengers')\n",
    "plt.xticks(rotation = 60)\n",
    "plt.ylabel('Count')\n",
    "plt.legend()\n",
    "plt.title('Survived Passengers by Gender or Sex')\n",
    "plt.show()\n",
    "\n",
    "#Categorical Scatter plot (to create a variety of plots that show the relationship between two categorical variables) - #Passengers Age by Sex and Passenger Class\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.catplot(x = \"Sex\", y = \"Age\", hue = \"Pclass\", data = df_train,palette='mako',orient='v')\n",
    "plt.show()\n",
    "\n",
    "OR\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.catplot(x = \"Sex\", y = \"Fare\", hue = \"Pclass\",data = df_train,palette='mako',orient='v',kind='box')\n",
    "plt.show()\n",
    "\n",
    "#hexbin plot (A hexbin plot is a 2D histogram that divides the data into hexagonal bins and shows the counts of observations in each bin with a color scale.)  - Attack vs Speed Pokemons\n",
    "plt.figure(figsize=(10,8))\n",
    "df.plot.hexbin(x='Attack', y='Speed', gridsize=20)\n",
    "plt.title(\"Attack vs Speed Pokemons \")\n",
    "plt.show()\n",
    "\n",
    "#Violin Plot (a graphical representation of the distribution of data that displays all the data points in a kernel density estimate.)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.title('Defense by Type1')\n",
    "sns.violinplot(x = \"Type 1\", y = \"Defense\",data = df,palette='mako')\n",
    "plt.ylim(0,300)\n",
    "plt.show()\n",
    "\n",
    "#Joint plot (a graphical representation of the relationship between two variables, along with the distribution of each variable.)\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.title('Attack vs Defense')\n",
    "sns.jointplot(x=\"Attack\",y=\"Defense\",data=df,kind=\"hex\",color='darkcyan')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Multivariate Analysis (more than two variables/features are analyzed together and the relationship/association between them is studied)\n",
    "#Pair plot (a graphical representation of the relationships between all pairs of variables in a dataset.)\n",
    "plt.figure(figsize=(25,20))\n",
    "sns.pairplot(df_train, diag_kind = \"kde\",palette='mako',hue=\"Survived\",markers='*')\n",
    "plt.show()\n",
    "\n",
    "#Scatterplot with regression lines\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.lmplot(x='Attack', y='Speed', hue='Legendary',  markers=['*', 'D'], fit_reg=False, data=df,palette='mako')\n",
    "plt.title('Attack Vs Speed by Legendary Status')\n",
    "plt.show()\n",
    "\n",
    "#box plot ( box plot is a graphical representation of the distribution of a dataset that shows the median, interquartile range, and range of the data.)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.boxplot(x=\"Generation\", y=\"Speed\", hue='Legendary', data=df, palette='mako')\n",
    "plt.title('Generation vs Speed  by Legendary Status')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "#Correlation Analysis (used to measure the strength of the linear association/relation between two variable)\n",
    "# heatmap correlation\n",
    "corrmat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(15, 10))\n",
    "sns.heatmap(corrmat, vmax=.8, square=True,annot=True,fmt=\".2f\",cmap='mako')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "#np.triue : It gives the upper triangle of the array\n",
    "plt.figure(dpi = 150,figsize= (15,10))\n",
    "mask = np.triu(np.ones_like(df_train.corr(),dtype = bool))\n",
    "sns.heatmap(df_train.corr(),mask = mask, fmt = \".2f\",annot=True,lw=1,cmap = 'mako')\n",
    "plt.yticks(rotation = 45)\n",
    "plt.xticks(rotation = 45)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4e8e3096",
   "metadata": {},
   "source": [
    "##### Categorical Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae6bb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bar plot\n",
    "sns.barplot(x,y,data,palette)\n",
    "\n",
    "#Scatter plot\n",
    "sns.swarmplot(x,y,data)\n",
    "\n",
    "#count plot\n",
    "sns.countplot(x,data,palette)\n",
    "\n",
    "#Point plot\n",
    "sns.pointplot(s,y,data,palette,hue)\n",
    "\n",
    "#Box plot\n",
    "sns.boxplot(x,y,data)\n",
    "\n",
    "#Violin plot\n",
    "sns.violinplot(x,y,data)\n",
    "\n",
    "#Regression Plot (It is used to showa linear regression model fit over the plotted data.)\n",
    "sns.regplot(x,y,data,axis)\n",
    "\n",
    "#Distribution plot (It is used to plot univariate distribution)\n",
    "sns.distplot(data,y,kde)\n",
    "\n",
    "#Heatmap\n",
    "sns.heatmap(uniform_data,vmin,vmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee81ff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70657c5d",
   "metadata": {},
   "source": [
    "##### Preprocessing Iris Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd7c153d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.cross_validation'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datasets\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcross_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[0;32m      5\u001b[0m iris \u001b[38;5;241m=\u001b[39m datasets\u001b[38;5;241m.\u001b[39mload_iris() \u001b[38;5;66;03m# Load the iris data\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.cross_validation'"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "iris = datasets.load_iris() # Load the iris data\n",
    "X = iris.data # Create a variable for the feature data\n",
    "y = iris.target # Create a variable for the target data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42) # Random split the data into four new datasets, training features, training outcome, test features, \n",
    "# and test outcome. Set the size of the test data to be 30% of the full dataset.\n",
    "sc = StandardScaler() # Load the standard scaler\n",
    "sc.fit(X_train) # Compute the mean and standard deviation based on the training data\n",
    "X_train_std = sc.transform(X_train) # Scale the training data to be of mean 0 and of unit variance\n",
    "X_test_std = sc.transform(X_test) # Scale the test data to be of mean 0 and of unit variance\n",
    "X_test[0:5] # Feature Test Data, non-standardized\n",
    "X_test_std[0:5] # Feature Test Data, standardized.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ebe0f0bf",
   "metadata": {},
   "source": [
    "##### Rescale and Standardize a Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e70097",
   "metadata": {},
   "outputs": [],
   "source": [
    "#A. Rescale A Feature\n",
    "from sklearn import preprocessing\n",
    "x = np.array([[-500.5], [-100.1], [0], [100.1], [900.9]]) # Create feature\n",
    "minmax_scale = preprocessing.MinMaxScaler(feature_range=(0, 1)) # Create scaler\n",
    "x_scale = minmax_scale.fit_transform(x) # Scale feature\n",
    "x_scale # Show feature\n",
    "\n",
    "#B. Standardize A Feature\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler() # Create scaler\n",
    "standardized = scaler.fit_transform(x) # Transform the feature\n",
    "standardized # Show feature\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "61d5e64e",
   "metadata": {},
   "source": [
    "#### Preprocessing Structured Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8da0946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "811f8da0",
   "metadata": {},
   "source": [
    "#### Preprocessing Structured Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d939cd29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0896fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcac5f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51fc00cb",
   "metadata": {},
   "source": [
    "### Machine learning "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2309b949",
   "metadata": {},
   "source": [
    "#### Preparation of data for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39385ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Draw samples and split dataset\n",
    "data2 = data1.sample(1000) #Draw a random sample from a dataset:\n",
    "X = #independent variable\n",
    "y = #dependent variable\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=0) #Split test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07117c9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f6448f1e",
   "metadata": {},
   "source": [
    "<img src=\"aa.jgp/\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5f9a2f8a",
   "metadata": {},
   "source": [
    "#### Performance Metrics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f636d788",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "source": [
    "''\n",
    "Performance metrics for Classification Models can be divided into \n",
    "\n",
    "a. Class Labels\n",
    "\n",
    "b. Probabilities\n",
    "\n",
    "Under Class labels \n",
    "    Confusion Matrix\n",
    "    F1 Score\n",
    "    Recall\n",
    "    Precision\n",
    "    Type I error\n",
    "    Type II error\n",
    "    Accuracy\n",
    "\n",
    "Under Probabilities \n",
    "    ROC Curve\n",
    "    AUC score\n",
    "\n",
    "\n",
    "for more information on this, check - https://medium.com/coders-mojo/day-27-of-30-days-of-data-analytics-with-projects-series-performance-metrics-9f0abdd7b9dd\n",
    "\n",
    "Low bias ML algorithms : KNN, SVM and Decision Trees\n",
    "High bias ML algorithms : Logistic Regression and Linear Regression\n",
    "\n",
    "Low variance ML algorithms : Logistic Regression, Linear Regression and Linear Discriminant Analysis\n",
    "High variance ML algorithms : Decision Trees, Support Vector Machines and KNN\n",
    "''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "250.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "144.844px",
    "left": "996px",
    "right": "20px",
    "top": "118px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
