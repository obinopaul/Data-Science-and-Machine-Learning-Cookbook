{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H2O AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.automl import H2OAutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h2o.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = h2o.import_file('data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test,df_valid = df.split_frame(ratios=[.7, .15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = \"Churn\"\n",
    "x = df.columns\n",
    "x.remove(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml = H2OAutoML(max_models = 10, seed = 10, exclude_algos = [\"StackedEnsemble\", \"DeepLearning\"], verbosity=\"info\", nfolds=0) #you dont need to exclude those models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.train(x = x, y = y, training_frame = df_train, validation_frame=df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lb = aml.leaderboard    # List of Models and their performances\n",
    "lb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred=aml.leader.predict(df_test)   # predict with the best model\n",
    "df_pred.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model Evaluation (Performance Analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.model_performance(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the performance of a certain model\n",
    "model_ids = list(aml.leaderboard['model_id'].as_data_frame().iloc[:,0])\n",
    "out = h2o.get_model([mid for mid in model_ids if \"XGBoost\" in mid][0])\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dir(out)\n",
    "out.confusion_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aml.leader.download_mojo(path = \"./\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyCaret AutoML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pycaret\n",
    "# from pycaret.classification import * \n",
    "# from pycaret.regression import setup, compare_models, create_model, tune_model, plot_model, evaluate_model, save_model\n",
    "from pycaret.classification import setup, compare_models, create_model, tune_model, plot_model, evaluate_model, save_model\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True) \n",
    "X['target'] = y \n",
    "# Initialize classification setup \n",
    "# clf1 = setup(data=X, target='target', train_size = 0.8, \n",
    "#              preprocess = True, polynomial_features = True, \n",
    "#              polynomial_degree = 2, fix_imbalance = True,\n",
    "#              fix_imbalance_method = 'SMOTE', feature_selection = True,\n",
    "#              feature_selection_method = ' ', feature_selection_estimator = ,\n",
    "#              n_features_to_select = 0.2) \n",
    "\n",
    "\n",
    "clf1 = setup(data=X, target='target', train_size = 0.8, session_id = 123)\n",
    "# all_models = models()   #use this to visualize a table of models available in the model library.\n",
    "\n",
    "# Compare models \n",
    "compare_results = compare_models(n_select=5)    #the best 5 models will be highlighted\n",
    "\n",
    "compare_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate each model\n",
    "for model in compare_results:\n",
    "    evaluate_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a model\n",
    "model = create_model('knn')     #change knn to any of the top 5 models from above\n",
    "            # from pycaret.regression import models     #change to classification when needed, then do: exp = setup(X, y)\n",
    "            # regression_models = models() # Get all regression model estimators  \n",
    "            # print(regression_models) # Display the list of model names\n",
    "\n",
    "\n",
    "# # Tune the model\n",
    "tuned_model = tune_model(model)\n",
    "\n",
    "# # Evaluate the model\n",
    "evaluate_model(tuned_model)\n",
    "\n",
    "# # Fit the model\n",
    "final_model = tune_model(tuned_model)\n",
    "\n",
    "# Save the final model in the \"ML\" folder\n",
    "model_path = 'models/pycaret_ExtraTreesRegressor_r2'\n",
    "save_model(final_model, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EvalML (AutoML using EvalML doesn't just give you the best model, it also gives the best pipeline)\n",
    "import evalml\n",
    "\n",
    "# Timeseries (https://evalml.alteryx.com/en/stable/user_guide/timeseries.html?highlight=time%20series#AutoMLSearch-for-time-series-problems)\n",
    "\n",
    "\n",
    "\n",
    "X, y = evalml.demos.load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, regression=True) #change the problem type\n",
    "                    #to see all problem types, use (evalml.problem_types.ProblemTypes.all_problem_types)\n",
    "                    #to see all objectives, use (evalml.objectives.get_all_objective_names())\n",
    "\n",
    "#Use EvalML's AutoML to perform the following steps:\n",
    "\n",
    "#Step 1: search multiple ML methods and parameters\n",
    "from evalml.automl import AutoMLSearch\n",
    "automl = AutoMLSearch(problem_type='regression', objective='r2', \n",
    "                    additional_objectives=[ 'mse', 'mae', 'root mean squared error'])  #also change the problem type\n",
    "automl.search(X_train, y_train)  \n",
    "\n",
    "#Step 2: Rank each of the multiple ML algorithms to see their parameters and then choose the best\n",
    "automl.rankings\n",
    "# automl.describe_pipeline(automl.rankings.iloc[0][\"id\"]) #use this to describe each of the model/pipeline. change 0 to other values\n",
    "\n",
    "#Step 3: Choose the best pipeline\n",
    "best_pipeline=automl.best_pipeline\n",
    "best_pipeline\n",
    "\n",
    "#Step 4: You can evaluate other objective functions, or optimize the model for a specific objective\n",
    "best_pipeline.score(X_test, y_test, objectives=[\"auc\",\"f1\",\"Precision\",\"Recall\"]) #evaluate other objective functions\n",
    "automl_auc = AutoMLSearch(X_train=X_train, y_train=y_train,             #optimize step 1 for a specific objective\n",
    "                        problem_type='binary',\n",
    "                        objective='auc',\n",
    "                        additional_objectives=['f1', 'precision'],\n",
    "                        max_batches=1,\n",
    "                        optimize_thresholds=True)\n",
    "\n",
    "automl_auc.search()\n",
    "\n",
    "#Step 5: Make predictions, save and load the model\n",
    "best_pipeline.predict_proba(X_test).to_dataframe()\n",
    "best_pipeline.save(\"models/model.pkl\")\n",
    "check_model=automl.load('models/model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
