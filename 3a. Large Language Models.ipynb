{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models Explained Visually\n",
    "\n",
    "Discover comprehensive visual explanations of language models through the following resources:\n",
    "\n",
    "- [BBY Croft's Large Language Models Guide](https://bbycroft.net/llm)\n",
    "- [Polo Club's Transformer Explainer](https://poloclub.github.io/transformer-explainer/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corpus\n",
    "    # A collection of text documents used to train a language model. The corpus can be a collection of books, articles, or any text data.\n",
    "    \n",
    "    \n",
    "# Vocabulary\n",
    "    # The set of unique tokens (words, sub-words, or characters) that a model can understand. The vocabulary is typically derived from\n",
    "    # the training corpus and includes common words and special tokens like [PAD], [UNK], [CLS], and [SEP].\n",
    "    \n",
    "    \n",
    "# Attention Mechanism\n",
    "    # A mechanism that allows the model to focus on important words in a sequence, enabling the model to handle long-range dependencies \n",
    "    # and capture context.\n",
    "\n",
    "\n",
    "# Tokens\n",
    "    # The smallest unit of input or output the model processes. Tokens can represent words, sub-words, or even characters, depending \n",
    "    # on the tokenization strategy.\n",
    "    \n",
    "    \n",
    "# Tokenization\n",
    "    # The process of converting raw text into tokens (usually words, sub-words, or characters) that the model can process. \n",
    "    # Tokenizers break down text based on a model's vocabulary (e.g., Byte Pair Encoding or WordPiece).\n",
    "    # Byte Pair Encoding (BPE):\n",
    "        # A tokenization algorithm that iteratively merges the most frequent pairs of characters in a corpus to create a vocabulary of \n",
    "        # variable-length tokens. BPE is widely used in NLP tasks, including machine translation and text generation.\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "    # Dense vector representations of tokens (words/sub-words) that capture semantic meaning. Used in LLMs to map input tokens into \n",
    "    # a continuous vector space where similar meanings are close together. Instead of treating each word as a unique, isolated token, \n",
    "    # embeddings allow words with similar meanings to be represented by vectors (arrays of numbers) that are close together \n",
    "    # in a multi-dimensional space.\n",
    "    \n",
    "    # example: Word2Vec, GloVe, FastText, BERT embeddings.\n",
    "        # Imagine you have the words: \n",
    "        # \"dog\", \"cat\", \"apple\", \"banana\" \n",
    "        # \"dog\" -> [0.1, 0.2, 0.3, 0.4], \n",
    "        # \"cat\" -> [0.2, 0.3, 0.4, 0.5], \n",
    "        # \"apple\" -> [0.3, 0.4, 0.5, 0.6], \n",
    "        # \"banana\" -> [0.4, 0.5, 0.6, 0.7]\n",
    "        # The embeddings for \"dog\" and \"cat\" are closer together than \"dog\" and \"apple\" because \"dog\" and \"cat\" are semantically\n",
    "        # similar (both animals) compared to \"dog\" and \"apple\" (different categories).\n",
    "        \n",
    "        \n",
    "# Part-of-Speech (POS) Tagging:\n",
    "    # Assigning each word in a sentence a grammatical category (e.g., noun, verb, adjective). Helps the model understand the \n",
    "    # structure of sentences and the role of each word, which is useful for tasks like parsing, translation, and question answering\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER):\n",
    "    # Identifying and categorizing entities (names, dates, locations, organizations, etc.) in text.\n",
    "\n",
    "\n",
    "# Stemming or Lemmatization:\n",
    "    # Reducing words to their base or root form. Stemming is a rule-based process that removes prefixes or suffixes, while lemmatization \n",
    "    # uses a vocabulary and morphological analysis to return the base form of a word. \n",
    "    # examples are: \n",
    "    \n",
    "\n",
    "# Sampling Techniques\n",
    "    # Methods used to generate outputs from a model, such as greedy search (selecting the most likely next token), beam search (exploring \n",
    "    # multiple token sequences), and temperature sampling (introducing randomness to outputs).\n",
    "\n",
    "\n",
    "# Beam Search\n",
    "    # A search strategy used during text generation to explore multiple possible token sequences and select the most likely ones. \n",
    "    # It reduces the likelihood of poor-quality outputs compared to greedy search.\n",
    "    \n",
    "    \n",
    "# Greedy Search\n",
    "    # A simpler search method where the model always selects the most probable next token. It is fast but may lead to less coherent \n",
    "    # or repetitive outputs.\n",
    "\n",
    "\n",
    "# Autoregressive Models\n",
    "    # LLMs like GPT, which generate text one token at a time, predicting the next token based on previously generated tokens. \n",
    "    # This type of model is suitable for tasks like text generation.\n",
    "    \n",
    "    \n",
    "# Masked Language Models (MLM)\n",
    "    # Models like BERT that learn by predicting masked-out tokens in a sentence, using the surrounding context. These models are \n",
    "    # bidirectional, meaning they consider context from both directions.\n",
    "    \n",
    "    \n",
    "# Zero-Shot Learning\n",
    "    # The model’s ability to perform tasks without explicit examples in the training data. For example, a zero-shot LLM can classify \n",
    "    # text without having seen labeled examples for that specific task.\n",
    "    \n",
    "    \n",
    "# Few-Shot Learning\n",
    "    # The model can generalize from only a few examples during inference. For instance, by providing the model a few sample questions \n",
    "    # and answers, it can handle similar tasks effectively.\n",
    "    \n",
    "    \n",
    "# Fine-Tuning vs. Transfer Learning\n",
    "    # Fine-Tuning: The process of adapting a pretrained LLM to a specific task (e.g., classification, question answering) by training \n",
    "        # it further on task-specific labeled data.\n",
    "    # Transfer Learning: Leveraging knowledge from a pretrained model and applying it to a new but related task, without needing to \n",
    "        # retrain from scratch.\n",
    "\n",
    "\n",
    "# Temperature Sampling\n",
    "    # A technique used during text generation to control the randomness of the output. Higher temperatures (e.g., 1.0) result in \n",
    "    # more diverse outputs, while lower temperatures (e.g., 0.2) make the model more deterministic.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PEFT + LoRA (Parameter Efficient Fine-tuning + Low-Rank Adaptation)\n",
    "    # Description: Fine-tunes only a small adapter layer added on top of a pre-trained model, conserving memory and improving efficiency.\n",
    "    # Use Case: Helps in training large models by keeping the original model frozen and updating only small parts.\n",
    "\n",
    "\n",
    "# 2. Quantization-Aware Training (QAT)\n",
    "    # Description: Reduces model size by converting high-precision weights (e.g., FP32) to lower precision formats (e.g., FP16 or INT8).\n",
    "    # Benefits: Saves memory and reduces training time but may affect model accuracy.\n",
    "    # Challenges: Requires careful monitoring to ensure model quality isn’t degraded.\n",
    "\n",
    "\n",
    "# 3. Gradient Checkpointing\n",
    "    # Description: Saves memory by storing only certain intermediate values during backpropagation.\n",
    "    # Use Case: Reduces memory usage but slows down training.\n",
    "\n",
    "\n",
    "# 4. Distributed Training\n",
    "    # Description: Splits the model and data across multiple devices or nodes for faster training.\n",
    "    # Key Techniques:\n",
    "        # FSDP (Fully Sharded Data Parallel): Shards model weights and optimizer states across devices.\n",
    "        # Deepspeed Zero Redundancy Optimizer (ZeRO): Distributes model parameters to save memory and optimize training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Post-Training Quantization (PTQ)\n",
    "    # Description: Quantizes a model’s weights and activations after training to reduce memory usage.\n",
    "    # Use Case: Reduces memory footprint for serving models at lower precision (e.g., FP32 → INT8).\n",
    "\n",
    "\n",
    "# 2. Distributed Inference\n",
    "    # Description: Partitioning model weights across multiple devices to handle large models.\n",
    "    # Techniques:\n",
    "    # Model Partitioning: Divides a large model across multiple GPUs or nodes for more efficient computation.\n",
    "    # In-flight Batching: Enables the processing of new requests while others are still being computed, improving GPU utilization.\n",
    "\n",
    "\n",
    "# 3. Dynamic Batching & Continuous Batching\n",
    "    # Description: Dynamically adjusts batch sizes during inference to maximize GPU utilization, reducing latency.\n",
    "    # Benefits: Ensures high throughput and efficiency, especially for models with varying input lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TensorRT-LLM\n",
    "    # Description: Optimizes models with kernel fusion and memory techniques like KV caching, Paged Attention, and FlashAttention.\n",
    "    # Benefits: Improves performance but requires conversion into TensorRT format for use.\n",
    "\n",
    "\n",
    "# 2. vLLM\n",
    "    # Description: An inference engine that uses Paged Attention to reduce resource wastage, optimizing memory usage and improving throughput.\n",
    "    # Benefits: High efficiency in processing tokens compared to traditional methods.\n",
    "\n",
    "\n",
    "# 3. DeepSpeed-Fastgen\n",
    "    # Description: Combines DeepSpeed's training and inference capabilities for fast, efficient model serving.\n",
    "    # Key Features: Supports Dynamic Splitfuse batching, improving latency and throughput for large models.\n",
    "    \n",
    "\n",
    "# Key Considerations\n",
    "    # Memory Constraints: LLM training and inference are memory-intensive processes. Techniques like PEFT, QAT, and gradient \n",
    "        # checkpointing can help mitigate memory limitations.\n",
    "    # Model Size: Models with billions of parameters may require distributed training or inference strategies to handle the memory demands.\n",
    "    # Efficiency: Methods like mixed precision, distributed training, and dynamic batching are key to improving efficiency in \n",
    "        # training and inference.\n",
    "    # Latency: Techniques like dynamic batching and continuous batching can help reduce inference latency, especially for real-time \n",
    "        # applications.\n",
    "    # Throughput: Distributed inference and model partitioning can improve throughput by leveraging multiple devices for \n",
    "        # parallel processing.\n",
    "    # Resource Optimization: Techniques like TensorRT-LLM and vLLM optimize memory usage and improve performance for large models.\n",
    "    # Scalability: Distributed training and inference methods enable scaling LLMs to handle larger models and datasets efficiently.\n",
    "    # Model Serving: Techniques like DeepSpeed-Fastgen provide end-to-end solutions for training and serving large language \n",
    "        # models effectively.\n",
    "    # Performance Trade-offs: Quantization and distributed strategies may impact model accuracy, so careful monitoring and tuning \n",
    "        # are essential to maintain performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in Machine Learning\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost (or loss) function in machine learning. It works by iteratively updating the parameters (weights) of the model in the direction that reduces the error (cost) the most, i.e., in the direction of the **negative gradient** of the loss function.\n",
    "\n",
    "Let’s assume we have a loss function $J(\\theta)$, where $\\theta$ represents the parameters (weights) of our model. The goal is to minimize this loss function, i.e., find the set of parameters that gives us the lowest possible value for $J(\\theta)$.\n",
    "\n",
    "Gradient Descent works by updating the parameters as follows:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ is the **parameter** (or weight) of the model.\n",
    "- $ \\alpha $ is the **learning rate** (a small positive value, typically between 0 and 1).\n",
    "- $ \\nabla_\\theta J(\\theta) $ is the **gradient** (the partial derivative) of the loss function with respect to the parameter $ \\theta $.\n",
    "\n",
    "\n",
    "## Loss Function Example: Binary Cross-Entropy (BCE)\n",
    "\n",
    "For binary classification problems, one commonly used loss function is the **Binary Cross-Entropy (BCE)** loss. The BCE loss function is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{y}_i $ is the predicted probability for the $ i^{th} $ sample.\n",
    "- $ y_i $ is the true label (0 or 1) for the $ i^{th} $ sample.\n",
    "- $ N $ is the number of samples in the dataset.\n",
    "\n",
    "The predicted value $ \\hat{y}_i $ is typically computed using a **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\sigma(w^T x_i + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ w $ are the weights of the model.\n",
    "- $ x_i $ is the input features of the $ i^{th} $ sample.\n",
    "- $ b $ is the bias term.\n",
    "- $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid function.\n",
    "\n",
    "The goal of training is to minimize this loss function using gradient descent, iteratively adjusting the weights \\( w \\) and bias \\( b \\) to reduce the difference between predicted and true labels.\n",
    "\n",
    "\n",
    "## Explanation:\n",
    "\n",
    "- Gradient ($ \\nabla_\\theta J(\\theta) $): This tells us the direction in which the function $ J(\\theta) $ increases the most. In other words, it shows us how steep the slope is at any point on the function. We want to move **in the opposite direction** (down the slope) to minimize the loss.\n",
    "  \n",
    "- **Learning rate ($ \\alpha $)**: This determines the size of the steps we take in the direction of the gradient. A small learning rate means small steps, and a large learning rate means larger steps. Too small of a learning rate will make the process slow, while too large of a learning rate could cause overshooting and prevent convergence.\n",
    "\n",
    "- **loss.backward()** - Computes the gradients of the loss with respect to the model's parameters using backpropagation.\n",
    "- **optimizer.step()** - Updates the model's parameters based on the computed gradients, performing the gradient descent step.\n",
    "\n",
    "\n",
    "\n",
    "## Calculating Gradients with Respect to Parameters\n",
    "\n",
    "To update the parameters using gradient descent, we need to calculate the gradient of the loss function with respect to each parameter (weights and biases). Here's how it's done for the Binary Cross-Entropy (BCE) loss in a logistic regression model.\n",
    "\n",
    "### 1. Binary Cross-Entropy Loss:\n",
    "\n",
    "For binary classification, the BCE loss function is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, \\hat{y}) = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Where $ \\hat{y} = \\sigma(w \\cdot x + b) $, and $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid activation function.\n",
    "\n",
    "### 2. Gradient of Loss with respect to Parameters:\n",
    "\n",
    "- The gradient of the loss with respect to \\( w \\) (weight):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = (\\hat{y} - y) \\cdot x\n",
    "$$\n",
    "\n",
    "- The gradient of the loss with respect to \\( b \\) (bias):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{y} $ is the predicted probability ($ \\hat{y} = \\sigma(w \\cdot x + b) $).\n",
    "- $ y $ is the true label (0 or 1).\n",
    "- $ x $ is the input feature vector.\n",
    "\n",
    "These gradients are used to update the parameters in the direction that minimizes the loss:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where $ \\alpha $ is the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of Gradient Descent in Machine Learning\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a variant of the basic gradient descent algorithm where the parameters are updated using only one data point (randomly selected) at a time, rather than the entire dataset. This leads to faster convergence but with noisier updates.\n",
    "\n",
    "The update rule for SGD is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ is the parameter (or weight).\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)}) $ is the gradient of the cost function, computed with respect to the $i$-th training example $ (x^{(i)}, y^{(i)}) $.\n",
    "\n",
    "Since SGD uses one data point at a time, it is computationally more efficient but introduces more variance in the updates. This often causes the algorithm to fluctuate around the minimum rather than converging smoothly.\n",
    "\n",
    "## Mini-Batch Gradient Descent (MBGD)\n",
    "\n",
    "Mini-Batch Gradient Descent is a compromise between the standard (Batch) Gradient Descent and Stochastic Gradient Descent. In mini-batch GD, instead of using the full dataset or a single data point, a small random subset of the data (mini-batch) is used to compute the gradient.\n",
    "\n",
    "The update rule for Mini-Batch GD is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of samples in the mini-batch.\n",
    "\n",
    "Mini-batch gradient descent provides a balance between the computational efficiency of batch gradient descent and the faster convergence of stochastic gradient descent.\n",
    "\n",
    "## Adagrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "Adagrad is an adaptive learning rate method. It adjusts the learning rate for each parameter individually based on the past gradient updates. Parameters that have larger gradients (more significant updates) will have smaller learning rates, while parameters with smaller gradients will have larger learning rates.\n",
    "\n",
    "The update rule for Adagrad is:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ G_t $ is the sum of the squared gradients up to time step $ t $:\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{i=1}^{t} \\nabla_\\theta J(\\theta_i)^2\n",
    "$$\n",
    "\n",
    "- $ \\epsilon $ is a small constant added to prevent division by zero (typically $ 10^{-8} $).\n",
    "- $ \\alpha $ is the learning rate.\n",
    "\n",
    "The adaptive nature of Adagrad makes it effective for sparse data (where most features are zero) or data with varying levels of gradients.\n",
    "\n",
    "## RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "RMSprop is another adaptive learning rate method, designed to solve some issues with Adagrad, especially the fact that Adagrad's learning rate can shrink too much over time. RMSprop uses a moving average of squared gradients to scale the learning rate, which helps keep the updates stable.\n",
    "\n",
    "The update rule for RMSprop is:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ E[g^2]_t $ is the moving average of squared gradients at time step $ t $:\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\beta \\cdot E[g^2]_{t-1} + (1-\\beta) \\cdot g_t^2\n",
    "$$\n",
    "\n",
    "where $ \\beta $ is a smoothing factor (often close to $ 0.9 $).\n",
    "- $ g_t $ is the gradient at time step $ t $.\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\epsilon $ is a small constant added to prevent division by zero.\n",
    "\n",
    "RMSprop helps improve convergence by adapting the learning rate to the magnitude of recent gradients, which is useful for non-stationary objectives.\n",
    "\n",
    "## Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam combines ideas from both Adagrad and RMSprop. It computes adaptive learning rates for each parameter, but also takes into account the momentum of past gradients (i.e., the exponentially decaying average of past gradients) to improve optimization.\n",
    "\n",
    "The update rule for Adam is:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} \\cdot m_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m_t $ is the first moment estimate (mean of gradients), typically:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "with $ \\beta_1 $ being the decay rate (typically $ 0.9 $).\n",
    "\n",
    "- $ v_t $ is the second moment estimate (variance of gradients), typically:\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "with $ \\beta_2 $ being another decay rate (typically $ 0.999 $).\n",
    "\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\epsilon $ is a small constant added to prevent division by zero.\n",
    "\n",
    "Adam is widely used because it combines the benefits of both momentum and adaptive learning rates, making it well-suited for a variety of machine learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"transformer.png\" alt=\"Feed-Forward Network\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "# Transformer Model: Step-by-Step Workflow\n",
    "\n",
    "Transformers are a foundational architecture in modern deep learning, particularly in natural language processing (NLP). Below is a comprehensive step-by-step guide outlining the complete workflow of a transformer model, from input tensors to the final output.\n",
    "\n",
    "## 1. Input Preparation\n",
    "\n",
    "### 1.1. Raw Text Input\n",
    "- **Description:** The process begins with raw text data, such as sentences or paragraphs.\n",
    "- **Example:** `\"The cat sat on the mat.\"`\n",
    "\n",
    "### 1.2. Tokenization\n",
    "- **Description:** Converts raw text into discrete tokens (words, subwords, or characters).\n",
    "- **Substeps:**\n",
    "  - **a. Splitting:** Break text into tokens based on spaces or specific rules.\n",
    "  - **b. Subword Tokenization (Optional):** Further splits rare words into subword units using algorithms like Byte Pair Encoding (BPE) or WordPiece.\n",
    "- **Example:** `\"The cat sat on the mat.\"` → `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]`\n",
    "\n",
    "### 1.3. Numerical Encoding\n",
    "- **Description:** Maps tokens to unique numerical identifiers using a vocabulary index.\n",
    "- **Substeps:**\n",
    "  - **a. Vocabulary Lookup:** Each token is assigned an integer based on its position in the vocabulary.\n",
    "  - **b. Handling Unknown Tokens:** Tokens not present in the vocabulary are mapped to a special `[UNK]` token.\n",
    "- **Example:** `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]` → `[101, 2024, 2003, 1037, 7099, 2527, 1012]`\n",
    "\n",
    "## 2. Embedding Layer\n",
    "\n",
    "### 2.1. Token Embeddings\n",
    "- **Description:** Transforms numerical token IDs into dense vector representations.\n",
    "- **Substeps:**\n",
    "  - **a. Embedding Matrix:** A learnable matrix where each row corresponds to a token's embedding.\n",
    "  - **b. Lookup:** Each token ID retrieves its corresponding embedding vector.\n",
    "- **Example:** `Embedding Matrix [Vocab Size x d_model]` → `X = [n_tokens x d_model]`\n",
    "\n",
    "### 2.2. Positional Encodings\n",
    "- **Description:** Adds information about the position of each token in the sequence to the token embeddings.\n",
    "- **Substeps:**\n",
    "  - **a. Sinusoidal Encoding (Fixed):** Uses sine and cosine functions of different frequencies.\n",
    "  - **b. Learned Positional Embeddings (Learnable):** Embeddings are learned during training.\n",
    "- **Example:**\n",
    "  $$\n",
    "  \\text{Positional Encoding}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\\\\n",
    "  \\text{Positional Encoding}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  $$\n",
    "  \n",
    "### 2.3. Combined Embeddings\n",
    "- **Description:** Summing token embeddings with positional encodings to incorporate positional information.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  E = X + \\text{Positional Encodings}\n",
    "  $$\n",
    "- **Result:** A matrix `E` representing the input sequence with positional information.\n",
    "\n",
    "## 3. Transformer Architecture\n",
    "\n",
    "The transformer consists of an **Encoder** and a **Decoder**, each composed of multiple layers. Below, we outline the main components and their subcomponents.\n",
    "\n",
    "### 3.1. Encoder\n",
    "\n",
    "#### 3.1.1. Multi-Head Self-Attention\n",
    "- **Description:** Allows the model to focus on different parts of the input sequence simultaneously.\n",
    "- **Substeps:**\n",
    "  - **a. Linear Projections:** Compute queries (Q), keys (K), and values (V) using learned weight matrices.\n",
    "    $$\n",
    "    Q = E W_Q, \\quad K = E W_K, \\quad V = E W_V\n",
    "    $$\n",
    "  \n",
    "  - **b. Attention Scores:** For each query vector in Q, compute the dot product with all key vectors in K. These scores indicate the relevance or similarity between each query and all keys.\n",
    "    - *High Score:* The corresponding value should be given more attention.\n",
    "    - *Low Score:* The corresponding value should be given less attention.\n",
    "      $$\n",
    "      Attention Scores = Q * K^T\n",
    "      $$\n",
    "  - **c. Scaled Dot-Product Attention:** Calculate attention scores, apply scaling, softmax, and compute weighted sums.\n",
    "    $$\n",
    "    \\text{Scores} = \\frac{Q K^T}{\\sqrt{d_k}} \\\\\n",
    "    \\text{Attention Weights} = \\text{softmax}(\\text{Scores}) \\\\ \n",
    "    \\text{Attention Output} = \\text{Attention Weights} \\times V\n",
    "    $$\n",
    "    - Scaling prevents the dot products from growing too large, which can push the softmax function into regions with very small gradients.\n",
    "    - These weights determine how much each value vector (from V) contributes to the final output. \\\\\n",
    "    - Weights aggregates the value vectors based on their relevance to each query, producing a weighted sum that captures contextual information.\n",
    "  \n",
    "  - **d. Concatenation:** Concatenate outputs from all attention heads.\n",
    "  - **e. Final Linear Projection:** Apply a linear transformation to the concatenated output.\n",
    "    $$\n",
    "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O\n",
    "    $$\n",
    "\n",
    "#### 3.1.2. Add & Norm\n",
    "- **Description:**  \n",
    "  Incorporates a **residual (skip) connection** by adding the sub-layer's input \\( E \\) to its output, followed by **layer normalization**. This helps preserve the original information and stabilizes the training process.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(E + \\text{MultiHead}(Q, K, V))\n",
    "  $$\n",
    "  $$\n",
    "  \\text{or}\n",
    "  $$\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(E + \\text{Attention Output})\n",
    "  $$\n",
    "\n",
    "\n",
    "#### 3.1.3. Feed-Forward Network (FFN)\n",
    "- **Description:**  \n",
    "  Processes each position independently through a two-layer fully connected network to capture complex patterns and non-linear relationships.\n",
    "  \n",
    "- **Substeps:**\n",
    "  - **a. Linear Transformation:**  \n",
    "    **Purpose:** Expands the dimensionality of the input to increase the model's capacity to learn.  \n",
    "    **Formula:**  \n",
    "    $$\n",
    "    \\text{FFN}_1 = \\text{ReLU}(E W_1 + b_1)\n",
    "    $$\n",
    "    - $ E $: **Input** from the previous **Add & Norm** step ($E$ here is the **Output** from the Add & Norm step).\n",
    "    - $ W_1 $: **Weight matrix** for the first linear layer.\n",
    "    - $ b_1 $: **Bias vector** for the first linear layer.\n",
    "    - **ReLU:** Activation function introducing non-linearity.\n",
    "  \n",
    "  - **b. Linear Transformation:**  \n",
    "    **Purpose:** Reduces the dimensionality back to the original size, ensuring consistency in the model's architecture.  \n",
    "    **Formula:**  \n",
    "    $$\n",
    "    \\text{FFN}_2 = \\text{FFN}_1 W_2 + b_2\n",
    "    $$\n",
    "    - $ W_2 $: **Weight matrix** for the second linear layer.\n",
    "    - $ b_2 $: **Bias vector** for the second linear layer.\n",
    "\n",
    "#### 3.1.4. Add & Norm\n",
    "- **Description:**  \n",
    "  Adds a **residual (skip) connection** by combining the FFN's output with its input $ E $, followed by **layer normalization**. This step helps preserve the original information and stabilizes the training process.\n",
    "  \n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Encoder Output} = \\text{LayerNorm}(E + \\text{FFN}_2)\n",
    "  $$\n",
    "  - $ E $: **Input** to the Feed-Forward Network (output from the previous **Add & Norm** step).\n",
    "  - $ \\text{FFN}_2 $: **Output** from the Feed-Forward Network.\n",
    "  - **LayerNorm:** Normalizes the combined output to have a mean of 0 and a variance of 1, enhancing training stability and performance.\n",
    "\n",
    "---\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "---\n",
    "\n",
    "### 3.2. Decoder\n",
    "\n",
    "While the **Encoder** processes the input sequence to generate contextualized representations, the **Decoder** generates the output sequence by leveraging these representations and the previously generated tokens. The Decoder consists of multiple layers, each containing three main sub-layers:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "2. **Multi-Head Cross-Attention**\n",
    "3. **Position-wise Feed-Forward Network (FFN)**\n",
    "\n",
    "Each sub-layer is followed by **Residual Connections** and **Layer Normalization**, similar to the Encoder.\n",
    "\n",
    "\n",
    "#### 3.2.1. Masked Multi-Head Self-Attention\n",
    "- **Description:** Similar to encoder's self-attention but with masking to prevent attending to future tokens.\n",
    "- **Purpose:**  \n",
    "  - **Prevent Information Leakage:** By masking future tokens, the model ensures that the prediction for the current token doesn't incorporate information from tokens that haven't been generated yet.\n",
    "  - **Maintain Causality:** Ensures that the generation process respects the sequential nature of language.\n",
    "\n",
    "- **Substeps:**\n",
    "  - **a. Masking:** Apply a causal mask to ensure autoregressive property.\n",
    "    $$\n",
    "    \\text{Masked Scores} = \\frac{Q K^T}{\\sqrt{d_k}} + M \\\\\n",
    "    \\text{Attention Weights} = \\text{softmax}(\\text{Masked Scores})\n",
    "    $$\n",
    "  - **b. Compute Attention Output:** As in encoder.\n",
    "    $$\n",
    "    \\text{MaskedAttention Output} = \\text{MultiHead}(Q, K, V) \\quad \\text{with Masking}\n",
    "    $$\n",
    "    $$\n",
    "    or\n",
    "    $$ \n",
    "    $$\n",
    "    \\text{MaskedAttention Output} = \\text{Attention Weights} \\times V\n",
    "    $$\n",
    "\n",
    "- **How Masking Works:**\n",
    "  - **Mask Matrix:** A triangular matrix that masks (sets to \\(-\\infty\\)) the attention scores for future tokens.\n",
    "  - **Application:** Before applying the softmax function, the mask is added to the attention scores to nullify the influence of future tokens.\n",
    "\n",
    "\n",
    "#### 3.2.2. Add & Norm (Post Masked Self-Attention)\n",
    "- **Description:** Residual connection and layer normalization.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(E + \\text{Masked MultiHead}(Q, K, V))\n",
    "  $$\n",
    "\n",
    "#### 3.2.3. Multi-Head Attention over Encoder Outputs (or Multi-Head Cross-Attention)\n",
    "- **Description:** Allows the decoder to attend to the encoder's output.\n",
    "- **Components:**\n",
    "  - **Queries (Q):** Derived from the Decoder's previous sub-layer (Masked Self-Attention output).\n",
    "  - **Keys (K) & Values (V):** Derived from the Encoder's final output.\n",
    "\n",
    "- **Substeps:**\n",
    "  - **a. Compute Q from Decoder:** \n",
    "    $$\n",
    "    Q = \\text{Decoder Output} W_Q'\n",
    "    $$\n",
    "  - **b. Compute K and V from Encoder:**\n",
    "    $$\n",
    "    K = \\text{Encoder Output} W_K', \\quad V = \\text{Encoder Output} W_V'\n",
    "    $$\n",
    "  - **c. Compute Attention Output:** As in encoder.\n",
    "\n",
    "#### 3.2.4. Add & Norm\n",
    "- **Description:**   Adds another **residual (skip) connection** by combining the input $ E' $ (output from the previous Add & Norm step) with the cross-attention output, followed by **layer normalization**. This step further integrates information from the Encoder while maintaining training stability.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(\\text{Decoder Output} + \\text{MultiHead}(Q, K, V))\n",
    "  $$\n",
    "\n",
    "#### 3.2.5. Feed-Forward Network (FFN) - Position Wise Feed Forward Network\n",
    "- **Description:** Same as encoder's FFN.\n",
    "- **Substeps:**\n",
    "  - **a. Linear Transformation and Activation:**\n",
    "    $$\n",
    "    \\text{FFN}_1 = \\text{ReLU}(\\text{Output} W_1 + b_1)\n",
    "    $$\n",
    "  - **b. Linear Transformation:**\n",
    "    $$\n",
    "    \\text{FFN}_2 = \\text{FFN}_1 W_2 + b_2\n",
    "    $$\n",
    "\n",
    "#### 3.2.6. Add & Norm (Post Feed-Forward Network)\n",
    "- **Description:** Adds a final **residual (skip) connection** by combining the FFN's output $ \\text{FFN}_2 $ with its input $ E'' $ (output from the previous Add & Norm step), followed by **layer normalization**. This final normalization ensures that the Encoder's output is well-conditioned for generating predictions.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Decoder Output} = \\text{LayerNorm}(\\text{Output from Add & Norm} + \\text{FFN}_2)\n",
    "  $$\n",
    "\n",
    "\n",
    "## 4. Output Embeddings and Generation\n",
    "\n",
    "- **Description:**  \n",
    "  The **Decoder Output** is transformed into **output embeddings** which are then used to generate the final predictions (e.g., token probabilities).\n",
    "\n",
    "### 4.1. Final Linear Layer\n",
    "- **Description:** Transforms decoder outputs to match the size of the target vocabulary.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Logits} = \\text{Decoder Output} W_O + b_O\n",
    "  $$\n",
    "\n",
    "### 4.2. Softmax\n",
    "- **Description:** Converts logits into probability distributions over the vocabulary.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Probabilities} = \\text{softmax}(\\text{Logits})\n",
    "  $$\n",
    "\n",
    "### 4.3. Token Selection\n",
    "- **Description:**  The token with the highest probability is selected as the next token in the sequence.\n",
    "- **Methods:**\n",
    "  - **a. Greedy Search:** Select the token with the highest probability.\n",
    "  - **b. Beam Search:** Explore multiple sequences to find the most likely output.\n",
    "  - **c. Sampling:** Randomly sample tokens based on their probabilities.\n",
    "\n",
    "### 4.4. Output Text\n",
    "- **Description:** Converts selected token IDs back to human-readable text.\n",
    "- **Example:** `[101, 2024, 2003, 102]` → `\"The cat sat.\"`\n",
    "\n",
    "## 5. Summary of Transformer Steps\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - Raw Text Input\n",
    "   - Tokenization\n",
    "   - Numerical Encoding\n",
    "\n",
    "2. **Embedding Layer:**\n",
    "   - Token Embeddings\n",
    "   - Positional Encodings\n",
    "   - Combined Embeddings\n",
    "\n",
    "3. **Transformer Architecture:**\n",
    "   - **Encoder:**\n",
    "     - Multi-Head Self-Attention\n",
    "     - Add & Norm\n",
    "     - Feed-Forward Network (FFN)\n",
    "     - Add & Norm\n",
    "   - **Decoder:**\n",
    "     - Masked Multi-Head Self-Attention\n",
    "     - Add & Norm\n",
    "     - Multi-Head Attention over Encoder Outputs\n",
    "     - Add & Norm\n",
    "     - Feed-Forward Network (FFN)\n",
    "     - Add & Norm\n",
    "\n",
    "4. **Output Generation:**\n",
    "   - Final Linear Layer\n",
    "   - Softmax\n",
    "   - Token Selection\n",
    "   - Output Text\n",
    "\n",
    "\n",
    "\n",
    "## 6. Masked Multi-Head Attention vs. Regular Multi-Head Attention\n",
    "\n",
    "- **Masked Multi-Head Attention:**  \n",
    "  - **Used In:** Decoder's self-attention sub-layer.\n",
    "  - **Function:** Prevents the model from attending to future tokens in the sequence during training, maintaining causality.\n",
    "  \n",
    "- **Regular Multi-Head Attention:**  \n",
    "  - **Used In:** Encoder's self-attention sub-layers and Decoder's cross-attention sub-layer.\n",
    "  - **Function:** Allows the model to attend to all positions in the input or encoder's output without restrictions.\n",
    "\n",
    "- **Key Difference:**  \n",
    "  The **masking** in the Decoder's self-attention ensures that each position can only attend to previous and current positions, not future ones, which is essential for tasks like language generation where the model should not peek ahead.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.1 Differences Between Encoder Inputs and Decoder Outputs\n",
    "\n",
    "- **Encoder Inputs:**\n",
    "  - **Nature:** The source sequence (e.g., a sentence in the source language).\n",
    "  - **Processing:** Fully accessible to all Encoder sub-layers; each token can attend to every other token in the sequence.\n",
    "  \n",
    "- **Decoder Outputs:**\n",
    "  - **Nature:** The target sequence being generated (e.g., a sentence in the target language).\n",
    "  - **Processing:** \n",
    "    - **Masked Self-Attention:** Each position can only attend to previous tokens in the target sequence.\n",
    "    - **Cross-Attention:** Each position can attend to all tokens in the Encoder's output, integrating source information.\n",
    "    - **Autoregressive Generation:** Each new token is generated based on previously generated tokens and the Encoder's context.\n",
    "\n",
    "- **Summary of Differences:**\n",
    "  | Feature                     | Encoder Inputs                      | Decoder Outputs                                |\n",
    "  |-----------------------------|-------------------------------------|------------------------------------------------|\n",
    "  | **Access to Sequence**      | Full access to entire input sequence| Limited to past and current tokens (masked)    |\n",
    "  | **Attention Mechanism**     | Self-Attention (no masking)         | Masked Self-Attention & Cross-Attention        |\n",
    "  | **Generation Flow**         | Processes input in parallel         | Generates output sequentially (autoregressive) |\n",
    "  | **Integration with Encoder**| Independent of Encoder               | Attends to Encoder's output for context        |\n",
    "\n",
    "---\n",
    "\n",
    "### 6.2 Flow Overview of Decoder Layer\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention:**\n",
    "   - **Input:** Previously generated tokens (shifted right target sequence).\n",
    "   - **Process:** Allows attention only to past and present tokens.\n",
    "   - **Output:** Contextualized representation of the target sequence up to the current token.\n",
    "\n",
    "2. **Add & Norm:**\n",
    "   - **Input:** Output from Masked Self-Attention.\n",
    "   - **Process:** Residual connection + Layer Normalization.\n",
    "   - **Output:** Normalized representation for the next sub-layer.\n",
    "\n",
    "3. **Multi-Head Cross-Attention:**\n",
    "   - **Input:** Encoder's output and normalized target representation.\n",
    "   - **Process:** Integrates information from the source sequence.\n",
    "   - **Output:** Enhanced representation combining source and target contexts.\n",
    "\n",
    "4. **Add & Norm:**\n",
    "   - **Input:** Output from Cross-Attention.\n",
    "   - **Process:** Residual connection + Layer Normalization.\n",
    "   - **Output:** Normalized representation for the FFN.\n",
    "\n",
    "5. **Feed-Forward Network (FFN):**\n",
    "   - **Input:** Normalized representation from Cross-Attention.\n",
    "   - **Process:** Two linear transformations with ReLU activation.\n",
    "   - **Output:** Transformed representation ready for final normalization.\n",
    "\n",
    "6. **Add & Norm:**\n",
    "   - **Input:** FFN's output.\n",
    "   - **Process:** Residual connection + Layer Normalization.\n",
    "   - **Output:** **Decoder Output**, ready for the final prediction layer.\n",
    "\n",
    "7. **Output Embeddings and Generation:**\n",
    "   - **Input:** Decoder Output.\n",
    "   - **Process:** Linear transformation + Softmax to generate token probabilities.\n",
    "   - **Output:** Predicted next token in the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.3. Key Differences Between Encoder Inputs and Decoder Outputs\n",
    "\n",
    "Understanding the distinction between what the Encoder receives and what the Decoder produces is crucial for grasping the Transformer's functionality.\n",
    "\n",
    "- **Encoder:**\n",
    "  - **Input:** Entire source sequence (e.g., a sentence in English).\n",
    "  - **Output:** Contextualized representations of the source sequence, capturing relationships and dependencies between tokens.\n",
    "  \n",
    "- **Decoder:**\n",
    "  - **Input:** Previously generated tokens in the target sequence (shifted right during training) and the Encoder's output.\n",
    "  - **Output:** Predictions for the next token in the target sequence, based on both the generated tokens so far and the source context.\n",
    "\n",
    "- **Autoregressive Nature:**\n",
    "  - **Encoder:** Processes all input tokens simultaneously.\n",
    "  - **Decoder:** Generates tokens one by one, each time conditioning on the previously generated tokens and the Encoder's output.\n",
    "\n",
    "- **Interaction:**\n",
    "  - **Encoder:** Independent of the Decoder.\n",
    "  - **Decoder:** Relies on the Encoder's output through the Cross-Attention mechanism to inform its token generation.\n",
    "\n",
    "---\n",
    "\n",
    "### 6.4. Summary\n",
    "\n",
    "The **Transformer Decoder** complements the Encoder by generating the output sequence in a manner that is both informed by the source context and conditioned on previously generated tokens. Key components like **Masked Multi-Head Self-Attention** ensure that the generation process remains autoregressive and causally consistent, while **Multi-Head Cross-Attention** integrates valuable information from the Encoder's output. The **Add & Norm** steps maintain stability and facilitate effective gradient flow, enabling the model to learn deep and complex representations.\n",
    "\n",
    "Understanding the interplay between these components is essential for leveraging the Transformer architecture in tasks such as machine translation, text generation, and more.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. What Are Text Embeddings?\n",
    "\n",
    "Text embeddings map pieces of text, such as sentences or paragraphs, into vectors in a high-dimensional space. The core idea is to represent words or sentences as points in this space such that semantically similar pieces of text are positioned closer together. This space is often referred to as the **embedding space** or **latent space**.\n",
    "\n",
    "- Each text element (word, sentence, or document) is converted into a vector of numbers.\n",
    "- These vectors typically have hundreds or even thousands of dimensions (e.g., 768, 1536, etc.).\n",
    "  \n",
    "For example, the sentence \"I love programming\" might be represented as:\n",
    "\n",
    "$$\n",
    "\\mathbf{v} = \\begin{bmatrix} 0.25 & 0.13 & 0.98 & \\dots & 0.51 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "where each entry in the vector corresponds to a learned feature in the embedding space.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Tokenization and Embedding Calculation\n",
    "\n",
    "Text embeddings are typically generated by processing text in smaller units called **tokens**. These tokens are often words, subwords, or characters, depending on the model architecture. Here's the general process:\n",
    "\n",
    "### Step 1: **Tokenization**\n",
    "\n",
    "Tokenization is the process of splitting text into smaller units, usually words or subwords. For example:\n",
    "\n",
    "- \"I love programming\" → Tokens: [\"I\", \"love\", \"programming\"]\n",
    "\n",
    "Some tokenizers break down words into subwords to handle out-of-vocabulary (OOV) words, like:\n",
    "\n",
    "- \"unhappiness\" → Tokens: [\"un\", \"happiness\"]\n",
    "\n",
    "### Step 2: Mapping Tokens to Vectors\n",
    "\n",
    "#### A. Tokenization\n",
    "\n",
    "Tokenization breaks down text into smaller units called **tokens**, which can be words, subwords, or characters. For example:\n",
    "\n",
    "- Sentence: \"I love programming.\"\n",
    "- Tokens: [\"I\", \"love\", \"programming\"]\n",
    "\n",
    "For **subword-level tokenization**:\n",
    "- Sentence: \"unhappiness\"\n",
    "- Tokens: [\"un\", \"happiness\"]\n",
    "\n",
    "\n",
    "#### B. Mapping Tokens to Vectors\n",
    "\n",
    "The first step in processing a token (word) is to convert it into a **vector representation** called an **embedding**. \n",
    "\n",
    "- **Token Embeddings**: Each word or token is mapped to a high-dimensional vector, represented as **E**.\n",
    "  \n",
    "    $$\n",
    "    \\mathbf{E} = W_{\\text{emb}} \\cdot \\mathbf{x}\n",
    "    $$\n",
    "    where:\n",
    "    - **E** is the token embedding vector (dimension $d$),\n",
    "    - **$W_{\\text{emb}}$** is the embedding matrix,\n",
    "    - **x** is the token index (one-hot vector representing the word).\n",
    "\n",
    "##### Explanation of Embedding Matrix ($ W_{\\text{emb}} $)\n",
    "\n",
    "To understand embeddings more deeply, let’s break down the **embedding matrix**:\n",
    "\n",
    "The **embedding matrix** $ W_{\\text{emb}} $ is essentially a lookup table that maps each word in the vocabulary to a vector in a high-dimensional space. This matrix has dimensions $ V \\times d $, where:\n",
    "- **V** is the size of the vocabulary (the number of unique words or tokens),\n",
    "- **d** is the dimensionality of the vector space (embedding size).\n",
    "\n",
    "Each row of this matrix corresponds to the **embedding** of a word in the vocabulary. If the token **x** corresponds to the word \"dog,\" its embedding is simply the **x-th row** of the matrix.\n",
    "\n",
    "##### Example:\n",
    "Consider a small vocabulary with **V = 4** words: [\"cat\", \"dog\", \"sat\", \"mat\"], and each word is represented by a 3-dimensional embedding vector (**d = 3**). The embedding matrix might look like this:\n",
    "\n",
    "$$\n",
    "W_{\\text{emb}} = \\begin{bmatrix}\n",
    "0.1 & 0.3 & 0.5 \\\\\n",
    "0.4 & 0.2 & 0.7 \\\\\n",
    "0.6 & 0.8 & 0.1 \\\\\n",
    "0.9 & 0.5 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "If the word \"dog\" corresponds to index 2, its embedding vector **E_dog** will be the 2nd row:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}_{\\text{dog}} = \\begin{bmatrix} 0.4 \\\\ 0.2 \\\\ 0.7 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "#### **Positional Encoding**  \n",
    "To account for the order of tokens in a sequence, we introduce **positional encodings**. These are vectors that encode the position of each token in the sequence.\n",
    "\n",
    "The positional encoding for a position $ p $ and dimension $ i $ is defined as:\n",
    "\n",
    "$$\n",
    "\\text{PE}_{p, 2i} = \\sin \\left( \\frac{p}{n^{2i/d}} \\right)\n",
    "$$\n",
    "$$\n",
    "\\text{PE}_{p, 2i+1} = \\cos \\left( \\frac{p}{n^{2i/d}} \\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- **p** is the position of the token in the sequence (e.g., 1 for the first word),\n",
    "- **n** is the User-defined scalar, set to 10,000 by the authors of Attention Is All You Need.\n",
    "- **d** is the dimensionality of the embedding space (e.g., 512),\n",
    "- **i** is the index of the dimension (from 0 to $ d-1 $).\n",
    "\n",
    "For example, for a sequence of 5 tokens, the positional encoding for the first token (at position $ p=1 $) with dimension $ d=4 $ will be calculated as:\n",
    "\n",
    "- $ \\text{PE}_{1, 0} = \\sin\\left(\\frac{1}{10000^{0/4}}\\right) $\n",
    "- $ \\text{PE}_{1, 1} = \\cos\\left(\\frac{1}{10000^{1/4}}\\right) $\n",
    "- $ \\text{PE}_{1, 2} = \\sin\\left(\\frac{1}{10000^{2/4}}\\right) $\n",
    "- $ \\text{PE}_{1, 3} = \\cos\\left(\\frac{1}{10000^{3/4}}\\right) $\n",
    "\n",
    "These values are added to the token embeddings to form the final input for the model:\n",
    "\n",
    "$$\n",
    "\\mathbf{X}_{\\text{final}} = \\mathbf{E} + \\mathbf{PE}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Attention Mechanism in Transformers: Detailed Mathematical Explanation\n",
    "\n",
    "In Transformer models like **BERT** and **GPT**, the **self-attention mechanism** allows each token in a sequence to attend to all other tokens dynamically, adjusting its representation based on the context of the sentence. Below is the breakdown of key components of attention and their mathematical formulation.\n",
    "\n",
    "### 1. **Query, Key, and Value Vectors (Q, K, V)**\n",
    "\n",
    "In the context of attention, each token is projected into three different vectors: **Query (Q)**, **Key (K)**, and **Value (V)**. These vectors help compute which tokens are relevant to one another based on their interactions.\n",
    "\n",
    "#### 1.1. **Query (Q)**, **Key (K)**, and **Value (V) Definition:\n",
    "\n",
    "- **Query (Q)**: Represents a question or request for information. For each token, a query vector is created that defines what information it is looking for in the other tokens.\n",
    "- **Key (K)**: Represents the context or feature of each token. It's used to determine if the token is relevant to the current query.\n",
    "- **Value (V)**: Represents the actual information that will be passed through if the token is relevant to the query.\n",
    "\n",
    "#### 1.2. **Mathematical Formulation of Q, K, V:**\n",
    "\n",
    "Each token's embedding $ t_i $ is projected into the query, key, and value vectors using learned weight matrices:\n",
    "\n",
    "$$\n",
    "Q_i = W_Q \\cdot t_i \\quad \\text{(Query vector for token \\( t_i \\))}\n",
    "$$\n",
    "$$\n",
    "K_i = W_K \\cdot t_i \\quad \\text{(Key vector for token \\( t_i \\))}\n",
    "$$\n",
    "$$\n",
    "V_i = W_V \\cdot t_i \\quad \\text{(Value vector for token \\( t_i \\))}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_Q, W_K, W_V $ are learned weight matrices,\n",
    "- $ t_i $ is the token embedding for token $ t_i $.\n",
    "\n",
    "\n",
    "### 2. **Attention Score Calculation**\n",
    "\n",
    "To determine the relevance of each token in the sequence to a given token (based on the query and key), we calculate an **attention score** using the **dot product** between the query vector of token $ t_i $ and the key vector of token $ t_j $.\n",
    "\n",
    "#### 2.1. **Attention Score Formula:**\n",
    "\n",
    "$$\n",
    "\\text{Attention\\_Score}_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ d_k $ is the dimensionality of the key vector (scaling factor to stabilize gradients),\n",
    "- $ Q_i $ is the query vector of token $ t_i $,\n",
    "- $ K_j $ is the key vector of token $ t_j $.\n",
    "\n",
    "The dot product between the query and key determines the **similarity** or **relevance** between two tokens.\n",
    "\n",
    "\n",
    "### 3. **Attention Weights Calculation**\n",
    "\n",
    "Once the attention scores are computed, we apply the **softmax** function to convert these scores into **attention weights**. The softmax function normalizes the attention scores to ensure that the sum of the attention weights across all tokens is 1.\n",
    "\n",
    "#### 3.1. **Attention Weight Formula:**\n",
    "\n",
    "$$\n",
    "\\alpha_{ij} = \\frac{\\exp(\\text{Attention\\_Score}_{ij})}{\\sum_{k=1}^{n} \\exp(\\text{Attention\\_Score}_{ik})}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\alpha_{ij} $ is the attention weight for the pair of tokens $ (t_i, t_j) $,\n",
    "- The denominator sums over all tokens $ k $ in the sequence, ensuring the weights are normalized.\n",
    "\n",
    "This step determines how much attention token $ t_i $ should give to token $ t_j $.\n",
    "\n",
    "\n",
    "### 4. **Contextualized Embedding Calculation**\n",
    "\n",
    "The final step is to compute the **contextualized embedding** for each token. This is done by taking a weighted sum of the **value vectors** based on the attention weights.\n",
    "\n",
    "#### 4.1. **Contextualized Embedding Formula:**\n",
    "\n",
    "$$\n",
    "\\mathbf{t}_i^\\text{contextualized} = \\sum_{j=1}^{n} \\alpha_{ij} \\cdot V_j\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\mathbf{t}_i^\\text{contextualized} $ is the updated embedding for token $ t_i $ after attending to all other tokens,\n",
    "- $ \\alpha_{ij} $ is the attention weight for token $ t_j $ as calculated earlier,\n",
    "- $ V_j $ is the value vector for token $ t_j $.\n",
    "\n",
    "This gives us a new representation for each token, which now incorporates contextual information from the entire sequence.\n",
    "\n",
    "\n",
    "### 5. **Multi-Head Attention**\n",
    "\n",
    "Instead of performing attention only once, **multi-head attention** performs attention multiple times in parallel. Each \"head\" operates on the input using different learned weight matrices, allowing the model to focus on different aspects of the sequence simultaneously.\n",
    "\n",
    "### 5.1. **Multi-Head Attention Formula:**\n",
    "\n",
    "For each head $ i $, attention is calculated as:\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_i^Q, W_i^K, W_i^V $ are the weight matrices for the $ i $-th attention head.\n",
    "\n",
    "After computing attention for all heads, the outputs are concatenated and passed through a final linear transformation:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ h $ is the number of attention heads,\n",
    "- $ W^O $ is the output weight matrix.\n",
    "\n",
    "This allows the model to capture a rich set of relationships in the input sequence, each head focusing on different features or aspects of the data.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of Key Components\n",
    "\n",
    "- **Query (Q)**: A vector representing the information we’re looking for.  $ Q_i = W_Q t_i$\n",
    "- **Key (K)**: A vector representing the context of each token. $\\quad K_i = W_K t_i$\n",
    "- **Value (V)**: A vector containing the actual information that will be passed on if the token matches the query. $\\quad V_i = W_V t_i $\n",
    "- **Attention Score**: The similarity between the query and the key, used to determine relevance. $ \\text{Attention\\_Score}_{ij} = \\frac{Q_i \\cdot K_j}{\\sqrt{d_k}} $\n",
    "- **Attention Weight**: Normalized attention score using softmax, which decides how much influence each token has on others.\n",
    "  $ \\alpha_{ij} = \\frac{\\exp(\\text{Attention\\_Score}_{ij})}{\\sum_{k=1}^{n} \\exp(\\text{Attention\\_Score}_{ik})} $\n",
    "- **Contextualized Embedding**: The weighted sum of the value vectors, giving each token a new representation based on its context in the sequence. $ \\mathbf{t}_i^\\text{contextualized} = \\sum_{j=1}^{n} \\alpha_{ij} V_j $\n",
    "- **Multi-Head Attention**: Performs multiple attention operations in parallel, allowing the model to focus on different aspects of the sequence simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Distance and Similarity in Embedding Space\n",
    "\n",
    "Once we have our tokens represented as vectors, we can use these embeddings for various tasks, such as measuring text similarity. A key feature of embedding spaces is that semantically similar items are placed closer together. \n",
    "\n",
    "Two common distance metrics are:\n",
    "\n",
    "### a. Euclidean Distance\n",
    "\n",
    "Euclidean distance is the straight-line distance between two vectors in the embedding space. For two vectors $\\mathbf{v}_1 = [v_{1,1}, v_{1,2}, \\dots, v_{1,n}]$ and $\\mathbf{v}_2 = [v_{2,1}, v_{2,2}, \\dots, v_{2,n}]$, the Euclidean distance $d(\\mathbf{v}_1, \\mathbf{v}_2)$ is given by:\n",
    "\n",
    "$$\n",
    "d(\\mathbf{v}_1, \\mathbf{v}_2) = \\sqrt{\\sum_{i=1}^{n} (v_{1,i} - v_{2,i})^2}\n",
    "$$\n",
    "\n",
    "### b. Cosine Similarity\n",
    "\n",
    "Cosine similarity measures the cosine of the angle between two vectors. It ranges from \\(-1\\) (completely opposite) to \\(1\\) (completely similar). The formula for cosine similarity is:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\mathbf{v}_1, \\mathbf{v}_2) = \\frac{\\mathbf{v}_1 \\cdot \\mathbf{v}_2}{||\\mathbf{v}_1|| ||\\mathbf{v}_2||}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $\\mathbf{v}_1 \\cdot \\mathbf{v}_2$ is the dot product of the vectors.\n",
    "- $||\\mathbf{v}_1||$ and $||\\mathbf{v}_2||$ are the magnitudes (norms) of the vectors.\n",
    "\n",
    "Cosine similarity works well when we care about the **relative weights** of features (terms) rather than their absolute magnitudes. It is widely used in NLP for comparing text.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Example: Text Embedding in Action\n",
    "\n",
    "Let’s say we have the following three sentences:\n",
    "\n",
    "1. \"I love programming.\"\n",
    "2. \"I enjoy coding.\"\n",
    "3. \"I dislike programming.\"\n",
    "\n",
    "Assume the embeddings of these sentences (in a 3-dimensional space) are as follows:\n",
    "\n",
    "$$\n",
    "\\mathbf{v_1} = \\begin{bmatrix} 0.2 & 0.4 & 0.9 \\end{bmatrix}, \\quad \\mathbf{v_2} = \\begin{bmatrix} 0.1 & 0.5 & 0.8 \\end{bmatrix}, \\quad \\mathbf{v_3} = \\begin{bmatrix} 0.7 & 0.2 & 0.1 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Now, we compute cosine similarity between:\n",
    "\n",
    "- **Sentence 1 and Sentence 2**:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\mathbf{v_1}, \\mathbf{v_2}) = \\frac{(0.2)(0.1) + (0.4)(0.5) + (0.9)(0.8)}{\\sqrt{(0.2^2 + 0.4^2 + 0.9^2)} \\cdot \\sqrt{(0.1^2 + 0.5^2 + 0.8^2)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.02 + 0.2 + 0.72}{\\sqrt{0.2 + 0.16 + 0.81} \\cdot \\sqrt{0.01 + 0.25 + 0.64}} = \\frac{0.94}{\\sqrt{1.17} \\cdot \\sqrt{0.9}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.94}{1.08 \\cdot 0.95} = \\frac{0.94}{1.026} \\approx 0.916\n",
    "$$\n",
    "\n",
    "This indicates a **high similarity** between \"I love programming\" and \"I enjoy coding.\"\n",
    "\n",
    "- **Sentence 1 and Sentence 3**:\n",
    "\n",
    "Now, calculate the cosine similarity between the first and third sentences:\n",
    "\n",
    "$$\n",
    "\\text{cosine\\_similarity}(\\mathbf{v_1}, \\mathbf{v_3}) = \\frac{(0.2)(0.7) + (0.4)(0.2) + (0.9)(0.1)}{\\sqrt{(0.2^2 + 0.4^2 + 0.9^2)} \\cdot \\sqrt{(0.7^2 + 0.2^2 + 0.1^2)}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.14 + 0.08 + 0.09}{\\sqrt{0.2 + 0.16 + 0.81} \\cdot \\sqrt{0.49 + 0.04 + 0.01}} = \\frac{0.31}{\\sqrt{1.17} \\cdot \\sqrt{0.54}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "= \\frac{0.31}{1.08 \\cdot 0.73} = \\frac{0.31}{0.7884} \\approx 0.393\n",
    "$$\n",
    "\n",
    "This indicates a **low similarity** between \"I love programming\" and \"I dislike programming.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Conclusion\n",
    "\n",
    "Text embeddings are a powerful way to represent text data numerically, allowing machines to perform tasks such as text classification, sentiment analysis, and semantic search. Through the use of distance metrics like **Euclidean distance** and **Cosine similarity**, we can quantify how similar two pieces of text are and apply this to a variety of NLP applications. Understanding embeddings and how they map text into high-dimensional spaces is crucial for building more effective and intuitive machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Text Embeddings in Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, dataloader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate a simple corpus (same as before)\n",
    "words = [\"apple\", \"banana\", \"cherry\", \"dog\", \"elephant\", \"fish\", \"grape\", \"house\", \"ice\", \"jungle\", \"kite\", \"lemon\"]\n",
    "num_sentences = 5\n",
    "max_sentence_length = 6\n",
    "\n",
    "# Generate random sentences\n",
    "corpus = []\n",
    "for _ in range(num_sentences):\n",
    "    sentence_length = np.random.randint(3, max_sentence_length)\n",
    "    sentence = np.random.choice(words, sentence_length, replace=True)\n",
    "    corpus.append(sentence)\n",
    "\n",
    "# Step 2: Create a token-to-index dictionary for words in the corpus\n",
    "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "# Step 3: Convert words in the corpus to indices using the dictionary\n",
    "corpus_indices = [[word_to_index[word] for word in sentence] for sentence in corpus]\n",
    "\n",
    "# Step 4: Define an embedding layer\n",
    "embedding_dim = 512\n",
    "vocab_size = len(words)\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Step 5: Pass the indices through the embedding layer\n",
    "embeddings = [token_embedding_table(torch.tensor(sentence)) for sentence in corpus_indices]\n",
    "\n",
    "# Print the embeddings for each word in the corpus\n",
    "print(\"\\nWord Embeddings for the Corpus:\")\n",
    "for i, sentence_embedding in enumerate(embeddings):\n",
    "    print(f\"Sentence {i + 1}:\")\n",
    "    for word_idx, word_embedding in zip(corpus_indices[i], sentence_embedding):\n",
    "        print(f\"Word '{words[word_idx]}': {word_embedding[:5]}...\")  # Print first 5 elements of each embedding\n",
    "    print()  # New line between sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # Fetch logits (predicted scores)\n",
    "        logits = self.token_embedding_table(index)\n",
    "\n",
    "        # Compute loss if targets are provided\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            # Reshape logits and targets for cross-entropy\n",
    "            B, T, C = logits.shape  # Batch size, Vocab size, Vector size\n",
    "            logits = logits.view(B * T, C)  \n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        # Generate a sequence of tokens\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Predict next token based on current sequence\n",
    "            logits, _ = self.forward(index)\n",
    "            logits = logits[:, -1, :]  # Focus on the last time step\n",
    "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
    "\n",
    "            # Sample next token from the probability distribution\n",
    "            index_next = torch.multinomial(probs, num_samples=1)    # Sample one token\n",
    "            \n",
    "            # Append the new token to the sequence\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "        \n",
    "        return index\n",
    "\n",
    "# Instantiate and run the model\n",
    "vocab_size = 100  # Example vocab size\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = BigramLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Example context and generation\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)  # Initial context\n",
    "generated_tokens = model.generate(context, max_new_tokens=500)\n",
    "# print(generated_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "**Word2Vec** is a neural network-based technique used to generate vector representations (embeddings) of words in a corpus of text. The key idea behind Word2Vec is to transform words into dense, continuous vectors in a high-dimensional space, where words with similar meanings are located closer to each other. These word embeddings capture semantic relationships between words, such as synonyms, antonyms, and analogies (e.g., \"king\" - \"man\" + \"woman\" = \"queen\"). Word2Vec achieves this by training a model to predict the surrounding words (context) given a target word, or vice versa, depending on the chosen model. The two main architectures for Word2Vec are the **Skip-gram model** and the **Continuous Bag of Words (CBOW) model**.\n",
    "\n",
    "In the **Skip-gram** model, the goal is to predict the context words given a target word. For example, in the sentence \"The cat sat on the mat\", if \"sat\" is the target word, the model tries to predict the surrounding words \"The\", \"cat\", \"on\", \"the\", and \"mat\". The model learns the vector representation for each word by adjusting the embeddings so that words occurring in similar contexts (like \"cat\" and \"dog\") have similar vectors. This process is repeated over a large corpus of text, allowing the embeddings to capture semantic information based on word co-occurrences. Word2Vec is efficient because it leverages a shallow neural network with a simple architecture that can be trained quickly on large datasets. The result is a set of word vectors that can be used in various downstream NLP tasks, such as sentiment analysis, machine translation, and information retrieval.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_pairs[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Loss: 0.21508773763974506\n",
      "Epoch [20/100], Loss: 0.07223054816325505\n",
      "Epoch [30/100], Loss: 0.038460445155700046\n",
      "Epoch [40/100], Loss: 0.025005883164703847\n",
      "Epoch [50/100], Loss: 0.01808421897391478\n",
      "Epoch [60/100], Loss: 0.013966864347457886\n",
      "Epoch [70/100], Loss: 0.011278403705606859\n",
      "Epoch [80/100], Loss: 0.00939424525325497\n",
      "Epoch [90/100], Loss: 0.008014310896396638\n",
      "Epoch [100/100], Loss: 0.006963123753666878\n",
      "Word: the, Embedding: [ 0.65964425 -1.0634614  -0.07938081 -1.0143552  -0.42179868  1.3326547\n",
      " -0.32117814 -1.9897895  -1.6684294  -0.01161219]\n",
      "Word: quick, Embedding: [ 2.8160932  -0.24703808  0.58954406 -0.66629213 -0.77631253 -0.5413049\n",
      " -1.2220969  -1.0079648  -1.1804886  -1.2120961 ]\n",
      "Word: brown, Embedding: [ 1.0991999  -1.2361306  -0.11001078 -0.96111774 -0.1683051   0.40376255\n",
      " -1.2056465  -1.2161037  -1.1382229   1.7400821 ]\n",
      "Word: fox, Embedding: [ 0.29302096 -0.86034954 -1.805973    0.1150915  -0.595443   -0.59565365\n",
      " -0.04477623 -1.3482325  -0.30772996 -0.987991  ]\n",
      "Word: jumps, Embedding: [ 1.3177413  -0.70657116  0.27684647  0.9118989  -0.36899444 -0.11429747\n",
      "  1.3753065  -1.06243     0.00828359  0.5598823 ]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Step 1: Prepare the data\n",
    "text = \"The quick brown fox jumps over the lazy dog\"\n",
    "\n",
    "# Tokenize the text and build a vocabulary\n",
    "tokens = text.lower().split()\n",
    "vocab = Counter(tokens)  # Count frequency of each word\n",
    "vocab_size = len(vocab)\n",
    "\n",
    "# Map words to indices and vice versa\n",
    "word_to_index = {word: idx for idx, (word, _) in enumerate(vocab.items())}\n",
    "index_to_word = {idx: word for word, idx in word_to_index.items()}\n",
    "\n",
    "# Hyperparameters\n",
    "embedding_dim = 10  # Size of the word embeddings\n",
    "window_size = 2  # Context window size (skip-gram)\n",
    "\n",
    "# Create training pairs (target, context)\n",
    "training_pairs = []\n",
    "for i, word in enumerate(tokens):\n",
    "    target = word_to_index[word]\n",
    "    # Get the context window around the target word\n",
    "    context = list(set(tokens[max(i - window_size, 0):i] + tokens[i + 1:i + window_size + 1]))\n",
    "    context = [word_to_index[context_word] for context_word in context if context_word != word]\n",
    "    \n",
    "    for context_word in context:\n",
    "        training_pairs.append((target, context_word))\n",
    "\n",
    "# Step 2: Define the Word2Vec Model (Skip-gram)\n",
    "class Word2Vec(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(Word2Vec, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Input embedding layer\n",
    "        self.out_embeddings = nn.Embedding(vocab_size, embedding_dim)  # Output embedding layer\n",
    "    \n",
    "    def forward(self, target, context):\n",
    "        # Get embeddings for target and context words\n",
    "        in_emb = self.in_embeddings(target)  # (B, E)\n",
    "        out_emb = self.out_embeddings(context)  # (B, E)\n",
    "        \n",
    "        # Compute logits (dot product between target and context word embeddings)\n",
    "        return torch.sum(in_emb * out_emb, dim=1)\n",
    "\n",
    "# Step 3: Create Dataset and DataLoader\n",
    "class Word2VecDataset(Dataset):\n",
    "    def __init__(self, pairs):\n",
    "        # Convert pairs to tensors\n",
    "        self.targets = torch.tensor([pair[0] for pair in pairs])\n",
    "        self.contexts = torch.tensor([pair[1] for pair in pairs])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.targets[idx], self.contexts[idx]\n",
    "\n",
    "# Step 4: Train the model\n",
    "dataset = Word2VecDataset(training_pairs)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "model = Word2Vec(vocab_size, embedding_dim)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "loss_fn = nn.BCEWithLogitsLoss()  # Binary cross-entropy loss\n",
    "\n",
    "# Training loop\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for target, context in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute model output\n",
    "        output = model(target, context)\n",
    "        \n",
    "        # Create positive labels (1 for correct context)\n",
    "        labels = torch.ones_like(output, dtype=torch.float)\n",
    "        \n",
    "        # Calculate the loss \n",
    "        loss = loss_fn(output, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backpropagate and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(dataloader)}\")\n",
    "\n",
    "# Step 5: Check the word embeddings learned by the model\n",
    "word_embeddings = model.in_embeddings.weight.data.numpy()\n",
    "\n",
    "# Show word embeddings for the first 5 words\n",
    "for idx in range(min(5, vocab_size)):\n",
    "    word = index_to_word[idx]\n",
    "    embedding = word_embeddings[idx]\n",
    "    print(f\"Word: {word}, Embedding: {embedding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 8.687405735254288\n",
      "Epoch [2/5], Loss: 8.092840701341629\n",
      "Epoch [3/5], Loss: 7.657473549246788\n",
      "Epoch [4/5], Loss: 7.372279062867165\n",
      "Epoch [5/5], Loss: 7.210036337375641\n",
      "Predicted next word: 3797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the GPT-like Model with Embedding Layers\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, num_classes, max_len=512):\n",
    "        super(SimpleGPT, self).__init__()\n",
    "        \n",
    "        # Embedding layer: Map tokens to embedding vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Positional encoding (used to add positions to the embeddings)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_size))\n",
    "        \n",
    "        # Transformer Layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=embed_size*4\n",
    "        )\n",
    "        \n",
    "        # Output layer for classification or next-token prediction\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get token embeddings\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding to embeddings\n",
    "        seq_len = x.size(1)\n",
    "        embeddings = embeddings + self.positional_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Pass through the transformer\n",
    "        transformer_out = self.transformer(embeddings, embeddings)\n",
    "        \n",
    "        # Use the last token's representation for classification or next token prediction\n",
    "        output = self.fc_out(transformer_out[:, -1, :])\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 5000  # Example vocabulary size\n",
    "embed_size = 256  # Embedding dimension\n",
    "num_heads = 8     # Number of attention heads\n",
    "num_layers = 6    # Number of transformer layers\n",
    "num_classes = vocab_size  # For next-token prediction (same size as vocab)\n",
    "max_len = 512     # Max sequence length\n",
    "batch_size = 32   # Batch size\n",
    "learning_rate = 1e-4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleGPT(vocab_size, embed_size, num_heads, num_layers, num_classes, max_len)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Sample DataLoader for training (replace with actual data)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Simulated training data (replace with actual tokenized text)\n",
    "train_data = torch.randint(0, vocab_size, (1000, max_len))  # 1000 sequences\n",
    "train_labels = torch.randint(0, vocab_size, (1000,))  # Corresponding next token (or class)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Inference function for a sample text\n",
    "def infer(model, text, vocab, max_len=512):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input text (basic tokenizer for demo purposes)\n",
    "    tokens = torch.tensor([vocab.get(word, 0) for word in text.split()]).unsqueeze(0).to(device)  # Adding batch dimension and moving to device\n",
    "    tokens = tokens[:, :max_len]  # Ensure the sequence length is within max_len\n",
    "    \n",
    "    # Get the model's prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)\n",
    "    \n",
    "    # Convert output to probabilities (for next token prediction)\n",
    "    prob = torch.softmax(output, dim=-1)\n",
    "    \n",
    "    # Get the predicted token (highest probability)\n",
    "    predicted_token_idx = torch.argmax(prob, dim=-1).item()\n",
    "    \n",
    "    # Decode the token back to text (inverse vocab lookup)\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    predicted_word = reverse_vocab.get(predicted_token_idx, \"<UNK>\")\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Sample Vocab (just for illustration)\n",
    "vocab = {str(i): i for i in range(vocab_size)}  # Just a simple numerical vocabulary, replace with actual vocab\n",
    "\n",
    "# Sample input text\n",
    "sample_text = \"this is a sample sentence\"\n",
    "\n",
    "# Perform inference\n",
    "predicted_word = infer(model, sample_text, vocab)\n",
    "print(f\"Predicted next word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked Example of Attention with a Toy Corpus\n",
    "\n",
    "We’ve explained the mathematics behind single-head and multi-head attention. Now, let’s apply these steps to a simple, concrete example. This will demonstrate how the calculations flow from inputs all the way through to the final attention output.\n",
    "\n",
    "## Toy Setup\n",
    "\n",
    "### Assumptions and Simplifications\n",
    "\n",
    "- **Vocabulary and Embeddings:**  \n",
    "  We have a small vocabulary with three tokens:  \n",
    "  1. \"The\"\n",
    "  2. \"cat\"\n",
    "  3. \"sat\"\n",
    "\n",
    "  We assume we already have embeddings for these tokens. Let's say `d_model = 4` for simplicity. Thus, each token is represented by a 4-dimensional vector. For demonstration:\n",
    "  - \"The\" → $[0.1, 0.3, 0.5, 0.7]$\n",
    "  - \"cat\" → $[0.2, 0.4, 0.4, 0.6]$\n",
    "  - \"sat\" → $[0.15, 0.25, 0.5, 0.1]$\n",
    "\n",
    "- **Input Sequence:**  \n",
    "  Suppose our input sequence is: \"The cat sat\"\n",
    "\n",
    "So we have 3 tokens, hence $n=3$.\n",
    "\n",
    "- **Positional Encodings:**  \n",
    "For simplicity, let’s not add complex positional encodings. Suppose we have a simple positional encoding that just adds a small unique offset for each token position:\n",
    "- Position 1 encoding: $[0.0, 0.1, 0.0, 0.0]$\n",
    "- Position 2 encoding: $[0.0, 0.0, 0.1, 0.0]$\n",
    "- Position 3 encoding: $[0.0, 0.0, 0.0, 0.1]$\n",
    "\n",
    "After adding positional encodings:\n",
    "- For \"The\" (position 1): $[0.1, 0.3+0.1, 0.5, 0.7] = [0.1, 0.4, 0.5, 0.7]$\n",
    "- For \"cat\" (position 2): $[0.2, 0.4, 0.4+0.1, 0.6] = [0.2, 0.4, 0.5, 0.6]$\n",
    "- For \"sat\" (position 3): $[0.15, 0.25, 0.5, 0.1+0.1] = [0.15, 0.25, 0.5, 0.2]$\n",
    "\n",
    "Thus, our input matrix $X$ is:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\[6pt]\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\[6pt]\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where each row is a token embedding with positional info.\n",
    "\n",
    "- **Parameters (W_Q, W_K, W_V):**  \n",
    "Let’s define:\n",
    "- $d_{\\text{model}} = 4$\n",
    "- Single-head attention: $d_k = d_{\\text{model}} = 4$\n",
    "\n",
    "Example parameter matrices:\n",
    "$$\n",
    "W_Q = \\begin{bmatrix}\n",
    "0.5 & 0.1 & 0.0 & 0.3 \\\\[6pt]\n",
    "0.4 & 0.2 & 0.1 & 0.0 \\\\[6pt]\n",
    "0.3 & 0.3 & 0.3 & 0.3 \\\\[6pt]\n",
    "0.2 & 0.1 & 0.5 & 0.4\n",
    "\\end{bmatrix}, \\quad\n",
    "W_K = \\begin{bmatrix}\n",
    "0.1 & 0.4 & 0.0 & 0.0 \\\\[6pt]\n",
    "0.0 & 0.5 & 0.2 & 0.1 \\\\[6pt]\n",
    "0.3 & 0.0 & 0.3 & 0.3 \\\\[6pt]\n",
    "0.2 & 0.2 & 0.1 & 0.0\n",
    "\\end{bmatrix}, \\quad\n",
    "W_V = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 & 0.0 \\\\[6pt]\n",
    "0.0 & 0.3 & 0.5 & 0.0 \\\\[6pt]\n",
    "0.1 & 0.1 & 0.1 & 0.4 \\\\[6pt]\n",
    "0.0 & 0.2 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step-by-Step Computation\n",
    "\n",
    "#### 1. Compute Q, K, V\n",
    "\n",
    "- $Q = XW_Q$:\n",
    "Let's multiply $X$ (3x4) by $W_Q$ (4x4):\n",
    "\n",
    "For the first token \"The\":\n",
    "$$\n",
    "Q_1 = [0.1, 0.4, 0.5, 0.7]\n",
    "\\begin{bmatrix}\n",
    "  0.5 & 0.1 & 0.0 & 0.3 \\\\\n",
    "  0.4 & 0.2 & 0.1 & 0.0 \\\\\n",
    "  0.3 & 0.3 & 0.3 & 0.3 \\\\\n",
    "  0.2 & 0.1 & 0.5 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute row-by-column:\n",
    "- $Q_1[1] = 0.1*0.5 + 0.4*0.4 + 0.5*0.3 + 0.7*0.2 = 0.05 + 0.16 + 0.15 + 0.14 = 0.5$\n",
    "- $Q_1[2] = 0.1*0.1 + 0.4*0.2 + 0.5*0.3 + 0.7*0.1 = 0.01 + 0.08 + 0.15 + 0.07 = 0.31$\n",
    "- $Q_1[3] = 0.1*0.0 + 0.4*0.1 + 0.5*0.3 + 0.7*0.5 = 0 + 0.04 + 0.15 + 0.35 = 0.54$\n",
    "- $Q_1[4] = 0.1*0.3 + 0.4*0.0 + 0.5*0.3 + 0.7*0.4 = 0.03 + 0 + 0.15 + 0.28 = 0.46$\n",
    "\n",
    "So $Q_1 = [0.5, 0.31, 0.54, 0.46]$.\n",
    "\n",
    "For the second token \"cat\":\n",
    "$$\n",
    "Q_2 = [0.2, 0.4, 0.5, 0.6]W_Q\n",
    "$$\n",
    "- $Q_2[1] = 0.2*0.5 + 0.4*0.4 + 0.5*0.3 + 0.6*0.2 = 0.1 + 0.16 + 0.15 + 0.12 = 0.53$\n",
    "- $Q_2[2] = 0.2*0.1 + 0.4*0.2 + 0.5*0.3 + 0.6*0.1 = 0.02 + 0.08 + 0.15 + 0.06 = 0.31$\n",
    "- $Q_2[3] = 0.2*0.0 + 0.4*0.1 + 0.5*0.3 + 0.6*0.5 = 0 + 0.04 + 0.15 + 0.3 = 0.49$\n",
    "- $Q_2[4] = 0.2*0.3 + 0.4*0.0 + 0.5*0.3 + 0.6*0.4 = 0.06 + 0 + 0.15 + 0.24 = 0.45$\n",
    "\n",
    "So $Q_2 = [0.53, 0.31, 0.49, 0.45]$.\n",
    "\n",
    "For the third token \"sat\":\n",
    "$$\n",
    "Q_3 = [0.15, 0.25, 0.5, 0.2]W_Q\n",
    "$$\n",
    "- $Q_3[1] = 0.15*0.5 + 0.25*0.4 + 0.5*0.3 + 0.2*0.2 = 0.075 + 0.1 + 0.15 + 0.04 = 0.365$\n",
    "- $Q_3[2] = 0.15*0.1 + 0.25*0.2 + 0.5*0.3 + 0.2*0.1 = 0.015 + 0.05 + 0.15 + 0.02 = 0.235$\n",
    "- $Q_3[3] = 0.15*0.0 + 0.25*0.1 + 0.5*0.3 + 0.2*0.5 = 0 + 0.025 + 0.15 + 0.1 = 0.275$\n",
    "- $Q_3[4] = 0.15*0.3 + 0.25*0.0 + 0.5*0.3 + 0.2*0.4 = 0.045 + 0 + 0.15 + 0.08 = 0.275$\n",
    "\n",
    "So $Q_3 = [0.365, 0.235, 0.275, 0.275]$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "  0.5 & 0.31 & 0.54 & 0.46 \\\\[4pt]\n",
    "  0.53 & 0.31 & 0.49 & 0.45 \\\\[4pt]\n",
    "  0.365 & 0.235 & 0.275 & 0.275\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $K = XW_K$:\n",
    "Similarly, multiply $X$ by $W_K$.\n",
    "\n",
    "For \"The\":\n",
    "$$\n",
    "K_1 = [0.1,0.4,0.5,0.7]W_K\n",
    "$$\n",
    "Compute:\n",
    "- $K_1[1] = 0.1*0.1 + 0.4*0.0 + 0.5*0.3 + 0.7*0.2 = 0.01+0+0.15+0.14=0.3$\n",
    "- $K_1[2] = 0.1*0.4 + 0.4*0.5 + 0.5*0.0 + 0.7*0.2 = 0.04+0.2+0+0.14=0.38$\n",
    "- $K_1[3] = 0.1*0.0 + 0.4*0.2 + 0.5*0.3 + 0.7*0.1 = 0+0.08+0.15+0.07=0.3$\n",
    "- $K_1[4] = 0.1*0.0 + 0.4*0.1 + 0.5*0.3 + 0.7*0.0 = 0+0.04+0.15+0=0.19$\n",
    "\n",
    "$K_1 = [0.3, 0.38, 0.3, 0.19]$.\n",
    "\n",
    "For \"cat\":\n",
    "$$\n",
    "K_2 = [0.2,0.4,0.5,0.6]W_K\n",
    "$$\n",
    "- $K_2[1] = 0.2*0.1 + 0.4*0.0 + 0.5*0.3 + 0.6*0.2 = 0.02+0+0.15+0.12=0.29$\n",
    "- $K_2[2] = 0.2*0.4 + 0.4*0.5 + 0.5*0.0 + 0.6*0.2 = 0.08+0.2+0+0.12=0.4$\n",
    "- $K_2[3] = 0.2*0.0 + 0.4*0.2 + 0.5*0.3 + 0.6*0.1 = 0+0.08+0.15+0.06=0.29$\n",
    "- $K_2[4] = 0.2*0.0 + 0.4*0.1 + 0.5*0.3 + 0.6*0.0 = 0+0.04+0.15+0=0.19$\n",
    "\n",
    "$K_2 = [0.29, 0.4, 0.29, 0.19]$.\n",
    "\n",
    "For \"sat\":\n",
    "$$\n",
    "K_3 = [0.15,0.25,0.5,0.2]W_K\n",
    "$$\n",
    "- $K_3[1] = 0.15*0.1 + 0.25*0.0 + 0.5*0.3 + 0.2*0.2 = 0.015+0+0.15+0.04=0.205$\n",
    "- $K_3[2] = 0.15*0.4 + 0.25*0.5 + 0.5*0.0 + 0.2*0.2 = 0.06+0.125+0+0.04=0.225$\n",
    "- $K_3[3] = 0.15*0.0 + 0.25*0.2 + 0.5*0.3 + 0.2*0.1 = 0+0.05+0.15+0.02=0.22$\n",
    "- $K_3[4] = 0.15*0.0 + 0.25*0.1 + 0.5*0.3 + 0.2*0.0 = 0+0.025+0.15+0=0.175$\n",
    "\n",
    "$K_3 = [0.205,0.225,0.22,0.175]$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "  0.3 & 0.38 & 0.3 & 0.19 \\\\[4pt]\n",
    "  0.29 & 0.4 & 0.29 & 0.19 \\\\[4pt]\n",
    "  0.205 & 0.225 & 0.22 & 0.175\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $V = XW_V$:\n",
    "Similarly:\n",
    "\n",
    "For \"The\":\n",
    "- $V_1[1] = 0.1*0.2 + 0.4*0.0 + 0.5*0.1 + 0.7*0.0 = 0.02+0+0.05+0=0.07$\n",
    "- $V_1[2] = 0.1*0.1 + 0.4*0.3 + 0.5*0.1 + 0.7*0.2 = 0.01+0.12+0.05+0.14=0.32$\n",
    "- $V_1[3] = 0.1*0.0 + 0.4*0.5 + 0.5*0.1 + 0.7*0.1 = 0+0.2+0.05+0.07=0.32$\n",
    "- $V_1[4] = 0.1*0.0 + 0.4*0.0 + 0.5*0.4 + 0.7*0.1 = 0+0+0.2+0.07=0.27$\n",
    "\n",
    "$V_1 = [0.07, 0.32, 0.32, 0.27]$.\n",
    "\n",
    "For \"cat\":\n",
    "- $V_2[1] = 0.2*0.2 + 0.4*0.0 + 0.5*0.1 + 0.6*0.0 = 0.04+0+0.05+0=0.09$\n",
    "- $V_2[2] = 0.2*0.1 + 0.4*0.3 + 0.5*0.1 + 0.6*0.2 = 0.02+0.12+0.05+0.12=0.31$\n",
    "- $V_2[3] = 0.2*0.0 + 0.4*0.5 + 0.5*0.1 + 0.6*0.1 = 0+0.2+0.05+0.06=0.31$\n",
    "- $V_2[4] = 0.2*0.0 + 0.4*0.0 + 0.5*0.4 + 0.6*0.1 = 0+0+0.2+0.06=0.26$\n",
    "\n",
    "$V_2 = [0.09,0.31,0.31,0.26]$.\n",
    "\n",
    "For \"sat\":\n",
    "- $V_3[1] = 0.15*0.2 + 0.25*0.0 + 0.5*0.1 + 0.2*0.0 = 0.03+0+0.05+0=0.08$\n",
    "- $V_3[2] = 0.15*0.1 + 0.25*0.3 + 0.5*0.1 + 0.2*0.2 = 0.015+0.075+0.05+0.04=0.18$\n",
    "- $V_3[3] = 0.15*0.0 + 0.25*0.5 + 0.5*0.1 + 0.2*0.1 = 0+0.125+0.05+0.02=0.195$\n",
    "- $V_3[4] = 0.15*0.0 + 0.25*0.0 + 0.5*0.4 + 0.2*0.1 = 0+0+0.2+0.02=0.22$\n",
    "\n",
    "$V_3 = [0.08,0.18,0.195,0.22]$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "V = \\begin{bmatrix}\n",
    "  0.07 & 0.32 & 0.32 & 0.27 \\\\[4pt]\n",
    "  0.09 & 0.31 & 0.31 & 0.26 \\\\[4pt]\n",
    "  0.08 & 0.18 & 0.195 & 0.22\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. Compute the Attention Scores\n",
    "\n",
    "$$\n",
    "\\text{Scores} = QK^T\n",
    "$$\n",
    "\n",
    "- Dimension check: $Q \\in \\mathbb{R}^{3 \\times 4}, K \\in \\mathbb{R}^{3 \\times 4}$, so $K^T \\in \\mathbb{R}^{4 \\times 3}$. Thus, $\\text{Scores} \\in \\mathbb{R}^{3 \\times 3}$.\n",
    "\n",
    "Compute $\\text{Scores}[i,j]$ = $Q_i \\cdot K_j$:\n",
    "\n",
    "- $\\text{Scores}[1,1] = Q_1 \\cdot K_1 = [0.5*0.3 + 0.31*0.38 + 0.54*0.3 + 0.46*0.19]$\n",
    "= $0.15 + 0.1178 + 0.162 + 0.0874 = 0.5172$\n",
    "\n",
    "- $\\text{Scores}[1,2] = Q_1 \\cdot K_2 = [0.5*0.29 + 0.31*0.4 + 0.54*0.29 + 0.46*0.19]$\n",
    "= $0.145 + 0.124 + 0.1566 + 0.0874 = 0.513$\n",
    "\n",
    "- $\\text{Scores}[1,3] = Q_1 \\cdot K_3 = [0.5*0.205 + 0.31*0.225 + 0.54*0.22 + 0.46*0.175]$\n",
    "= $0.1025 + 0.06975 + 0.1188 + 0.0805 = 0.37155$\n",
    "\n",
    "- $\\text{Scores}[2,1] = Q_2 \\cdot K_1$\n",
    "= $[0.53*0.3 + 0.31*0.38 + 0.49*0.3 + 0.45*0.19]$\n",
    "= $0.159 + 0.1178 + 0.147 + 0.0855 = 0.5093$\n",
    "\n",
    "- $\\text{Scores}[2,2] = Q_2 \\cdot K_2$\n",
    "= $[0.53*0.29 + 0.31*0.4 + 0.49*0.29 + 0.45*0.19]$\n",
    "= $0.1537 + 0.124 + 0.1421 + 0.0855 = 0.5053$\n",
    "\n",
    "- $\\text{Scores}[2,3] = Q_2 \\cdot K_3$\n",
    "= $[0.53*0.205 + 0.31*0.225 + 0.49*0.22 + 0.45*0.175]$\n",
    "= $0.10865 + 0.06975 + 0.1078 + 0.07875 = 0.36495$\n",
    "\n",
    "- $\\text{Scores}[3,1] = Q_3 \\cdot K_1$\n",
    "= $[0.365*0.3 + 0.235*0.38 + 0.275*0.3 + 0.275*0.19]$\n",
    "= $0.1095 + 0.0893 + 0.0825 + 0.05225 = 0.33355$\n",
    "\n",
    "- $\\text{Scores}[3,2] = Q_3 \\cdot K_2$\n",
    "= $[0.365*0.29 + 0.235*0.4 + 0.275*0.29 + 0.275*0.19]$\n",
    "= $0.10585 + 0.094 + 0.07975 + 0.05225 = 0.33185$\n",
    "\n",
    "- \\(\\text{Scores}[3,3] = Q_3 \\cdot K_3\\)\n",
    "= \\([0.365*0.205 + 0.235*0.225 + 0.275*0.22 + 0.275*0.175]\\)\n",
    "= \\(0.074825 + 0.052875 + 0.0605 + 0.048125 = 0.236325\\)\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\text{Scores} = \\begin{bmatrix}\n",
    "0.5172 & 0.513  & 0.37155 \\\\[4pt]\n",
    "0.5093 & 0.5053 & 0.36495 \\\\[4pt]\n",
    "0.33355 & 0.33185 & 0.236325\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 3. Scale the Scores\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\frac{\\text{Scores}}{\\sqrt{d_k}} = \\frac{\\text{Scores}}{\\sqrt{4}} = \\frac{\\text{Scores}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\begin{bmatrix}\n",
    "0.2586 & 0.2565 & 0.185775 \\\\[4pt]\n",
    "0.25465 & 0.25265 & 0.182475 \\\\[4pt]\n",
    "0.166775 & 0.165925 & 0.1181625\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 4. Apply Softmax to Each Row\n",
    "\n",
    "The **softmax** function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z} = (z_1, z_2, \\ldots, z_N)$ is a vector of real numbers and $N$ is the dimension of the vector $\\mathbf{z}$.\n",
    "\n",
    "\n",
    "For the first row:\n",
    "- Sum = $\\exp(0.2586) + \\exp(0.2565) + \\exp(0.185775)$\n",
    "- $\\exp(0.2586) \\approx 1.295$\n",
    "- $\\exp(0.2565) \\approx 1.292$\n",
    "- $\\exp(0.185775) \\approx 1.204$\n",
    "\n",
    "Sum ≈ 1.295 + 1.292 + 1.204 = 3.791\n",
    "\n",
    "So:\n",
    "- $A[1,1] = 1.295/3.791 ≈ 0.3418$\n",
    "- $A[1,2] = 1.292/3.791 ≈ 0.3408$\n",
    "- $A[1,3] = 1.204/3.791 ≈ 0.3177$\n",
    "\n",
    "For the second row:\n",
    "- $\\exp(0.25465) \\approx 1.290$\n",
    "- $\\exp(0.25265) \\approx 1.287$\n",
    "- $\\exp(0.182475) \\approx 1.200$\n",
    "\n",
    "Sum ≈ 1.290 + 1.287 + 1.200 = 3.777\n",
    "\n",
    "- $A[2,1] = 1.290/3.777 ≈ 0.3415$\n",
    "- $A[2,2] = 1.287/3.777 ≈ 0.3407$\n",
    "- $A[2,3] = 1.200/3.777 ≈ 0.3178$\n",
    "\n",
    "For the third row:\n",
    "- $\\exp(0.166775) \\approx 1.181$\n",
    "- $\\exp(0.165925) \\approx 1.180$\n",
    "- $\\exp(0.1181625) \\approx 1.1255$\n",
    "\n",
    "Sum ≈ 1.181 + 1.180 + 1.1255 = 3.4865\n",
    "\n",
    "- $A[3,1] = 1.181/3.4865 ≈ 0.3388$\n",
    "- $A[3,2] = 1.180/3.4865 ≈ 0.3385$\n",
    "- $A[3,3] = 1.1255/3.4865 ≈ 0.3227$\n",
    "\n",
    "So our attention weight matrix $A$ is approximately:\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0.3418 & 0.3408 & 0.3177 \\\\[4pt]\n",
    "0.3415 & 0.3407 & 0.3178 \\\\[4pt]\n",
    "0.3388 & 0.3385 & 0.3227\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 5. Compute the Final Output\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} = A V\n",
    "$$\n",
    "\n",
    "- Dimension: $A \\in \\mathbb{R}^{3 \\times 3}, V \\in \\mathbb{R}^{3 \\times 4}$, resulting in $\\text{AttOutput} \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "\n",
    "For each row $i$:\n",
    "$$\n",
    "\\text{AttOutput}_i = \\sum_{j=1}^{3} A[i,j] V_j\n",
    "$$\n",
    "\n",
    "- For the first token:\n",
    "$$\n",
    "\\text{AttOutput}_1 = 0.3418V_1 + 0.3408V_2 + 0.3177V_3\n",
    "$$\n",
    "Recall:\n",
    "- $V_1 = [0.07,0.32,0.32,0.27]$\n",
    "- $V_2 = [0.09,0.31,0.31,0.26]$\n",
    "- $V_3 = [0.08,0.18,0.195,0.22]$\n",
    "\n",
    "Compute component-wise:\n",
    "- Dim1: $0.3418*0.07 + 0.3408*0.09 + 0.3177*0.08 = 0.0233 + 0.0307 + 0.0254 = 0.0794$\n",
    "- Dim2: $0.3418*0.32 + 0.3408*0.31 + 0.3177*0.18 = 0.1094 + 0.1056 + 0.0572 = 0.2722$\n",
    "- Dim3: $0.3418*0.32 + 0.3408*0.31 + 0.3177*0.195 = 0.1094 + 0.1056 + 0.0629 = 0.2779$\n",
    "- Dim4: $0.3418*0.27 + 0.3408*0.26 + 0.3177*0.22 = 0.0923 + 0.0886 + 0.0699 = 0.2508$\n",
    "\n",
    "$\\text{AttOutput}_1 = [0.0794, 0.2722, 0.2779, 0.2508]$\n",
    "\n",
    "- For the second token:\n",
    "$$\n",
    "\\text{AttOutput}_2 = 0.3415V_1 + 0.3407V_2 + 0.3178V_3\n",
    "$$\n",
    "Repeat similarly:\n",
    "- Dim1: $0.3415*0.07 + 0.3407*0.09 + 0.3178*0.08 = 0.023105 + 0.030663 + 0.025424 = 0.079192$\n",
    "- Dim2: $0.3415*0.32 + 0.3407*0.31 + 0.3178*0.18 = 0.10928 + 0.105617 + 0.057204 = 0.272101$\n",
    "- Dim3: $0.3415*0.32 + 0.3407*0.31 + 0.3178*0.195 = 0.10928 + 0.105617 + 0.062971 = 0.277868$\n",
    "- Dim4: $0.3415*0.27 + 0.3407*0.26 + 0.3178*0.22 = 0.092205 + 0.088582 + 0.069916 = 0.250703$\n",
    "\n",
    "$\\text{AttOutput}_2 \\approx [0.0792, 0.2721, 0.2779, 0.2507]$\n",
    "\n",
    "- For the third token:\n",
    "$$\n",
    "\\text{AttOutput}_3 = 0.3388V_1 + 0.3385V_2 + 0.3227V_3\n",
    "$$\n",
    "- Dim1: $0.3388*0.07 + 0.3385*0.09 + 0.3227*0.08 = 0.023716 + 0.030465 + 0.025816 = 0.079997$\n",
    "- Dim2: $0.3388*0.32 + 0.3385*0.31 + 0.3227*0.18 = 0.108416 + 0.104935 + 0.058086 = 0.271437$\n",
    "- Dim3: $0.3388*0.32 + 0.3385*0.31 + 0.3227*0.195 = 0.108416 + 0.104935 + 0.062933 = 0.276284$\n",
    "- Dim4: $0.3388*0.27 + 0.3385*0.26 + 0.3227*0.22 = 0.091476 + 0.08801 + 0.071 = 0.250486$\n",
    "\n",
    "$\\text{AttOutput}_3 \\approx [0.08, 0.2714, 0.2763, 0.2505]$\n",
    "\n",
    "Final $\\text{AttOutput}$:\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\[4pt]\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\[4pt]\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- Each row of $\\text{AttOutput}$ is the transformed representation of the corresponding token after attending to all tokens in the sequence (including itself).\n",
    "- Notice that the rows are fairly similar, which reflects the similarity in the queries and keys for this small artificial example. In a more complex and varied sequence, these values would differ more significantly.\n",
    "- In practice, multiple heads are used, and their outputs are concatenated to capture various patterns. Here, we demonstrated just a single-head scenario.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Decoder Architecture in Transformers\n",
    "#### 6. Apply Masking\n",
    "\n",
    "**Masking** is used to prevent attention to certain positions. In this example, we will apply a **causal mask** to prevent each token from attending to future tokens. This is particularly useful in decoder architectures to maintain the autoregressive property.\n",
    "\n",
    "- **Causal Mask Matrix ($M$):**\n",
    "\n",
    "  The causal mask ensures that each position can only attend to itself and previous positions. For our 3-token sequence:\n",
    "\n",
    "  $$\n",
    "  M = \\begin{bmatrix}\n",
    "  0 & -\\infty & -\\infty \\\\[4pt]\n",
    "  0 & 0 & -\\infty \\\\[4pt]\n",
    "  0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  - $0$ allows attention.\n",
    "  - $-\\infty$ effectively masks out the position by making its softmax probability zero.\n",
    "\n",
    "- **Apply Mask to Scores:**\n",
    "\n",
    "  $$\n",
    "  \\text{Masked Scores} = \\text{Scores} + M\n",
    "  $$\n",
    "\n",
    "  Performing element-wise addition:\n",
    "\n",
    "  $$\n",
    "  \\text{Masked Scores} = \\begin{bmatrix}\n",
    "  0.5172 & 0.5130 & -\\infty \\\\[4pt]\n",
    "  0.5093 & 0.5053 & -\\infty \\\\[4pt]\n",
    "  0.33355 & 0.33185 & 0.236325\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  **Explanation:**  \n",
    "  - For the first token (\"The\"), it cannot attend to the third token (\"sat\"), hence $-\\infty$.\n",
    "  - For the second token (\"cat\"), it cannot attend to the third token (\"sat\"), hence $-\\infty$.\n",
    "  - The third token (\"sat\") can attend to all tokens, including itself.\n",
    "\n",
    "#### 7. Scale the Scores\n",
    "\n",
    "Scaling helps in stabilizing gradients during training.\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\frac{\\text{Masked Scores}}{\\sqrt{d_k}} = \\frac{\\text{Masked Scores}}{\\sqrt{4}} = \\frac{\\text{Masked Scores}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\begin{bmatrix}\n",
    "0.2586 & 0.2565 & -\\infty \\\\[4pt]\n",
    "0.25465 & 0.25265 & -\\infty \\\\[4pt]\n",
    "0.166775 & 0.165925 & 0.1181625\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 8. Apply Softmax to Each Row\n",
    "\n",
    "The **softmax** function converts the scaled scores into probabilities.\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "**Applying Softmax:**\n",
    "\n",
    "- **First Row:**\n",
    "  $$\n",
    "  \\mathbf{z}_1 = [0.2586, 0.2565, -\\infty]\n",
    "  $$\n",
    "  - $\\exp(0.2586) \\approx 1.295$\n",
    "  - $\\exp(0.2565) \\approx 1.292$\n",
    "  - $\\exp(-\\infty) = 0$\n",
    "\n",
    "  Sum: $1.295 + 1.292 + 0 = 2.587$\n",
    "\n",
    "  Softmax:\n",
    "  $$\n",
    "  A[1,1] = \\frac{1.295}{2.587} \\approx 0.500 \\\\\n",
    "  A[1,2] = \\frac{1.292}{2.587} \\approx 0.500 \\\\\n",
    "  A[1,3] = \\frac{0}{2.587} = 0.0\n",
    "  $$\n",
    "\n",
    "- **Second Row:**\n",
    "  $$\n",
    "  \\mathbf{z}_2 = [0.25465, 0.25265, -\\infty]\n",
    "  $$\n",
    "  - $\\exp(0.25465) \\approx 1.290$\n",
    "  - $\\exp(0.25265) \\approx 1.287$\n",
    "  - $\\exp(-\\infty) = 0$\n",
    "\n",
    "  Sum: $1.290 + 1.287 + 0 = 2.577$\n",
    "\n",
    "  Softmax:\n",
    "  $$\n",
    "  A[2,1] = \\frac{1.290}{2.577} \\approx 0.500 \\\\\n",
    "  A[2,2] = \\frac{1.287}{2.577} \\approx 0.500 \\\\\n",
    "  A[2,3] = \\frac{0}{2.577} = 0.0\n",
    "  $$\n",
    "\n",
    "- **Third Row:**\n",
    "  $$\n",
    "  \\mathbf{z}_3 = [0.166775, 0.165925, 0.1181625]\n",
    "  $$\n",
    "  - $\\exp(0.166775) \\approx 1.181$\n",
    "  - $\\exp(0.165925) \\approx 1.180$\n",
    "  - $\\exp(0.1181625) \\approx 1.1255$\n",
    "\n",
    "  Sum: $1.181 + 1.180 + 1.1255 = 3.4865$\n",
    "\n",
    "  Softmax:\n",
    "  $$\n",
    "  A[3,1] = \\frac{1.181}{3.4865} \\approx 0.339 \\\\\n",
    "  A[3,2] = \\frac{1.180}{3.4865} \\approx 0.339 \\\\\n",
    "  A[3,3] = \\frac{1.1255}{3.4865} \\approx 0.323\n",
    "  $$\n",
    "\n",
    "Thus, our attention weight matrix $A$ is approximately:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0.500 & 0.500 & 0.0 \\\\[4pt]\n",
    "0.500 & 0.500 & 0.0 \\\\[4pt]\n",
    "0.339 & 0.339 & 0.323\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Effect of Masking:**\n",
    "- **First Token (\"The\"):** Can only attend to itself and \"cat\". No attention to \"sat\".\n",
    "- **Second Token (\"cat\"):** Can only attend to itself and \"The\". No attention to \"sat\".\n",
    "- **Third Token (\"sat\"):** Can attend to all tokens, including itself.\n",
    "\n",
    "#### 6. Compute the Final Output\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} = A V\n",
    "$$\n",
    "\n",
    "- **Dimension Check:**  \n",
    "  $A \\in \\mathbb{R}^{3 \\times 3}$, $V \\in \\mathbb{R}^{3 \\times 4}$, so $\\text{AttOutput} \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "\n",
    "- **Calculation:**\n",
    "\n",
    "  - **For the first token (\"The\"):**\n",
    "    $$\n",
    "    \\text{AttOutput}_1 = 0.500 \\times V_1 + 0.500 \\times V_2 + 0.0 \\times V_3 \\\\\n",
    "    = 0.500 \\times [0.07, 0.32, 0.32, 0.27] + 0.500 \\times [0.09, 0.31, 0.31, 0.26] + 0.0 \\times [0.08, 0.18, 0.195, 0.22] \\\\\n",
    "    = [0.035, 0.16, 0.16, 0.135] + [0.045, 0.155, 0.155, 0.13] + [0.0, 0.0, 0.0, 0.0] \\\\\n",
    "    = [0.080, 0.315, 0.315, 0.265]\n",
    "    $$\n",
    "\n",
    "  - **For the second token (\"cat\"):**\n",
    "    $$\n",
    "    \\text{AttOutput}_2 = 0.500 \\times V_1 + 0.500 \\times V_2 + 0.0 \\times V_3 \\\\\n",
    "    = 0.500 \\times [0.07, 0.32, 0.32, 0.27] + 0.500 \\times [0.09, 0.31, 0.31, 0.26] + 0.0 \\times [0.08, 0.18, 0.195, 0.22] \\\\\n",
    "    = [0.035, 0.16, 0.16, 0.135] + [0.045, 0.155, 0.155, 0.13] + [0.0, 0.0, 0.0, 0.0] \\\\\n",
    "    = [0.080, 0.315, 0.315, 0.265]\n",
    "    $$\n",
    "\n",
    "  - **For the third token (\"sat\"):**\n",
    "    $$\n",
    "    \\text{AttOutput}_3 = 0.339 \\times V_1 + 0.339 \\times V_2 + 0.323 \\times V_3 \\\\\n",
    "    = 0.339 \\times [0.07, 0.32, 0.32, 0.27] + 0.339 \\times [0.09, 0.31, 0.31, 0.26] + 0.323 \\times [0.08, 0.18, 0.195, 0.22] \\\\\n",
    "    = [0.02373, 0.10848, 0.10848, 0.07269] + [0.03051, 0.10509, 0.10509, 0.08814] + [0.02584, 0.05814, 0.06309, 0.07106] \\\\\n",
    "    = [0.07908, 0.27171, 0.27666, 0.23189]\n",
    "    $$\n",
    "\n",
    "Thus, the final attention output matrix $\\text{AttOutput}$ is approximately:\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.080 & 0.315 & 0.315 & 0.265 \\\\[4pt]\n",
    "0.080 & 0.315 & 0.315 & 0.265 \\\\[4pt]\n",
    "0.079 & 0.271 & 0.277 & 0.232\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **First Token (\"The\"):** Its representation is an average of \"The\" and \"cat\", ignoring \"sat\" due to the causal mask.\n",
    "- **Second Token (\"cat\"):** Similarly, it averages \"The\" and \"cat\".\n",
    "- **Third Token (\"sat\"):** It attends to all three tokens, incorporating information from \"The\", \"cat\", and itself.\n",
    "\n",
    "#### 7. Incorporate Multi-Head Attention (Optional)\n",
    "\n",
    "**Multi-Head Attention** allows the model to attend to information from different representation subspaces at different positions. Here's how to extend our example to multi-head attention.\n",
    "\n",
    "- **Assumptions:**\n",
    "  - Number of heads: $h = 2$\n",
    "  - Dimension per head: $d_k = d_v = d_{\\text{model}} / h = 2$\n",
    "\n",
    "- **Parameter Matrices for Each Head:**\n",
    "  For simplicity, we define separate $W_Q$, $W_K$, and $W_V$ for each head. Assume these are predefined.\n",
    "\n",
    "  **Head 1:**\n",
    "  $$\n",
    "  W_{Q}^{(1)} = \\begin{bmatrix}\n",
    "  0.5 & 0.1 \\\\\n",
    "  0.4 & 0.2 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.2 & 0.1\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{K}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.0 & 0.5 \\\\\n",
    "  0.3 & 0.0 \\\\\n",
    "  0.2 & 0.2\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{V}^{(1)} = \\begin{bmatrix}\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.1 \\\\\n",
    "  0.0 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  **Head 2:**\n",
    "  $$\n",
    "  W_{Q}^{(2)} = \\begin{bmatrix}\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.0 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.5 & 0.4\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{K}^{(2)} = \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.1 & 0.0\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{V}^{(2)} = \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.5 & 0.0 \\\\\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Compute Q, K, V for Each Head:**\n",
    "\n",
    "  **Head 1:**\n",
    "  $$\n",
    "  Q^{(1)} = X W_{Q}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.5 & 0.1 \\\\\n",
    "  0.4 & 0.2 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.2 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.5 & 0.31 \\\\\n",
    "  0.53 & 0.31 \\\\\n",
    "  0.365 & 0.235\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  K^{(1)} = X W_{K}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.0 & 0.5 \\\\\n",
    "  0.3 & 0.0 \\\\\n",
    "  0.2 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.3 & 0.38 \\\\\n",
    "  0.29 & 0.4 \\\\\n",
    "  0.205 & 0.225\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  V^{(1)} = X W_{V}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.1 \\\\\n",
    "  0.0 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.07 & 0.32 \\\\\n",
    "  0.09 & 0.31 \\\\\n",
    "  0.08 & 0.18\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  **Head 2:**\n",
    "  $$\n",
    "  Q^{(2)} = X W_{Q}^{(2)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.0 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.5 & 0.4\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.38 & 0.59 \\\\\n",
    "  0.39 & 0.57 \\\\\n",
    "  0.305 & 0.255\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  K^{(2)} = X W_{K}^{(2)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.1 & 0.0\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.1 & 0.13 \\\\\n",
    "  0.11 & 0.13 \\\\\n",
    "  0.125 & 0.11\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  V^{(2)} = X W_{V}^{(2)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.5 & 0.0 \\\\\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.05 & 0.04 \\\\\n",
    "  0.06 & 0.03 \\\\\n",
    "  0.07 & 0.07\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Compute Attention for Each Head:**\n",
    "\n",
    "  For each head, perform the same steps as single-head attention:\n",
    "  \n",
    "  1. Compute Scores: $Q^{(h)} {K^{(h)}}^T$\n",
    "  2. Apply Masking (if necessary)\n",
    "  3. Scale Scores\n",
    "  4. Apply Softmax\n",
    "  5. Compute Attention Output: $A^{(h)} V^{(h)}$\n",
    "\n",
    "  **Head 1:**\n",
    "\n",
    "  - **Scores:**\n",
    "    $$\n",
    "    \\text{Scores}^{(1)} = Q^{(1)} {K^{(1)}}^T = \\begin{bmatrix}\n",
    "    0.5 & 0.31 & 0.54 & 0.46 \\\\\n",
    "    0.53 & 0.31 & 0.49 & 0.45 \\\\\n",
    "    0.365 & 0.235 & 0.275 & 0.275\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.3 & 0.38 & 0.3 & 0.19 \\\\\n",
    "    0.29 & 0.4 & 0.29 & 0.19 \\\\\n",
    "    0.205 & 0.225 & 0.22 & 0.175\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.5172 & 0.5130 & 0.37155 \\\\\n",
    "    0.5093 & 0.5053 & 0.36495 \\\\\n",
    "    0.33355 & 0.33185 & 0.236325\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Apply Masking:**\n",
    "    $$\n",
    "    M = \\begin{bmatrix}\n",
    "    0 & -\\infty & -\\infty \\\\\n",
    "    0 & 0 & -\\infty \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\text{Masked Scores}^{(1)} = \\text{Scores}^{(1)} + M = \\begin{bmatrix}\n",
    "    0.5172 & 0.5130 & -\\infty \\\\\n",
    "    0.5093 & 0.5053 & -\\infty \\\\\n",
    "    0.33355 & 0.33185 & 0.236325\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Scale:**\n",
    "    $$\n",
    "    \\tilde{\\text{Scores}}^{(1)} = \\frac{\\text{Masked Scores}^{(1)}}{2} = \\begin{bmatrix}\n",
    "    0.2586 & 0.2565 & -\\infty \\\\\n",
    "    0.25465 & 0.25265 & -\\infty \\\\\n",
    "    0.166775 & 0.165925 & 0.1181625\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Softmax:**\n",
    "    $$\n",
    "    A^{(1)} = \\text{softmax}(\\tilde{\\text{Scores}}^{(1)}) = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Attention Output:**\n",
    "    $$\n",
    "    \\text{AttOutput}^{(1)} = A^{(1)} V^{(1)} = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.07 & 0.32 \\\\\n",
    "    0.09 & 0.31 \\\\\n",
    "    0.08 & 0.18\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.080 & 0.315 \\\\\n",
    "    0.080 & 0.315 \\\\\n",
    "    0.079 & 0.271\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  **Head 2:**\n",
    "\n",
    "  - **Scores:**\n",
    "    $$\n",
    "    \\text{Scores}^{(2)} = Q^{(2)} {K^{(2)}}^T = \\begin{bmatrix}\n",
    "    0.38 & 0.59 \\\\\n",
    "    0.39 & 0.57 \\\\\n",
    "    0.305 & 0.255\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.1 & 0.13 \\\\\n",
    "    0.11 & 0.13 \\\\\n",
    "    0.125 & 0.11\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.38*0.1 + 0.59*0.11 & 0.38*0.13 + 0.59*0.13 \\\\\n",
    "    0.39*0.1 + 0.57*0.11 & 0.39*0.13 + 0.57*0.13 \\\\\n",
    "    0.305*0.1 + 0.255*0.11 & 0.305*0.13 + 0.255*0.13\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.038 + 0.0649 & 0.0494 + 0.0767 \\\\\n",
    "    0.039 + 0.0627 & 0.0507 + 0.0741 \\\\\n",
    "    0.0305 + 0.02805 & 0.03965 + 0.03315\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.1029 & 0.1261 \\\\\n",
    "    0.1017 & 0.1248 \\\\\n",
    "    0.05855 & 0.0728\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Apply Masking:**\n",
    "    $$\n",
    "    M = \\begin{bmatrix}\n",
    "    0 & -\\infty & -\\infty \\\\\n",
    "    0 & 0 & -\\infty \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    Since $K^{(2)}$ has dimension $2$, and our sequence length is $3$, we need to adjust the mask accordingly. However, for simplicity, assume a similar causal mask applies:\n",
    "\n",
    "    $$\n",
    "    \\text{Masked Scores}^{(2)} = \\text{Scores}^{(2)} + M = \\begin{bmatrix}\n",
    "    0.1029 & 0.1261 & -\\infty \\\\\n",
    "    0.1017 & 0.1248 & -\\infty \\\\\n",
    "    0.05855 & 0.0728 & 0.0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Scale:**\n",
    "    $$\n",
    "    \\tilde{\\text{Scores}}^{(2)} = \\frac{\\text{Masked Scores}^{(2)}}{2} = \\begin{bmatrix}\n",
    "    0.05145 & 0.06305 & -\\infty \\\\\n",
    "    0.05085 & 0.0624 & -\\infty \\\\\n",
    "    0.029275 & 0.0364 & 0.0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Softmax:**\n",
    "    $$\n",
    "    A^{(2)} = \\text{softmax}(\\tilde{\\text{Scores}}^{(2)}) = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Attention Output:**\n",
    "    $$\n",
    "    \\text{AttOutput}^{(2)} = A^{(2)} V^{(2)} = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.05 & 0.04 \\\\\n",
    "    0.06 & 0.03 \\\\\n",
    "    0.07 & 0.07\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.500*0.05 + 0.500*0.06 & 0.500*0.04 + 0.500*0.03 \\\\\n",
    "    0.500*0.05 + 0.500*0.06 & 0.500*0.04 + 0.500*0.03 \\\\\n",
    "    0.339*0.05 + 0.339*0.06 + 0.323*0.07 & 0.339*0.04 + 0.339*0.03 + 0.323*0.07\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.01695 + 0.02034 + 0.02261 & 0.01356 + 0.01017 + 0.02261\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.0599 & 0.04634\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "- **Concatenate Heads and Project:**\n",
    "\n",
    "  After computing attention outputs for all heads, concatenate them:\n",
    "\n",
    "  $$\n",
    "  \\text{Concat} = [\\text{AttOutput}^{(1)}, \\text{AttOutput}^{(2)}] = \\begin{bmatrix}\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.079 & 0.271 & 0.0599 & 0.04634\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  Then apply a final linear projection:\n",
    "\n",
    "  Let’s define $W_O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d_{\\text{model}}}$ as:\n",
    "\n",
    "  $$\n",
    "  W_O = \\begin{bmatrix}\n",
    "  0.1 & 0.0 & 0.2 & 0.1 \\\\\n",
    "  0.0 & 0.1 & 0.0 & 0.2 \\\\\n",
    "  0.3 & 0.1 & 0.0 & 0.0 \\\\\n",
    "  0.0 & 0.2 & 0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  Compute the final output:\n",
    "\n",
    "  $$\n",
    "  \\text{FinalOutput} = \\text{Concat} \\cdot W_O = \\begin{bmatrix}\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.079 & 0.271 & 0.0599 & 0.04634\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.0 & 0.2 & 0.1 \\\\\n",
    "  0.0 & 0.1 & 0.0 & 0.2 \\\\\n",
    "  0.3 & 0.1 & 0.0 & 0.0 \\\\\n",
    "  0.0 & 0.2 & 0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.080*0.1 + 0.315*0.0 + 0.055*0.3 + 0.035*0.0 & \\ldots \\\\\n",
    "  0.080*0.1 + 0.315*0.0 + 0.055*0.3 + 0.035*0.0 & \\ldots \\\\\n",
    "  0.079*0.1 + 0.271*0.0 + 0.0599*0.3 + 0.04634*0.0 & \\ldots\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  (Complete the matrix multiplication as needed.)\n",
    "\n",
    "#### 8. Final Representation\n",
    "\n",
    "The final output represents the attended information for each token, enriched by multiple attention heads capturing diverse patterns.\n",
    "\n",
    "## Summary of Enhanced Steps\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - Create input embeddings and add positional encodings.\n",
    "\n",
    "2. **Linear Projections:**\n",
    "   - Compute $Q = XW_Q$, $K = XW_K$, $V = XW_V$ for each head.\n",
    "\n",
    "3. **Attention Scores:**\n",
    "   - Compute $\\text{Scores} = QK^T$.\n",
    "\n",
    "4. **Apply Masking:**\n",
    "   - Add mask matrix $M$ to $\\text{Scores}$ to obtain $\\text{Masked Scores}$.\n",
    "\n",
    "5. **Scaling:**\n",
    "   - Scale the scores by $\\sqrt{d_k}$.\n",
    "\n",
    "6. **Softmax:**\n",
    "   - Apply softmax to obtain attention weights $A$.\n",
    "\n",
    "7. **Attention Output:**\n",
    "   - Compute $\\text{AttOutput} = A V$.\n",
    "\n",
    "8. **Multi-Head Concatenation (if applicable):**\n",
    "   - Concatenate outputs from all heads and apply final linear projection.\n",
    "\n",
    "9. **Final Representation:**\n",
    "   - The final output represents the attended information for each token.\n",
    "\n",
    "\n",
    "\n",
    "## Summary of Steps\n",
    "\n",
    "1. Took the input sequence and created $X$.\n",
    "2. Computed $Q = XW_Q$, $K = XW_K$, $V = XW_V$.\n",
    "3. Calculated $\\text{Scores} = QK^T$, then scaled them.\n",
    "4. Applied softmax to get attention weights $A$.\n",
    "5. Computed the final output as $A V$.\n",
    "\n",
    "This step-by-step example shows how the linear algebra operations translate into the final attended representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Inference Steps for a Transformer-based Language Model\n",
    "\n",
    "We have obtained the final attention output (`AttOutput`) for a sequence. Recall the final `AttOutput` (for a single layer or after the attention mechanism) is:\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\[4pt]\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\[4pt]\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This represents the transformed representations of each token in the input sequence after applying attention. Each row corresponds to a token position in the sequence, and each column is one of the model’s hidden dimensions $(d_{\\text{model}} = 4$ in this example).\n",
    "\n",
    "## From AttOutput to Predictions\n",
    "\n",
    "To make predictions about the next token, the model uses a final linear layer and a softmax to produce a probability distribution over the vocabulary.\n",
    "\n",
    "### Final Linear Projection\n",
    "\n",
    "Suppose we have a vocabulary $\\mathcal{V}$ of size $|\\mathcal{V}| = V$. We introduce a parameter matrix:\n",
    "$$\n",
    "W_{\\text{out}} \\in \\mathbb{R}^{d_{\\text{model}} \\times V}\n",
    "$$\n",
    "For our example, let’s assume $V = 5$ (a tiny vocabulary for demonstration) and $d_{\\text{model}} = 4$.\n",
    "\n",
    "A single row of `AttOutput` (say the last token’s representation) is multiplied by $W_{\\text{out}}$ to produce logits (unnormalized scores) for the next token:\n",
    "$$\n",
    "Z_t = \\text{AttOutput}_t W_{\\text{out}}\n",
    "$$\n",
    "$\\text{AttOutput}_t \\in \\mathbb{R}^{1 \\times 4}$ and $W_{\\text{out}} \\in \\mathbb{R}^{4 \\times 5}$, thus $Z_t \\in \\mathbb{R}^{1 \\times 5}$.\n",
    "\n",
    "For the sake of example, let’s pick the last row of AttOutput:\n",
    "$$\n",
    "\\text{AttOutput}_3 = [0.08, 0.2714, 0.2763, 0.2505].\n",
    "$$\n",
    "\n",
    "Assume:\n",
    "$$\n",
    "W_{\\text{out}} = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 & 0.3 & 0.1 \\\\[4pt]\n",
    "0.0 & 0.5 & 0.1 & 0.0 & 0.2 \\\\[4pt]\n",
    "0.1 & 0.1 & 0.3 & 0.3 & 0.1 \\\\[4pt]\n",
    "0.2 & 0.2 & 0.0 & 0.1 & 0.0 \n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute $Z_3$:\n",
    "$$\n",
    "Z_3 = \\text{AttOutput}_3 W_{\\text{out}}\n",
    "$$\n",
    "\n",
    "Component-wise:\n",
    "- $Z_3[1] = 0.08*0.2 + 0.2714*0.0 + 0.2763*0.1 + 0.2505*0.2 = 0.016 + 0 + 0.02763 + 0.0501 = 0.09373$\n",
    "- $Z_3[2] = 0.08*0.1 + 0.2714*0.5 + 0.2763*0.1 + 0.2505*0.2 = 0.008 + 0.1357 + 0.02763 + 0.0501 = 0.22143$\n",
    "- $Z_3[3] = 0.08*0.0 + 0.2714*0.1 + 0.2763*0.3 + 0.2505*0.0 = 0 + 0.02714 + 0.08289 + 0 = 0.11003$\n",
    "- $Z_3[4] = 0.08*0.3 + 0.2714*0.0 + 0.2763*0.3 + 0.2505*0.1 = 0.024 + 0 + 0.08289 + 0.02505 = 0.13194$\n",
    "- $Z_3[5] = 0.08*0.1 + 0.2714*0.2 + 0.2763*0.1 + 0.2505*0.0 = 0.008 + 0.05428 + 0.02763 + 0 = 0.08991$\n",
    "\n",
    "So:\n",
    "$$\n",
    "Z_3 \\approx [0.09373, \\; 0.22143, \\; 0.11003, \\; 0.13194, \\; 0.08991].\n",
    "$$\n",
    "\n",
    "### Softmax for Next-Token Prediction\n",
    "\n",
    "The probability distribution for the next token given the first three tokens is:\n",
    "$$\n",
    "p_{\\theta}(w_4 \\mid w_1, w_2, w_3) = \\text{softmax}(Z_3).\n",
    "$$\n",
    "\n",
    "Compute softmax:\n",
    "$$\n",
    "\\exp(Z_3[1]) = \\exp(0.09373) \\approx 1.0981\n",
    "$$\n",
    "$$\n",
    "\\exp(Z_3[2]) = \\exp(0.22143) \\approx 1.2479\n",
    "$$\n",
    "$$\n",
    "\\exp(Z_3[3]) = \\exp(0.11003) \\approx 1.1162\n",
    "$$\n",
    "$$\n",
    "\\exp(Z_3[4]) = \\exp(0.13194) \\approx 1.1410\n",
    "$$\n",
    "$$\n",
    "\\exp(Z_3[5]) = \\exp(0.08991) \\approx 1.0940\n",
    "$$\n",
    "\n",
    "Sum these:\n",
    "$$\n",
    "S = 1.0981 + 1.2479 + 1.1162 + 1.1410 + 1.0940 \\approx 5.6972\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "p_{\\theta}(w_4 = i \\mid w_{1:3}) = \\frac{\\exp(Z_3[i])}{S}\n",
    "$$\n",
    "\n",
    "- $p_{\\theta}(w_4=1) \\approx 1.0981 / 5.6972 \\approx 0.1927$\n",
    "- $p_{\\theta}(w_4=2) \\approx 1.2479 / 5.6972 \\approx 0.2190$\n",
    "- $p_{\\theta}(w_4=3) \\approx 1.1162 / 5.6972 \\approx 0.1960$\n",
    "- $p_{\\theta}(w_4=4) \\approx 1.1410 / 5.6972 \\approx 0.2003$\n",
    "- $p_{\\theta}(w_4=5) \\approx 1.0940 / 5.6972 \\approx 0.1920$\n",
    "\n",
    "We have a probability distribution over the next token.\n",
    "\n",
    "## Target Labels and Loss Calculation\n",
    "\n",
    "In training, we have a ground truth next token. Suppose the correct next token (the label) is $w_4 = 2$. The one-hot target vector $y$ for this position is:\n",
    "$$\n",
    "y = [0, 1, 0, 0, 0].\n",
    "$$\n",
    "\n",
    "The model’s predicted distribution for $w_4$ is:\n",
    "$$\n",
    "\\hat{p} = [0.1927,\\;0.2190,\\;0.1960,\\;0.2003,\\;0.1920].\n",
    "$$\n",
    "\n",
    "The loss function commonly used is the cross-entropy:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\sum_{i=1}^{V} y_i \\log \\hat{p}_i.\n",
    "$$\n",
    "\n",
    "Since $y_i=0$ for all $i \\neq 2$ and $y_2=1$:\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\log(\\hat{p}_2) = -\\log(0.2190) \\approx 1.517.\n",
    "$$\n",
    "\n",
    "## Backpropagation and Parameter Updates\n",
    "\n",
    "To train the model, we compute the gradient of the loss $\\mathcal{L}(\\theta)$ with respect to all parameters ($\\theta$ includes $W_Q, W_K, W_V, W_{\\text{out}}$, and others).\n",
    "\n",
    "1. **Gradient with respect to Output Layer:**\n",
    "\n",
    "   For the output weights $W_{\\text{out}}$, the gradient is derived from:\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial Z_3[i]} = \\hat{p}_i - y_i.\n",
    "   $$\n",
    "\n",
    "   Here:\n",
    "   - For $i=2$ (correct token), $\\hat{p}_2 - y_2 = 0.2190 - 1 = -0.7810$.\n",
    "   - For other $i$, $\\hat{p}_i - 0 = \\hat{p}_i$.\n",
    "\n",
    "   Thus:\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial Z_3} = [0.1927,\\; -0.7810,\\; 0.1960,\\;0.2003,\\;0.1920].\n",
    "   $$\n",
    "\n",
    "   Then:\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}} = \\text{AttOutput}_3^T \\frac{\\partial \\mathcal{L}}{\\partial Z_3}\n",
    "   $$\n",
    "   where $\\text{AttOutput}_3 \\in \\mathbb{R}^{1 \\times 4}$ and $\\frac{\\partial \\mathcal{L}}{\\partial Z_3} \\in \\mathbb{R}^{1 \\times 5}$, so the resulting gradient is $\\in \\mathbb{R}^{4 \\times 5}$.\n",
    "\n",
    "   This will update $W_{\\text{out}}$.\n",
    "\n",
    "2. **Gradient flows backward:**\n",
    "   \n",
    "   The gradient also propagates back into `AttOutput_3`:\n",
    "   $$\n",
    "   \\frac{\\partial \\mathcal{L}}{\\partial \\text{AttOutput}_3} = \\frac{\\partial \\mathcal{L}}{\\partial Z_3} W_{\\text{out}}^T.\n",
    "   $$\n",
    "\n",
    "   From there, we continue backpropagation:\n",
    "   - Through the attention mechanism: $\\frac{\\partial \\mathcal{L}}{\\partial V}$, $\\frac{\\partial \\mathcal{L}}{\\partial A}$, and hence $\\frac{\\partial \\mathcal{L}}{\\partial Q}, \\frac{\\partial \\mathcal{L}}{\\partial K}$, and so on.\n",
    "   - Through $Q = XW_Q$, $K = XW_K$, $V = XW_V$, we get gradients w.r.t. $W_Q, W_K, W_V$.\n",
    "\n",
    "   Every parameter in the model receives a gradient signal that tells it how to adjust to reduce the loss.\n",
    "\n",
    "3. **Gradient Descent Update:**\n",
    "   \n",
    "   Once we have $\\frac{\\partial \\mathcal{L}}{\\partial \\theta}$, we perform a parameter update:\n",
    "   $$\n",
    "   \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta}\\mathcal{L}(\\theta),\n",
    "   $$\n",
    "   where $\\eta$ is the learning rate.\n",
    "\n",
    "   Over many training steps with many examples, the parameters $W_Q, W_K, W_V, W_{\\text{out}}$, and others, adjust so that the model becomes better at predicting correct next tokens, thus giving Q, K, and V their functional meaning.\n",
    "\n",
    "## Training Summary\n",
    "\n",
    "- We start with a forward pass that produces probability distributions over next tokens.\n",
    "- We compute the loss using the target token.\n",
    "- Backpropagate the loss to find gradients w.r.t. all parameters.\n",
    "- Use gradient descent (or Adam or another optimizer) to update parameters.\n",
    "- Repeat for many examples and epochs until the model converges.\n",
    "\n",
    "## Inference (After Training)\n",
    "\n",
    "After training, we fix the parameters $\\theta$. Given a prompt $(w_1, w_2, w_3)$:\n",
    "1. Compute `AttOutput` just like during training (forward pass only, no backprop).\n",
    "2. Compute $\\hat{p}(w_4 \\mid w_{1:3}) = \\text{softmax}(Z_3)$.\n",
    "3. Sample a token from the distribution. Suppose we pick the token with the highest probability or sample stochastically.\n",
    "4. Append the chosen token to the sequence and repeat until a termination condition.\n",
    "\n",
    "During inference, no gradients are computed, and no parameter updates are made. We simply use the learned parameters to generate new text.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detailed Training and Inference Process for a Transformer-based Language Model\n",
    "\n",
    "In the previous sections, we computed the attention output (`AttOutput`) for a given input sequence. Now, we will delve deeper into the training process of a Transformer-based language model, starting from the `AttOutput`, and progress through loss calculation, backpropagation, gradient descent, and finally inference. This explanation focuses on the mathematical foundations and step-by-step calculations involved.\n",
    "\n",
    "## Transformer Layer Overview\n",
    "\n",
    "A **Transformer layer** is the core component of Transformer-based models like GPT and BERT. Each Transformer layer comprises two main sub-layers:\n",
    "\n",
    "1. **Multi-Head Self-Attention Mechanism**\n",
    "2. **Position-Wise Feed-Forward Network (FFN)**\n",
    "\n",
    "Each sub-layer is followed by **residual connections** and **layer normalization** to stabilize and enhance training.\n",
    "\n",
    "### 1. Multi-Head Self-Attention\n",
    "\n",
    "The multi-head self-attention mechanism allows the model to focus on different parts of the input sequence simultaneously. Mathematically, for each head $ h $ in the multi-head attention:\n",
    "\n",
    "$$\n",
    "\\text{head}_h = \\text{Attention}(QW_Q^{(h)}, KW_K^{(h)}, VW_V^{(h)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ Q = XW_Q $\n",
    "- $ K = XW_K $\n",
    "- $ V = XW_V $\n",
    "- $ W_Q^{(h)}, W_K^{(h)}, W_V^{(h)} $ are the projection matrices for head $ h $\n",
    "\n",
    "The attention mechanism is defined as:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^\\top}{\\sqrt{d_k}}\\right)V\n",
    "$$\n",
    "\n",
    "Here, $ d_k $ is the dimensionality of the key vectors.\n",
    "\n",
    "After computing all heads, their outputs are concatenated and projected:\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\text{head}_2, \\ldots, \\text{head}_h)W_O\n",
    "$$\n",
    "\n",
    "Where $ W_O $ is the output projection matrix.\n",
    "\n",
    "### 2. Position-Wise Feed-Forward Network (FFN)\n",
    "\n",
    "After the multi-head attention, each Transformer layer applies a feed-forward neural network independently to each position:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(xW_1 + b_1)W_2 + b_2\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}} $\n",
    "- $ W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}} $\n",
    "- $ b_1, b_2 $ are bias vectors\n",
    "- $ d_{\\text{ff}} $ is the dimensionality of the feed-forward layer\n",
    "\n",
    "### 3. Residual Connections and Layer Normalization\n",
    "\n",
    "Each sub-layer is wrapped with residual connections and followed by layer normalization:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text{LayerNorm}(X + \\text{MultiHead}(Q, K, V)) \\\\\n",
    "& \\text{LayerNorm}(X + \\text{FFN}(X))\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This structure helps in mitigating issues like vanishing gradients and stabilizes the training process.\n",
    "\n",
    "## From `AttOutput` to Predictions\n",
    "\n",
    "Given the attention output matrix (`AttOutput`):\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "Each row corresponds to a token in the input sequence, and each column represents a hidden dimension ($d_{\\text{model}} = 4$ in this example).\n",
    "\n",
    "### 1. Position-Wise Feed-Forward Network (FFN)\n",
    "\n",
    "Assume simple FFN parameters for illustration:\n",
    "\n",
    "$$\n",
    "W_1 = \\begin{bmatrix}\n",
    "0.1 & 0.2 & 0.3 \\\\\n",
    "0.4 & 0.5 & 0.6 \\\\\n",
    "0.7 & 0.8 & 0.9 \\\\\n",
    "0.1 & 0.3 & 0.5 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}, \\quad\n",
    "W_2 = \\begin{bmatrix}\n",
    "0.2 & 0.4 & 0.6 & 0.8 \\\\\n",
    "0.1 & 0.3 & 0.5 & 0.7 \\\\\n",
    "0.0 & 0.2 & 0.4 & 0.6 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "**No biases** are considered for simplicity.\n",
    "\n",
    "#### Applying FFN to Each Position\n",
    "\n",
    "For each row in `AttOutput`, compute:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x) = \\text{ReLU}(x W_1) W_2\n",
    "$$\n",
    "\n",
    "Let's compute the FFN output for the **first token**:\n",
    "\n",
    "1. **Compute $x W_1$:**\n",
    "\n",
    "$$\n",
    "x = [0.0794, 0.2722, 0.2779, 0.2508] \\in \\mathbb{R}^{1 \\times 4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x W_1 = [0.0794 \\times 0.1 + 0.2722 \\times 0.4 + 0.2779 \\times 0.7 + 0.2508 \\times 0.1, \\; \\ldots ]\n",
    "$$\n",
    "\n",
    "Calculate each component:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& y_1 = 0.0794 \\times 0.1 + 0.2722 \\times 0.4 + 0.2779 \\times 0.7 + 0.2508 \\times 0.1 \\\\\n",
    "& \\quad = 0.00794 + 0.10888 + 0.19453 + 0.02508 \\approx 0.33643 \\\\\n",
    "& y_2 = 0.0794 \\times 0.2 + 0.2722 \\times 0.5 + 0.2779 \\times 0.8 + 0.2508 \\times 0.3 \\\\\n",
    "& \\quad = 0.01588 + 0.1361 + 0.22232 + 0.07524 \\approx 0.44954 \\\\\n",
    "& y_3 = 0.0794 \\times 0.3 + 0.2722 \\times 0.6 + 0.2779 \\times 0.9 + 0.2508 \\times 0.5 \\\\\n",
    "& \\quad = 0.02382 + 0.16332 + 0.25011 + 0.1254 \\approx 0.56265 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "x W_1 \\approx [0.33643, 0.44954, 0.56265]\n",
    "$$\n",
    "\n",
    "2. **Apply ReLU:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x W_1) = [\\max(0, 0.33643), \\max(0, 0.44954), \\max(0, 0.56265)] = [0.33643, 0.44954, 0.56265]\n",
    "$$\n",
    "\n",
    "3. **Compute $\\text{ReLU}(x W_1) W_2$:**\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x W_1) W_2 = [0.33643, 0.44954, 0.56265] \\begin{bmatrix}\n",
    "0.2 & 0.4 & 0.6 & 0.8 \\\\\n",
    "0.1 & 0.3 & 0.5 & 0.7 \\\\\n",
    "0.0 & 0.2 & 0.4 & 0.6 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculate each component:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& o_1 = 0.33643 \\times 0.2 + 0.44954 \\times 0.1 + 0.56265 \\times 0.0 = 0.067286 + 0.044954 + 0 = 0.11224 \\\\\n",
    "& o_2 = 0.33643 \\times 0.4 + 0.44954 \\times 0.3 + 0.56265 \\times 0.2 = 0.134572 + 0.134862 + 0.11253 \\approx 0.3810 \\\\\n",
    "& o_3 = 0.33643 \\times 0.6 + 0.44954 \\times 0.5 + 0.56265 \\times 0.4 = 0.201858 + 0.22477 + 0.22506 \\approx 0.6517 \\\\\n",
    "& o_4 = 0.33643 \\times 0.8 + 0.44954 \\times 0.7 + 0.56265 \\times 0.6 = 0.269144 + 0.314678 + 0.33759 \\approx 0.9214 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{FFNOutput}_1 \\approx [0.11224, 0.3810, 0.6517, 0.9214]\n",
    "$$\n",
    "\n",
    "Repeat similarly for the **second** and **third tokens**:\n",
    "\n",
    "$$\n",
    "\\text{FFNOutput} \\approx \\begin{bmatrix}\n",
    "0.11224 & 0.3810 & 0.6517 & 0.9214 \\\\\n",
    "0.1105  & 0.3752 & 0.6431 & 0.9108 \\\\\n",
    "0.1130  & 0.3798 & 0.6485 & 0.9150 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "### 2. Output Projection and Loss Calculation\n",
    "\n",
    "#### 1. Final Linear Projection\n",
    "\n",
    "To produce predictions for the next token, apply a linear projection followed by a softmax to obtain a probability distribution over the vocabulary.\n",
    "\n",
    "Assume the vocabulary size $ V = 5 $, and \\( W_{\\text{out}} \\) is defined as:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}} = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 & 0.3 & 0.1 \\\\\n",
    "0.0 & 0.5 & 0.1 & 0.0 & 0.2 \\\\\n",
    "0.1 & 0.1 & 0.3 & 0.3 & 0.1 \\\\\n",
    "0.2 & 0.2 & 0.0 & 0.1 & 0.0 \n",
    "\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 5}\n",
    "$$\n",
    "\n",
    "For the **third token** ($ t = 3 $), compute the logits $ Z_3 $:\n",
    "\n",
    "$$\n",
    "Z_3 = \\text{FFNOutput}_3 W_{\\text{out}} \\in \\mathbb{R}^{1 \\times 5}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\text{FFNOutput}_3 = [0.1130, 0.3798, 0.6485, 0.9150]\n",
    "$$\n",
    "\n",
    "Compute each component:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& Z_3[1] = 0.1130 \\times 0.2 + 0.3798 \\times 0.0 + 0.6485 \\times 0.1 + 0.9150 \\times 0.2 = 0.0226 + 0 + 0.06485 + 0.1830 = 0.27045 \\\\\n",
    "& Z_3[2] = 0.1130 \\times 0.1 + 0.3798 \\times 0.5 + 0.6485 \\times 0.1 + 0.9150 \\times 0.2 = 0.0113 + 0.1899 + 0.06485 + 0.1830 = 0.44805 \\\\\n",
    "& Z_3[3] = 0.1130 \\times 0.0 + 0.3798 \\times 0.1 + 0.6485 \\times 0.3 + 0.9150 \\times 0.0 = 0 + 0.03798 + 0.19455 + 0 = 0.23253 \\\\\n",
    "& Z_3[4] = 0.1130 \\times 0.3 + 0.3798 \\times 0.0 + 0.6485 \\times 0.3 + 0.9150 \\times 0.1 = 0.0339 + 0 + 0.19455 + 0.0915 = 0.3200 \\\\\n",
    "& Z_3[5] = 0.1130 \\times 0.1 + 0.3798 \\times 0.2 + 0.6485 \\times 0.1 + 0.9150 \\times 0.0 = 0.0113 + 0.07596 + 0.06485 + 0 = 0.1521 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z_3 \\approx [0.27045, \\; 0.44805, \\; 0.23253, \\; 0.3200, \\; 0.1521]\n",
    "$$\n",
    "\n",
    "#### 2. Softmax for Next-Token Prediction\n",
    "\n",
    "Apply the softmax function to convert logits into probabilities:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(w_4 \\mid w_1, w_2, w_3) = \\text{softmax}(Z_3)\n",
    "$$\n",
    "\n",
    "Compute exponentials:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\exp(Z_3[1]) = \\exp(0.27045) \\approx 1.310 \\\\\n",
    "& \\exp(Z_3[2]) = \\exp(0.44805) \\approx 1.565 \\\\\n",
    "& \\exp(Z_3[3]) = \\exp(0.23253) \\approx 1.262 \\\\\n",
    "& \\exp(Z_3[4]) = \\exp(0.3200) \\approx 1.377 \\\\\n",
    "& \\exp(Z_3[5]) = \\exp(0.1521) \\approx 1.164 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Sum of exponentials:\n",
    "\n",
    "$$\n",
    "S = 1.310 + 1.565 + 1.262 + 1.377 + 1.164 \\approx 6.678\n",
    "$$\n",
    "\n",
    "Compute probabilities:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& p(w_4=1) = \\frac{\\exp(Z_3[1])}{S} \\approx \\frac{1.310}{6.678} \\approx 0.196 \\\\\n",
    "& p(w_4=2) = \\frac{\\exp(Z_3[2])}{S} \\approx \\frac{1.565}{6.678} \\approx 0.235 \\\\\n",
    "& p(w_4=3) = \\frac{\\exp(Z_3[3])}{S} \\approx \\frac{1.262}{6.678} \\approx 0.189 \\\\\n",
    "& p(w_4=4) = \\frac{\\exp(Z_3[4])}{S} \\approx \\frac{1.377}{6.678} \\approx 0.206 \\\\\n",
    "& p(w_4=5) = \\frac{\\exp(Z_3[5])}{S} \\approx \\frac{1.164}{6.678} \\approx 0.175 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Thus:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(w_4 \\mid w_1, w_2, w_3) \\approx [0.196, \\; 0.235, \\; 0.189, \\; 0.206, \\; 0.175]\n",
    "$$\n",
    "\n",
    "## Target Labels and Loss Calculation\n",
    "\n",
    "During training, each input sequence has a corresponding target sequence. The goal is for the model to predict the next token in the sequence. \n",
    "\n",
    "**Example:**\n",
    "\n",
    "Assume the correct next token ($ w_4 $) is token **2**.\n",
    "\n",
    "### 1. One-Hot Encoding of Target\n",
    "\n",
    "The target vector $ y $ is a one-hot encoded vector indicating the correct next token:\n",
    "\n",
    "$$\n",
    "y = [0, 1, 0, 0, 0]\n",
    "$$\n",
    "\n",
    "### 2. Cross-Entropy Loss\n",
    "\n",
    "The loss function commonly used is the cross-entropy loss, which measures the difference between the predicted probability distribution $ \\hat{p} $ and the target distribution $ y $:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\sum_{i=1}^{V} y_i \\log \\hat{p}_i\n",
    "$$\n",
    "\n",
    "Given $ y = [0, 1, 0, 0, 0] $ and $ \\hat{p} = [0.196, \\; 0.235, \\; 0.189, \\; 0.206, \\; 0.175] $:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\theta) = -\\log(\\hat{p}_2) = -\\log(0.235) \\approx 1.447\n",
    "$$\n",
    "\n",
    "## Backpropagation and Parameter Updates\n",
    "\n",
    "To minimize the loss $ \\mathcal{L}(\\theta) $, we perform backpropagation to compute gradients and update the model parameters accordingly.\n",
    "\n",
    "### 1. Gradient with respect to Logits ($ Z_3 $)\n",
    "\n",
    "The gradient of the loss with respect to each logit $ Z_3[i] $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z_3[i]} = \\hat{p}_i - y_i\n",
    "$$\n",
    "\n",
    "For our example:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z_3} = [0.196, \\; 0.235 - 1, \\; 0.189, \\; 0.206, \\; 0.175] = [0.196, \\; -0.765, \\; 0.189, \\; 0.206, \\; 0.175]\n",
    "$$\n",
    "\n",
    "### 2. Gradient with respect to Output Weights ($ W_{\\text{out}} $)\n",
    "\n",
    "The gradient of the loss with respect to $ W_{\\text{out}} $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}} = \\text{FFNOutput}_3^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial Z_3}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\text{FFNOutput}_3 = [0.1130, \\; 0.3798, \\; 0.6485, \\; 0.9150] \\in \\mathbb{R}^{1 \\times 4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial Z_3} = [0.196, \\; -0.765, \\; 0.189, \\; 0.206, \\; 0.175] \\in \\mathbb{R}^{1 \\times 5}\n",
    "$$\n",
    "\n",
    "Compute each element of $ \\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}} = \\begin{bmatrix}\n",
    "0.1130 \\times 0.196 & 0.1130 \\times (-0.765) & 0.1130 \\times 0.189 & 0.1130 \\times 0.206 & 0.1130 \\times 0.175 \\\\\n",
    "0.3798 \\times 0.196 & 0.3798 \\times (-0.765) & 0.3798 \\times 0.189 & 0.3798 \\times 0.206 & 0.3798 \\times 0.175 \\\\\n",
    "0.6485 \\times 0.196 & 0.6485 \\times (-0.765) & 0.6485 \\times 0.189 & 0.6485 \\times 0.206 & 0.6485 \\times 0.175 \\\\\n",
    "0.9150 \\times 0.196 & 0.9150 \\times (-0.765) & 0.9150 \\times 0.189 & 0.9150 \\times 0.206 & 0.9150 \\times 0.175 \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculating each element:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}} \\approx \\begin{bmatrix}\n",
    "0.022148 & -0.086295 & 0.021357 & 0.023278 & 0.019775 \\\\\n",
    "0.074408 & -0.290577 & 0.071922 & 0.078308 & 0.066465 \\\\\n",
    "0.127486 & -0.490653 & 0.122317 & 0.132282 & 0.113387 \\\\\n",
    "0.178980 & -0.696975 & 0.173415 & 0.189030 & 0.160125 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 5}\n",
    "$$\n",
    "\n",
    "### 3. Gradient with respect to FFN Output ($ \\text{FFNOutput}_3 $)\n",
    "\n",
    "The gradient of the loss with respect to $ \\text{FFNOutput}_3 $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} = \\frac{\\partial \\mathcal{L}}{\\partial Z_3} W_{\\text{out}}^\\top\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "W_{\\text{out}}^\\top = \\begin{bmatrix}\n",
    "0.2 & 0.0 & 0.1 & 0.2 \\\\\n",
    "0.1 & 0.5 & 0.1 & 0.2 \\\\\n",
    "0.0 & 0.1 & 0.3 & 0.0 \\\\\n",
    "0.3 & 0.0 & 0.3 & 0.1 \\\\\n",
    "0.1 & 0.2 & 0.1 & 0.0 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{5 \\times 4}\n",
    "$$\n",
    "\n",
    "Compute each component of $ \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} $:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3}[1] = 0.196 \\times 0.2 + (-0.765) \\times 0.0 + 0.189 \\times 0.1 + 0.206 \\times 0.2 + 0.175 \\times 0.1 \\\\\n",
    "& \\quad = 0.0392 + 0 + 0.0189 + 0.0412 + 0.0175 \\approx 0.1168 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3}[2] = 0.196 \\times 0.1 + (-0.765) \\times 0.5 + 0.189 \\times 0.1 + 0.206 \\times 0.0 + 0.175 \\times 0.2 \\\\\n",
    "& \\quad = 0.0196 - 0.3825 + 0.0189 + 0 + 0.0350 \\approx -0.3090 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3}[3] = 0.196 \\times 0.0 + (-0.765) \\times 0.1 + 0.189 \\times 0.3 + 0.206 \\times 0.3 + 0.175 \\times 0.1 \\\\\n",
    "& \\quad = 0 + (-0.0765) + 0.0567 + 0.0618 + 0.0175 \\approx -0.0415 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3}[4] = 0.196 \\times 0.2 + (-0.765) \\times 0.2 + 0.189 \\times 0.0 + 0.206 \\times 0.1 + 0.175 \\times 0.0 \\\\\n",
    "& \\quad = 0.0392 - 0.1530 + 0 + 0.0206 + 0 \\approx -0.0932 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} \\approx [0.1168, \\; -0.3090, \\; -0.0415, \\; -0.0932] \\in \\mathbb{R}^{1 \\times 4}\n",
    "$$\n",
    "\n",
    "### 4. Gradient Through FFN\n",
    "\n",
    "#### 4.1 Gradient Through $ W_2 $\n",
    "\n",
    "The gradient with respect to $ W_2 $ is:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2} = \\text{ReLU}(x W_1)^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x W_1) = [0.33643, \\; 0.44954, \\; 0.56265] \\in \\mathbb{R}^{1 \\times 3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} = [0.1168, \\; -0.3090, \\; -0.0415, \\; -0.0932] \\in \\mathbb{R}^{1 \\times 4}\n",
    "$$\n",
    "\n",
    "Compute each element of $ \\frac{\\partial \\mathcal{L}}{\\partial W_2} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2} = \\begin{bmatrix}\n",
    "0.33643 \\times 0.1168 & 0.33643 \\times (-0.3090) & 0.33643 \\times (-0.0415) & 0.33643 \\times (-0.0932) \\\\\n",
    "0.44954 \\times 0.1168 & 0.44954 \\times (-0.3090) & 0.44954 \\times (-0.0415) & 0.44954 \\times (-0.0932) \\\\\n",
    "0.56265 \\times 0.1168 & 0.56265 \\times (-0.3090) & 0.56265 \\times (-0.0415) & 0.56265 \\times (-0.0932) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculating each element:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[1,1] = 0.33643 \\times 0.1168 \\approx 0.0393 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[1,2] = 0.33643 \\times (-0.3090) \\approx -0.1040 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[1,3] = 0.33643 \\times (-0.0415) \\approx -0.01397 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[1,4] = 0.33643 \\times (-0.0932) \\approx -0.03134 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[2,1] = 0.44954 \\times 0.1168 \\approx 0.0524 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[2,2] = 0.44954 \\times (-0.3090) \\approx -0.1389 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[2,3] = 0.44954 \\times (-0.0415) \\approx -0.0186 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[2,4] = 0.44954 \\times (-0.0932) \\approx -0.0418 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[3,1] = 0.56265 \\times 0.1168 \\approx 0.0657 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[3,2] = 0.56265 \\times (-0.3090) \\approx -0.1739 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[3,3] = 0.56265 \\times (-0.0415) \\approx -0.0233 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_2}[3,4] = 0.56265 \\times (-0.0932) \\approx -0.0524 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_2} \\approx \\begin{bmatrix}\n",
    "0.0393 & -0.1040 & -0.01397 & -0.03134 \\\\\n",
    "0.0524 & -0.1389 & -0.0186 & -0.0418 \\\\\n",
    "0.0657 & -0.1739 & -0.0233 & -0.0524 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "#### 4.2 Gradient Through $ W_1 $\n",
    "\n",
    "The gradient with respect to $ W_1 $ involves the chain rule through the FFN:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1} = X^\\top \\cdot \\left( \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} W_2^\\top \\right) \\cdot \\text{ReLU}'(x W_1)\n",
    "$$\n",
    "\n",
    "Since all pre-activations were positive in ReLU, \\( \\text{ReLU}'(x W_1) = 1 \\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1} = X^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} W_2^\\top\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} = [0.1168, \\; -0.3090, \\; -0.0415, \\; -0.0932] \\in \\mathbb{R}^{1 \\times 4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "W_2^\\top = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 \\\\\n",
    "0.4 & 0.3 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4 \\\\\n",
    "0.8 & 0.7 & 0.6 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}\n",
    "$$\n",
    "\n",
    "Compute the intermediate product:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} W_2^\\top = [0.1168, \\; -0.3090, \\; -0.0415, \\; -0.0932] \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 \\\\\n",
    "0.4 & 0.3 & 0.2 \\\\\n",
    "0.6 & 0.5 & 0.4 \\\\\n",
    "0.8 & 0.7 & 0.6 \\\\\n",
    "\\end{bmatrix} = [ \\text{...}, \\text{...}, \\text{...} ]\n",
    "$$\n",
    "\n",
    "Calculating each component:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\text{Component 1} = 0.1168 \\times 0.2 + (-0.3090) \\times 0.4 + (-0.0415) \\times 0.6 + (-0.0932) \\times 0.8 \\\\\n",
    "& \\quad = 0.02336 - 0.1236 - 0.0249 - 0.07456 \\approx -0.1997 \\\\\n",
    "& \\text{Component 2} = 0.1168 \\times 0.1 + (-0.3090) \\times 0.3 + (-0.0415) \\times 0.5 + (-0.0932) \\times 0.7 \\\\\n",
    "& \\quad = 0.01168 - 0.0927 - 0.02075 - 0.06524 \\approx -0.1660 \\\\\n",
    "& \\text{Component 3} = 0.1168 \\times 0.0 + (-0.3090) \\times 0.2 + (-0.0415) \\times 0.4 + (-0.0932) \\times 0.6 \\\\\n",
    "& \\quad = 0 + (-0.0618) + (-0.0166) + (-0.05592) \\approx -0.1343 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial \\text{FFNOutput}_3} W_2^\\top \\approx [-0.1997, \\; -0.1660, \\; -0.1343] \\in \\mathbb{R}^{1 \\times 3}\n",
    "$$\n",
    "\n",
    "Now, compute $ \\frac{\\partial \\mathcal{L}}{\\partial W_1} $:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1} = X^\\top \\cdot [-0.1997, \\; -0.1660, \\; -0.1343] \\in \\mathbb{R}^{4 \\times 3}\n",
    "$$\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "X^\\top = \\begin{bmatrix}\n",
    "0.0794 & 0.0792 & 0.08 \\\\\n",
    "0.2722 & 0.2721 & 0.2714 \\\\\n",
    "0.2779 & 0.2779 & 0.2763 \\\\\n",
    "0.2508 & 0.2507 & 0.2505 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}\n",
    "$$\n",
    "\n",
    "Multiply each row of $ X^\\top $ with the gradient vector:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1} = \\begin{bmatrix}\n",
    "0.0794 \\times (-0.1997) + 0.0792 \\times (-0.1660) + 0.08 \\times (-0.1343) \\\\\n",
    "0.2722 \\times (-0.1997) + 0.2721 \\times (-0.1660) + 0.2714 \\times (-0.1343) \\\\\n",
    "0.2779 \\times (-0.1997) + 0.2779 \\times (-0.1660) + 0.2763 \\times (-0.1343) \\\\\n",
    "0.2508 \\times (-0.1997) + 0.2507 \\times (-0.1660) + 0.2505 \\times (-0.1343) \\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Calculating each element:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_1}[1, :] = 0.0794 \\times (-0.1997) + 0.0792 \\times (-0.1660) + 0.08 \\times (-0.1343) \\\\\n",
    "& \\quad = -0.01583 - 0.01315 - 0.01074 \\approx -0.03972 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_1}[2, :] = 0.2722 \\times (-0.1997) + 0.2721 \\times (-0.1660) + 0.2714 \\times (-0.1343) \\\\\n",
    "& \\quad = -0.05431 - 0.04518 - 0.03645 \\approx -0.1350 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_1}[3, :] = 0.2779 \\times (-0.1997) + 0.2779 \\times (-0.1660) + 0.2763 \\times (-0.1343) \\\\\n",
    "& \\quad = -0.05546 - 0.04621 - 0.03710 \\approx -0.1388 \\\\\n",
    "& \\frac{\\partial \\mathcal{L}}{\\partial W_1}[4, :] = 0.2508 \\times (-0.1997) + 0.2507 \\times (-0.1660) + 0.2505 \\times (-0.1343) \\\\\n",
    "& \\quad = -0.05014 - 0.04161 - 0.03361 \\approx -0.12536 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W_1} \\approx \\begin{bmatrix}\n",
    "-0.03972 & -0.03972 & -0.03972 \\\\\n",
    "-0.1350 & -0.1350 & -0.1350 \\\\\n",
    "-0.1388 & -0.1388 & -0.1388 \\\\\n",
    "-0.12536 & -0.12536 & -0.12536 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{4 \\times 3}\n",
    "$$\n",
    "\n",
    "### 5. Parameter Updates\n",
    "\n",
    "After computing the gradients for all parameters, update them using an optimization algorithm like **Gradient Descent** or **Adam**.\n",
    "\n",
    "#### Gradient Descent Update Rule\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ represents all model parameters ($ W_Q, W_K, W_V, W_{\\text{out}}, W_1, W_2, \\ldots $)\n",
    "- $ \\eta $ is the learning rate\n",
    "- $ \\nabla_{\\theta} \\mathcal{L}(\\theta) $ is the gradient of the loss with respect to the parameters\n",
    "\n",
    "**Example Update for $ W_{\\text{out}} $:**\n",
    "\n",
    "Assume a learning rate $ \\eta = 0.01 $.\n",
    "\n",
    "$$\n",
    "W_{\\text{out}}^{\\text{new}} = W_{\\text{out}} - 0.01 \\times \\frac{\\partial \\mathcal{L}}{\\partial W_{\\text{out}}}\n",
    "$$\n",
    "\n",
    "Each element of $ W_{\\text{out}} $ is updated accordingly.\n",
    "\n",
    "**Example Update for $ W_1 $:**\n",
    "\n",
    "$$\n",
    "W_1^{\\text{new}} = W_1 - 0.01 \\times \\frac{\\partial \\mathcal{L}}{\\partial W_1}\n",
    "$$\n",
    "\n",
    "Similarly, update $ W_K, W_V, W_2, $ etc.\n",
    "\n",
    "### 6. Iterative Training Process\n",
    "\n",
    "The training process involves iterating over numerous input sequences (or mini-batches), performing forward and backward passes, and updating the model parameters to minimize the loss.\n",
    "\n",
    "**Training Steps:**\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - Embed input tokens and add positional encodings.\n",
    "   - Pass embeddings through multi-head attention and FFN within Transformer layers.\n",
    "   - Project to output logits and compute softmax probabilities.\n",
    "   - Calculate loss using target labels.\n",
    "\n",
    "2. **Backward Pass:**\n",
    "   - Compute gradients of the loss with respect to all parameters via backpropagation.\n",
    "\n",
    "3. **Parameter Update:**\n",
    "   - Adjust parameters in the direction that minimizes the loss using the optimization algorithm.\n",
    "\n",
    "4. **Repeat:**\n",
    "   - Continue this process for many examples and epochs until the model converges (i.e., the loss stabilizes or decreases below a threshold).\n",
    "\n",
    "## Inference (Generation Phase)\n",
    "\n",
    "After training, the model can generate new text by performing forward passes without updating parameters.\n",
    "\n",
    "### 1. Given a Prompt\n",
    "\n",
    "Suppose the trained model receives the prompt:\n",
    "\n",
    "$$\n",
    "(w_1, w_2, w_3) = (\\text{\"The\"}, \\text{\"cat\"}, \\text{\"sat\"})\n",
    "$$\n",
    "\n",
    "### 2. Forward Pass to Predict Next Token\n",
    "\n",
    "1. **Embedding and Positional Encoding:**\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "2. **Multi-Head Attention and FFN:**\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\text{FFNOutput} \\approx \\begin{bmatrix}\n",
    "0.11224 & 0.3810 & 0.6517 & 0.9214 \\\\\n",
    "0.1105  & 0.3752 & 0.6431 & 0.9108 \\\\\n",
    "0.1130  & 0.3798 & 0.6485 & 0.9150 \\\\\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{3 \\times 4}\n",
    "$$\n",
    "\n",
    "3. **Output Projection:**\n",
    "\n",
    "For the **third token** (R t = 3 R):\n",
    "\n",
    "$$\n",
    "Z_3 \\approx [0.27045, \\; 0.44805, \\; 0.23253, \\; 0.3200, \\; 0.1521] \\in \\mathbb{R}^{1 \\times 5}\n",
    "$$\n",
    "\n",
    "4. **Softmax to Get Probabilities:**\n",
    "\n",
    "$$\n",
    "p_{\\theta}(w_4 \\mid w_{1:3}) \\approx [0.196, \\; 0.235, \\; 0.189, \\; 0.206, \\; 0.175]\n",
    "$$\n",
    "\n",
    "### 3. Sampling the Next Token\n",
    "\n",
    "Based on the probability distribution:\n",
    "\n",
    "$$\n",
    "p_{\\theta}(w_4 \\mid w_{1:3}) \\approx [0.196, \\; 0.235, \\; 0.189, \\; 0.206, \\; 0.175]\n",
    "$$\n",
    "\n",
    "- **Greedy Decoding:** Choose the token with the highest probability.\n",
    "\n",
    "$$\n",
    "w_4 = \\text{argmax}(p_{\\theta}(w_4 \\mid w_{1:3})) = 2 \\quad (\\text{\"cat\"})\n",
    "$$\n",
    "\n",
    "- **Stochastic Sampling:** Sample a token based on the probability distribution.\n",
    "\n",
    "### 4. Iterative Generation\n",
    "\n",
    "Append $ w_4 $ to the sequence and repeat the forward pass to generate $ w_5 $:\n",
    "\n",
    "$$\n",
    "(w_1, w_2, w_3, w_4) = (\\text{\"The\"}, \\text{\"cat\"}, \\text{\"sat\"}, \\text{\"cat\"})\n",
    "$$\n",
    "\n",
    "Continue this process until reaching a termination condition (e.g., maximum length, end-of-sequence token).\n",
    "\n",
    "## Summary of Mathematical Operations\n",
    "\n",
    "1. **Forward Pass:**\n",
    "   - **Embedding:** $ X = \\text{Embeddings}(w_1, w_2, w_3) $\n",
    "   - **Multi-Head Attention:** Compute $ \\text{AttOutput} $\n",
    "   - **Feed-Forward Network:** Compute $$ \\text{FFNOutput} $\n",
    "   - **Output Projection:** Compute $ Z_t = \\text{FFNOutput}_t W_{\\text{out}} $\n",
    "   - **Softmax:** Compute $ p_{\\theta}(w_{t+1} \\mid w_{1:t}) $\n",
    "\n",
    "2. **Loss Calculation (Training):**\n",
    "   - **Cross-Entropy Loss:** $ \\mathcal{L}(\\theta) = -\\log(p_{\\theta}(w_{t+1} \\mid w_{1:t})) $\n",
    "\n",
    "3. **Backward Pass (Training):**\n",
    "   - **Compute Gradients:** $ \\nabla_{\\theta} \\mathcal{L}(\\theta) $ via backpropagation.\n",
    "   - **Update Parameters:** $ \\theta \\leftarrow \\theta - \\eta \\nabla_{\\theta} \\mathcal{L}(\\theta) $\n",
    "\n",
    "4. **Inference (Generation):**\n",
    "   - **Generate Next Token:** Sample $ w_{t+1} $ from $ p_{\\theta}(w_{t+1} \\mid w_{1:t}) $\n",
    "   - **Repeat:** Append $ w_{t+1} $ and continue.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This detailed mathematical walkthrough illustrates how a Transformer-based language model is trained and used for inference:\n",
    "\n",
    "1. **Transformer Layer:**\n",
    "   - **Multi-Head Attention:** Computes context-aware representations by attending to different parts of the input sequence.\n",
    "   - **Feed-Forward Network:** Applies non-linear transformations to each position independently.\n",
    "   - **Residual Connections and Layer Normalization:** Stabilize training and facilitate gradient flow.\n",
    "\n",
    "2. **Training Process:**\n",
    "   - **Forward Pass:** Compute predictions using the Transformer layers.\n",
    "   - **Loss Calculation:** Measure discrepancy between predictions and true labels using cross-entropy loss.\n",
    "   - **Backward Pass:** Compute gradients via backpropagation.\n",
    "   - **Parameter Update:** Adjust model parameters to minimize loss using gradient descent or optimizers like Adam.\n",
    "\n",
    "3. **Inference:**\n",
    "   - **Prompting:** Provide an initial sequence.\n",
    "   - **Generating Tokens:** Use the trained model to predict and sample the next token iteratively.\n",
    "\n",
    "Through end-to-end training, the model learns to adjust $ W_Q $, $ W_K $, and $ W_V $ (along with all other parameters) to produce meaningful attention distributions that enhance its ability to predict and generate coherent and contextually relevant text.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Word Embeddings for the Corpus (shape: B, T, embedding_dim):\n",
      "Input Tensor Shape: torch.Size([5, 5])\n",
      "Embeddings Shape: torch.Size([5, 5, 512])\n",
      "\n",
      "Embeddings for the first sentence:\n",
      "tensor([[ 0.7161,  0.5239,  1.9768,  ..., -1.5047,  0.8310, -0.2109],\n",
      "        [ 0.3967, -0.8570, -0.6796,  ..., -1.0295,  0.6432, -0.0169],\n",
      "        [-0.1029, -0.2704, -1.1114,  ..., -1.3107, -0.2067, -1.4294],\n",
      "        [-0.0183, -0.7638, -0.5150,  ..., -0.0061, -0.2643, -0.8754],\n",
      "        [-0.0183, -0.7638, -0.5150,  ..., -0.0061, -0.2643, -0.8754]],\n",
      "       grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate a simple corpus (same as before)\n",
    "words = [\"apple\", \"banana\", \"cherry\", \"dog\", \"elephant\", \"fish\", \"grape\", \"house\", \"ice\", \"jungle\", \"kite\", \"lemon\"]\n",
    "num_sentences = 5\n",
    "max_sentence_length = 6\n",
    "\n",
    "# Generate random sentences\n",
    "corpus = []\n",
    "for _ in range(num_sentences):\n",
    "    sentence_length = np.random.randint(3, max_sentence_length)\n",
    "    sentence = np.random.choice(words, sentence_length, replace=True)\n",
    "    corpus.append(sentence)\n",
    "\n",
    "# Step 2: Create a token-to-index dictionary for words in the corpus\n",
    "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "# Step 3: Convert words in the corpus to indices using the dictionary\n",
    "corpus_indices = [[word_to_index[word] for word in sentence] for sentence in corpus]\n",
    "\n",
    "# Step 4: Define an embedding layer\n",
    "embedding_dim = 512\n",
    "vocab_size = len(words)\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "# Step 5: Pass the indices through the embedding layer\n",
    "# Ensure input tensor has shape (B, T), where B is batch size and T is sequence length\n",
    "# We pad sequences to the same length to maintain uniformity\n",
    "max_length = max(len(sentence) for sentence in corpus_indices)\n",
    "padded_corpus_indices = [sentence + [0] * (max_length - len(sentence)) for sentence in corpus_indices]\n",
    "\n",
    "# Convert to a tensor of shape (B, T)\n",
    "input_tensor = torch.tensor(padded_corpus_indices)\n",
    "\n",
    "# Step 6: Get the embeddings for the input tensor\n",
    "embeddings = token_embedding_table(input_tensor)  # This will have shape (B, T, embedding_dim)\n",
    "\n",
    "# Print the embeddings for each word in the corpus\n",
    "print(\"\\nWord Embeddings for the Corpus (shape: B, T, embedding_dim):\")\n",
    "B, T = input_tensor.shape  # B = batch size, T = sequence length\n",
    "print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
    "print(f\"Embeddings Shape: {embeddings.shape}\")\n",
    "\n",
    "# Optionally: To view the embeddings of each token in the first sentence\n",
    "print(\"\\nEmbeddings for the first sentence:\")\n",
    "print(embeddings[0])  # This will show the embeddings for the first sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(padded_corpus_indices).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Shape: torch.Size([5, 5, 512])\n",
      "Logits Shape: torch.Size([25, 512])\n",
      "Targets Shape: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, dataloader, random_split\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Generate a simple corpus (same as before)\n",
    "words = [\"apple\", \"banana\", \"cherry\", \"dog\", \"elephant\", \"fish\", \"grape\", \"house\", \"ice\", \"jungle\", \"kite\", \"lemon\"]\n",
    "num_sentences = 5\n",
    "max_sentence_length = 6\n",
    "\n",
    "# Generate random sentences\n",
    "corpus = []\n",
    "for _ in range(num_sentences):\n",
    "    sentence_length = np.random.randint(3, max_sentence_length)\n",
    "    sentence = np.random.choice(words, sentence_length, replace=True)\n",
    "    corpus.append(sentence)\n",
    "\n",
    "# Step 2: Create a token-to-index dictionary for words in the corpus\n",
    "word_to_index = {word: idx for idx, word in enumerate(words)}\n",
    "\n",
    "# Step 3: Convert words in the corpus to indices using the dictionary\n",
    "corpus_indices = [[word_to_index[word] for word in sentence] for sentence in corpus]\n",
    "\n",
    "# Step 4: Define an embedding layer\n",
    "embedding_dim = 512\n",
    "vocab_size = len(words)\n",
    "token_embedding_table = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "max_length = max(len(sentence) for sentence in corpus_indices)\n",
    "padded_corpus_indices = [sentence + [0] * (max_length - len(sentence)) for sentence in corpus_indices]\n",
    "\n",
    "# Convert to a tensor of shape (B, T)\n",
    "input_tensor = torch.tensor(padded_corpus_indices)\n",
    "\n",
    "# Step 5: Pass the indices through the embedding layer\n",
    "embeddings = token_embedding_table(input_tensor)\n",
    "\n",
    "B, T, C = embeddings.shape\n",
    "\n",
    "logits = embeddings.view(B*T, C)  # Reshape to (B, C*T) for linear layer\n",
    "targets = torch.randint(0, vocab_size, (B,))  # Random targets for illustration\n",
    "\n",
    "print(f\"Input Shape: {embeddings.shape}\")\n",
    "print(f\"Logits Shape: {logits.shape}\")\n",
    "print(f\"Targets Shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
