{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models Explained Visually\n",
    "\n",
    "Discover comprehensive visual explanations of language models through the following resources:\n",
    "\n",
    "- [BBY Croft's Large Language Models Guide](https://bbycroft.net/llm)\n",
    "- [Polo Club's Transformer Explainer](https://poloclub.github.io/transformer-explainer/)\n",
    "- [HuggingFace Transformers code for all Tasks](https://huggingface.co/docs/transformers/en/tasks/sequence_classification)\n",
    "- [HuggingFace Tasks](https://huggingface.co/tasks)\n",
    "\n",
    "\n",
    "## Transformers Tutorial\n",
    "-[Transformers from scratch](https://peterbloem.nl/blog/transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corpus\n",
    "    # A collection of text documents used to train a language model. The corpus can be a collection of books, articles, or any text data.\n",
    "    \n",
    "    \n",
    "# Vocabulary\n",
    "    # The set of unique tokens (words, sub-words, or characters) that a model can understand. The vocabulary is typically derived from\n",
    "    # the training corpus and includes common words and special tokens like [PAD], [UNK], [CLS], and [SEP].\n",
    "    \n",
    "    \n",
    "# Attention Mechanism\n",
    "    # A mechanism that allows the model to focus on important words in a sequence, enabling the model to handle long-range dependencies \n",
    "    # and capture context.\n",
    "\n",
    "    # Self-Attention:\n",
    "        # a mechanism in deep learning that allows a system to focus on different parts of input data to make predictions or estimations\n",
    "            # Encoder: Uses full attention, allowing each token to attend to every other token in the input sequence.\n",
    "            # Decoder: Uses causal (autoregressive) masking to prevent tokens from attending to future tokens in the output sequence.\n",
    "\n",
    "    # Cross-Attention:\n",
    "        # a mechanism in transformers where one sequence (e.g., the decoder's sequence) attends to another sequence \n",
    "        # (e.g., the encoder's output) to extract relevant context.\n",
    "            # Encoder-Decoder Attention: The decoder attends to the encoder's output to incorporate relevant information.\n",
    "    \n",
    "# Tokens\n",
    "    # The smallest unit of input or output the model processes. Tokens can represent words, sub-words, or even characters, depending \n",
    "    # on the tokenization strategy.\n",
    "    \n",
    "    \n",
    "# Tokenization\n",
    "    # The process of converting raw text into tokens (usually words, sub-words, or characters) that the model can process. \n",
    "    # Tokenizers break down text based on a model's vocabulary (e.g., Byte Pair Encoding or WordPiece).\n",
    "    # Byte Pair Encoding (BPE):\n",
    "        # A tokenization algorithm that iteratively merges the most frequent pairs of characters in a corpus to create a vocabulary of \n",
    "        # variable-length tokens. BPE is widely used in NLP tasks, including machine translation and text generation.\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "    # Dense vector representations of tokens (words/sub-words) that capture semantic meaning. Used in LLMs to map input tokens into \n",
    "    # a continuous vector space where similar meanings are close together. Instead of treating each word as a unique, isolated token, \n",
    "    # embeddings allow words with similar meanings to be represented by vectors (arrays of numbers) that are close together \n",
    "    # in a multi-dimensional space.\n",
    "    \n",
    "    # example: Word2Vec, GloVe, FastText, BERT embeddings.\n",
    "        # Imagine you have the words: \n",
    "        # \"dog\", \"cat\", \"apple\", \"banana\" \n",
    "        # \"dog\" -> [0.1, 0.2, 0.3, 0.4], \n",
    "        # \"cat\" -> [0.2, 0.3, 0.4, 0.5], \n",
    "        # \"apple\" -> [0.3, 0.4, 0.5, 0.6], \n",
    "        # \"banana\" -> [0.4, 0.5, 0.6, 0.7]\n",
    "        # The embeddings for \"dog\" and \"cat\" are closer together than \"dog\" and \"apple\" because \"dog\" and \"cat\" are semantically\n",
    "        # similar (both animals) compared to \"dog\" and \"apple\" (different categories).\n",
    "        \n",
    "        \n",
    "# Part-of-Speech (POS) Tagging:\n",
    "    # Assigning each word in a sentence a grammatical category (e.g., noun, verb, adjective). Helps the model understand the \n",
    "    # structure of sentences and the role of each word, which is useful for tasks like parsing, translation, and question answering\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER):\n",
    "    # Identifying and categorizing entities (names, dates, locations, organizations, etc.) in text.\n",
    "\n",
    "\n",
    "# Stemming or Lemmatization:\n",
    "    # Reducing words to their base or root form. Stemming is a rule-based process that removes prefixes or suffixes, while lemmatization \n",
    "    # uses a vocabulary and morphological analysis to return the base form of a word. \n",
    "    # examples are: \n",
    "    \n",
    "\n",
    "# Sampling Techniques\n",
    "    # Methods used to generate outputs from a model, such as greedy search (selecting the most likely next token), beam search (exploring \n",
    "    # multiple token sequences), and temperature sampling (introducing randomness to outputs).\n",
    "\n",
    "\n",
    "# Beam Search\n",
    "    # A search strategy used during text generation to explore multiple possible token sequences and select the most likely ones. \n",
    "    # It reduces the likelihood of poor-quality outputs compared to greedy search.\n",
    "    \n",
    "    \n",
    "# Greedy Search\n",
    "    # A simpler search method where the model always selects the most probable next token. It is fast but may lead to less coherent \n",
    "    # or repetitive outputs.\n",
    "\n",
    "\n",
    "# Autoregressive Models\n",
    "    # LLMs like GPT, which generate text one token at a time, predicting the next token based on previously generated tokens. \n",
    "    # This type of model is suitable for tasks like text generation.\n",
    "    \n",
    "    \n",
    "# Masked Language Models (MLM)\n",
    "    # Models like BERT that learn by predicting masked-out tokens in a sentence, using the surrounding context. These models are \n",
    "    # bidirectional, meaning they consider context from both directions.\n",
    "    \n",
    "    \n",
    "# Zero-Shot Learning\n",
    "    # The model’s ability to perform tasks without explicit examples in the training data. For example, a zero-shot LLM can classify \n",
    "    # text without having seen labeled examples for that specific task.\n",
    "    \n",
    "    \n",
    "# Few-Shot Learning\n",
    "    # The model can generalize from only a few examples during inference. For instance, by providing the model a few sample questions \n",
    "    # and answers, it can handle similar tasks effectively.\n",
    "    \n",
    "    \n",
    "# Fine-Tuning vs. Transfer Learning\n",
    "    # Fine-Tuning: The process of adapting a pretrained LLM to a specific task (e.g., classification, question answering) by training \n",
    "        # it further on task-specific labeled data.\n",
    "    # Transfer Learning: Leveraging knowledge from a pretrained model and applying it to a new but related task, without needing to \n",
    "        # retrain from scratch.\n",
    "\n",
    "\n",
    "# Temperature Sampling\n",
    "    # A technique used during text generation to control the randomness of the output. Higher temperatures (e.g., 1.0) result in \n",
    "    # more diverse outputs, while lower temperatures (e.g., 0.2) make the model more deterministic.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PEFT + LoRA (Parameter Efficient Fine-tuning + Low-Rank Adaptation)\n",
    "    # Description: Fine-tunes only a small adapter layer added on top of a pre-trained model, conserving memory and improving efficiency.\n",
    "    # Use Case: Helps in training large models by keeping the original model frozen and updating only small parts.\n",
    "\n",
    "\n",
    "# 2. Quantization-Aware Training (QAT)\n",
    "    # Description: Reduces model size by converting high-precision weights (e.g., FP32) to lower precision formats (e.g., FP16 or INT8).\n",
    "    # Benefits: Saves memory and reduces training time but may affect model accuracy.\n",
    "    # Challenges: Requires careful monitoring to ensure model quality isn’t degraded.\n",
    "\n",
    "\n",
    "# 3. Gradient Checkpointing\n",
    "    # Description: Saves memory by storing only certain intermediate values during backpropagation.\n",
    "    # Use Case: Reduces memory usage but slows down training.\n",
    "\n",
    "\n",
    "# 4. Distributed Training\n",
    "    # Description: Splits the model and data across multiple devices or nodes for faster training.\n",
    "    # Key Techniques:\n",
    "        # FSDP (Fully Sharded Data Parallel): Shards model weights and optimizer states across devices.\n",
    "        # Deepspeed Zero Redundancy Optimizer (ZeRO): Distributes model parameters to save memory and optimize training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Post-Training Quantization (PTQ)\n",
    "    # Description: Quantizes a model’s weights and activations after training to reduce memory usage.\n",
    "    # Use Case: Reduces memory footprint for serving models at lower precision (e.g., FP32 → INT8).\n",
    "\n",
    "\n",
    "# 2. Distributed Inference\n",
    "    # Description: Partitioning model weights across multiple devices to handle large models.\n",
    "    # Techniques:\n",
    "    # Model Partitioning: Divides a large model across multiple GPUs or nodes for more efficient computation.\n",
    "    # In-flight Batching: Enables the processing of new requests while others are still being computed, improving GPU utilization.\n",
    "\n",
    "\n",
    "# 3. Dynamic Batching & Continuous Batching\n",
    "    # Description: Dynamically adjusts batch sizes during inference to maximize GPU utilization, reducing latency.\n",
    "    # Benefits: Ensures high throughput and efficiency, especially for models with varying input lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TensorRT-LLM\n",
    "    # Description: Optimizes models with kernel fusion and memory techniques like KV caching, Paged Attention, and FlashAttention.\n",
    "    # Benefits: Improves performance but requires conversion into TensorRT format for use.\n",
    "\n",
    "\n",
    "# 2. vLLM\n",
    "    # Description: An inference engine that uses Paged Attention to reduce resource wastage, optimizing memory usage and improving throughput.\n",
    "    # Benefits: High efficiency in processing tokens compared to traditional methods.\n",
    "\n",
    "\n",
    "# 3. DeepSpeed-Fastgen\n",
    "    # Description: Combines DeepSpeed's training and inference capabilities for fast, efficient model serving.\n",
    "    # Key Features: Supports Dynamic Splitfuse batching, improving latency and throughput for large models.\n",
    "    \n",
    "\n",
    "# Key Considerations\n",
    "    # Memory Constraints: LLM training and inference are memory-intensive processes. Techniques like PEFT, QAT, and gradient \n",
    "        # checkpointing can help mitigate memory limitations.\n",
    "    # Model Size: Models with billions of parameters may require distributed training or inference strategies to handle the memory demands.\n",
    "    # Efficiency: Methods like mixed precision, distributed training, and dynamic batching are key to improving efficiency in \n",
    "        # training and inference.\n",
    "    # Latency: Techniques like dynamic batching and continuous batching can help reduce inference latency, especially for real-time \n",
    "        # applications.\n",
    "    # Throughput: Distributed inference and model partitioning can improve throughput by leveraging multiple devices for \n",
    "        # parallel processing.\n",
    "    # Resource Optimization: Techniques like TensorRT-LLM and vLLM optimize memory usage and improve performance for large models.\n",
    "    # Scalability: Distributed training and inference methods enable scaling LLMs to handle larger models and datasets efficiently.\n",
    "    # Model Serving: Techniques like DeepSpeed-Fastgen provide end-to-end solutions for training and serving large language \n",
    "        # models effectively.\n",
    "    # Performance Trade-offs: Quantization and distributed strategies may impact model accuracy, so careful monitoring and tuning \n",
    "        # are essential to maintain performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent in Machine Learning\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the cost (or loss) function in machine learning. It works by iteratively updating the parameters (weights) of the model in the direction that reduces the error (cost) the most, i.e., in the direction of the **negative gradient** of the loss function.\n",
    "\n",
    "Let’s assume we have a loss function $J(\\theta)$, where $\\theta$ represents the parameters (weights) of our model. The goal is to minimize this loss function, i.e., find the set of parameters that gives us the lowest possible value for $J(\\theta)$.\n",
    "\n",
    "Gradient Descent works by updating the parameters as follows:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ is the **parameter** (or weight) of the model.\n",
    "- $ \\alpha $ is the **learning rate** (a small positive value, typically between 0 and 1).\n",
    "- $ \\nabla_\\theta J(\\theta) $ is the **gradient** (the partial derivative) of the loss function with respect to the parameter $ \\theta $.\n",
    "\n",
    "\n",
    "## Loss Function Example: Binary Cross-Entropy (BCE)\n",
    "\n",
    "For binary classification problems, one commonly used loss function is the **Binary Cross-Entropy (BCE)** loss. The BCE loss function is given by:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\hat{y}, y) = - \\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{y}_i $ is the predicted probability for the $ i^{th} $ sample.\n",
    "- $ y_i $ is the true label (0 or 1) for the $ i^{th} $ sample.\n",
    "- $ N $ is the number of samples in the dataset.\n",
    "\n",
    "The predicted value $ \\hat{y}_i $ is typically computed using a **sigmoid activation function**:\n",
    "\n",
    "$$\n",
    "\\hat{y}_i = \\sigma(w^T x_i + b)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ w $ are the weights of the model.\n",
    "- $ x_i $ is the input features of the $ i^{th} $ sample.\n",
    "- $ b $ is the bias term.\n",
    "- $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid function.\n",
    "\n",
    "The goal of training is to minimize this loss function using gradient descent, iteratively adjusting the weights \\( w \\) and bias \\( b \\) to reduce the difference between predicted and true labels.\n",
    "\n",
    "\n",
    "## Explanation:\n",
    "\n",
    "- Gradient ($ \\nabla_\\theta J(\\theta) $): This tells us the direction in which the function $ J(\\theta) $ increases the most. In other words, it shows us how steep the slope is at any point on the function. We want to move **in the opposite direction** (down the slope) to minimize the loss.\n",
    "  \n",
    "- **Learning rate ($ \\alpha $)**: This determines the size of the steps we take in the direction of the gradient. A small learning rate means small steps, and a large learning rate means larger steps. Too small of a learning rate will make the process slow, while too large of a learning rate could cause overshooting and prevent convergence.\n",
    "\n",
    "- **loss.backward()** - Computes the gradients of the loss with respect to the model's parameters using backpropagation.\n",
    "- **optimizer.step()** - Updates the model's parameters based on the computed gradients, performing the gradient descent step.\n",
    "\n",
    "\n",
    "\n",
    "## Calculating Gradients with Respect to Parameters\n",
    "\n",
    "To update the parameters using gradient descent, we need to calculate the gradient of the loss function with respect to each parameter (weights and biases). Here's how it's done for the Binary Cross-Entropy (BCE) loss in a logistic regression model.\n",
    "\n",
    "### 1. Binary Cross-Entropy Loss:\n",
    "\n",
    "For binary classification, the BCE loss function is:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(y, \\hat{y}) = - \\left[ y \\log(\\hat{y}) + (1 - y) \\log(1 - \\hat{y}) \\right]\n",
    "$$\n",
    "\n",
    "Where $ \\hat{y} = \\sigma(w \\cdot x + b) $, and $ \\sigma(z) = \\frac{1}{1 + e^{-z}} $ is the sigmoid activation function.\n",
    "\n",
    "### 2. Gradient of Loss with respect to Parameters:\n",
    "\n",
    "- The gradient of the loss with respect to \\( w \\) (weight):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w} = (\\hat{y} - y) \\cdot x\n",
    "$$\n",
    "\n",
    "- The gradient of the loss with respect to \\( b \\) (bias):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial b} = \\hat{y} - y\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\hat{y} $ is the predicted probability ($ \\hat{y} = \\sigma(w \\cdot x + b) $).\n",
    "- $ y $ is the true label (0 or 1).\n",
    "- $ x $ is the input feature vector.\n",
    "\n",
    "These gradients are used to update the parameters in the direction that minimizes the loss:\n",
    "\n",
    "$$\n",
    "w = w - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial w}\n",
    "$$\n",
    "$$\n",
    "b = b - \\alpha \\cdot \\frac{\\partial \\mathcal{L}}{\\partial b}\n",
    "$$\n",
    "\n",
    "Where $ \\alpha $ is the learning rate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variants of Gradient Descent in Machine Learning\n",
    "\n",
    "## Stochastic Gradient Descent (SGD)\n",
    "\n",
    "Stochastic Gradient Descent (SGD) is a variant of the basic gradient descent algorithm where the parameters are updated using only one data point (randomly selected) at a time, rather than the entire dataset. This leads to faster convergence but with noisier updates.\n",
    "\n",
    "The update rule for SGD is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ \\theta $ is the parameter (or weight).\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)}) $ is the gradient of the cost function, computed with respect to the $i$-th training example $ (x^{(i)}, y^{(i)}) $.\n",
    "\n",
    "Since SGD uses one data point at a time, it is computationally more efficient but introduces more variance in the updates. This often causes the algorithm to fluctuate around the minimum rather than converging smoothly.\n",
    "\n",
    "## Mini-Batch Gradient Descent (MBGD)\n",
    "\n",
    "Mini-Batch Gradient Descent is a compromise between the standard (Batch) Gradient Descent and Stochastic Gradient Descent. In mini-batch GD, instead of using the full dataset or a single data point, a small random subset of the data (mini-batch) is used to compute the gradient.\n",
    "\n",
    "The update rule for Mini-Batch GD is:\n",
    "\n",
    "$$\n",
    "\\theta = \\theta - \\alpha \\cdot \\frac{1}{m} \\sum_{i=1}^{m} \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m $ is the number of samples in the mini-batch.\n",
    "\n",
    "Mini-batch gradient descent provides a balance between the computational efficiency of batch gradient descent and the faster convergence of stochastic gradient descent.\n",
    "\n",
    "## Adagrad (Adaptive Gradient Algorithm)\n",
    "\n",
    "Adagrad is an adaptive learning rate method. It adjusts the learning rate for each parameter individually based on the past gradient updates. Parameters that have larger gradients (more significant updates) will have smaller learning rates, while parameters with smaller gradients will have larger learning rates.\n",
    "\n",
    "The update rule for Adagrad is:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{G_t + \\epsilon}} \\cdot \\nabla_\\theta J(\\theta)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ G_t $ is the sum of the squared gradients up to time step $ t $:\n",
    "\n",
    "$$\n",
    "G_t = \\sum_{i=1}^{t} \\nabla_\\theta J(\\theta_i)^2\n",
    "$$\n",
    "\n",
    "- $ \\epsilon $ is a small constant added to prevent division by zero (typically $ 10^{-8} $).\n",
    "- $ \\alpha $ is the learning rate.\n",
    "\n",
    "The adaptive nature of Adagrad makes it effective for sparse data (where most features are zero) or data with varying levels of gradients.\n",
    "\n",
    "## RMSprop (Root Mean Square Propagation)\n",
    "\n",
    "RMSprop is another adaptive learning rate method, designed to solve some issues with Adagrad, especially the fact that Adagrad's learning rate can shrink too much over time. RMSprop uses a moving average of squared gradients to scale the learning rate, which helps keep the updates stable.\n",
    "\n",
    "The update rule for RMSprop is:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{E[g^2]_t + \\epsilon}} \\cdot g_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ E[g^2]_t $ is the moving average of squared gradients at time step $ t $:\n",
    "\n",
    "$$\n",
    "E[g^2]_t = \\beta \\cdot E[g^2]_{t-1} + (1-\\beta) \\cdot g_t^2\n",
    "$$\n",
    "\n",
    "where $ \\beta $ is a smoothing factor (often close to $ 0.9 $).\n",
    "- $ g_t $ is the gradient at time step $ t $.\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\epsilon $ is a small constant added to prevent division by zero.\n",
    "\n",
    "RMSprop helps improve convergence by adapting the learning rate to the magnitude of recent gradients, which is useful for non-stationary objectives.\n",
    "\n",
    "## Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam combines ideas from both Adagrad and RMSprop. It computes adaptive learning rates for each parameter, but also takes into account the momentum of past gradients (i.e., the exponentially decaying average of past gradients) to improve optimization.\n",
    "\n",
    "The update rule for Adam is:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\alpha}{\\sqrt{v_t} + \\epsilon} \\cdot m_t\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $ m_t $ is the first moment estimate (mean of gradients), typically:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "$$\n",
    "\n",
    "with $ \\beta_1 $ being the decay rate (typically $ 0.9 $).\n",
    "\n",
    "- $ v_t $ is the second moment estimate (variance of gradients), typically:\n",
    "\n",
    "$$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "$$\n",
    "\n",
    "with $ \\beta_2 $ being another decay rate (typically $ 0.999 $).\n",
    "\n",
    "- $ \\alpha $ is the learning rate.\n",
    "- $ \\epsilon $ is a small constant added to prevent division by zero.\n",
    "\n",
    "Adam is widely used because it combines the benefits of both momentum and adaptive learning rates, making it well-suited for a variety of machine learning tasks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"transformer.png\" alt=\"Feed-Forward Network\" />\n",
    "</p>\n",
    "\n",
    "\n",
    "# Transformer Model: Step-by-Step Workflow\n",
    "\n",
    "Transformers are a foundational architecture in modern deep learning, particularly in natural language processing (NLP). Below is a comprehensive step-by-step guide outlining the complete workflow of a transformer model, from input tensors to the final output.\n",
    "\n",
    "## 1. Input Preparation\n",
    "\n",
    "### 1.1. Raw Text Input\n",
    "- **Description:** The process begins with raw text data, such as sentences or paragraphs.\n",
    "- **Example:** `\"The cat sat on the mat.\"`\n",
    "\n",
    "### 1.2. Tokenization\n",
    "- **Description:** Converts raw text into discrete tokens (words, subwords, or characters).\n",
    "- **Substeps:**\n",
    "  - **a. Splitting:** Break text into tokens based on spaces or specific rules.\n",
    "  - **b. Subword Tokenization (Optional):** Further splits rare words into subword units using algorithms like Byte Pair Encoding (BPE) or WordPiece.\n",
    "- **Example:** `\"The cat sat on the mat.\"` → `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]`\n",
    "\n",
    "### 1.3. Numerical Encoding\n",
    "- **Description:** Maps tokens to unique numerical identifiers using a vocabulary index.\n",
    "- **Substeps:**\n",
    "  - **a. Vocabulary Lookup:** Each token is assigned an integer based on its position in the vocabulary.\n",
    "  - **b. Handling Unknown Tokens:** Tokens not present in the vocabulary are mapped to a special `[UNK]` token.\n",
    "- **Example:** `[\"The\", \"cat\", \"sat\", \"on\", \"the\", \"mat\", \".\"]` → `[101, 2024, 2003, 1037, 7099, 2527, 1012]`\n",
    "\n",
    "## 2. Embedding Layer\n",
    "\n",
    "### 2.1. Token Embeddings\n",
    "- **Description:** Transforms numerical token IDs into dense vector representations.\n",
    "- **Substeps:**\n",
    "  - **a. Embedding Matrix:** A learnable matrix where each row corresponds to a token's embedding.\n",
    "  - **b. Lookup:** Each token ID retrieves its corresponding embedding vector.\n",
    "- **Example:** `Embedding Matrix [Vocab Size x d_model]` → `X = [n_tokens x d_model]`\n",
    "\n",
    "    $$\n",
    "    \\mathbf{E} = W_{\\text{emb}} \\cdot \\mathbf{x}\n",
    "    $$\n",
    "    where:\n",
    "    - **E** is the token embedding vector (dimension $d$),\n",
    "    - **$W_{\\text{emb}}$** is the embedding matrix,\n",
    "    - **x** is the token index (one-hot vector representing the word).\n",
    "\n",
    "\n",
    "### 2.2. Positional Encodings\n",
    "- **Description:** Adds information about the position of each token in the sequence to the token embeddings.\n",
    "- **Substeps:**\n",
    "  - **a. Sinusoidal Encoding (Fixed):** Uses sine and cosine functions of different frequencies.\n",
    "  - **b. Learned Positional Embeddings (Learnable):** Embeddings are learned during training.\n",
    "- **Example:**\n",
    "  $$\n",
    "  \\text{Positional Encoding}_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right) \\\\\n",
    "  \\text{Positional Encoding}_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right)\n",
    "  $$\n",
    "  \n",
    "### 2.3. Combined Embeddings\n",
    "- **Description:** Summing token embeddings with positional encodings to incorporate positional information.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  E = X + \\text{Positional Encodings}\n",
    "  $$\n",
    "- **Result:** A matrix `E` representing the input sequence with positional information.\n",
    "\n",
    "## 3. Transformer Architecture\n",
    "\n",
    "The transformer consists of an **Encoder** and a **Decoder**, each composed of multiple layers. Below, we outline the main components and their subcomponents.\n",
    "\n",
    "### 3.1. Encoder\n",
    "\n",
    "#### 3.1.1. Multi-Head Self-Attention\n",
    "- **Description:** Allows the model to focus on different parts of the input sequence simultaneously.\n",
    "- **Substeps:**\n",
    "  - **a. Linear Projections:** Compute queries (Q), keys (K), and values (V) using learned weight matrices.\n",
    "    $$\n",
    "    Q = E W_Q, \\quad K = E W_K, \\quad V = E W_V\n",
    "    $$\n",
    "  \n",
    "    Each token's embedding $ E_i $ is projected into the query, key, and value vectors using learned weight matrices:\n",
    "\n",
    "    $$\n",
    "    Q_i = W_Q \\cdot E_i \\quad \\text{(Query vector for token $ E_i $)}\n",
    "    $$\n",
    "    $$\n",
    "    K_i = W_K \\cdot E_i \\quad \\text{(Key vector for token $ E_i $)}\n",
    "    $$\n",
    "    $$\n",
    "    V_i = W_V \\cdot E_i \\quad \\text{(Value vector for token $ E_i $)}\n",
    "    $$\n",
    "\n",
    "    Where:\n",
    "    - $ W_Q, W_K, W_V $ are learned weight matrices,\n",
    "    - $ E_i $ is the token embedding for token $ E_i $.\n",
    "\n",
    "\n",
    "  - **b. Attention Scores:** For each query vector in Q, compute the dot product with all key vectors in K. These scores indicate the relevance or similarity between each query and all keys.\n",
    "    - *High Score:* The corresponding value should be given more attention.\n",
    "    - *Low Score:* The corresponding value should be given less attention.\n",
    "      $$\n",
    "      Attention Scores = Q * K^T\n",
    "      $$\n",
    "       **Example for $ N=4 $:**\n",
    "          $$\n",
    "          Q = \n",
    "          \\begin{bmatrix}\n",
    "          q_{1} \\\\[6pt]\n",
    "          q_{2} \\\\[6pt]\n",
    "          q_{3} \\\\[6pt]\n",
    "          q_{4}\n",
    "          \\end{bmatrix}_{4 \\times d_k}, \\quad\n",
    "          K =\n",
    "          \\begin{bmatrix}\n",
    "          k_{1} \\\\[6pt]\n",
    "          k_{2} \\\\[6pt]\n",
    "          k_{3} \\\\[6pt]\n",
    "          k_{4}\n",
    "          \\end{bmatrix}_{4 \\times d_k}\n",
    "          $$\n",
    "\n",
    "      The score matrix (before scaling and softmax) is:\n",
    "          $$\n",
    "          QK^T = \n",
    "          \\begin{bmatrix}\n",
    "          q_{1}\\cdot k_{1}^T & q_{1}\\cdot k_{2}^T & q_{1}\\cdot k_{3}^T & q_{1}\\cdot k_{4}^T \\\\[6pt]\n",
    "          q_{2}\\cdot k_{1}^T & q_{2}\\cdot k_{2}^T & q_{2}\\cdot k_{3}^T & q_{2}\\cdot k_{4}^T \\\\[6pt]\n",
    "          q_{3}\\cdot k_{1}^T & q_{3}\\cdot k_{2}^T & q_{3}\\cdot k_{3}^T & q_{3}\\cdot k_{4}^T \\\\[6pt]\n",
    "          q_{4}\\cdot k_{1}^T & q_{4}\\cdot k_{2}^T & q_{4}\\cdot k_{3}^T & q_{4}\\cdot k_{4}^T\n",
    "          \\end{bmatrix}_{4 \\times 4}\n",
    "          $$\n",
    "      Each row now represents how much attention a single query token gives to every token in the sequence, and each row sums to 1.\n",
    "      \n",
    "  - **c. Scaled Dot-Product Attention:** Calculate attention scores, apply scaling, softmax, and compute weighted sums.\n",
    "    $$\n",
    "    \\text{Scores} = \\frac{Q K^T}{\\sqrt{d_k}} \\\\\n",
    "    \\text{Attention Weights} = \\text{softmax}(\\text{Scores}) \\\\ \n",
    "    \\text{Attention Output} = \\text{Attention Weights} \\times V\n",
    "    $$\n",
    "    - Scaling prevents the dot products from growing too large, which can push the softmax function into regions with very small gradients.\n",
    "    - These weights determine how much each value vector (from V) contributes to the final output. \\\\\n",
    "    - Weights aggregates the value vectors based on their relevance to each query, producing a weighted sum that captures contextual information.\n",
    "  \n",
    "  - **d. Concatenation:** Concatenate outputs from all attention heads.\n",
    "    $$\n",
    "    \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) \\in \\mathbb{R}^{N \\times (h \\cdot d_k)}\n",
    "    $$\n",
    "  - **e. Final Linear Projection:** Apply a linear transformation to the concatenated output.\n",
    "    $$\n",
    "    \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\ldots, \\text{head}_h) W_O\n",
    "    $$\n",
    "    - Here, $ W_O $ is a learnable weight matrix of size $(h \\cdot d_k) \\times d_{model}$.\n",
    "\n",
    "#### 3.1.2. Add & Norm\n",
    "- **Description:**  \n",
    "  Incorporates a **residual (skip) connection** by adding the sub-layer's input \\( E \\) to its output, followed by **layer normalization**. This helps preserve the original information and stabilizes the training process.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(E + \\text{MultiHead}(Q, K, V))\n",
    "  $$\n",
    "  $$\n",
    "  \\text{or}\n",
    "  $$\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(E + \\text{Attention Output})\n",
    "  $$\n",
    "\n",
    "\n",
    "#### 3.1.3. Feed-Forward Network (FFN)\n",
    "- **Description:**  \n",
    "  Processes each position independently through a two-layer fully connected network to capture complex patterns and non-linear relationships.\n",
    "  \n",
    "- **Substeps:**\n",
    "  - **a. Linear Transformation:**  \n",
    "    **Purpose:** Expands the dimensionality of the input to increase the model's capacity to learn.  \n",
    "    **Formula:**  \n",
    "    $$\n",
    "    \\text{FFN}_1 = \\text{ReLU}(E W_1 + b_1)\n",
    "    $$\n",
    "    - $ E $: **Input** from the previous **Add & Norm** step ($E$ here is the **Output** from the Add & Norm step).\n",
    "    - $ W_1 $: **Weight matrix** for the first linear layer.\n",
    "    - $ b_1 $: **Bias vector** for the first linear layer.\n",
    "    - **ReLU:** Activation function introducing non-linearity.\n",
    "  \n",
    "  - **b. Linear Transformation:**  \n",
    "    **Purpose:** Reduces the dimensionality back to the original size, ensuring consistency in the model's architecture.  \n",
    "    **Formula:**  \n",
    "    $$\n",
    "    \\text{FFN}_2 = \\text{FFN}_1 W_2 + b_2\n",
    "    $$\n",
    "    - $ W_2 $: **Weight matrix** for the second linear layer.\n",
    "    - $ b_2 $: **Bias vector** for the second linear layer.\n",
    "\n",
    "#### 3.1.4. Add & Norm\n",
    "- **Description:**  \n",
    "  Adds a **residual (skip) connection** by combining the FFN's output with its input $ E $, followed by **layer normalization**. This step helps preserve the original information and stabilizes the training process.\n",
    "  \n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Encoder Output} = \\text{LayerNorm}(E + \\text{FFN}_2)\n",
    "  $$\n",
    "  - $ E $: **Input** to the Feed-Forward Network (output from the previous **Add & Norm** step).\n",
    "  - $ \\text{FFN}_2 $: **Output** from the Feed-Forward Network.\n",
    "  - **LayerNorm:** Normalizes the combined output to have a mean of 0 and a variance of 1, enhancing training stability and performance.\n",
    "\n",
    "---\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "---\n",
    "\n",
    "### 3.2. Decoder\n",
    "\n",
    "While the **Encoder** processes the input sequence to generate contextualized representations, the **Decoder** generates the output sequence by leveraging these representations and the previously generated tokens. The Decoder consists of multiple layers, each containing three main sub-layers:\n",
    "\n",
    "1. **Masked Multi-Head Self-Attention**\n",
    "2. **Multi-Head Cross-Attention**\n",
    "3. **Position-wise Feed-Forward Network (FFN)**\n",
    "\n",
    "Each sub-layer is followed by **Residual Connections** and **Layer Normalization**, similar to the Encoder.\n",
    "\n",
    "\n",
    "#### 3.2.1. Masked Multi-Head Self-Attention\n",
    "- **Description:** Similar to encoder's self-attention but with masking to prevent attending to future tokens.\n",
    "- **Purpose:**  \n",
    "  - **Prevent Information Leakage:** By masking future tokens, the model ensures that the prediction for the current token doesn't incorporate information from tokens that haven't been generated yet.\n",
    "  - **Maintain Causality:** Ensures that the generation process respects the sequential nature of language.\n",
    "\n",
    "- **Substeps:**\n",
    "  - **a. Masking:** Apply a causal mask to ensure autoregressive property. $M$ (with `0` for allowed positions and `-∞` for future positions)\n",
    "    $$\n",
    "    \\text{Masked Scores} = \\frac{Q K^T}{\\sqrt{d_k}} + M \\\\\n",
    "    $$\n",
    "  \n",
    "  $$\n",
    "  \\text{Raw Scores (no mask)} = \n",
    "  \\begin{bmatrix}\n",
    "  s_{00} & s_{01} & s_{02} & s_{03} \\\\\n",
    "  s_{10} & s_{11} & s_{12} & s_{13} \\\\\n",
    "  s_{20} & s_{21} & s_{22} & s_{23} \\\\\n",
    "  s_{30} & s_{31} & s_{32} & s_{33} \\\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  \n",
    "  $$\n",
    "  \\text{Mask Matrix \\(M\\) = }\n",
    "  \\begin{bmatrix}\n",
    "  0 & -\\infty & -\\infty & -\\infty \\\\\n",
    "  0 & 0 & -\\infty & -\\infty \\\\\n",
    "  0 & 0 & 0 & -\\infty \\\\\n",
    "  0 & 0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "**At Each Timestep \\(t\\):**\n",
    "- At $t=0$: Only token 0 visible:\n",
    "  $$\n",
    "  \\text{Masked Scores} = \n",
    "  \\begin{bmatrix}\n",
    "  s_{00} & -\\infty & -\\infty & -\\infty\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- At $t=1$: Tokens 0 and 1 visible:\n",
    "  $$\n",
    "  \\text{Masked Scores} = \n",
    "  \\begin{bmatrix}\n",
    "  s_{00} & -\\infty \\\\\n",
    "  s_{10} & s_{11}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "- At $t=2$: Tokens 0,1,2 visible:\n",
    "  $$\n",
    "  \\begin{bmatrix}\n",
    "  s_{00} & -\\infty & -\\infty \\\\\n",
    "  s_{10} & s_{11} & -\\infty \\\\\n",
    "  s_{20} & s_{21} & s_{22}\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Masked Scores} = \n",
    "  \\begin{bmatrix}\n",
    "  s_{00} & -\\infty & -\\infty & -\\infty \\\\\n",
    "  s_{10} & s_{11} & -\\infty & -\\infty \\\\\n",
    "  s_{20} & s_{21} & s_{22} & -\\infty \\\\\n",
    "  s_{30} & s_{31} & s_{32} & s_{33} \\\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    " - Applying Softmax:\n",
    "  $$\n",
    "  \\text{Attention Weights} = \\text{softmax}(\\text{Masked Scores})\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  \\text{Attention Weights} = \n",
    "  \\begin{bmatrix}\n",
    "  s_{00} & 0 & 0 & 0 \\\\\n",
    "  s_{10} & s_{11} & 0 & 0 \\\\\n",
    "  s_{20} & s_{21} & s_{22} & 0 \\\\\n",
    "  s_{30} & s_{31} & s_{32} & s_{33} \\\\\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **b. Compute Attention Output:** As in encoder.\n",
    "  $$\n",
    "  \\text{MaskedAttention Output} = \\text{MultiHead}(Q, K, V) \\quad \\text{with Masking}\n",
    "  $$\n",
    "  $$\n",
    "  or\n",
    "  $$ \n",
    "  $$\n",
    "  \\text{MaskedAttention Output} = \\text{Attention Weights} \\times V\n",
    "  $$\n",
    "\n",
    "- **How Masking Works:**\n",
    "  - **Mask Matrix:** A triangular matrix that masks (sets to $-\\infty$) the attention scores for future tokens.\n",
    "  - **Application:** Before applying the softmax function, the mask is added to the attention scores to nullify the influence of future tokens.\n",
    "\n",
    "\n",
    "#### 3.2.2. Add & Norm (Post Masked Self-Attention)\n",
    "- **Description:** Residual connection and layer normalization.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(E + \\text{Masked MultiHead}(Q, K, V))\n",
    "  $$\n",
    "\n",
    "#### 3.2.3. Multi-Head Attention over Encoder Outputs (or Multi-Head Cross-Attention)\n",
    "- **Description:** Allows the decoder to attend to the encoder's output.\n",
    "- **Components:**\n",
    "  - **Queries (Q):** Derived from the Decoder's previous sub-layer (Masked Self-Attention output).\n",
    "  - **Keys (K) & Values (V):** Derived from the Encoder's final output.\n",
    "\n",
    "- **Substeps:**\n",
    "  - **a. Compute Q from Decoder:** \n",
    "    $$\n",
    "    Q = \\text{Decoder Output} W_Q'\n",
    "    $$\n",
    "  - **b. Compute K and V from Encoder:**\n",
    "    $$\n",
    "    K = \\text{Encoder Output} W_K', \\quad V = \\text{Encoder Output} W_V'\n",
    "    $$\n",
    "  - **c. Compute Attention Output:** As in encoder.\n",
    "\n",
    "#### 3.2.4. Add & Norm\n",
    "- **Description:**   Adds another **residual (skip) connection** by combining the input $ E' $ (output from the previous Add & Norm step) with the cross-attention output, followed by **layer normalization**. This step further integrates information from the Encoder while maintaining training stability.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Output} = \\text{LayerNorm}(\\text{Decoder Output} + \\text{MultiHead}(Q, K, V))\n",
    "  $$\n",
    "\n",
    "#### 3.2.5. Feed-Forward Network (FFN) - Position Wise Feed Forward Network\n",
    "- **Description:** Same as encoder's FFN.\n",
    "- **Substeps:**\n",
    "  - **a. Linear Transformation and Activation:**\n",
    "    $$\n",
    "    \\text{FFN}_1 = \\text{ReLU}(\\text{Output} W_1 + b_1)\n",
    "    $$\n",
    "  - **b. Linear Transformation:**\n",
    "    $$\n",
    "    \\text{FFN}_2 = \\text{FFN}_1 W_2 + b_2\n",
    "    $$\n",
    "\n",
    "#### 3.2.6. Add & Norm (Post Feed-Forward Network)\n",
    "- **Description:** Adds a final **residual (skip) connection** by combining the FFN's output $ \\text{FFN}_2 $ with its input $ E'' $ (output from the previous Add & Norm step), followed by **layer normalization**. This final normalization ensures that the Encoder's output is well-conditioned for generating predictions.\n",
    "\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Decoder Output} = \\text{LayerNorm}(\\text{Output from Add & Norm} + \\text{FFN}_2)\n",
    "  $$\n",
    "\n",
    "\n",
    "## 4. Output Embeddings and Generation\n",
    "\n",
    "- **Description:**  \n",
    "  The **Decoder Output** is transformed into **output embeddings** which are then used to generate the final predictions (e.g., token probabilities).\n",
    "\n",
    "### 4.1. Final Linear Layer\n",
    "- **Description:** Transforms decoder outputs to match the size of the target vocabulary.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Logits} = \\text{Decoder Output} W_O + b_O\n",
    "  $$\n",
    "\n",
    "### 4.2. Softmax\n",
    "- **Description:** Converts logits into probability distributions over the vocabulary.\n",
    "- **Formula:**\n",
    "  $$\n",
    "  \\text{Probabilities} = \\text{softmax}(\\text{Logits})\n",
    "  $$\n",
    "\n",
    "### 4.3. Token Selection\n",
    "- **Description:**  The token with the highest probability is selected as the next token in the sequence.\n",
    "- **Methods:**\n",
    "  - **a. Greedy Search:** Select the token with the highest probability.\n",
    "  - **b. Beam Search:** Explore multiple sequences to find the most likely output.\n",
    "  - **c. Sampling:** Randomly sample tokens based on their probabilities.\n",
    "\n",
    "### 4.4. Output Text\n",
    "- **Description:** Converts selected token IDs back to human-readable text.\n",
    "- **Example:** `[101, 2024, 2003, 102]` → `\"The cat sat.\"`\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Masked Multi-Head Attention vs. Regular Multi-Head Attention\n",
    "\n",
    "- **Masked Multi-Head Attention:**  \n",
    "  - **Used In:** Decoder's self-attention sub-layer.\n",
    "  - **Function:** Prevents the model from attending to future tokens in the sequence during training, maintaining causality.\n",
    "  \n",
    "- **Regular Multi-Head Attention:**  \n",
    "  - **Used In:** Encoder's self-attention sub-layers and Decoder's cross-attention sub-layer.\n",
    "  - **Function:** Allows the model to attend to all positions in the input or encoder's output without restrictions.\n",
    "\n",
    "- **Key Difference:**  \n",
    "  The **masking** in the Decoder's self-attention ensures that each position can only attend to previous and current positions, not future ones, which is essential for tasks like language generation where the model should not peek ahead.\n",
    "\n",
    "---\n",
    "\n",
    "### 5.1 Differences Between Encoder Inputs and Decoder Outputs\n",
    "\n",
    "- **Encoder Inputs:**\n",
    "  - **Nature:** The source sequence (e.g., a sentence in the source language).\n",
    "  - **Processing:** Fully accessible to all Encoder sub-layers; each token can attend to every other token in the sequence.\n",
    "  \n",
    "- **Decoder Outputs:**\n",
    "  - **Nature:** The target sequence being generated (e.g., a sentence in the target language).\n",
    "  - **Processing:** \n",
    "    - **Masked Self-Attention:** Each position can only attend to previous tokens in the target sequence.\n",
    "    - **Cross-Attention:** Each position can attend to all tokens in the Encoder's output, integrating source information.\n",
    "    - **Autoregressive Generation:** Each new token is generated based on previously generated tokens and the Encoder's context.\n",
    "\n",
    "- **Summary of Differences:**\n",
    "  | Feature                     | Encoder Inputs                      | Decoder Outputs                                |\n",
    "  |-----------------------------|-------------------------------------|------------------------------------------------|\n",
    "  | **Access to Sequence**      | Full access to entire input sequence| Limited to past and current tokens (masked)    |\n",
    "  | **Attention Mechanism**     | Self-Attention (no masking)         | Masked Self-Attention & Cross-Attention        |\n",
    "  | **Generation Flow**         | Processes input in parallel         | Generates output sequentially (autoregressive) |\n",
    "  | **Integration with Encoder**| Independent of Encoder               | Attends to Encoder's output for context        |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'over']\n",
      "['the', 'over', 'fox']\n",
      "['the', 'over', 'fox', 'lazy']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps', 'lazy']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps', 'lazy', 'jumps']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps', 'lazy', 'jumps', 'quick']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps', 'lazy', 'jumps', 'quick', 'brown']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown']\n",
      "['the', 'over', 'fox', 'lazy', 'jumps', 'jumps', 'jumps', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'lazy', 'jumps', 'lazy', 'jumps', 'quick', 'brown', 'brown', 'fox']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 5, 3, 6, 4, 4, 4, 4, 6, 4, 1, 2, 2, 6, 4, 6, 4, 1, 2, 2, 3]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Example small vocabulary (just for demonstration, real training would require more data)\n",
    "word_list = [\"the\", \"quick\", \"brown\", \"fox\", \"jumps\", \"over\", \"lazy\", \"dog\"]\n",
    "vocab_size = len(word_list)\n",
    "word_to_idx = {w: i for i, w in enumerate(word_list)}\n",
    "idx_to_word = {i: w for w, i in word_to_idx.items()}\n",
    "\n",
    "class BigramLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(BigramLanguageModel, self).__init__()\n",
    "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
    "\n",
    "    def forward(self, index, targets=None):\n",
    "        # logits: [Batch, Time, Vocab]\n",
    "        logits = self.token_embedding_table(index)\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B * T, C)\n",
    "            targets = targets.view(B * T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, index, max_new_tokens):\n",
    "        for _ in range(max_new_tokens):\n",
    "            logits, _ = self.forward(index)\n",
    "            # Focus on last timestep for next token prediction\n",
    "            logits = logits[:, -1, :]\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            index_next = torch.multinomial(probs, num_samples=1)    # Sample from the distribution\n",
    "            index = torch.cat((index, index_next), dim=1)\n",
    "            \n",
    "            next_word = index.view(-1).tolist()\n",
    "            print([idx_to_word[i] for i in next_word])\n",
    "\n",
    "        # print()  # New line at the end of generation\n",
    "        return index\n",
    "\n",
    "# Instantiate and run the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BigramLanguageModel(vocab_size).to(device)\n",
    "\n",
    "# Start generation with the token \"the\"\n",
    "start_word = \"the\"\n",
    "context = torch.tensor([[word_to_idx[start_word]]], dtype=torch.long, device=device)\n",
    "\n",
    "# Generate more words and print as they come out\n",
    "model.generate(context, max_new_tokens=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM in code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 8.687405735254288\n",
      "Epoch [2/5], Loss: 8.092840701341629\n",
      "Epoch [3/5], Loss: 7.657473549246788\n",
      "Epoch [4/5], Loss: 7.372279062867165\n",
      "Epoch [5/5], Loss: 7.210036337375641\n",
      "Predicted next word: 3797\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define the GPT-like Model with Embedding Layers\n",
    "class SimpleGPT(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, num_heads, num_layers, num_classes, max_len=512):\n",
    "        super(SimpleGPT, self).__init__()\n",
    "        \n",
    "        # Embedding layer: Map tokens to embedding vectors\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        \n",
    "        # Positional encoding (used to add positions to the embeddings)\n",
    "        self.positional_encoding = nn.Parameter(torch.zeros(1, max_len, embed_size))\n",
    "        \n",
    "        # Transformer Layer\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_size,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers,\n",
    "            dim_feedforward=embed_size*4\n",
    "        )\n",
    "        \n",
    "        # Output layer for classification or next-token prediction\n",
    "        self.fc_out = nn.Linear(embed_size, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Get token embeddings\n",
    "        embeddings = self.embedding(x)\n",
    "        \n",
    "        # Add positional encoding to embeddings\n",
    "        seq_len = x.size(1)\n",
    "        embeddings = embeddings + self.positional_encoding[:, :seq_len, :]\n",
    "        \n",
    "        # Pass through the transformer\n",
    "        transformer_out = self.transformer(embeddings, embeddings)\n",
    "        \n",
    "        # Use the last token's representation for classification or next token prediction\n",
    "        output = self.fc_out(transformer_out[:, -1, :])\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "vocab_size = 5000  # Example vocabulary size\n",
    "embed_size = 256  # Embedding dimension\n",
    "num_heads = 8     # Number of attention heads\n",
    "num_layers = 6    # Number of transformer layers\n",
    "num_classes = vocab_size  # For next-token prediction (same size as vocab)\n",
    "max_len = 512     # Max sequence length\n",
    "batch_size = 32   # Batch size\n",
    "learning_rate = 1e-4\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleGPT(vocab_size, embed_size, num_heads, num_layers, num_classes, max_len)\n",
    "model.to(device)\n",
    "\n",
    "# Loss and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "def train(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch in train_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to device\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader)}\")\n",
    "\n",
    "# Sample DataLoader for training (replace with actual data)\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Simulated training data (replace with actual tokenized text)\n",
    "train_data = torch.randint(0, vocab_size, (1000, max_len))  # 1000 sequences\n",
    "train_labels = torch.randint(0, vocab_size, (1000,))  # Corresponding next token (or class)\n",
    "\n",
    "# DataLoader\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "train(model, train_loader, criterion, optimizer)\n",
    "\n",
    "# Inference function for a sample text\n",
    "def infer(model, text, vocab, max_len=512):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenize input text (basic tokenizer for demo purposes)\n",
    "    tokens = torch.tensor([vocab.get(word, 0) for word in text.split()]).unsqueeze(0).to(device)  # Adding batch dimension and moving to device\n",
    "    tokens = tokens[:, :max_len]  # Ensure the sequence length is within max_len\n",
    "    \n",
    "    # Get the model's prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(tokens)\n",
    "    \n",
    "    # Convert output to probabilities (for next token prediction)\n",
    "    prob = torch.softmax(output, dim=-1)\n",
    "    \n",
    "    # Get the predicted token (highest probability)\n",
    "    predicted_token_idx = torch.argmax(prob, dim=-1).item()\n",
    "    \n",
    "    # Decode the token back to text (inverse vocab lookup)\n",
    "    reverse_vocab = {v: k for k, v in vocab.items()}\n",
    "    predicted_word = reverse_vocab.get(predicted_token_idx, \"<UNK>\")\n",
    "    \n",
    "    return predicted_word\n",
    "\n",
    "# Sample Vocab (just for illustration)\n",
    "vocab = {str(i): i for i in range(vocab_size)}  # Just a simple numerical vocabulary, replace with actual vocab\n",
    "\n",
    "# Sample input text\n",
    "sample_text = \"this is a sample sentence\"\n",
    "\n",
    "# Perform inference\n",
    "predicted_word = infer(model, sample_text, vocab)\n",
    "print(f\"Predicted next word: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Worked Example of Attention with a Toy Corpus\n",
    "\n",
    "We’ve explained the mathematics behind single-head and multi-head attention. Now, let’s apply these steps to a simple, concrete example. This will demonstrate how the calculations flow from inputs all the way through to the final attention output.\n",
    "\n",
    "## Toy Setup\n",
    "\n",
    "### Assumptions and Simplifications\n",
    "\n",
    "- **Vocabulary and Embeddings:**  \n",
    "  We have a small vocabulary with three tokens:  \n",
    "  1. \"The\"\n",
    "  2. \"cat\"\n",
    "  3. \"sat\"\n",
    "\n",
    "  We assume we already have embeddings for these tokens. Let's say `d_model = 4` for simplicity. Thus, each token is represented by a 4-dimensional vector. For demonstration:\n",
    "  - \"The\" → $[0.1, 0.3, 0.5, 0.7]$\n",
    "  - \"cat\" → $[0.2, 0.4, 0.4, 0.6]$\n",
    "  - \"sat\" → $[0.15, 0.25, 0.5, 0.1]$\n",
    "\n",
    "- **Input Sequence:**  \n",
    "  Suppose our input sequence is: \"The cat sat\"\n",
    "\n",
    "So we have 3 tokens, hence $n=3$.\n",
    "\n",
    "- **Positional Encodings:**  \n",
    "For simplicity, let’s not add complex positional encodings. Suppose we have a simple positional encoding that just adds a small unique offset for each token position:\n",
    "- Position 1 encoding: $[0.0, 0.1, 0.0, 0.0]$\n",
    "- Position 2 encoding: $[0.0, 0.0, 0.1, 0.0]$\n",
    "- Position 3 encoding: $[0.0, 0.0, 0.0, 0.1]$\n",
    "\n",
    "After adding positional encodings:\n",
    "- For \"The\" (position 1): $[0.1, 0.3+0.1, 0.5, 0.7] = [0.1, 0.4, 0.5, 0.7]$\n",
    "- For \"cat\" (position 2): $[0.2, 0.4, 0.4+0.1, 0.6] = [0.2, 0.4, 0.5, 0.6]$\n",
    "- For \"sat\" (position 3): $[0.15, 0.25, 0.5, 0.1+0.1] = [0.15, 0.25, 0.5, 0.2]$\n",
    "\n",
    "Thus, our input matrix $X$ is:\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\[6pt]\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\[6pt]\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "where each row is a token embedding with positional info.\n",
    "\n",
    "- **Parameters (W_Q, W_K, W_V):**  \n",
    "Let’s define:\n",
    "- $d_{\\text{model}} = 4$\n",
    "- Single-head attention: $d_k = d_{\\text{model}} = 4$\n",
    "\n",
    "Example parameter matrices:\n",
    "$$\n",
    "W_Q = \\begin{bmatrix}\n",
    "0.5 & 0.1 & 0.0 & 0.3 \\\\[6pt]\n",
    "0.4 & 0.2 & 0.1 & 0.0 \\\\[6pt]\n",
    "0.3 & 0.3 & 0.3 & 0.3 \\\\[6pt]\n",
    "0.2 & 0.1 & 0.5 & 0.4\n",
    "\\end{bmatrix}, \\quad\n",
    "W_K = \\begin{bmatrix}\n",
    "0.1 & 0.4 & 0.0 & 0.0 \\\\[6pt]\n",
    "0.0 & 0.5 & 0.2 & 0.1 \\\\[6pt]\n",
    "0.3 & 0.0 & 0.3 & 0.3 \\\\[6pt]\n",
    "0.2 & 0.2 & 0.1 & 0.0\n",
    "\\end{bmatrix}, \\quad\n",
    "W_V = \\begin{bmatrix}\n",
    "0.2 & 0.1 & 0.0 & 0.0 \\\\[6pt]\n",
    "0.0 & 0.3 & 0.5 & 0.0 \\\\[6pt]\n",
    "0.1 & 0.1 & 0.1 & 0.4 \\\\[6pt]\n",
    "0.0 & 0.2 & 0.1 & 0.1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "### Step-by-Step Computation\n",
    "\n",
    "#### 1. Compute Q, K, V\n",
    "\n",
    "- $Q = XW_Q$:\n",
    "Let's multiply $X$ (3x4) by $W_Q$ (4x4):\n",
    "\n",
    "For the first token \"The\":\n",
    "$$\n",
    "Q_1 = [0.1, 0.4, 0.5, 0.7]\n",
    "\\begin{bmatrix}\n",
    "  0.5 & 0.1 & 0.0 & 0.3 \\\\\n",
    "  0.4 & 0.2 & 0.1 & 0.0 \\\\\n",
    "  0.3 & 0.3 & 0.3 & 0.3 \\\\\n",
    "  0.2 & 0.1 & 0.5 & 0.4\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Compute row-by-column:\n",
    "- $Q_1[1] = 0.1*0.5 + 0.4*0.4 + 0.5*0.3 + 0.7*0.2 = 0.05 + 0.16 + 0.15 + 0.14 = 0.5$\n",
    "- $Q_1[2] = 0.1*0.1 + 0.4*0.2 + 0.5*0.3 + 0.7*0.1 = 0.01 + 0.08 + 0.15 + 0.07 = 0.31$\n",
    "- $Q_1[3] = 0.1*0.0 + 0.4*0.1 + 0.5*0.3 + 0.7*0.5 = 0 + 0.04 + 0.15 + 0.35 = 0.54$\n",
    "- $Q_1[4] = 0.1*0.3 + 0.4*0.0 + 0.5*0.3 + 0.7*0.4 = 0.03 + 0 + 0.15 + 0.28 = 0.46$\n",
    "\n",
    "So $Q_1 = [0.5, 0.31, 0.54, 0.46]$.\n",
    "\n",
    "For the second token \"cat\":\n",
    "$$\n",
    "Q_2 = [0.2, 0.4, 0.5, 0.6]W_Q\n",
    "$$\n",
    "- $Q_2[1] = 0.2*0.5 + 0.4*0.4 + 0.5*0.3 + 0.6*0.2 = 0.1 + 0.16 + 0.15 + 0.12 = 0.53$\n",
    "- $Q_2[2] = 0.2*0.1 + 0.4*0.2 + 0.5*0.3 + 0.6*0.1 = 0.02 + 0.08 + 0.15 + 0.06 = 0.31$\n",
    "- $Q_2[3] = 0.2*0.0 + 0.4*0.1 + 0.5*0.3 + 0.6*0.5 = 0 + 0.04 + 0.15 + 0.3 = 0.49$\n",
    "- $Q_2[4] = 0.2*0.3 + 0.4*0.0 + 0.5*0.3 + 0.6*0.4 = 0.06 + 0 + 0.15 + 0.24 = 0.45$\n",
    "\n",
    "So $Q_2 = [0.53, 0.31, 0.49, 0.45]$.\n",
    "\n",
    "For the third token \"sat\":\n",
    "$$\n",
    "Q_3 = [0.15, 0.25, 0.5, 0.2]W_Q\n",
    "$$\n",
    "- $Q_3[1] = 0.15*0.5 + 0.25*0.4 + 0.5*0.3 + 0.2*0.2 = 0.075 + 0.1 + 0.15 + 0.04 = 0.365$\n",
    "- $Q_3[2] = 0.15*0.1 + 0.25*0.2 + 0.5*0.3 + 0.2*0.1 = 0.015 + 0.05 + 0.15 + 0.02 = 0.235$\n",
    "- $Q_3[3] = 0.15*0.0 + 0.25*0.1 + 0.5*0.3 + 0.2*0.5 = 0 + 0.025 + 0.15 + 0.1 = 0.275$\n",
    "- $Q_3[4] = 0.15*0.3 + 0.25*0.0 + 0.5*0.3 + 0.2*0.4 = 0.045 + 0 + 0.15 + 0.08 = 0.275$\n",
    "\n",
    "So $Q_3 = [0.365, 0.235, 0.275, 0.275]$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "Q = \\begin{bmatrix}\n",
    "  0.5 & 0.31 & 0.54 & 0.46 \\\\[4pt]\n",
    "  0.53 & 0.31 & 0.49 & 0.45 \\\\[4pt]\n",
    "  0.365 & 0.235 & 0.275 & 0.275\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $K = XW_K$:\n",
    "Similarly, multiply $X$ by $W_K$.\n",
    "\n",
    "For \"The\":\n",
    "$$\n",
    "K_1 = [0.1,0.4,0.5,0.7]W_K\n",
    "$$\n",
    "Compute:\n",
    "- $K_1[1] = 0.1*0.1 + 0.4*0.0 + 0.5*0.3 + 0.7*0.2 = 0.01+0+0.15+0.14=0.3$\n",
    "- $K_1[2] = 0.1*0.4 + 0.4*0.5 + 0.5*0.0 + 0.7*0.2 = 0.04+0.2+0+0.14=0.38$\n",
    "- $K_1[3] = 0.1*0.0 + 0.4*0.2 + 0.5*0.3 + 0.7*0.1 = 0+0.08+0.15+0.07=0.3$\n",
    "- $K_1[4] = 0.1*0.0 + 0.4*0.1 + 0.5*0.3 + 0.7*0.0 = 0+0.04+0.15+0=0.19$\n",
    "\n",
    "$K_1 = [0.3, 0.38, 0.3, 0.19]$.\n",
    "\n",
    "For \"cat\":\n",
    "$$\n",
    "K_2 = [0.2,0.4,0.5,0.6]W_K\n",
    "$$\n",
    "- $K_2[1] = 0.2*0.1 + 0.4*0.0 + 0.5*0.3 + 0.6*0.2 = 0.02+0+0.15+0.12=0.29$\n",
    "- $K_2[2] = 0.2*0.4 + 0.4*0.5 + 0.5*0.0 + 0.6*0.2 = 0.08+0.2+0+0.12=0.4$\n",
    "- $K_2[3] = 0.2*0.0 + 0.4*0.2 + 0.5*0.3 + 0.6*0.1 = 0+0.08+0.15+0.06=0.29$\n",
    "- $K_2[4] = 0.2*0.0 + 0.4*0.1 + 0.5*0.3 + 0.6*0.0 = 0+0.04+0.15+0=0.19$\n",
    "\n",
    "$K_2 = [0.29, 0.4, 0.29, 0.19]$.\n",
    "\n",
    "For \"sat\":\n",
    "$$\n",
    "K_3 = [0.15,0.25,0.5,0.2]W_K\n",
    "$$\n",
    "- $K_3[1] = 0.15*0.1 + 0.25*0.0 + 0.5*0.3 + 0.2*0.2 = 0.015+0+0.15+0.04=0.205$\n",
    "- $K_3[2] = 0.15*0.4 + 0.25*0.5 + 0.5*0.0 + 0.2*0.2 = 0.06+0.125+0+0.04=0.225$\n",
    "- $K_3[3] = 0.15*0.0 + 0.25*0.2 + 0.5*0.3 + 0.2*0.1 = 0+0.05+0.15+0.02=0.22$\n",
    "- $K_3[4] = 0.15*0.0 + 0.25*0.1 + 0.5*0.3 + 0.2*0.0 = 0+0.025+0.15+0=0.175$\n",
    "\n",
    "$K_3 = [0.205,0.225,0.22,0.175]$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "K = \\begin{bmatrix}\n",
    "  0.3 & 0.38 & 0.3 & 0.19 \\\\[4pt]\n",
    "  0.29 & 0.4 & 0.29 & 0.19 \\\\[4pt]\n",
    "  0.205 & 0.225 & 0.22 & 0.175\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "- $V = XW_V$:\n",
    "Similarly:\n",
    "\n",
    "For \"The\":\n",
    "- $V_1[1] = 0.1*0.2 + 0.4*0.0 + 0.5*0.1 + 0.7*0.0 = 0.02+0+0.05+0=0.07$\n",
    "- $V_1[2] = 0.1*0.1 + 0.4*0.3 + 0.5*0.1 + 0.7*0.2 = 0.01+0.12+0.05+0.14=0.32$\n",
    "- $V_1[3] = 0.1*0.0 + 0.4*0.5 + 0.5*0.1 + 0.7*0.1 = 0+0.2+0.05+0.07=0.32$\n",
    "- $V_1[4] = 0.1*0.0 + 0.4*0.0 + 0.5*0.4 + 0.7*0.1 = 0+0+0.2+0.07=0.27$\n",
    "\n",
    "$V_1 = [0.07, 0.32, 0.32, 0.27]$.\n",
    "\n",
    "For \"cat\":\n",
    "- $V_2[1] = 0.2*0.2 + 0.4*0.0 + 0.5*0.1 + 0.6*0.0 = 0.04+0+0.05+0=0.09$\n",
    "- $V_2[2] = 0.2*0.1 + 0.4*0.3 + 0.5*0.1 + 0.6*0.2 = 0.02+0.12+0.05+0.12=0.31$\n",
    "- $V_2[3] = 0.2*0.0 + 0.4*0.5 + 0.5*0.1 + 0.6*0.1 = 0+0.2+0.05+0.06=0.31$\n",
    "- $V_2[4] = 0.2*0.0 + 0.4*0.0 + 0.5*0.4 + 0.6*0.1 = 0+0+0.2+0.06=0.26$\n",
    "\n",
    "$V_2 = [0.09,0.31,0.31,0.26]$.\n",
    "\n",
    "For \"sat\":\n",
    "- $V_3[1] = 0.15*0.2 + 0.25*0.0 + 0.5*0.1 + 0.2*0.0 = 0.03+0+0.05+0=0.08$\n",
    "- $V_3[2] = 0.15*0.1 + 0.25*0.3 + 0.5*0.1 + 0.2*0.2 = 0.015+0.075+0.05+0.04=0.18$\n",
    "- $V_3[3] = 0.15*0.0 + 0.25*0.5 + 0.5*0.1 + 0.2*0.1 = 0+0.125+0.05+0.02=0.195$\n",
    "- $V_3[4] = 0.15*0.0 + 0.25*0.0 + 0.5*0.4 + 0.2*0.1 = 0+0+0.2+0.02=0.22$\n",
    "\n",
    "$V_3 = [0.08,0.18,0.195,0.22]$.\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "V = \\begin{bmatrix}\n",
    "  0.07 & 0.32 & 0.32 & 0.27 \\\\[4pt]\n",
    "  0.09 & 0.31 & 0.31 & 0.26 \\\\[4pt]\n",
    "  0.08 & 0.18 & 0.195 & 0.22\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 2. Compute the Attention Scores\n",
    "\n",
    "$$\n",
    "\\text{Scores} = QK^T\n",
    "$$\n",
    "\n",
    "- Dimension check: $Q \\in \\mathbb{R}^{3 \\times 4}, K \\in \\mathbb{R}^{3 \\times 4}$, so $K^T \\in \\mathbb{R}^{4 \\times 3}$. Thus, $\\text{Scores} \\in \\mathbb{R}^{3 \\times 3}$.\n",
    "\n",
    "Compute $\\text{Scores}[i,j]$ = $Q_i \\cdot K_j$:\n",
    "\n",
    "- $\\text{Scores}[1,1] = Q_1 \\cdot K_1 = [0.5*0.3 + 0.31*0.38 + 0.54*0.3 + 0.46*0.19]$\n",
    "= $0.15 + 0.1178 + 0.162 + 0.0874 = 0.5172$\n",
    "\n",
    "- $\\text{Scores}[1,2] = Q_1 \\cdot K_2 = [0.5*0.29 + 0.31*0.4 + 0.54*0.29 + 0.46*0.19]$\n",
    "= $0.145 + 0.124 + 0.1566 + 0.0874 = 0.513$\n",
    "\n",
    "- $\\text{Scores}[1,3] = Q_1 \\cdot K_3 = [0.5*0.205 + 0.31*0.225 + 0.54*0.22 + 0.46*0.175]$\n",
    "= $0.1025 + 0.06975 + 0.1188 + 0.0805 = 0.37155$\n",
    "\n",
    "- $\\text{Scores}[2,1] = Q_2 \\cdot K_1$\n",
    "= $[0.53*0.3 + 0.31*0.38 + 0.49*0.3 + 0.45*0.19]$\n",
    "= $0.159 + 0.1178 + 0.147 + 0.0855 = 0.5093$\n",
    "\n",
    "- $\\text{Scores}[2,2] = Q_2 \\cdot K_2$\n",
    "= $[0.53*0.29 + 0.31*0.4 + 0.49*0.29 + 0.45*0.19]$\n",
    "= $0.1537 + 0.124 + 0.1421 + 0.0855 = 0.5053$\n",
    "\n",
    "- $\\text{Scores}[2,3] = Q_2 \\cdot K_3$\n",
    "= $[0.53*0.205 + 0.31*0.225 + 0.49*0.22 + 0.45*0.175]$\n",
    "= $0.10865 + 0.06975 + 0.1078 + 0.07875 = 0.36495$\n",
    "\n",
    "- $\\text{Scores}[3,1] = Q_3 \\cdot K_1$\n",
    "= $[0.365*0.3 + 0.235*0.38 + 0.275*0.3 + 0.275*0.19]$\n",
    "= $0.1095 + 0.0893 + 0.0825 + 0.05225 = 0.33355$\n",
    "\n",
    "- $\\text{Scores}[3,2] = Q_3 \\cdot K_2$\n",
    "= $[0.365*0.29 + 0.235*0.4 + 0.275*0.29 + 0.275*0.19]$\n",
    "= $0.10585 + 0.094 + 0.07975 + 0.05225 = 0.33185$\n",
    "\n",
    "- \\(\\text{Scores}[3,3] = Q_3 \\cdot K_3\\)\n",
    "= \\([0.365*0.205 + 0.235*0.225 + 0.275*0.22 + 0.275*0.175]\\)\n",
    "= \\(0.074825 + 0.052875 + 0.0605 + 0.048125 = 0.236325\\)\n",
    "\n",
    "Thus:\n",
    "$$\n",
    "\\text{Scores} = \\begin{bmatrix}\n",
    "0.5172 & 0.513  & 0.37155 \\\\[4pt]\n",
    "0.5093 & 0.5053 & 0.36495 \\\\[4pt]\n",
    "0.33355 & 0.33185 & 0.236325\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 3. Scale the Scores\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\frac{\\text{Scores}}{\\sqrt{d_k}} = \\frac{\\text{Scores}}{\\sqrt{4}} = \\frac{\\text{Scores}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\begin{bmatrix}\n",
    "0.2586 & 0.2565 & 0.185775 \\\\[4pt]\n",
    "0.25465 & 0.25265 & 0.182475 \\\\[4pt]\n",
    "0.166775 & 0.165925 & 0.1181625\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 4. Apply Softmax to Each Row\n",
    "\n",
    "The **softmax** function is defined as follows:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{z} = (z_1, z_2, \\ldots, z_N)$ is a vector of real numbers and $N$ is the dimension of the vector $\\mathbf{z}$.\n",
    "\n",
    "\n",
    "For the first row:\n",
    "- Sum = $\\exp(0.2586) + \\exp(0.2565) + \\exp(0.185775)$\n",
    "- $\\exp(0.2586) \\approx 1.295$\n",
    "- $\\exp(0.2565) \\approx 1.292$\n",
    "- $\\exp(0.185775) \\approx 1.204$\n",
    "\n",
    "Sum ≈ 1.295 + 1.292 + 1.204 = 3.791\n",
    "\n",
    "So:\n",
    "- $A[1,1] = 1.295/3.791 ≈ 0.3418$\n",
    "- $A[1,2] = 1.292/3.791 ≈ 0.3408$\n",
    "- $A[1,3] = 1.204/3.791 ≈ 0.3177$\n",
    "\n",
    "For the second row:\n",
    "- $\\exp(0.25465) \\approx 1.290$\n",
    "- $\\exp(0.25265) \\approx 1.287$\n",
    "- $\\exp(0.182475) \\approx 1.200$\n",
    "\n",
    "Sum ≈ 1.290 + 1.287 + 1.200 = 3.777\n",
    "\n",
    "- $A[2,1] = 1.290/3.777 ≈ 0.3415$\n",
    "- $A[2,2] = 1.287/3.777 ≈ 0.3407$\n",
    "- $A[2,3] = 1.200/3.777 ≈ 0.3178$\n",
    "\n",
    "For the third row:\n",
    "- $\\exp(0.166775) \\approx 1.181$\n",
    "- $\\exp(0.165925) \\approx 1.180$\n",
    "- $\\exp(0.1181625) \\approx 1.1255$\n",
    "\n",
    "Sum ≈ 1.181 + 1.180 + 1.1255 = 3.4865\n",
    "\n",
    "- $A[3,1] = 1.181/3.4865 ≈ 0.3388$\n",
    "- $A[3,2] = 1.180/3.4865 ≈ 0.3385$\n",
    "- $A[3,3] = 1.1255/3.4865 ≈ 0.3227$\n",
    "\n",
    "So our attention weight matrix $A$ is approximately:\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0.3418 & 0.3408 & 0.3177 \\\\[4pt]\n",
    "0.3415 & 0.3407 & 0.3178 \\\\[4pt]\n",
    "0.3388 & 0.3385 & 0.3227\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 5. Compute the Final Output\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} = A V\n",
    "$$\n",
    "\n",
    "- Dimension: $A \\in \\mathbb{R}^{3 \\times 3}, V \\in \\mathbb{R}^{3 \\times 4}$, resulting in $\\text{AttOutput} \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "\n",
    "For each row $i$:\n",
    "$$\n",
    "\\text{AttOutput}_i = \\sum_{j=1}^{3} A[i,j] V_j\n",
    "$$\n",
    "\n",
    "- For the first token:\n",
    "$$\n",
    "\\text{AttOutput}_1 = 0.3418V_1 + 0.3408V_2 + 0.3177V_3\n",
    "$$\n",
    "Recall:\n",
    "- $V_1 = [0.07,0.32,0.32,0.27]$\n",
    "- $V_2 = [0.09,0.31,0.31,0.26]$\n",
    "- $V_3 = [0.08,0.18,0.195,0.22]$\n",
    "\n",
    "Compute component-wise:\n",
    "- Dim1: $0.3418*0.07 + 0.3408*0.09 + 0.3177*0.08 = 0.0233 + 0.0307 + 0.0254 = 0.0794$\n",
    "- Dim2: $0.3418*0.32 + 0.3408*0.31 + 0.3177*0.18 = 0.1094 + 0.1056 + 0.0572 = 0.2722$\n",
    "- Dim3: $0.3418*0.32 + 0.3408*0.31 + 0.3177*0.195 = 0.1094 + 0.1056 + 0.0629 = 0.2779$\n",
    "- Dim4: $0.3418*0.27 + 0.3408*0.26 + 0.3177*0.22 = 0.0923 + 0.0886 + 0.0699 = 0.2508$\n",
    "\n",
    "$\\text{AttOutput}_1 = [0.0794, 0.2722, 0.2779, 0.2508]$\n",
    "\n",
    "- For the second token:\n",
    "$$\n",
    "\\text{AttOutput}_2 = 0.3415V_1 + 0.3407V_2 + 0.3178V_3\n",
    "$$\n",
    "Repeat similarly:\n",
    "- Dim1: $0.3415*0.07 + 0.3407*0.09 + 0.3178*0.08 = 0.023105 + 0.030663 + 0.025424 = 0.079192$\n",
    "- Dim2: $0.3415*0.32 + 0.3407*0.31 + 0.3178*0.18 = 0.10928 + 0.105617 + 0.057204 = 0.272101$\n",
    "- Dim3: $0.3415*0.32 + 0.3407*0.31 + 0.3178*0.195 = 0.10928 + 0.105617 + 0.062971 = 0.277868$\n",
    "- Dim4: $0.3415*0.27 + 0.3407*0.26 + 0.3178*0.22 = 0.092205 + 0.088582 + 0.069916 = 0.250703$\n",
    "\n",
    "$\\text{AttOutput}_2 \\approx [0.0792, 0.2721, 0.2779, 0.2507]$\n",
    "\n",
    "- For the third token:\n",
    "$$\n",
    "\\text{AttOutput}_3 = 0.3388V_1 + 0.3385V_2 + 0.3227V_3\n",
    "$$\n",
    "- Dim1: $0.3388*0.07 + 0.3385*0.09 + 0.3227*0.08 = 0.023716 + 0.030465 + 0.025816 = 0.079997$\n",
    "- Dim2: $0.3388*0.32 + 0.3385*0.31 + 0.3227*0.18 = 0.108416 + 0.104935 + 0.058086 = 0.271437$\n",
    "- Dim3: $0.3388*0.32 + 0.3385*0.31 + 0.3227*0.195 = 0.108416 + 0.104935 + 0.062933 = 0.276284$\n",
    "- Dim4: $0.3388*0.27 + 0.3385*0.26 + 0.3227*0.22 = 0.091476 + 0.08801 + 0.071 = 0.250486$\n",
    "\n",
    "$\\text{AttOutput}_3 \\approx [0.08, 0.2714, 0.2763, 0.2505]$\n",
    "\n",
    "Final $\\text{AttOutput}$:\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.0794 & 0.2722 & 0.2779 & 0.2508 \\\\[4pt]\n",
    "0.0792 & 0.2721 & 0.2779 & 0.2507 \\\\[4pt]\n",
    "0.08   & 0.2714 & 0.2763 & 0.2505\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "## Interpretation\n",
    "\n",
    "- Each row of $\\text{AttOutput}$ is the transformed representation of the corresponding token after attending to all tokens in the sequence (including itself).\n",
    "- Notice that the rows are fairly similar, which reflects the similarity in the queries and keys for this small artificial example. In a more complex and varied sequence, these values would differ more significantly.\n",
    "- In practice, multiple heads are used, and their outputs are concatenated to capture various patterns. Here, we demonstrated just a single-head scenario.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Decoder Architecture in Transformers\n",
    "#### 6. Apply Masking\n",
    "\n",
    "**Masking** is used to prevent attention to certain positions. In this example, we will apply a **causal mask** to prevent each token from attending to future tokens. This is particularly useful in decoder architectures to maintain the autoregressive property.\n",
    "\n",
    "- **Causal Mask Matrix ($M$):**\n",
    "\n",
    "  The causal mask ensures that each position can only attend to itself and previous positions. For our 3-token sequence:\n",
    "\n",
    "  $$\n",
    "  M = \\begin{bmatrix}\n",
    "  0 & -\\infty & -\\infty \\\\[4pt]\n",
    "  0 & 0 & -\\infty \\\\[4pt]\n",
    "  0 & 0 & 0\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  - $0$ allows attention.\n",
    "  - $-\\infty$ effectively masks out the position by making its softmax probability zero.\n",
    "\n",
    "- **Apply Mask to Scores:**\n",
    "\n",
    "  $$\n",
    "  \\text{Masked Scores} = \\text{Scores} + M\n",
    "  $$\n",
    "\n",
    "  Performing element-wise addition:\n",
    "\n",
    "  $$\n",
    "  \\text{Masked Scores} = \\begin{bmatrix}\n",
    "  0.5172 & 0.5130 & -\\infty \\\\[4pt]\n",
    "  0.5093 & 0.5053 & -\\infty \\\\[4pt]\n",
    "  0.33355 & 0.33185 & 0.236325\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  **Explanation:**  \n",
    "  - For the first token (\"The\"), it cannot attend to the third token (\"sat\"), hence $-\\infty$.\n",
    "  - For the second token (\"cat\"), it cannot attend to the third token (\"sat\"), hence $-\\infty$.\n",
    "  - The third token (\"sat\") can attend to all tokens, including itself.\n",
    "\n",
    "#### 7. Scale the Scores\n",
    "\n",
    "Scaling helps in stabilizing gradients during training.\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\frac{\\text{Masked Scores}}{\\sqrt{d_k}} = \\frac{\\text{Masked Scores}}{\\sqrt{4}} = \\frac{\\text{Masked Scores}}{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\tilde{\\text{Scores}} = \\begin{bmatrix}\n",
    "0.2586 & 0.2565 & -\\infty \\\\[4pt]\n",
    "0.25465 & 0.25265 & -\\infty \\\\[4pt]\n",
    "0.166775 & 0.165925 & 0.1181625\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "#### 8. Apply Softmax to Each Row\n",
    "\n",
    "The **softmax** function converts the scaled scores into probabilities.\n",
    "\n",
    "$$\n",
    "\\text{softmax}(\\mathbf{z})_i = \\frac{\\exp(z_i)}{\\sum_{j=1}^{N} \\exp(z_j)}\n",
    "$$\n",
    "\n",
    "**Applying Softmax:**\n",
    "\n",
    "- **First Row:**\n",
    "  $$\n",
    "  \\mathbf{z}_1 = [0.2586, 0.2565, -\\infty]\n",
    "  $$\n",
    "  - $\\exp(0.2586) \\approx 1.295$\n",
    "  - $\\exp(0.2565) \\approx 1.292$\n",
    "  - $\\exp(-\\infty) = 0$\n",
    "\n",
    "  Sum: $1.295 + 1.292 + 0 = 2.587$\n",
    "\n",
    "  Softmax:\n",
    "  $$\n",
    "  A[1,1] = \\frac{1.295}{2.587} \\approx 0.500 \\\\\n",
    "  A[1,2] = \\frac{1.292}{2.587} \\approx 0.500 \\\\\n",
    "  A[1,3] = \\frac{0}{2.587} = 0.0\n",
    "  $$\n",
    "\n",
    "- **Second Row:**\n",
    "  $$\n",
    "  \\mathbf{z}_2 = [0.25465, 0.25265, -\\infty]\n",
    "  $$\n",
    "  - $\\exp(0.25465) \\approx 1.290$\n",
    "  - $\\exp(0.25265) \\approx 1.287$\n",
    "  - $\\exp(-\\infty) = 0$\n",
    "\n",
    "  Sum: $1.290 + 1.287 + 0 = 2.577$\n",
    "\n",
    "  Softmax:\n",
    "  $$\n",
    "  A[2,1] = \\frac{1.290}{2.577} \\approx 0.500 \\\\\n",
    "  A[2,2] = \\frac{1.287}{2.577} \\approx 0.500 \\\\\n",
    "  A[2,3] = \\frac{0}{2.577} = 0.0\n",
    "  $$\n",
    "\n",
    "- **Third Row:**\n",
    "  $$\n",
    "  \\mathbf{z}_3 = [0.166775, 0.165925, 0.1181625]\n",
    "  $$\n",
    "  - $\\exp(0.166775) \\approx 1.181$\n",
    "  - $\\exp(0.165925) \\approx 1.180$\n",
    "  - $\\exp(0.1181625) \\approx 1.1255$\n",
    "\n",
    "  Sum: $1.181 + 1.180 + 1.1255 = 3.4865$\n",
    "\n",
    "  Softmax:\n",
    "  $$\n",
    "  A[3,1] = \\frac{1.181}{3.4865} \\approx 0.339 \\\\\n",
    "  A[3,2] = \\frac{1.180}{3.4865} \\approx 0.339 \\\\\n",
    "  A[3,3] = \\frac{1.1255}{3.4865} \\approx 0.323\n",
    "  $$\n",
    "\n",
    "Thus, our attention weight matrix $A$ is approximately:\n",
    "\n",
    "$$\n",
    "A = \\begin{bmatrix}\n",
    "0.500 & 0.500 & 0.0 \\\\[4pt]\n",
    "0.500 & 0.500 & 0.0 \\\\[4pt]\n",
    "0.339 & 0.339 & 0.323\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Effect of Masking:**\n",
    "- **First Token (\"The\"):** Can only attend to itself and \"cat\". No attention to \"sat\".\n",
    "- **Second Token (\"cat\"):** Can only attend to itself and \"The\". No attention to \"sat\".\n",
    "- **Third Token (\"sat\"):** Can attend to all tokens, including itself.\n",
    "\n",
    "#### 6. Compute the Final Output\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} = A V\n",
    "$$\n",
    "\n",
    "- **Dimension Check:**  \n",
    "  $A \\in \\mathbb{R}^{3 \\times 3}$, $V \\in \\mathbb{R}^{3 \\times 4}$, so $\\text{AttOutput} \\in \\mathbb{R}^{3 \\times 4}$.\n",
    "\n",
    "- **Calculation:**\n",
    "\n",
    "  - **For the first token (\"The\"):**\n",
    "    $$\n",
    "    \\text{AttOutput}_1 = 0.500 \\times V_1 + 0.500 \\times V_2 + 0.0 \\times V_3 \\\\\n",
    "    = 0.500 \\times [0.07, 0.32, 0.32, 0.27] + 0.500 \\times [0.09, 0.31, 0.31, 0.26] + 0.0 \\times [0.08, 0.18, 0.195, 0.22] \\\\\n",
    "    = [0.035, 0.16, 0.16, 0.135] + [0.045, 0.155, 0.155, 0.13] + [0.0, 0.0, 0.0, 0.0] \\\\\n",
    "    = [0.080, 0.315, 0.315, 0.265]\n",
    "    $$\n",
    "\n",
    "  - **For the second token (\"cat\"):**\n",
    "    $$\n",
    "    \\text{AttOutput}_2 = 0.500 \\times V_1 + 0.500 \\times V_2 + 0.0 \\times V_3 \\\\\n",
    "    = 0.500 \\times [0.07, 0.32, 0.32, 0.27] + 0.500 \\times [0.09, 0.31, 0.31, 0.26] + 0.0 \\times [0.08, 0.18, 0.195, 0.22] \\\\\n",
    "    = [0.035, 0.16, 0.16, 0.135] + [0.045, 0.155, 0.155, 0.13] + [0.0, 0.0, 0.0, 0.0] \\\\\n",
    "    = [0.080, 0.315, 0.315, 0.265]\n",
    "    $$\n",
    "\n",
    "  - **For the third token (\"sat\"):**\n",
    "    $$\n",
    "    \\text{AttOutput}_3 = 0.339 \\times V_1 + 0.339 \\times V_2 + 0.323 \\times V_3 \\\\\n",
    "    = 0.339 \\times [0.07, 0.32, 0.32, 0.27] + 0.339 \\times [0.09, 0.31, 0.31, 0.26] + 0.323 \\times [0.08, 0.18, 0.195, 0.22] \\\\\n",
    "    = [0.02373, 0.10848, 0.10848, 0.07269] + [0.03051, 0.10509, 0.10509, 0.08814] + [0.02584, 0.05814, 0.06309, 0.07106] \\\\\n",
    "    = [0.07908, 0.27171, 0.27666, 0.23189]\n",
    "    $$\n",
    "\n",
    "Thus, the final attention output matrix $\\text{AttOutput}$ is approximately:\n",
    "\n",
    "$$\n",
    "\\text{AttOutput} \\approx \\begin{bmatrix}\n",
    "0.080 & 0.315 & 0.315 & 0.265 \\\\[4pt]\n",
    "0.080 & 0.315 & 0.315 & 0.265 \\\\[4pt]\n",
    "0.079 & 0.271 & 0.277 & 0.232\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- **First Token (\"The\"):** Its representation is an average of \"The\" and \"cat\", ignoring \"sat\" due to the causal mask.\n",
    "- **Second Token (\"cat\"):** Similarly, it averages \"The\" and \"cat\".\n",
    "- **Third Token (\"sat\"):** It attends to all three tokens, incorporating information from \"The\", \"cat\", and itself.\n",
    "\n",
    "#### 7. Incorporate Multi-Head Attention (Optional)\n",
    "\n",
    "**Multi-Head Attention** allows the model to attend to information from different representation subspaces at different positions. Here's how to extend our example to multi-head attention.\n",
    "\n",
    "- **Assumptions:**\n",
    "  - Number of heads: $h = 2$\n",
    "  - Dimension per head: $d_k = d_v = d_{\\text{model}} / h = 2$\n",
    "\n",
    "- **Parameter Matrices for Each Head:**\n",
    "  For simplicity, we define separate $W_Q$, $W_K$, and $W_V$ for each head. Assume these are predefined.\n",
    "\n",
    "  **Head 1:**\n",
    "  $$\n",
    "  W_{Q}^{(1)} = \\begin{bmatrix}\n",
    "  0.5 & 0.1 \\\\\n",
    "  0.4 & 0.2 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.2 & 0.1\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{K}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.0 & 0.5 \\\\\n",
    "  0.3 & 0.0 \\\\\n",
    "  0.2 & 0.2\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{V}^{(1)} = \\begin{bmatrix}\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.1 \\\\\n",
    "  0.0 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  **Head 2:**\n",
    "  $$\n",
    "  W_{Q}^{(2)} = \\begin{bmatrix}\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.0 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.5 & 0.4\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{K}^{(2)} = \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.1 & 0.0\n",
    "  \\end{bmatrix}, \\quad\n",
    "  W_{V}^{(2)} = \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.5 & 0.0 \\\\\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Compute Q, K, V for Each Head:**\n",
    "\n",
    "  **Head 1:**\n",
    "  $$\n",
    "  Q^{(1)} = X W_{Q}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.5 & 0.1 \\\\\n",
    "  0.4 & 0.2 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.2 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.5 & 0.31 \\\\\n",
    "  0.53 & 0.31 \\\\\n",
    "  0.365 & 0.235\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  K^{(1)} = X W_{K}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.0 & 0.5 \\\\\n",
    "  0.3 & 0.0 \\\\\n",
    "  0.2 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.3 & 0.38 \\\\\n",
    "  0.29 & 0.4 \\\\\n",
    "  0.205 & 0.225\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  V^{(1)} = X W_{V}^{(1)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.1 \\\\\n",
    "  0.0 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.07 & 0.32 \\\\\n",
    "  0.09 & 0.31 \\\\\n",
    "  0.08 & 0.18\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  **Head 2:**\n",
    "  $$\n",
    "  Q^{(2)} = X W_{Q}^{(2)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.0 & 0.3 \\\\\n",
    "  0.1 & 0.0 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.5 & 0.4\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.38 & 0.59 \\\\\n",
    "  0.39 & 0.57 \\\\\n",
    "  0.305 & 0.255\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  K^{(2)} = X W_{K}^{(2)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.2 & 0.1 \\\\\n",
    "  0.3 & 0.3 \\\\\n",
    "  0.1 & 0.0\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.1 & 0.13 \\\\\n",
    "  0.11 & 0.13 \\\\\n",
    "  0.125 & 0.11\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  $$\n",
    "  V^{(2)} = X W_{V}^{(2)} = \\begin{bmatrix}\n",
    "  0.1 & 0.4 & 0.5 & 0.7 \\\\\n",
    "  0.2 & 0.4 & 0.5 & 0.6 \\\\\n",
    "  0.15 & 0.25 & 0.5 & 0.2\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.0 & 0.0 \\\\\n",
    "  0.5 & 0.0 \\\\\n",
    "  0.1 & 0.4 \\\\\n",
    "  0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.05 & 0.04 \\\\\n",
    "  0.06 & 0.03 \\\\\n",
    "  0.07 & 0.07\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "- **Compute Attention for Each Head:**\n",
    "\n",
    "  For each head, perform the same steps as single-head attention:\n",
    "  \n",
    "  1. Compute Scores: $Q^{(h)} {K^{(h)}}^T$\n",
    "  2. Apply Masking (if necessary)\n",
    "  3. Scale Scores\n",
    "  4. Apply Softmax\n",
    "  5. Compute Attention Output: $A^{(h)} V^{(h)}$\n",
    "\n",
    "  **Head 1:**\n",
    "\n",
    "  - **Scores:**\n",
    "    $$\n",
    "    \\text{Scores}^{(1)} = Q^{(1)} {K^{(1)}}^T = \\begin{bmatrix}\n",
    "    0.5 & 0.31 & 0.54 & 0.46 \\\\\n",
    "    0.53 & 0.31 & 0.49 & 0.45 \\\\\n",
    "    0.365 & 0.235 & 0.275 & 0.275\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.3 & 0.38 & 0.3 & 0.19 \\\\\n",
    "    0.29 & 0.4 & 0.29 & 0.19 \\\\\n",
    "    0.205 & 0.225 & 0.22 & 0.175\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.5172 & 0.5130 & 0.37155 \\\\\n",
    "    0.5093 & 0.5053 & 0.36495 \\\\\n",
    "    0.33355 & 0.33185 & 0.236325\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Apply Masking:**\n",
    "    $$\n",
    "    M = \\begin{bmatrix}\n",
    "    0 & -\\infty & -\\infty \\\\\n",
    "    0 & 0 & -\\infty \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    $$\n",
    "    \\text{Masked Scores}^{(1)} = \\text{Scores}^{(1)} + M = \\begin{bmatrix}\n",
    "    0.5172 & 0.5130 & -\\infty \\\\\n",
    "    0.5093 & 0.5053 & -\\infty \\\\\n",
    "    0.33355 & 0.33185 & 0.236325\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Scale:**\n",
    "    $$\n",
    "    \\tilde{\\text{Scores}}^{(1)} = \\frac{\\text{Masked Scores}^{(1)}}{2} = \\begin{bmatrix}\n",
    "    0.2586 & 0.2565 & -\\infty \\\\\n",
    "    0.25465 & 0.25265 & -\\infty \\\\\n",
    "    0.166775 & 0.165925 & 0.1181625\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Softmax:**\n",
    "    $$\n",
    "    A^{(1)} = \\text{softmax}(\\tilde{\\text{Scores}}^{(1)}) = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Attention Output:**\n",
    "    $$\n",
    "    \\text{AttOutput}^{(1)} = A^{(1)} V^{(1)} = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.07 & 0.32 \\\\\n",
    "    0.09 & 0.31 \\\\\n",
    "    0.08 & 0.18\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.080 & 0.315 \\\\\n",
    "    0.080 & 0.315 \\\\\n",
    "    0.079 & 0.271\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  **Head 2:**\n",
    "\n",
    "  - **Scores:**\n",
    "    $$\n",
    "    \\text{Scores}^{(2)} = Q^{(2)} {K^{(2)}}^T = \\begin{bmatrix}\n",
    "    0.38 & 0.59 \\\\\n",
    "    0.39 & 0.57 \\\\\n",
    "    0.305 & 0.255\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.1 & 0.13 \\\\\n",
    "    0.11 & 0.13 \\\\\n",
    "    0.125 & 0.11\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.38*0.1 + 0.59*0.11 & 0.38*0.13 + 0.59*0.13 \\\\\n",
    "    0.39*0.1 + 0.57*0.11 & 0.39*0.13 + 0.57*0.13 \\\\\n",
    "    0.305*0.1 + 0.255*0.11 & 0.305*0.13 + 0.255*0.13\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.038 + 0.0649 & 0.0494 + 0.0767 \\\\\n",
    "    0.039 + 0.0627 & 0.0507 + 0.0741 \\\\\n",
    "    0.0305 + 0.02805 & 0.03965 + 0.03315\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.1029 & 0.1261 \\\\\n",
    "    0.1017 & 0.1248 \\\\\n",
    "    0.05855 & 0.0728\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Apply Masking:**\n",
    "    $$\n",
    "    M = \\begin{bmatrix}\n",
    "    0 & -\\infty & -\\infty \\\\\n",
    "    0 & 0 & -\\infty \\\\\n",
    "    0 & 0 & 0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "    Since $K^{(2)}$ has dimension $2$, and our sequence length is $3$, we need to adjust the mask accordingly. However, for simplicity, assume a similar causal mask applies:\n",
    "\n",
    "    $$\n",
    "    \\text{Masked Scores}^{(2)} = \\text{Scores}^{(2)} + M = \\begin{bmatrix}\n",
    "    0.1029 & 0.1261 & -\\infty \\\\\n",
    "    0.1017 & 0.1248 & -\\infty \\\\\n",
    "    0.05855 & 0.0728 & 0.0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Scale:**\n",
    "    $$\n",
    "    \\tilde{\\text{Scores}}^{(2)} = \\frac{\\text{Masked Scores}^{(2)}}{2} = \\begin{bmatrix}\n",
    "    0.05145 & 0.06305 & -\\infty \\\\\n",
    "    0.05085 & 0.0624 & -\\infty \\\\\n",
    "    0.029275 & 0.0364 & 0.0\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Softmax:**\n",
    "    $$\n",
    "    A^{(2)} = \\text{softmax}(\\tilde{\\text{Scores}}^{(2)}) = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "  - **Attention Output:**\n",
    "    $$\n",
    "    \\text{AttOutput}^{(2)} = A^{(2)} V^{(2)} = \\begin{bmatrix}\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.500 & 0.500 & 0.0 \\\\\n",
    "    0.339 & 0.339 & 0.323\n",
    "    \\end{bmatrix}\n",
    "    \\begin{bmatrix}\n",
    "    0.05 & 0.04 \\\\\n",
    "    0.06 & 0.03 \\\\\n",
    "    0.07 & 0.07\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.500*0.05 + 0.500*0.06 & 0.500*0.04 + 0.500*0.03 \\\\\n",
    "    0.500*0.05 + 0.500*0.06 & 0.500*0.04 + 0.500*0.03 \\\\\n",
    "    0.339*0.05 + 0.339*0.06 + 0.323*0.07 & 0.339*0.04 + 0.339*0.03 + 0.323*0.07\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.01695 + 0.02034 + 0.02261 & 0.01356 + 0.01017 + 0.02261\n",
    "    \\end{bmatrix}\n",
    "    = \\begin{bmatrix}\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.055 & 0.035 \\\\\n",
    "    0.0599 & 0.04634\n",
    "    \\end{bmatrix}\n",
    "    $$\n",
    "\n",
    "- **Concatenate Heads and Project:**\n",
    "\n",
    "  After computing attention outputs for all heads, concatenate them:\n",
    "\n",
    "  $$\n",
    "  \\text{Concat} = [\\text{AttOutput}^{(1)}, \\text{AttOutput}^{(2)}] = \\begin{bmatrix}\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.079 & 0.271 & 0.0599 & 0.04634\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  Then apply a final linear projection:\n",
    "\n",
    "  Let’s define $W_O \\in \\mathbb{R}^{(h \\cdot d_v) \\times d_{\\text{model}}}$ as:\n",
    "\n",
    "  $$\n",
    "  W_O = \\begin{bmatrix}\n",
    "  0.1 & 0.0 & 0.2 & 0.1 \\\\\n",
    "  0.0 & 0.1 & 0.0 & 0.2 \\\\\n",
    "  0.3 & 0.1 & 0.0 & 0.0 \\\\\n",
    "  0.0 & 0.2 & 0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  Compute the final output:\n",
    "\n",
    "  $$\n",
    "  \\text{FinalOutput} = \\text{Concat} \\cdot W_O = \\begin{bmatrix}\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.080 & 0.315 & 0.055 & 0.035 \\\\\n",
    "  0.079 & 0.271 & 0.0599 & 0.04634\n",
    "  \\end{bmatrix}\n",
    "  \\begin{bmatrix}\n",
    "  0.1 & 0.0 & 0.2 & 0.1 \\\\\n",
    "  0.0 & 0.1 & 0.0 & 0.2 \\\\\n",
    "  0.3 & 0.1 & 0.0 & 0.0 \\\\\n",
    "  0.0 & 0.2 & 0.1 & 0.1\n",
    "  \\end{bmatrix}\n",
    "  = \\begin{bmatrix}\n",
    "  0.080*0.1 + 0.315*0.0 + 0.055*0.3 + 0.035*0.0 & \\ldots \\\\\n",
    "  0.080*0.1 + 0.315*0.0 + 0.055*0.3 + 0.035*0.0 & \\ldots \\\\\n",
    "  0.079*0.1 + 0.271*0.0 + 0.0599*0.3 + 0.04634*0.0 & \\ldots\n",
    "  \\end{bmatrix}\n",
    "  $$\n",
    "\n",
    "  (Complete the matrix multiplication as needed.)\n",
    "\n",
    "#### 8. Final Representation\n",
    "\n",
    "The final output represents the attended information for each token, enriched by multiple attention heads capturing diverse patterns.\n",
    "\n",
    "## Summary of Enhanced Steps\n",
    "\n",
    "1. **Input Preparation:**\n",
    "   - Create input embeddings and add positional encodings.\n",
    "\n",
    "2. **Linear Projections:**\n",
    "   - Compute $Q = XW_Q$, $K = XW_K$, $V = XW_V$ for each head.\n",
    "\n",
    "3. **Attention Scores:**\n",
    "   - Compute $\\text{Scores} = QK^T$.\n",
    "\n",
    "4. **Apply Masking:**\n",
    "   - Add mask matrix $M$ to $\\text{Scores}$ to obtain $\\text{Masked Scores}$.\n",
    "\n",
    "5. **Scaling:**\n",
    "   - Scale the scores by $\\sqrt{d_k}$.\n",
    "\n",
    "6. **Softmax:**\n",
    "   - Apply softmax to obtain attention weights $A$.\n",
    "\n",
    "7. **Attention Output:**\n",
    "   - Compute $\\text{AttOutput} = A V$.\n",
    "\n",
    "8. **Multi-Head Concatenation (if applicable):**\n",
    "   - Concatenate outputs from all heads and apply final linear projection.\n",
    "\n",
    "9. **Final Representation:**\n",
    "   - The final output represents the attended information for each token.\n",
    "\n",
    "\n",
    "\n",
    "## Summary of Steps\n",
    "\n",
    "1. Took the input sequence and created $X$.\n",
    "2. Computed $Q = XW_Q$, $K = XW_K$, $V = XW_V$.\n",
    "3. Calculated $\\text{Scores} = QK^T$, then scaled them.\n",
    "4. Applied softmax to get attention weights $A$.\n",
    "5. Computed the final output as $A V$.\n",
    "\n",
    "This step-by-step example shows how the linear algebra operations translate into the final attended representation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "###############################################################################\n",
    "# NOTE:\n",
    "# This is an illustrative (pseudo) code template designed to show how you might\n",
    "# structure training pipelines for different tasks using Transformers in PyTorch.\n",
    "# It is NOT fully functional or optimized for production use.\n",
    "#\n",
    "# The key idea:\n",
    "# - The core Transformer architecture is often the same (or very similar).\n",
    "# - Task differences are primarily in:\n",
    "#   1. Data Representation (input-output format, multimodal encoders, etc.)\n",
    "#   2. Additional heads or modules depending on the modality and task.\n",
    "#   3. Training objective / loss function.\n",
    "#\n",
    "# Tasks shown conceptually:\n",
    "# - Q/A (Text-to-Text)\n",
    "# - Summarization (Text-to-Text)\n",
    "# - Content creation (e.g. text generation)\n",
    "# - Sentiment analysis (Text classification)\n",
    "# - Chatbot (Text-to-Text, possibly instruction-tuned)\n",
    "# - Image-to-Audio (Vision encoder + Audio decoder)\n",
    "# - Image-to-Image (Vision-to-Vision transform, possibly using diffusion models or VQ-VAE decoders)\n",
    "# - Text-to-Image (Text encoder + Image decoder/generator)\n",
    "# - Text-to-Video (Text encoder + Video generator)\n",
    "#\n",
    "# In practice, each of these tasks might require a different specialized architecture,\n",
    "# especially when moving beyond text (multimodal). Below, we give a rough template\n",
    "# and indicate what changes.\n",
    "###############################################################################\n",
    "\n",
    "########################################\n",
    "# Basic Transformer Building Block\n",
    "########################################\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # Self-attention\n",
    "        attn_out, _ = self.self_attn(x, x, x, attn_mask=mask)\n",
    "        x = x + attn_out\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_out = self.fc(x)\n",
    "        x = x + ff_out\n",
    "        x = self.norm2(x)\n",
    "        return x\n",
    "\n",
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super().__init__()\n",
    "        self.self_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.cross_attn = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        self.norm3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None):\n",
    "        # Self-attention (causal mask for autoregressive decoding)\n",
    "        attn_out, _ = self.self_attn(tgt, tgt, tgt, attn_mask=tgt_mask)\n",
    "        tgt = tgt + attn_out\n",
    "        tgt = self.norm1(tgt)\n",
    "\n",
    "        # Cross-attention\n",
    "        cross_out, _ = self.cross_attn(tgt, memory, memory, attn_mask=memory_mask)\n",
    "        tgt = tgt + cross_out\n",
    "        tgt = self.norm2(tgt)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_out = self.fc(tgt)\n",
    "        tgt = tgt + ff_out\n",
    "        tgt = self.norm3(tgt)\n",
    "        return tgt\n",
    "\n",
    "########################################\n",
    "# Text-Only Transformer (e.g., for Q/A, Summarization, Chatbot)\n",
    "########################################\n",
    "class TextTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=4, ff_dim=1024, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(512, embed_dim)  # positional embedding\n",
    "        self.encoder = nn.ModuleList([TransformerEncoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.decoder = nn.ModuleList([TransformerDecoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, src_tokens, tgt_tokens, src_mask=None, tgt_mask=None):\n",
    "        # Encode\n",
    "        src_seq_len = src_tokens.size(1)\n",
    "        src_positions = torch.arange(src_seq_len, device=src_tokens.device).unsqueeze(0)\n",
    "        src_embed = self.embedding(src_tokens) + self.pos_embed(src_positions)\n",
    "\n",
    "        memory = src_embed\n",
    "        for layer in self.encoder:\n",
    "            memory = layer(memory, mask=src_mask)\n",
    "\n",
    "        # Decode\n",
    "        tgt_seq_len = tgt_tokens.size(1)\n",
    "        tgt_positions = torch.arange(tgt_seq_len, device=tgt_tokens.device).unsqueeze(0)\n",
    "        tgt_embed = self.embedding(tgt_tokens) + self.pos_embed(tgt_positions)\n",
    "\n",
    "        out = tgt_embed\n",
    "        for layer in self.decoder:\n",
    "            out = layer(out, memory, tgt_mask, src_mask)\n",
    "\n",
    "        logits = self.output_head(out)\n",
    "        return logits\n",
    "\n",
    "    # Training for Q/A, Summarization, Chatbot: \n",
    "    # - The model architecture remains the same.\n",
    "    # - The difference is in the data:\n",
    "    #   Q/A: src=input passage+question, tgt=answer\n",
    "    #   Summarization: src=original text, tgt=summary\n",
    "    #   Chatbot: src=dialogue history, tgt=next reply\n",
    "    # \n",
    "    # All are text-to-text. The difference is just *how you present the input-output pairs*.\n",
    "\n",
    "# Example dataset loaders for different tasks\n",
    "class TaskDataset(Dataset):\n",
    "    def __init__(self, inputs, targets, tokenizer):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.tokenizer(self.inputs[idx])\n",
    "        tgt = self.tokenizer(self.targets[idx])\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "# Dummy tokenizer for illustration\n",
    "def dummy_tokenizer(text, vocab_size=1000):\n",
    "    return [hash(token) % vocab_size for token in text.split()]\n",
    "\n",
    "# Datasets for different tasks\n",
    "question_answering_data = TaskDataset(\n",
    "    inputs=[\"What is the capital of France?\", \"Who wrote 1984?\"],\n",
    "    targets=[\"Paris\", \"George Orwell\"],\n",
    "    tokenizer=lambda x: dummy_tokenizer(x)\n",
    ")\n",
    "\n",
    "summarization_data = TaskDataset(\n",
    "    inputs=[\"This is a long article about AI and its applications.\"],\n",
    "    targets=[\"AI and its applications.\"],\n",
    "    tokenizer=lambda x: dummy_tokenizer(x)\n",
    ")\n",
    "\n",
    "chatbot_data = TaskDataset(\n",
    "    inputs=[\"chat: Hello, how are you?\", \"chat: What is your name?\"],\n",
    "    targets=[\"I am fine, thank you.\", \"I am a chatbot.\"],\n",
    "    tokenizer=lambda x: dummy_tokenizer(x)\n",
    ")\n",
    "\n",
    "########################################\n",
    "# Text Classification Head (e.g., Sentiment Analysis)\n",
    "########################################\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model  # a TransformerEncoder for example\n",
    "        self.classifier = nn.Linear(base_model.encoder[-1].norm2.normalized_shape[0], num_classes)\n",
    "\n",
    "    def forward(self, src_tokens, src_mask=None):\n",
    "        # We'll just use the encoder part of a transformer for classification\n",
    "        src_seq_len = src_tokens.size(1)\n",
    "        src_positions = torch.arange(src_seq_len, device=src_tokens.device).unsqueeze(0)\n",
    "        embed = self.base_model.embedding(src_tokens) + self.base_model.pos_embed(src_positions)\n",
    "        \n",
    "        x = embed\n",
    "        for layer in self.base_model.encoder:\n",
    "            x = layer(x, mask=src_mask)\n",
    "\n",
    "        # Take the representation of the first token (e.g. [CLS])\n",
    "        cls_repr = x[:, 0, :]\n",
    "        logits = self.classifier(cls_repr)\n",
    "        return logits\n",
    "\n",
    "    # For sentiment analysis, we don't need a decoder.\n",
    "    # We just encode the text and classify. This changes the output head\n",
    "    # (instead of text generation, we predict class logits).\n",
    "\n",
    "\n",
    "########################################\n",
    "# Multimodal Extensions\n",
    "########################################\n",
    "\n",
    "# For Image-to-Audio, Text-to-Image, etc., we need different encoders/decoders.\n",
    "\n",
    "# Example: Image Encoder (e.g., a Vision Transformer (ViT)-style encoder)\n",
    "class VisionEncoder(nn.Module):\n",
    "    def __init__(self, image_patch_dim, embed_dim=256, num_heads=4, ff_dim=1024, num_layers=4):\n",
    "        super().__init__()\n",
    "        # image_patch_dim: dimension after splitting image into patches and flattening\n",
    "        self.linear = nn.Linear(image_patch_dim, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(512, embed_dim)\n",
    "        self.layers = nn.ModuleList([TransformerEncoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, image_patches):\n",
    "        # image_patches: [B, num_patches, patch_dim]\n",
    "        B, T, _ = image_patches.size()\n",
    "        positions = torch.arange(T, device=image_patches.device).unsqueeze(0)\n",
    "        x = self.linear(image_patches) + self.pos_embed(positions)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x  # This acts like \"memory\" for the decoder\n",
    "\n",
    "# For Audio Decoding or Image Decoding, the architecture might vary significantly.\n",
    "# Here, we might have a Transformer decoder that outputs spectrogram tokens for audio,\n",
    "# or latent codes for an image decoder (like VQ-VAE codes).\n",
    "\n",
    "class AudioDecoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=256, num_heads=4, ff_dim=1024, num_layers=4):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.pos_embed = nn.Embedding(512, embed_dim)\n",
    "        self.layers = nn.ModuleList([TransformerDecoderLayer(embed_dim, num_heads, ff_dim) for _ in range(num_layers)])\n",
    "        self.output_head = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "    def forward(self, memory, tgt_tokens, tgt_mask=None):\n",
    "        # memory from vision encoder\n",
    "        T = tgt_tokens.size(1)\n",
    "        positions = torch.arange(T, device=tgt_tokens.device).unsqueeze(0)\n",
    "        x = self.embedding(tgt_tokens) + self.pos_embed(positions)\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask=tgt_mask)\n",
    "\n",
    "        logits = self.output_head(x)\n",
    "        return logits\n",
    "\n",
    "# For text-to-image or text-to-video: \n",
    "# - Text-to-image might have a text encoder (like the TextTransformer encoder) and a custom image decoder.\n",
    "# - Text-to-video might be similar, but the decoder would produce a sequence of video frames (tokens), possibly using a specialized tokenization.\n",
    "\n",
    "# The main changes are:\n",
    "# 1. Add a different encoder for non-text input (images, audio features).\n",
    "# 2. Add a different decoder for non-text output (image tokens, audio tokens).\n",
    "# 3. Data presentation: \n",
    "#    - Image-to-Audio: Input: image (converted to patches), Output: sequence of audio tokens (like spectrogram patches)\n",
    "#    - Image-to-Image: Input: image patches, Output: another set of image tokens (e.g., style transfer)\n",
    "#    - Text-to-Image: Input: text tokens, Output: image tokens\n",
    "#    - Text-to-Video: Input: text tokens, Output: video tokens (multiple frames encoded as patches)\n",
    "#\n",
    "# Each requires specialized data loaders that convert raw data (images, audio, video) into token-like sequences.\n",
    "# Often, a learned codebook or a pretrained VAE (for images) or audio tokenizer is used.\n",
    "\n",
    "########################################\n",
    "# Example Dummy Training Loop (Text-to-Text)\n",
    "########################################\n",
    "# Q/A training: Input: \"question + context\", Output: \"answer\"  \n",
    "class QADataset(Dataset):\n",
    "    def __init__(self, questions, contexts, answers, tokenizer):\n",
    "        self.questions = questions\n",
    "        self.contexts = contexts\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # This is just a conceptual example\n",
    "        src = self.tokenizer(self.contexts[idx] + \" \" + self.questions[idx])\n",
    "        tgt = self.tokenizer(self.answers[idx])\n",
    "        return torch.tensor(src), torch.tensor(tgt)\n",
    "\n",
    "# Instantiate a text model\n",
    "vocab_size = 10000\n",
    "model = TextTransformer(vocab_size)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Dummy data\n",
    "questions = [\"What is the capital of France?\", \"Who wrote 1984?\"]\n",
    "contexts = [\"France is a country in Europe. Its capital is Paris.\", \"1984 is a novel by George Orwell.\"]\n",
    "answers = [\"Paris\", \"George Orwell\"]\n",
    "tokenizer = lambda x: [hash(t)%vocab_size for t in x.split()] # Dummy tokenizer\n",
    "\n",
    "dataset = QADataset(questions, contexts, answers, tokenizer)\n",
    "loader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for src, tgt in loader:\n",
    "    # Shift tgt by one for teacher forcing:\n",
    "    # Input to decoder: tgt[:, :-1]\n",
    "    # Targets for loss: tgt[:, 1:]\n",
    "    logits = model(src, tgt[:, :-1])\n",
    "    # logits: [B, T, vocab_size]\n",
    "    # targets: [B, T]\n",
    "    loss = F.cross_entropy(logits.transpose(1, 2), tgt[:, 1:])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    print(\"Loss:\", loss.item())\n",
    "\n",
    "# This shows a typical text-to-text training step.\n",
    "# For other tasks, you'd change:\n",
    "# - The dataset (how data is loaded and tokenized)\n",
    "# - Possibly the architecture (add a vision encoder for image tasks, etc.)\n",
    "# - The loss function (maybe multiple heads or different output spaces)\n",
    "#\n",
    "# For advanced multimodal tasks, you'd have something like:\n",
    "#   encoder = VisionEncoder(image_patch_dim=...),\n",
    "#   decoder = AudioDecoder(vocab_size=...)\n",
    "# and train similarly, but feeding image patches to the encoder and audio tokens to the decoder.\n",
    "\n",
    "###############################################################################\n",
    "# Summary:\n",
    "# - Q/A, Summarization, Content creation, Chatbot: Mostly text-to-text.\n",
    "#   * Same model architecture (encoder-decoder).\n",
    "#   * Different datasets and prompts.\n",
    "# \n",
    "# - Sentiment Analysis: Classification head on top of an encoder. Same model layers,\n",
    "#   but different final layer (linear classifier) and data labeling.\n",
    "#\n",
    "# - Image-to-Audio, Image-to-Image, Text-to-Image, Text-to-Video:\n",
    "#   * Typically involves a multimodal architecture:\n",
    "#       - Add a Vision Encoder for images.\n",
    "#       - Add a different Decoder (or generation head) for the output modality.\n",
    "#   * Data presentation changes: images become patches, audio/video are tokenized,\n",
    "#     text remains text tokens.\n",
    "#   * Training objective still involves predicting the next \"token\" in the output modality.\n",
    "###############################################################################\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BERT TOKENS VS BERT EMBEDDINGS VS BERT MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# -------------------------- 1. BERT Tokens -----------------------------\n",
    "# Step 1: Tokenize a given input sentence\n",
    "pretrained_model = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "# Example sentence\n",
    "text = \"The cat sat on the mat.\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = tokenizer.tokenize(text)  # Step 1: Get BERT Tokens\n",
    "input_ids = tokenizer.convert_tokens_to_ids(tokens)  # Convert tokens to input IDs\n",
    "\n",
    "print(\"Step 1: BERT Tokens\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Token IDs: {input_ids}\\n\")\n",
    "\n",
    "# ---------------------- 2. BERT Embeddings ----------------------------\n",
    "# Step 2: Prepare input tensors and extract embeddings\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")  # Auto-prepare inputs\n",
    "\n",
    "# Load the BERT model\n",
    "model = BertModel.from_pretrained(pretrained_model)\n",
    "\n",
    "# Extract Input Embeddings\n",
    "with torch.no_grad():\n",
    "    # Word Embeddings\n",
    "    word_embeddings = model.embeddings.word_embeddings(encoded_input[\"input_ids\"])\n",
    "\n",
    "    # Position Embeddings\n",
    "    position_ids = torch.arange(0, encoded_input[\"input_ids\"].size(1)).unsqueeze(0)\n",
    "    position_embeddings = model.embeddings.position_embeddings(position_ids)\n",
    "\n",
    "    # Segment (Token Type) Embeddings\n",
    "    token_type_embeddings = model.embeddings.token_type_embeddings(encoded_input[\"token_type_ids\"])\n",
    "\n",
    "# Combine all embeddings\n",
    "input_embeddings = word_embeddings + position_embeddings + token_type_embeddings\n",
    "\n",
    "print(\"Step 2: BERT Embeddings\")\n",
    "print(f\"Word Embeddings Shape: {word_embeddings.shape}\")\n",
    "print(f\"Position Embeddings Shape: {position_embeddings.shape}\")\n",
    "print(f\"Segment Embeddings Shape: {token_type_embeddings.shape}\")\n",
    "print(f\"Combined Input Embeddings Shape: {input_embeddings.shape}\\n\")\n",
    "# Shape: (batch_size, sequence_length, hidden_size)\n",
    "\n",
    "# ---------------------- 3. BERT Model/Architecture ---------------------\n",
    "# Step 3: Pass embeddings through the Transformer architecture\n",
    "with torch.no_grad():\n",
    "    outputs = model(**encoded_input)\n",
    "\n",
    "# Outputs\n",
    "last_hidden_state = outputs.last_hidden_state  # Contextualized token representations\n",
    "pooler_output = outputs.pooler_output          # [CLS] token output for sentence-level tasks\n",
    "\n",
    "print(\"Step 3: BERT Model/Architecture\")\n",
    "print(f\"Last Hidden State Shape: {last_hidden_state.shape}\")\n",
    "print(f\"Pooler Output Shape: {pooler_output.shape}\")\n",
    "# Shape: (batch_size, sequence_length, hidden_size) for last_hidden_state\n",
    "# Shape: (batch_size, hidden_size) for pooler_output\n",
    "\n",
    "# --------------------------- Recap -------------------------------------\n",
    "\"\"\"\n",
    "1. BERT Tokens:\n",
    "   - Tokenization breaks text into subword tokens.\n",
    "   - Tokens are converted into IDs that BERT understands.\n",
    "\n",
    "2. BERT Embeddings:\n",
    "   - Input IDs are converted into embeddings (Word, Position, and Segment embeddings).\n",
    "        - Token Embeddings: Convert each token (word/subword) into a vector.\n",
    "        - Segment Embeddings: Distinguish between different sentences in sentence-pair tasks (e.g., sentence A vs. sentence B).\n",
    "        - Position Embeddings: Represent the position of each token in the sequence since Transformers lack inherent positional information.\n",
    "\n",
    "   - Combined embeddings are the input to BERT's Transformer layers.\n",
    "\n",
    "3. BERT Model/Architecture:\n",
    "   - The Transformer layers contextualize the embeddings.\n",
    "   - Outputs include:\n",
    "       - last_hidden_state: Token-level contextual embeddings.\n",
    "       - pooler_output: Sentence-level embedding from the [CLS] token.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AI Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative AI and LLM Projects Classification\n",
    "code in ``` C:\\Users\\pault\\Documents\\3. AI and Machine Learning\\2. Deep Learning\\1. VERY IMPORTANT\\AI notebooks\\notebooks ```\n",
    "\n",
    "### 1. Natural Language Processing (NLP)\n",
    "These projects focus on understanding, generating, or interacting with text.\n",
    "\n",
    "#### **Text Generation and Editing**\n",
    "- Grammar correction (`grammar-correction`)\n",
    "- Text summarization (`summarization-genai`)\n",
    "- Instruction-following LLMs (`dolly-2-instruction-following`)\n",
    "- Content creation (e.g., blogs, articles, or creative writing)\n",
    "- Text editing (`instruct-pix2pix-image-editing`)\n",
    "\n",
    "#### **Question Answering and Chatbots**\n",
    "- Question answering systems (`llm-question-answering`, `table-question-answering`)\n",
    "- Conversational AI/chatbots (`llm-chatbot`, `llava-multimodal-chatbot`)\n",
    "- Task-specific chatbots (`nano-llava-multimodal-chatbot`, `mobilevlm-language-assistant`)\n",
    "\n",
    "#### **Named Entity Recognition (NER)**\n",
    "- Extracting structured information from text (`named-entity-recognition`, `nuextract-structure-extraction`)\n",
    "\n",
    "#### **Language Translation and Multilingual Models**\n",
    "- Cross-lingual translation (`cross-lingual-books-alignment`)\n",
    "- Massively multilingual speech (`mms-massively-multilingual-speech`)\n",
    "\n",
    "#### **RAG and Knowledge Retrieval**\n",
    "- Retrieval-Augmented Generation (RAG) (`llm-rag-langchain`, `llm-rag-llamaindex`)\n",
    "- Knowledge-based question answering (`knowledge-graphs-conve`)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Computer Vision\n",
    "These projects involve image understanding, generation, or manipulation.\n",
    "\n",
    "#### **Image Generation**\n",
    "- Text-to-image (`stable-diffusion-text-to-image`, `text-to-image-genai`, `wuerstchen-image-generation`)\n",
    "- Paint-by-example (`paint-by-example`)\n",
    "- Style transfer (`style-transfer-webcam`, `pixart`)\n",
    "\n",
    "#### **Object Detection and Segmentation**\n",
    "- Object detection from webcam (`object-detection-webcam`, `hello-detection`)\n",
    "- Image segmentation (`segment-anything`, `oneformer-segmentation`)\n",
    "\n",
    "#### **Image-to-Image Transformation**\n",
    "- Sketch-to-image (`sketch-to-image-pix2pix-turbo`)\n",
    "- Image colorization (`ddcolor-image-colorization`)\n",
    "\n",
    "#### **Multimodal Image Understanding**\n",
    "- Visual language processing (`blip-visual-language-processing`)\n",
    "- Zero-shot image classification (`clip-zero-shot-image-classification`, `siglip-zero-shot-image-classification`)\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Speech and Audio Processing\n",
    "Projects that focus on generating, understanding, or manipulating audio.\n",
    "\n",
    "#### **Speech-to-Text**\n",
    "- Automatic Speech Recognition (ASR) (`distil-whisper-asr`, `whisper-asr-genai`)\n",
    "- Subtitle generation from audio (`whisper-subtitles-generation`)\n",
    "\n",
    "#### **Text-to-Speech**\n",
    "- Text-to-speech conversion (`parler-tts-text-to-speech`, `outetts-text-to-speech`)\n",
    "- Multilingual TTS systems (`bark-text-to-audio`)\n",
    "\n",
    "#### **Voice Conversion**\n",
    "- Voice cloning or conversion (`freevc-voice-conversion`, `softvc-voice-conversion`)\n",
    "\n",
    "#### **Music and Sound Generation**\n",
    "- Text-to-music (`riffusion-text-to-music`)\n",
    "- Audio generation (`sound-generation-audioldm2`, `stable-audio`)\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Multimodal AI\n",
    "Models combining text, images, audio, and video.\n",
    "\n",
    "#### **Multimodal Chatbots**\n",
    "- Multimodal assistants (`kosmos2-multimodal-large-language-model`, `llava-next-multimodal-chatbot`)\n",
    "\n",
    "#### **Image-to-Audio or Video**\n",
    "- Text-to-video generation (`zeroscope-text2video`)\n",
    "- Image-to-audio conversion (`bark-text-to-audio`)\n",
    "\n",
    "#### **Cross-Modal Retrieval**\n",
    "- Text-to-video retrieval (`s3d-mil-nce-text-to-video-retrieval`)\n",
    "- Mobile-based video search (`mobileclip-video-search`)\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Specialized Applications\n",
    "Projects addressing domain-specific challenges.\n",
    "\n",
    "#### **Healthcare**\n",
    "- 3D segmentation for medical images (`3D-segmentation-point-clouds`)\n",
    "- CT scan segmentation and quantization (`ct-segmentation-quantize`)\n",
    "\n",
    "#### **Education**\n",
    "- Explainable AI (`explainable-ai-1-basic`, `explainable-ai-3-map-interpretation`)\n",
    "- Knowledge-based tutors or learning assistants\n",
    "\n",
    "### **Industrial Applications**\n",
    "- Meter reading (`meter-reader`)\n",
    "- Vehicle detection and recognition (`vehicle-detection-and-recognition`)\n",
    "- Person tracking and counting (`person-tracking-webcam`, `person-counting-webcam`)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Optimization and Efficiency\n",
    "Improving model performance or enabling deployment on resource-constrained devices.\n",
    "\n",
    "#### **Model Quantization and Optimization**\n",
    "- Post-training quantization (`pytorch-post-training-quantization-nncf`, `tensorflow-quantization-aware-training`)\n",
    "- Quantization-aware training (`pytorch-quantization-aware-training`)\n",
    "\n",
    "#### **Deployment Tools**\n",
    "- Model conversion to OpenVINO (`pytorch-to-openvino`, `tensorflow-object-detection-to-openvino`)\n",
    "- Lightweight implementations for mobile devices (`amused-lightweight-text-to-image`)\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Emerging Domains\n",
    "Innovative and experimental areas in GenAI.\n",
    "\n",
    "#### **Real-Time Interaction**\n",
    "- Real-time animation of humans (`animate-anyone`, `dynamicrafter-animating-images`)\n",
    "- Pose estimation (`pose-estimation-webcam`, `3D-pose-estimation-webcam`)\n",
    "\n",
    "#### **Image Depth and Monodepth**\n",
    "- Depth estimation (`depth-anything`, `vision-monodepth`)\n",
    "\n",
    "#### **Generative Diffusion Models**\n",
    "- Stable diffusion for various tasks (`stable-diffusion-v2`, `stable-diffusion-v3`, `stable-diffusion-xl`)\n",
    "- Video generation with diffusion models (`stable-video-diffusion`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open AI Notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorization of AI Projects\n",
    "\n",
    "### 1. Multimodal Applications\n",
    "#### 1.1 Vision\n",
    "- GPT_with_vision_for_video_understanding.ipynb\n",
    "- Tag_caption_images_with_GPT4V.ipynb\n",
    "- Image_generations_edits_and_variations_with_DALL-E.ipynb\n",
    "- How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb\n",
    "- Creating_slides_with_Assistants_API_and_DALL-E3.ipynb\n",
    "- Vision_Fine_tuning_on_GPT4o_for_Visual_Question_Answering.ipynb\n",
    "- Using_GPT4_Vision_With_Function_Calling.ipynb\n",
    "\n",
    "#### 1.2 Audio\n",
    "- Whisper_correct_misspelling.ipynb\n",
    "- Whisper_processing_guide.ipynb\n",
    "- Whisper_prompting_guide.ipynb\n",
    "- voice_translation_into_different_languages_using_GPT-4o.ipynb\n",
    "- steering_tts.ipynb\n",
    "\n",
    "#### 1.3 Text + Visuals\n",
    "- chat_with_your_own_data.ipynb\n",
    "- Using_vision_modality_for_RAG_with_Pinecone.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Natural Language Processing (NLP)\n",
    "#### 2.1 Text Classification\n",
    "- Fine-tuned_classification.ipynb\n",
    "- Multiclass_classification_for_transactions.ipynb\n",
    "- Zero-shot_classification_with_embeddings.ipynb\n",
    "- Classification_using_embeddings.ipynb\n",
    "\n",
    "#### 2.2 Named Entity Recognition (NER) and Entity Extraction\n",
    "- Named_Entity_Recognition_to_enrich_text.ipynb\n",
    "- Entity_extraction_for_long_documents.ipynb\n",
    "\n",
    "#### 2.3 Question Answering (QA)\n",
    "- Question_answering_using_a_search_API.ipynb\n",
    "- Question_answering_using_embeddings.ipynb\n",
    "- QA_with_Langchain_AnalyticDB_and_OpenAI.ipynb\n",
    "- QA_with_Langchain_Qdrant_and_OpenAI.ipynb\n",
    "- QA_with_Langchain_Tair_and_OpenAI.ipynb\n",
    "- question-answering-with-weaviate-and-openai.ipynb\n",
    "- olympics-2-create-qa.ipynb\n",
    "- olympics-3-train-qa.ipynb\n",
    "\n",
    "#### 2.4 Summarization\n",
    "- Summarizing_long_documents.ipynb\n",
    "- How_to_eval_abstractive_summarization.ipynb\n",
    "\n",
    "#### 2.5 Semantic Search\n",
    "- Embedding_Wikipedia_articles_for_search.ipynb\n",
    "- Semantic_text_search_using_embeddings.ipynb\n",
    "- elasticsearch-semantic-search.ipynb\n",
    "- OpenAI_wikipedia_semantic_search.ipynb\n",
    "- Semantic_Search.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Machine Learning & Embeddings\n",
    "#### 3.1 Embedding Applications\n",
    "- Using_embeddings.ipynb\n",
    "- custom_image_embedding_search.ipynb\n",
    "- Recommendation_using_embeddings.ipynb\n",
    "- Regression_using_embeddings.ipynb\n",
    "- Visualizing_embeddings_in_2D.ipynb\n",
    "- Visualizing_embeddings_in_3D.ipynb\n",
    "- Visualizing_embeddings_in_Kangas.ipynb\n",
    "- Visualizing_embeddings_in_wandb.ipynb\n",
    "- Visualizing_embeddings_with_Atlas.ipynb\n",
    "\n",
    "#### 3.2 Clustering and Dimensionality Reduction\n",
    "- Clustering.ipynb\n",
    "- Clustering_for_transaction_classification.ipynb\n",
    "\n",
    "#### 3.3 Embedding Search\n",
    "- Using_Chroma_for_embeddings_search.ipynb\n",
    "- semantic_search_using_mongodb_atlas_vector_search.ipynb\n",
    "- Using_Pinecone_for_embeddings_search.ipynb\n",
    "- Using_Redis_for_embeddings_search.ipynb\n",
    "- Using_Qdrant_for_embeddings_search.ipynb\n",
    "- Using_Typesense_for_embeddings_search.ipynb\n",
    "- Using_MyScale_for_embeddings_search.ipynb\n",
    "- Using_Weaviate_for_embeddings_search.ipynb\n",
    "- Filtered_search_with_Milvus_and_OpenAI.ipynb\n",
    "- Filtered_search_with_Zilliz_and_OpenAI.ipynb\n",
    "- Redis-hybrid-query-examples.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Retrieval-Augmented Generation (RAG)\n",
    "#### 4.1 General\n",
    "- RAG_with_graph_db.ipynb\n",
    "- GPT4_Retrieval_Augmentation.ipynb\n",
    "- hyde-with-chroma-and-openai.ipynb\n",
    "- generative-search-with-weaviate-and-openai.ipynb\n",
    "- Evaluate_RAG_with_LlamaIndex.ipynb\n",
    "- ft_retrieval_augmented_generation_qdrant.ipynb\n",
    "\n",
    "#### 4.2 Document Parsing and Knowledge Base Integration\n",
    "- Parse_PDF_docs_for_RAG.ipynb\n",
    "- financial_document_analysis_with_llamaindex.ipynb\n",
    "- deeplake_langchain_qa.ipynb\n",
    "- Using_Pinecone_for_embeddings_search.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Assistants and Function Calling\n",
    "#### 5.1 Assistants API\n",
    "- Assistants_API_overview_python.ipynb\n",
    "- Using_tool_required_for_customer_service.ipynb\n",
    "- Using_reasoning_for_data_validation.ipynb\n",
    "\n",
    "#### 5.2 Function Calling\n",
    "- Function_calling_with_an_OpenAPI_spec.ipynb\n",
    "- Fine_tuning_for_function_calling.ipynb\n",
    "- Using_chained_calls_for_o1_structured_outputs.ipynb\n",
    "- Function_calling_finding_nearby_places.ipynb\n",
    "\n",
    "#### 5.3 Workflow and Automation\n",
    "- How_to_automate_S3_storage_with_functions.ipynb\n",
    "- Openai_monitoring_with_wandb_weave.ipynb\n",
    "- orchestration_agents.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multimodal Generative AI\n",
    "#### 6.1 DALL-E\n",
    "- DALL-E.ipynb\n",
    "- How_to_create_dynamic_masks_with_DALL-E_and_Segment_Anything.ipynb\n",
    "\n",
    "#### 6.2 Whisper\n",
    "- whisper.ipynb\n",
    "- Whisper_correct_misspelling.ipynb\n",
    "- Whisper_processing_guide.ipynb\n",
    "- Whisper_prompting_guide.ipynb\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Tools and Integration\n",
    "#### 7.1 Database and Vector Stores\n",
    "- Getting_started_with_bigquery_vector_search_and_openai.ipynb\n",
    "- Getting_started_with_AnalyticDB_and_OpenAI.ipynb\n",
    "- redisjson.ipynb\n",
    "\n",
    "#### 7.2 Miscellaneous\n",
    "- api_request_parallel_processor.py\n",
    "- batch_processing.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Llama FineTuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# A Quick Cheat Sheet for Fine-Tuning LLMs\n",
    "# Using LoRA and QLoRA (Step-by-Step Walkthrough)\n",
    "# ============================================\n",
    "#\n",
    "# This code cell provides a concise, step-by-step cheat sheet for \n",
    "# fine-tuning Large Language Models (LLMs) using LoRA and QLoRA. \n",
    "# Each step includes sample commands, key parameters, and explanations \n",
    "# in comments. Adapt as needed for your own training scripts!\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 1: INSTALL DEPENDENCIES\n",
    "# --------------------------------------------------\n",
    "# Make sure you have the following libraries installed:\n",
    "#   - transformers (latest)\n",
    "#   - accelerate\n",
    "#   - bitsandbytes\n",
    "#   - peft\n",
    "#   - datasets\n",
    "#   - (optional) wandb for experiment tracking\n",
    "#\n",
    "# !pip install -q -U bitsandbytes\n",
    "# !pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "# !pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "# !pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from datasets import load_dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, PeftModel\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 2: DATA PREPARATION\n",
    "# --------------------------------------------------\n",
    "# 1. Load your dataset(s). In many cases, you'll have train/eval/test splits.\n",
    "# 2. Define a prompt format function to guide how the data is turned into text.\n",
    "# 3. Tokenize the prompts to create input_ids and labels for causal LM.\n",
    "\n",
    "train_data = load_dataset('gem', 'viggo', split='train')\n",
    "eval_data  = load_dataset('gem', 'viggo', split='validation')\n",
    "\n",
    "# Example prompt format function\n",
    "def create_prompt(example):\n",
    "    # Customize your prompt as needed:\n",
    "    prompt = (\n",
    "        f\"Given a target sentence, construct the underlying meaning representation.\\n\\n\"\n",
    "        f\"Target Sentence:\\n{example['target']}\\n\\n\"\n",
    "        f\"Meaning Representation:\\n{example['meaning_representation']}\\n\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# We'll create a tokenizer later when we load the model. For demonstration, here's a placeholder.\n",
    "# tokenized_train_data = train_data.map(lambda x: tokenizer(create_prompt(x)), batched=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 3: LOAD THE BASE MODEL (LLAMA 2 EXAMPLE)\n",
    "# --------------------------------------------------\n",
    "# For QLoRA, we use bitsandbytes to load the model in 4-bit precision.\n",
    "# You can adapt for other LLMs as well.\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"  # Example: 7B Llama 2\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,                # 4-bit quantization\n",
    "    bnb_4bit_use_double_quant=True,   # Double quantization for improved memory usage\n",
    "    bnb_4bit_quant_type=\"nf4\",        # Normal Float 4 quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "# Make sure the tokenizer has a pad_token or set it to eos.\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 4: PREPARE MODEL FOR K-BIT TRAINING\n",
    "# --------------------------------------------------\n",
    "# We use PEFT's utility function to prepare the model for 8bit/4bit training. \n",
    "# This helps ensure certain layers are frozen appropriately.\n",
    "\n",
    "model.gradient_checkpointing_enable()  # Optional memory-saving trick\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 5: SET UP LoRA OR QLoRA ADAPTERS\n",
    "# --------------------------------------------------\n",
    "# LoRA config parameters:\n",
    "#   - r: Rank of the adapter matrix (larger => more capacity).\n",
    "#   - lora_alpha: Scaling factor.\n",
    "#   - lora_dropout: Dropout for LoRA layers.\n",
    "#   - target_modules: Which model modules to apply LoRA to.\n",
    "# For QLoRA, use the same approach but ensure you loaded the model in 4bit above.\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\", \"lm_head\"\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, lora_config)  # Apply LoRA adapters\n",
    "\n",
    "# Utility to see trainable params\n",
    "def print_trainable_parameters(m):\n",
    "    trainable_params = 0\n",
    "    all_params = 0\n",
    "    for _, p in m.named_parameters():\n",
    "        all_params += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable_params += p.numel()\n",
    "    print(f\"Trainable params: {trainable_params} | All params: {all_params} | Trainable%: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "model.config.use_cache = False  # re-enable for inference to speed up predictions for similar inputs\n",
    "# --------------------------------------------------\n",
    "# STEP 6: TOKENIZATION PIPELINE (UPDATED)\n",
    "# --------------------------------------------------\n",
    "# We'll tokenize with appropriate padding and truncation for your max_length.\n",
    "\n",
    "def tokenize_function(ex):\n",
    "    prompt = create_prompt(ex)\n",
    "    tokenized = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,         # Adjust as needed\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    # Causal LM training: labels match input_ids\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "tokenized_train_data = train_data.map(tokenize_function, batched=False)\n",
    "tokenized_eval_data  = eval_data.map(tokenize_function, batched=False)\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 7: TRAINING PREP WITH TRANSFORMERS\n",
    "# --------------------------------------------------\n",
    "# We'll create a Trainer or Accelerate-based training loop.\n",
    "# For demonstration, here's a minimal Trainer approach:\n",
    "\n",
    "from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./lora-qlora-output\",\n",
    "    per_device_train_batch_size=2,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=2,   # Adjust as needed\n",
    "    gradient_accumulation_steps=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    logging_steps=25,\n",
    "    num_train_epochs=1,            # or use max_steps if you prefer\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,                     # set bf16 or fp16 as your hardware allows\n",
    "    optim=\"paged_adamw_8bit\",      # recommended for 4-bit/8-bit\n",
    "    report_to=\"none\"               # or \"wandb\" if you want to track metrics\n",
    ")\n",
    "\n",
    "# Data collator: ensure we handle LM-style tasks properly\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train_data,\n",
    "    eval_dataset=tokenized_eval_data,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 8: TRAIN THE MODEL\n",
    "# --------------------------------------------------\n",
    "# This will run the LoRA/QLoRA fine-tuning. Watch out for OOM (Out of Memory) errors.\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 9: SAVING AND LOADING THE FINETUNED ADAPTER\n",
    "# --------------------------------------------------\n",
    "# Once training completes, you can save the PEFT LoRA adapter weights with:\n",
    "trainer.save_model(\"./lora-qlora-output\")\n",
    "\n",
    "# Later, to reload, you do:\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "lora_model = PeftModel.from_pretrained(base_model, \"./lora-qlora-output\")\n",
    "\n",
    "# --------------------------------------------------\n",
    "# STEP 10: INFERENCE\n",
    "# --------------------------------------------------\n",
    "# Let's see how the model performs with a test prompt after fine-tuning.\n",
    "\n",
    "sample_prompt = \"Please convert the following text into a meaning representation: ... \"\n",
    "tokens = tokenizer(sample_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    output_ids = lora_model.generate(**tokens, max_new_tokens=100)\n",
    "print(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "\n",
    "# --------------------------------------------------\n",
    "# NOTE ON QLoRA vs LoRA\n",
    "# --------------------------------------------------\n",
    "# - QLoRA: A memory-efficient form of LoRA that uses 4-bit quantization for base model weights.\n",
    "# - LoRA: Traditional approach that can run at full precision or 8-bit if loaded that way.\n",
    "#\n",
    "# Both rely on the PEFT library's approach of training rank-decomposed adapter matrices \n",
    "# while freezing most of the base model's parameters.\n",
    "\n",
    "# --------------------------------------------------\n",
    "# DONE!\n",
    "# --------------------------------------------------\n",
    "# This cheat sheet has walked you through:\n",
    "#  1) Installing dependencies\n",
    "#  2) Loading data and formatting prompts\n",
    "#  3) Loading a base LLM in 4-bit quantization (QLoRA approach)\n",
    "#  4) Preparing the model for k-bit training\n",
    "#  5) Setting LoRA config for target modules\n",
    "#  6) Tokenizing data for causal LM\n",
    "#  7) Creating a Transformers Trainer\n",
    "#  8) Fine-tuning with LoRA/QLoRA\n",
    "#  9) Saving/loading your LoRA adapters\n",
    "# 10) Running inference with the finetuned model\n",
    "\n",
    "print(\"Cheat Sheet for LoRA & QLoRA Fine-tuning Complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# CHEAT SHEET: ADVANCED TRAINING TECHNIQUES FOR LARGE MODELS\n",
    "# ==============================================================================\n",
    "#\n",
    "# This code cell is a walkthrough detailing how to implement:\n",
    "#   1) Parameter-Efficient Fine-Tuning (PEFT) + LoRA\n",
    "#   2) Quantization-Aware Training (QAT)\n",
    "#   3) Gradient Checkpointing\n",
    "#   4) Distributed Training (FSDP, ZeRO)\n",
    "#\n",
    "# Each section includes explanatory code and use cases. \n",
    "# Adapt these examples to your specific model and training environment.\n",
    "# ==============================================================================\n",
    "\n",
    "# ==============================================================================\n",
    "# 1. PEFT + LoRA (Parameter-Efficient Fine-Tuning + Low-Rank Adaptation)\n",
    "# ------------------------------------------------------------------------------\n",
    "# DESCRIPTION:\n",
    "#   - Fine-tunes only small \"adapter\" layers (LoRA) added on top of a large \n",
    "#     pre-trained model, freezing most of the base model's parameters.\n",
    "#   - This conserves memory and improves efficiency.\n",
    "# USE CASE:\n",
    "#   - Large language models (e.g., LLaMA 2) for domain-specific tasks with \n",
    "#     limited data. Train minimal parameters while keeping the rest of the \n",
    "#     model untouched.\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "# EXAMPLE: Loading a base LLM and applying PEFT + LoRA\n",
    "\n",
    "base_model_id = \"meta-llama/Llama-2-7b-hf\"  # Example: LLaMA 2 7B\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# OPTIONAL: Use 8-bit or 4-bit quantization for memory savings \n",
    "# (comment out if not needed)\n",
    "# from transformers import BitsAndBytesConfig\n",
    "# bnb_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     base_model_id,\n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map=\"auto\"\n",
    "# )\n",
    "# model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA Config: \n",
    "lora_config = LoraConfig(\n",
    "    r=8,                       # Rank of low-rank matrices\n",
    "    lora_alpha=16,            # Scaling factor\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],  # Which layers to adapt\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Convert your base model to a PEFT model using LoRA:\n",
    "lora_model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Now you can train 'lora_model' on your domain-specific data. \n",
    "# Example of checking trainable params:\n",
    "def print_trainable_parameters(m):\n",
    "    trainable_params = 0\n",
    "    total_params = 0\n",
    "    for _, p in m.named_parameters():\n",
    "        total_params += p.numel()\n",
    "        if p.requires_grad:\n",
    "            trainable_params += p.numel()\n",
    "    print(f\"Trainable params: {trainable_params} / {total_params} \"\n",
    "          f\"({100 * trainable_params/total_params:.2f}%)\")\n",
    "\n",
    "print_trainable_parameters(lora_model)\n",
    "\n",
    "# ==============================================================================\n",
    "# 2. Quantization-Aware Training (QAT)\n",
    "# ------------------------------------------------------------------------------\n",
    "# DESCRIPTION:\n",
    "#   - Converts model weights from high precision (e.g., FP32) to lower precision \n",
    "#     (e.g., FP16, INT8, or INT4). \n",
    "# BENEFITS:\n",
    "#   - Saves memory and can reduce training time.\n",
    "# CHALLENGES:\n",
    "#   - Must monitor potential accuracy degradation. \n",
    "#   - Not all ops and hardware fully support certain precisions.\n",
    "# EXAMPLE:\n",
    "#   - Using `bitsandbytes` or built-in Transformer/PEFT quantization for \n",
    "#     4-bit/8-bit training. \n",
    "# ==============================================================================\n",
    "\n",
    "# DEMO: Basic QAT with bitsandbytes (using an 8-bit or 4-bit quant config).\n",
    "# Already shown partial steps above, so here's a simplified snippet:\n",
    "\n",
    "\"\"\"\n",
    "from transformers import BitsAndBytesConfig\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0,  # threshold for outlier removal\n",
    "    llm_int8_has_fp16_weight=True\n",
    ")\n",
    "\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "# Then proceed with normal training steps while carefully tracking performance.\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: For actual QAT, you might combine these quantization steps with \n",
    "# further fine-tuning under quantized conditions.\n",
    "\n",
    "# ==============================================================================\n",
    "# 3. Gradient Checkpointing\n",
    "# ------------------------------------------------------------------------------\n",
    "# DESCRIPTION:\n",
    "#   - Saves memory by only storing select intermediate activations needed for \n",
    "#     backward pass, recomputing the rest during backprop.\n",
    "# USE CASE:\n",
    "#   - Useful for large models / limited GPU memory. \n",
    "# TRADE-OFF:\n",
    "#   - Slower training due to re-computation, but significantly lower memory usage.\n",
    "# EXAMPLE:\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "# If your model is a Transformers-based model:\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# This will reduce memory usage at the cost of extra compute during backprop.\n",
    "# Ensure that your training loop or Trainer approach is aware of this setting \n",
    "# so no unexpected issues arise (like weird memory or speed metrics).\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# 4. Distributed Training\n",
    "# ------------------------------------------------------------------------------\n",
    "# DESCRIPTION:\n",
    "#   - Splits data and/or model across multiple GPUs or nodes to reduce training \n",
    "#     time and handle bigger models. \n",
    "# KEY TECHNIQUES:\n",
    "#   - FSDP (Fully Sharded Data Parallel): \n",
    "#       Shards model weights and optimizer states across devices. \n",
    "#   - DeepSpeed ZeRO (Zero Redundancy Optimizer): \n",
    "#       Distributes model parameters, gradients, and optimizer states among \n",
    "#       workers to save memory and improve throughput.\n",
    "# ==============================================================================\n",
    "\"\"\"\n",
    "# EXAMPLE: Using PyTorch FSDP\n",
    "#   1. Import and wrap your model in FSDP:\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import wrap\n",
    "\n",
    "#   2. Initialize distributed environment (e.g., torchrun or manually)\n",
    "#       torchrun --nproc_per_node=4 your_script.py\n",
    "#   3. Wrap your model:\n",
    "model = wrap(model)\n",
    "#   4. Train as usual, ensuring the data is distributed among processes.\n",
    "\n",
    "# EXAMPLE: Using DeepSpeed ZeRO\n",
    "#   1. Install DeepSpeed.\n",
    "#   2. Use a deepspeed config file specifying \"zero_optimization\" stages 1, 2, or 3.\n",
    "#   3. Launch training with deepspeed script:\n",
    "#       deepspeed --num_gpus=4 your_script.py --deepspeed_config ds_config.json\n",
    "#   4. Model parameters, gradients, and optimizer states are partitioned \n",
    "#      across devices.\n",
    "\n",
    "# Both FSDP and ZeRO require a distributed environment setup. \n",
    "# They are powerful for large-scale training while controlling \n",
    "# memory usage effectively.\n",
    "\"\"\"\n",
    "\n",
    "# ==============================================================================\n",
    "# END CHEAT SHEET\n",
    "# ==============================================================================\n",
    "print(\"Cheat sheet complete! Review code, tailor for your environment, and happy training!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
