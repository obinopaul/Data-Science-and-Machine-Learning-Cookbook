{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "AI Agent Architectures\n",
    "├── Profiling Module or Perception Module (The Eyes and Ears of the Agent)\n",
    "│   ├── Sensory expertise\n",
    "│   ├── Perceives and interprets the environment and communicated with other agents.\n",
    "│   ├──  --> the agent may collect and analyze information from its environment like how human senses work.\n",
    "│   ├──  --> helps it comprehend visual signals, recognize speech patterns, and sense tactile inputs..\n",
    "│   └── Example: Recognizing objects via sensors in self-driving cars\n",
    "│\n",
    "├── Memory Module\n",
    "│   ├── Stores data, rules, and patterns\n",
    "│   ├── Enables knowledge recall and decision-making\n",
    "│   └── Example: Chatbots recalling customer preferences\n",
    "│   \n",
    "├── Planning Module\n",
    "│   ├── Analyzes current situations\n",
    "│   ├── Strategizes actions to meet goals\n",
    "│   └── Example: Optimizing delivery routes\n",
    "│\n",
    "├── Action Module\n",
    "│   ├── Executes planned actions\n",
    "│   ├── Interfaces with external systems\n",
    "│   └── Example: Robotic arms assembling parts\n",
    "│\n",
    "├── Learning Module\n",
    "│   ├── Adapts and improves performance\n",
    "│   ├── Methods include:\n",
    "│   │   ├── Supervised Learning\n",
    "│   │   ├── Unsupervised Learning\n",
    "│   │   └── Reinforcement Learning\n",
    "│   └── Example: Agents learning optimal decisions from feedback\n",
    "│\n",
    "├── Data Structuring and Transformation Module\n",
    "│   ├── Organizes and preprocesses data both from the environment as well as from the memory module\n",
    "│   ├── Converts data into trainable formats\n",
    "│   └── Example: Formatting images for neural network training\n",
    "│\n",
    "├── Training Module\n",
    "│   ├── Performs training operations and updates\n",
    "│   ├── Use methods like:\n",
    "│   │   ├── Supervised, Unsupervised, and Reinforcement Learning\n",
    "│   │   ├── Computer Vision, LLM, Time series Learning\n",
    "│   │   ├── ANN, CNN, Transformers, GANs, GNNs, \n",
    "│   │   └── Tools like TensorFlow, PyTorch, Keras, Scikit-learn\n",
    "│   └── Example: AI training itself in virtual environments\n",
    "│\n",
    "└── Other Modules\n",
    "\n",
    " \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "langchain_community\n",
    "tiktoken\n",
    "langchainhub\n",
    "langchain\n",
    "chromadb\n",
    "langgraph\n",
    "tavily-python\n",
    "python-dotenv\n",
    "google-generativeai\n",
    "langchain_google_genai\n",
    "langchain-nomic\n",
    "langchain-text-splitters\n",
    "langchain_mistralai\n",
    "wikipedia\n",
    "langchain_huggingface\n",
    "google-search-results\n",
    "faiss-cpu\n",
    "sentence-transformers\n",
    "youtube-search\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> n8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n8n interface\n",
    "\n",
    "# n8n is a workflow automation tool that enables you to connect your favorite apps, services, and devices.\n",
    "# It allows you to automate workflows and integrate your apps, services, and devices with each other.\n",
    "\n",
    "    # workflow: a sequence of connected steps that automate a process.\n",
    "    # node: a single step in a workflow.\n",
    "    # connection: a link between two nodes that passes data from one node to another.\n",
    "    # execution: a single run of a workflow.\n",
    "\n",
    "# Types of Nodes\n",
    "    # Trigger Node: The starting point of a workflow. It initiates the execution of a workflow.\n",
    "    # Regular Node: A node that performs a specific action or operation.\n",
    "    # Parameter Node: A node that stores and provides data to other nodes in the workflow.\n",
    "    # Sub-Workflow Node: A node that allows you to reuse a workflow within another workflow.\n",
    "    # Webhook Node: A node that receives data from an external service or application.\n",
    "    # Error Node: A node that handles errors that occur during the execution of a workflow.\n",
    "    # No-Operation Node: A node that does nothing. It is used for debugging and testing purposes.\n",
    "    \n",
    "    # OR\n",
    "    # Trigger Nodes: These nodes initiate the execution of a workflow. They are the starting points of a workflow.\n",
    "    # Data Transformation Nodes: These nodes perform operations on data. They transform, filter, or manipulate data in some way.\n",
    "    # Action Nodes: These nodes perform actions such as sending an email, making an API call, or updating a database.\n",
    "    # Logic Nodes: These nodes control the flow of a workflow. They make decisions based on conditions and determine the path a workflow should take.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import agent_types\n",
    "from langchain.agents.react.agent import create_react_agent\n",
    "from langchain.agents import tools, tool \n",
    "\n",
    "\"\"\" \n",
    "Directory structure:\n",
    "└── agents/\n",
    "    ├── __init__.py\n",
    "    ├── agent.py\n",
    "    ├── agent_iterator.py\n",
    "    ├── agent_types.py\n",
    "    ├── initialize.py\n",
    "    ├── load_tools.py\n",
    "    ├── loading.py\n",
    "    ├── schema.py\n",
    "    ├── tools.py\n",
    "    ├── types.py\n",
    "    ├── utils.py\n",
    "    ├── agent_toolkits/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── azure_cognitive_services.py\n",
    "    │   ├── base.py\n",
    "    │   ├── ainetwork/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── amadeus/\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── clickup/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── conversational_retrieval/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── openai_functions.py\n",
    "    │   │   └── tool.py\n",
    "    │   ├── csv/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── file_management/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── github/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── gitlab/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── gmail/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── jira/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── json/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── multion/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── nasa/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── nla/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── tool.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── office365/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── openapi/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── planner.py\n",
    "    │   │   ├── planner_prompt.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   ├── spec.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── pandas/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── playwright/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── powerbi/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── chat_base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── python/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── slack/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── spark/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── spark_sql/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── sql/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── steam/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── vectorstore/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── xorbits/\n",
    "    │   │   └── __init__.py\n",
    "    │   └── zapier/\n",
    "    │       ├── __init__.py\n",
    "    │       └── toolkit.py\n",
    "    ├── chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── conversational/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── conversational_chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── format_scratchpad/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── log.py\n",
    "    │   ├── log_to_messages.py\n",
    "    │   ├── openai_functions.py\n",
    "    │   ├── openai_tools.py\n",
    "    │   ├── tools.py\n",
    "    │   └── xml.py\n",
    "    ├── json_chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── mrkl/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── openai_assistant/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── openai_functions_agent/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── agent_token_buffer_memory.py\n",
    "    │   └── base.py\n",
    "    ├── openai_functions_multi_agent/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── openai_tools/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── output_parsers/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── json.py\n",
    "    │   ├── openai_functions.py\n",
    "    │   ├── openai_tools.py\n",
    "    │   ├── react_json_single_input.py\n",
    "    │   ├── react_single_input.py\n",
    "    │   ├── self_ask.py\n",
    "    │   ├── tools.py\n",
    "    │   └── xml.py\n",
    "    ├── react/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── agent.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   ├── textworld_prompt.py\n",
    "    │   └── wiki_prompt.py\n",
    "    ├── self_ask_with_search/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── structured_chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── tool_calling_agent/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    └── xml/\n",
    "        ├── __init__.py\n",
    "        ├── base.py\n",
    "        └── prompt.py\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "from langchain.agents import tool\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "print(f'Length of the word '{get_word_length.invoke(\"hello\")})\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import retriever\n",
    "\n",
    "\"\"\" \n",
    "Directory structure:\n",
    "└── tools/\n",
    "    ├── __init__.py\n",
    "    ├── base.py\n",
    "    ├── convert_to_openai.py\n",
    "    ├── ifttt.py\n",
    "    ├── plugin.py\n",
    "    ├── render.py\n",
    "    ├── retriever.py\n",
    "    ├── yahoo_finance_news.py\n",
    "    ├── ainetwork/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── app.py\n",
    "    │   ├── base.py\n",
    "    │   ├── owner.py\n",
    "    │   ├── rule.py\n",
    "    │   ├── transfer.py\n",
    "    │   └── value.py\n",
    "    ├── amadeus/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── closest_airport.py\n",
    "    │   └── flight_search.py\n",
    "    ├── arxiv/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── azure_cognitive_services/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── form_recognizer.py\n",
    "    │   ├── image_analysis.py\n",
    "    │   ├── speech2text.py\n",
    "    │   ├── text2speech.py\n",
    "    │   └── text_analytics_health.py\n",
    "    ├── bearly/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── bing_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── brave_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── clickup/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── dataforseo_api_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── ddg_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── e2b_data_analysis/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── edenai/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── audio_speech_to_text.py\n",
    "    │   ├── audio_text_to_speech.py\n",
    "    │   ├── edenai_base_tool.py\n",
    "    │   ├── image_explicitcontent.py\n",
    "    │   ├── image_objectdetection.py\n",
    "    │   ├── ocr_identityparser.py\n",
    "    │   ├── ocr_invoiceparser.py\n",
    "    │   └── text_moderation.py\n",
    "    ├── eleven_labs/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── models.py\n",
    "    │   └── text2speech.py\n",
    "    ├── file_management/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── copy.py\n",
    "    │   ├── delete.py\n",
    "    │   ├── file_search.py\n",
    "    │   ├── list_dir.py\n",
    "    │   ├── move.py\n",
    "    │   ├── read.py\n",
    "    │   └── write.py\n",
    "    ├── github/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── gitlab/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── gmail/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── create_draft.py\n",
    "    │   ├── get_message.py\n",
    "    │   ├── get_thread.py\n",
    "    │   ├── search.py\n",
    "    │   └── send_message.py\n",
    "    ├── golden_query/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_cloud/\n",
    "    │   ├── __init__.py\n",
    "    │   └── texttospeech.py\n",
    "    ├── google_finance/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_jobs/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_lens/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_places/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_scholar/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_serper/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_trends/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── graphql/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── human/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── interaction/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── jira/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── json/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── memorize/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── merriam_webster/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── metaphor_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── multion/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── close_session.py\n",
    "    │   ├── create_session.py\n",
    "    │   └── update_session.py\n",
    "    ├── nasa/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── nuclia/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── office365/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── create_draft_message.py\n",
    "    │   ├── events_search.py\n",
    "    │   ├── messages_search.py\n",
    "    │   ├── send_event.py\n",
    "    │   └── send_message.py\n",
    "    ├── openapi/\n",
    "    │   ├── __init__.py\n",
    "    │   └── utils/\n",
    "    │       ├── __init__.py\n",
    "    │       ├── api_models.py\n",
    "    │       └── openapi_utils.py\n",
    "    ├── openweathermap/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── playwright/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── click.py\n",
    "    │   ├── current_page.py\n",
    "    │   ├── extract_hyperlinks.py\n",
    "    │   ├── extract_text.py\n",
    "    │   ├── get_elements.py\n",
    "    │   ├── navigate.py\n",
    "    │   └── navigate_back.py\n",
    "    ├── powerbi/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── pubmed/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── python/\n",
    "    │   └── __init__.py\n",
    "    ├── reddit_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── requests/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── scenexplain/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── searchapi/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── searx_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── shell/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── slack/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── get_channel.py\n",
    "    │   ├── get_message.py\n",
    "    │   ├── schedule_message.py\n",
    "    │   └── send_message.py\n",
    "    ├── sleep/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── spark_sql/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── sql_database/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── prompt.py\n",
    "    │   └── tool.py\n",
    "    ├── stackexchange/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── steam/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── steamship_image_generation/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── tavily_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── vectorstore/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── wikipedia/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── wolfram_alpha/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── youtube/\n",
    "    │   ├── __init__.py\n",
    "    │   └── search.py\n",
    "    └── zapier/\n",
    "        ├── __init__.py\n",
    "        └── tool.py\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import *\n",
    "# from langchain_community.chains import *\n",
    "\n",
    "\"\"\" \n",
    "Directory structure:\n",
    "└── chains/\n",
    "    ├── __init__.py\n",
    "    ├── base.py\n",
    "    ├── example_generator.py\n",
    "    ├── history_aware_retriever.py\n",
    "    ├── llm.py\n",
    "    ├── llm_requests.py\n",
    "    ├── loading.py\n",
    "    ├── mapreduce.py\n",
    "    ├── moderation.py\n",
    "    ├── prompt_selector.py\n",
    "    ├── retrieval.py\n",
    "    ├── sequential.py\n",
    "    ├── transform.py\n",
    "    ├── api/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── news_docs.py\n",
    "    │   ├── open_meteo_docs.py\n",
    "    │   ├── podcast_docs.py\n",
    "    │   ├── prompt.py\n",
    "    │   ├── tmdb_docs.py\n",
    "    │   └── openapi/\n",
    "    │       ├── __init__.py\n",
    "    │       ├── chain.py\n",
    "    │       ├── prompts.py\n",
    "    │       ├── requests_chain.py\n",
    "    │       └── response_chain.py\n",
    "    ├── chat_vector_db/\n",
    "    │   ├── __init__.py\n",
    "    │   └── prompts.py\n",
    "    ├── combine_documents/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── map_reduce.py\n",
    "    │   ├── map_rerank.py\n",
    "    │   ├── reduce.py\n",
    "    │   ├── refine.py\n",
    "    │   └── stuff.py\n",
    "    ├── constitutional_ai/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── models.py\n",
    "    │   ├── principles.py\n",
    "    │   └── prompts.py\n",
    "    ├── conversation/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── memory.py\n",
    "    │   └── prompt.py\n",
    "    ├── conversational_retrieval/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── elasticsearch_database/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── ernie_functions/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── flare/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── graph_qa/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── arangodb.py\n",
    "    │   ├── base.py\n",
    "    │   ├── cypher.py\n",
    "    │   ├── cypher_utils.py\n",
    "    │   ├── falkordb.py\n",
    "    │   ├── gremlin.py\n",
    "    │   ├── hugegraph.py\n",
    "    │   ├── kuzu.py\n",
    "    │   ├── nebulagraph.py\n",
    "    │   ├── neptune_cypher.py\n",
    "    │   ├── neptune_sparql.py\n",
    "    │   ├── ontotext_graphdb.py\n",
    "    │   ├── prompts.py\n",
    "    │   └── sparql.py\n",
    "    ├── hyde/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── llm_bash/\n",
    "    │   └── __init__.py\n",
    "    ├── llm_checker/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── llm_math/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── llm_summarization_checker/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts/\n",
    "    │       ├── are_all_true_prompt.txt\n",
    "    │       ├── check_facts.txt\n",
    "    │       ├── create_facts.txt\n",
    "    │       └── revise_summary.txt\n",
    "    ├── llm_symbolic_math/\n",
    "    │   └── __init__.py\n",
    "    ├── natbot/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── crawler.py\n",
    "    │   └── prompt.py\n",
    "    ├── openai_functions/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── citation_fuzzy_match.py\n",
    "    │   ├── extraction.py\n",
    "    │   ├── openapi.py\n",
    "    │   ├── qa_with_structure.py\n",
    "    │   ├── tagging.py\n",
    "    │   └── utils.py\n",
    "    ├── openai_tools/\n",
    "    │   ├── __init__.py\n",
    "    │   └── extraction.py\n",
    "    ├── qa_generation/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── qa_with_sources/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── loading.py\n",
    "    │   ├── map_reduce_prompt.py\n",
    "    │   ├── refine_prompts.py\n",
    "    │   ├── retrieval.py\n",
    "    │   ├── stuff_prompt.py\n",
    "    │   └── vector_db.py\n",
    "    ├── query_constructor/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── ir.py\n",
    "    │   ├── parser.py\n",
    "    │   ├── prompt.py\n",
    "    │   └── schema.py\n",
    "    ├── question_answering/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── chain.py\n",
    "    │   ├── map_reduce_prompt.py\n",
    "    │   ├── map_rerank_prompt.py\n",
    "    │   ├── refine_prompts.py\n",
    "    │   └── stuff_prompt.py\n",
    "    ├── retrieval_qa/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── router/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── embedding_router.py\n",
    "    │   ├── llm_router.py\n",
    "    │   ├── multi_prompt.py\n",
    "    │   ├── multi_prompt_prompt.py\n",
    "    │   ├── multi_retrieval_prompt.py\n",
    "    │   └── multi_retrieval_qa.py\n",
    "    ├── sql_database/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── prompt.py\n",
    "    │   └── query.py\n",
    "    ├── structured_output/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    └── summarize/\n",
    "        ├── __init__.py\n",
    "        ├── chain.py\n",
    "        ├── map_reduce_prompt.py\n",
    "        ├── refine_prompts.py\n",
    "        └── stuff_prompt.py\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "from langchain.agents import tool\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "print(f'Length of the word '{get_word_length.invoke(\"hello\")})\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "tools = [tool_1, tool_2, tool_3]\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(text: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(text)\n",
    "\n",
    "print(get_word_length.invoke(\"hello\"))\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Web Answer\",\n",
    "        func = google_search.run,\n",
    "        description=\"Get an intermediate answer to a question.\",\n",
    "        verbose = True\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "#-----------------------------Custom Tool from a Custom Chain----------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "class AnnualReportChain(Chain):\n",
    "    chain: Chain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return list(self.chain.input_keys)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        # Queries the database to get the relevant documents for a given query\n",
    "        query = inputs.get(\"input_documents\", \"\")\n",
    "        docs = vectorstore.similarity_search(query, include_metadata=True)\n",
    "        output = chain.run(input_documents=docs, question=query)\n",
    "        return {'output': output}\n",
    "    \n",
    "    \n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize your custom Chain\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "chain = load_qa_chain(llm)\n",
    "annual_report_chain = AnnualReportChain(chain=chain)\n",
    "\n",
    "# Initialize your custom Tool\n",
    "annual_report_tool = Tool(\n",
    "    name=\"Annual Report\",\n",
    "    func=annual_report_chain.run,\n",
    "    description=\"\"\"\n",
    "    useful for when you need to answer questions about a company's income statement,\n",
    "    cash flow statement, or balance sheet. This tool can help you extract data points like\n",
    "    net income, revenue, free cash flow, and total debt, among other financial line items.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "#---------------------------------- Custom Tool -------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Union\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Define Input Schema: Use Pydantic to define input parameters and descriptions for your tool.\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str = Field(..., description=\"Description of param1.\")\n",
    "    param2: int = Field(default=10, description=\"Description of param2.\")\n",
    "    \n",
    "# Create the Tool: Use the @tool decorator to define a custom tool.\n",
    "@tool(\"my_tool_function\", args_schema=MyToolInput, return_direct=True)\n",
    "def my_tool_function(param1: str, param2: int = 10) -> Union[Dict, str]:\n",
    "    \"\"\"\n",
    "    Description of what the tool does.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = (\n",
    "            f'https://api.financialdatasets.ai/insider-transactions'\n",
    "            f'?ticker={param1}'\n",
    "            f'&limit={param2}'\n",
    "            )\n",
    "        # Perform the task (e.g., call an API, process data, etc.)\n",
    "        response = requests.get(url, headers={'X-API-Key': api_key})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "tools = [my_tool_function, annual_report_tool, get_word_length]\n",
    "\n",
    "\n",
    "#---------------------------------- Creating a Node from Tools ----------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "builder = StateGraph(State)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain, LangGraph, and LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain \n",
    "    # Tools\n",
    "    # Agents\n",
    "    # Chains\n",
    "    # Multi-Agent Systems\n",
    "    # Plan and Execute\n",
    "    # Reflection and Learning\n",
    "    # Communication\n",
    "    # Perception\n",
    "\n",
    "# LangChain is a platform that enables developers to build, test, and deploy blockchain applications using multiple programming languages.\n",
    "# It provides a set of tools and libraries that simplify the development process and make it easier to create blockchain applications.\n",
    "\n",
    "\n",
    "\n",
    "# Types of LangChain Agents\n",
    "    # LangChain offers several agentic patterns, each tailored to specific needs. These include:\n",
    "\n",
    "    # Tool Calling Agents: Designed for straightforward tool usage.\n",
    "    # React Agents: Use reasoning and action mechanisms to dynamically decide the best steps.\n",
    "    # Structured Chat Agents: Parse inputs and outputs into structured formats like JSON.\n",
    "    # Self-Ask with Search: Handle queries by splitting them into smaller, manageable steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Tool Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents.tool_calling_agent import base\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI as LangchainChatDeepSeek\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults, TavilyAnswer\n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "import os\n",
    "\n",
    "# Load API key\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "# Tools\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "tool_3 = TavilySearchResults(max_results=10)\n",
    "\n",
    "tools = [tool_1, tool_2, tool_3]\n",
    "\n",
    "# LLM\n",
    "llm = LangchainChatDeepSeek(\n",
    "            api_key=api_key,\n",
    "            model=\"deepseek-chat\",\n",
    "            base_url=\"https://api.deepseek.com\",\n",
    "        )\n",
    "\n",
    "# Agent\n",
    "\n",
    "# Create a tool-calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "# agent = base.create_tool_calling_agent()\n",
    "\n",
    "# Agent Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,  # Only final output. If True, returns all intermediate steps\n",
    "    handle_parsing_errors=True,  # Graceful parsing errors\n",
    ")\n",
    "        \n",
    "\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": [HumanMessage(content=query)]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from typing import Annotated\n",
    "\n",
    "template = ''' Answwer the following questions as best as you can. You have access to the following tools:\n",
    "{tools}\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action you should take, should be one of [{tool_names}]\n",
    "Action_input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this thouhgt/Action/Action input/Observation sequence can be repeated N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "'''\n",
    "\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "    \n",
    "prompt = PromptTemplate.from_template(template)\n",
    "search_agent = create_react_agent(llm, tools = [python_repl_tool], prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=search_agent, tools=tools, verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n",
    "# agent_executor.invoke({\"input\": [HumanMessage(content=\"What is the capital of France?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Self Ask with Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_self_ask_with_search_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Web Answer\",\n",
    "        func = google_search.run,\n",
    "        description=\"Get an intermediate answer to a question.\",\n",
    "        verbose = True\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "search_agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=search_agent, tools=tools, verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n",
    "# agent_executor.invoke({\"input\": [HumanMessage(content=\"What is the capital of France?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "class CustomAgent(BaseModel):\n",
    "    llm: BaseLanguageModel  # The LLM to use for decision-making\n",
    "    tools: List[BaseTool]  # List of tools the agent can use\n",
    "    max_loops: int = 5  # Maximum number of loops to prevent infinite execution\n",
    "    stop_pattern: List[str]  # Stop patterns for the LLM to avoid hallucinations\n",
    "\n",
    "    @property\n",
    "    def tool_by_names(self) -> Dict[str, BaseTool]:\n",
    "        \"\"\"Map tool names to tool objects.\"\"\"\n",
    "        return {tool.name: tool for tool in self.tools}\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        \"\"\"Run the agent to answer a question.\"\"\"\n",
    "        name_to_tool_map = self.tool_by_names\n",
    "        previous_responses = []\n",
    "        num_loops = 0\n",
    "\n",
    "        while num_loops < self.max_loops:\n",
    "            num_loops += 1\n",
    "\n",
    "            # Format the prompt with the current state\n",
    "            curr_prompt = PROMPT_TEMPLATE.format(\n",
    "                tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools]),\n",
    "                tool_names=\", \".join([tool.name for tool in self.tools]),\n",
    "                question=question,\n",
    "                previous_responses=\"\\n\".join(previous_responses),\n",
    "            )\n",
    "\n",
    "            # Get the next action from the LLM\n",
    "            output, tool, tool_input = self._get_next_action(curr_prompt)\n",
    "\n",
    "            # If the final answer is found, return it\n",
    "            if tool == \"Final Answer\":\n",
    "                return tool_input\n",
    "\n",
    "            # Execute the tool and get the result\n",
    "            tool_result = name_to_tool_map[tool].run(tool_input)\n",
    "            output += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
    "            print(output)  # Print the agent's reasoning\n",
    "            previous_responses.append(output)\n",
    "\n",
    "        return \"Max loops reached without finding a final answer.\"\n",
    "\n",
    "    def _get_next_action(self, prompt: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Get the next action from the LLM.\"\"\"\n",
    "        result = self.llm.generate([prompt], stop=self.stop_pattern)\n",
    "        output = result.generations[0][0].text  # Get the first generation\n",
    "\n",
    "        # Parse the output to extract the tool and input\n",
    "        tool, tool_input = self._get_tool_and_input(output)\n",
    "        return output, tool, tool_input\n",
    "\n",
    "    def _get_tool_and_input(self, generated: str) -> Tuple[str, str]:\n",
    "        \"\"\"Parse the LLM output to extract the tool and input.\"\"\"\n",
    "        if FINAL_ANSWER_TOKEN in generated:\n",
    "            return \"Final Answer\", generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
    "\n",
    "        # Use regex to extract the tool and input\n",
    "        regex = r\"Action: (.*?)\\nAction Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, generated, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Output of LLM is not parsable for next tool use: `{generated}`\")\n",
    "\n",
    "        tool = match.group(1).strip()\n",
    "        tool_input = match.group(2).strip(\" \").strip('\"')\n",
    "        return tool, tool_input\n",
    "    \n",
    "\n",
    "FINAL_ANSWER_TOKEN = \"Final Answer:\"\n",
    "OBSERVATION_TOKEN = \"Observation:\"\n",
    "THOUGHT_TOKEN = \"Thought:\"\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question as best as you can using the following tools: \n",
    "\n",
    "{tool_description}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: comment on what you want to do next\n",
    "Action: the action to take, exactly one element of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation repeats N times, use it until you are sure of the answer)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {question}\n",
    "Thought: {previous_responses}\n",
    "\"\"\"\n",
    "\n",
    "# The tool(s) that your Agent will use\n",
    "tools = [annual_report_tool]\n",
    "\n",
    "# The question that you will ask your Agent\n",
    "question = \"What was Meta's net income in 2022? What was net income the year before that?\"\n",
    "\n",
    "# The prompt that your Agent will use and update as it is \"reasoning\"\n",
    "prompt = PROMPT_TEMPLATE.format(\n",
    "  tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
    "  tool_names=\", \".join([tool.name for tool in tools]),\n",
    "  question=question,\n",
    "  previous_responses='{previous_responses}',\n",
    ")\n",
    "\n",
    "# The LLM that your Agent will use\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize your Agent\n",
    "agent = CustomAgent(\n",
    "  llm=llm, \n",
    "  tools=tools, \n",
    "  prompt=prompt, \n",
    "  stop_pattern=[f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}'],\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,  # Only final output. If True, returns all intermediate steps\n",
    "    handle_parsing_errors=True,  # Graceful parsing errors\n",
    ")\n",
    "# Run the Agent!\n",
    "result = agent.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Custom Agent Executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Key Concepts\n",
    "    # Graph : A workflow of nodes and edges.\n",
    "    # Nodes : Functions or agents that perform tasks.\n",
    "    # Edges : Connections between nodes that define the flow.\n",
    "    # State : A shared data structure passed between nodes.\n",
    "    # StateGraph : A graph that manages state transitions.\n",
    "\n",
    "# Draw a directory tree for the src directory for a LangChain project.\n",
    "\"\"\"\n",
    "src/\n",
    "├── agents/\n",
    "│   ├── __init__.py\n",
    "│   ├── agent.py\n",
    "│   ├── graph.py\n",
    "│   ├── tools.py\n",
    "│   ├── configuration.py\n",
    "│   ├── state.py\n",
    "│   ├── prompts.py\n",
    "│   └── utils.py\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, Sequence, List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "#------------------ Define the Memory Saver-----------------------\n",
    "memory = MemorySaver()\n",
    "\n",
    "#------------------ Define the State-----------------------     # You can write a custom state class by extending the TypedDict class.\n",
    "class AgentState(TypedDict):\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Selected Agents: Tracks which agents are active in the workflow\n",
    "    selected_analysts: List[str]\n",
    "    \n",
    "    # Current Agent Index: Tracks the progress through the selected agents\n",
    "    current_analyst_idx: int\n",
    "\n",
    "workflow = StateGraph(AgentState)   # Initialize the Graph\n",
    "\n",
    "#------------------ Create Nodes-----------------------\n",
    "def supervisor_router(state):\n",
    "    \"\"\"Route to appropriate analyst(s) based on the query\"\"\"\n",
    "    result = routing_chain.invoke(state)\n",
    "    selected_analysts = [a.strip() for a in result.content.strip().split(',')]\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [SystemMessage(content=f\"Routing query to: {', '.join(selected_analysts)}\", name=\"supervisor\")],\n",
    "        \"selected_analysts\": selected_analysts,\n",
    "        \"current_analyst_idx\": 0\n",
    "    }\n",
    "\n",
    "# or\n",
    "def agent_node(state: AgentState, agent, name: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Generic node function for an agent.\n",
    "    - `state`: The current state of the workflow.\n",
    "    - `agent`: The agent or function to process the state.\n",
    "    - `name`: The name of the agent (for logging or identification).\n",
    "    \"\"\"\n",
    "    # Invoke the agent with the current state\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # Update the state with the agent's output\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=result[\"messages\"][-1].content, name=name)],\n",
    "        \"selected_agents\": state[\"selected_agents\"],\n",
    "        \"current_agent_idx\": state[\"current_agent_idx\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "#--------------------- Wrap the agent in a node--------------------------\n",
    "\n",
    "# Create the analysts with their specific tools\n",
    "quant_strategist = create_react_agent(llm, tools=quant_strategist_tools)\n",
    "quant_strategist_node = functools.partial(agent_node, agent=quant_strategist, name=\"quant_strategist\")\n",
    "\n",
    "macro_analyst = create_react_agent(llm, tools=macro_analyst_tools)\n",
    "macro_analyst_node = functools.partial(agent_node, agent=macro_analyst, name=\"macro_analyst\")\n",
    "\n",
    "\n",
    "#------------------- Add Nodes to Graph-----------------------\n",
    "workflow = StateGraph(AgentState)   # Initialize the Graph\n",
    "workflow.add_node(\"supervisor\", supervisor_router)  # Add the supervisor node\n",
    "workflow.add_node(\"quant_strategist\", quant_strategist_node)    # Add the quant_strategist node\n",
    "workflow.add_node(\"macro_analyst\", macro_analyst_node)        # Add the macro_analyst node\n",
    "\n",
    "#------------------- Define the Prompt-----------------------\n",
    "class SupervisorPrompt(ChatPromptTemplate):\n",
    "    \"\"\"Prompt for the supervisor node\"\"\"\n",
    "    messages: MessagesPlaceholder\n",
    "    selected_analysts: List[str]\n",
    "    current_analyst_idx: int\n",
    "\n",
    "#------------------- Define Conditional Edge-----------------------\n",
    "def get_next_step(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next step in the workflow.\n",
    "    - If no agents are selected, go to the final summary.\n",
    "    - If all agents have processed, go to the final summary.\n",
    "    - Otherwise, go to the next agent.\n",
    "    \"\"\"\n",
    "    if not state[\"selected_agents\"]:\n",
    "        return \"final_summary\"\n",
    "    current_idx = state[\"current_agent_idx\"]\n",
    "    if current_idx >= len(state[\"selected_agents\"]):\n",
    "        return \"final_summary\"\n",
    "    return state[\"selected_agents\"][current_idx]\n",
    "\n",
    "\n",
    "# Add conditional edges:\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",  # Source node\n",
    "    get_next_step,  # Router node/Function to determine the next step\n",
    "    {\n",
    "        \"quant_strategist\": \"quant_strategist\",  # Route to quant_strategist node\n",
    "        \"macro_analyst\": \"macro_analyst\",        # Route to macro_analyst node\n",
    "        \"final_summary\": \"final_summary\"         # Route to final_summary node\n",
    "    }\n",
    ")\n",
    "\n",
    "#------------------ Add Final Edges ------------------------------------\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "workflow.add_edge(\"final_summary\", END)\n",
    "\n",
    "#-------------------- Compile the Graph --------------------------------\n",
    "graph = workflow.compile()\n",
    "# or\n",
    "graph = workflow.compile(checkpointer=memory)   # Compile the graph with memory\n",
    "# or\n",
    "graph = workflow.compile(checkpointer=memory, interrupt_before=[\"quant_strategist_node\"])  # Compile the graph with memory and interrupt before quant_strategist_node\n",
    "\n",
    "\n",
    "#------------------ Stream the Graph with Memory--------------------------------\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}   # add memory thread, we used thread_id = 2\n",
    "events = graph.stream({\"messages\": {\"Hi there, my name is Paul\"}}, config, stream_mode = \"values\")\n",
    "\n",
    "for event in events:    # Iterate over the events\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "memory.get(config)  # Retrieve the memory for a specific configuration or thread_id\n",
    "\n",
    "\n",
    "#-------------------- Accessing the Graph State --------------------------------\n",
    "graph.get_state(config).values  # get the state of the graph\n",
    "graph.update_state(config, {\"input\": \"Hello, World!\"})  # update the state of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nice way to execute the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------ATLERNATIVE WAY TO RUN THE GRAPH IN A BEAUTIFUL WAY------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Run the Graph------------------------------------\n",
    "#------------------------- Custom Function----------------------------------\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import re\n",
    "from langchain_core.messages import HumanMessage\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.rule import Rule\n",
    "\n",
    "#---------- Formatting Functions\n",
    "# Format Bold Text\n",
    "def format_bold_text(content: str) -> Text:\n",
    "    \"\"\"Convert **text** to rich Text with bold formatting.\"\"\"\n",
    "    text = Text()\n",
    "    pattern = r'\\*\\*(.*?)\\*\\*'\n",
    "    parts = re.split(pattern, content)\n",
    "    for i, part in enumerate(parts):\n",
    "        if i % 2 == 0:\n",
    "            text.append(part)\n",
    "        else:\n",
    "            text.append(part, style=\"bold\")\n",
    "    return text\n",
    "\n",
    "# Format Message Content\n",
    "def format_message_content(content: str) -> Union[str, Text]:\n",
    "    \"\"\"Format the message content, handling JSON and text with bold markers.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        return json.dumps(data, indent=2)\n",
    "    except:\n",
    "        if '**' in content:\n",
    "            return format_bold_text(content)\n",
    "        return content\n",
    "\n",
    "# Format Agent Message\n",
    "def format_agent_message(message: HumanMessage) -> Union[str, Text]:\n",
    "    \"\"\"Format a single agent message.\"\"\"\n",
    "    return format_message_content(message.content)\n",
    "\n",
    "# Get Agent Title\n",
    "def get_agent_title(agent: str, message: HumanMessage) -> str:\n",
    "    \"\"\"Get the title for the agent panel, with fallback handling.\"\"\"\n",
    "    base_title = agent.replace('_', ' ').title()\n",
    "    if hasattr(message, 'name') and message.name is not None:\n",
    "        try:\n",
    "            return message.name.replace('_', ' ').title()\n",
    "        except:\n",
    "            return base_title\n",
    "    return base_title\n",
    "\n",
    "# Print a Single Step\n",
    "def print_step(step: Dict[str, Any]) -> None:\n",
    "    \"\"\"Pretty print a single step of the agent execution.\"\"\"\n",
    "    console = Console()\n",
    "    for agent, data in step.items():\n",
    "        # Handle supervisor steps\n",
    "        if 'next' in data:\n",
    "            next_agent = data['next']\n",
    "            text = Text()\n",
    "            text.append(\"Portfolio Manager \", style=\"bold magenta\")\n",
    "            text.append(\"assigns next task to \", style=\"white\")\n",
    "            if next_agent == \"final_summary\":\n",
    "                text.append(\"FINAL SUMMARY\", style=\"bold yellow\")\n",
    "            elif next_agent == \"END\":\n",
    "                text.append(\"END\", style=\"bold red\")\n",
    "            else:\n",
    "                text.append(f\"{next_agent}\", style=\"bold green\")\n",
    "            console.print(Panel(\n",
    "                text,\n",
    "                title=\"[bold blue]Supervision Step\",\n",
    "                border_style=\"blue\"\n",
    "            ))\n",
    "        # Handle agent responses and final summary\n",
    "        if 'messages' in data:\n",
    "            message = data['messages'][0]\n",
    "            formatted_content = format_agent_message(message)\n",
    "            if agent == \"final_summary\":\n",
    "                # Final summary formatting\n",
    "                console.print(Rule(style=\"yellow\", title=\"Portfolio Analysis\"))\n",
    "                console.print(Panel(\n",
    "                    formatted_content,\n",
    "                    title=\"[bold yellow]Investment Summary and Recommendation\",\n",
    "                    border_style=\"yellow\",\n",
    "                    padding=(1, 2)\n",
    "                ))\n",
    "                console.print(Rule(style=\"yellow\"))\n",
    "            else:\n",
    "                # Regular analyst reports\n",
    "                title = get_agent_title(agent, message)\n",
    "                console.print(Panel(\n",
    "                    formatted_content,\n",
    "                    title=f\"[bold blue]{title} Report\",\n",
    "                    border_style=\"green\"\n",
    "                ))\n",
    "\n",
    "# Stream the Execution\n",
    "def stream_agent_execution(graph, input_data: Dict, config: Dict) -> None:\n",
    "    \"\"\"Stream and pretty print the agent execution.\"\"\"\n",
    "    console = Console()\n",
    "    console.print(\"\\n[bold blue]Starting Agent Execution...[/bold blue]\\n\")\n",
    "    for step in graph.stream(input_data, config):\n",
    "        if \"__end__\" not in step:\n",
    "            print_step(step)\n",
    "            console.print(\"\\n\")\n",
    "    console.print(\"[bold blue]Analysis Complete[/bold blue]\\n\")\n",
    "\n",
    "\n",
    "# Run the Graph\n",
    "# Define the input data\n",
    "input_data = {\n",
    "    \"messages\": [HumanMessage(content=\"What is AAPL's current price and latest revenue?\")]\n",
    "}\n",
    "\n",
    "# Define the configuration (e.g., recursion limit)\n",
    "config = {\"recursion_limit\": 10}\n",
    "\n",
    "# Stream the execution\n",
    "stream_agent_execution(graph, input_data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph State: --> Example\n",
    "# What is a LangGraph State?\n",
    "    # A LangGraph state is a data structure that holds the current state of the workflow. It is passed between nodes in the graph, \n",
    "    # and each node can modify the state as needed. The state typically contains all the information required for the workflow to function, \n",
    "    # such as inputs, intermediate results, and outputs.\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional, Annotated\n",
    "import operator\n",
    "from langgraph.graph import Graph, StateGraph, MessageGraph\n",
    "\n",
    " \n",
    "#------------------ Define the State (State.py) -----------------------\n",
    "DEFAULT_EXTRACTION_SCHEMA = {\n",
    "    \"title\": \"CompanyInfo\",\n",
    "    \"description\": \"Basic information about a company\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"company_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Official name of the company\",\n",
    "        },\n",
    "        \"founding_year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Year the company was founded\",\n",
    "        },\n",
    "        \"founder_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"Names of the founding team members\",\n",
    "        },\n",
    "        \"product_description\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Brief description of the company's main product or service\",\n",
    "        },\n",
    "        \"funding_summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Summary of the company's funding history\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"company_name\"],\n",
    "}\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class InputState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    company: str\n",
    "    \"Company to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: Optional[dict[str, Any]] = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class OverallState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    company: str\n",
    "    \"Company to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: str = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "    search_queries: list[str] = field(default=None)\n",
    "    \"List of generated search queries to find relevant information\"\n",
    "\n",
    "    completed_notes: Annotated[list, operator.add] = field(default_factory=list)\n",
    "    \"Notes from completed research related to the schema\"\n",
    "\n",
    "    info: dict[str, Any] = field(default=None)\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "    is_satisfactory: bool = field(default=None)\n",
    "    \"True if all required fields are well populated, False otherwise\"\n",
    "\n",
    "    reflection_steps_taken: int = field(default=0)\n",
    "    \"Number of times the reflection node has been executed\"\n",
    "\n",
    "    \n",
    "@dataclass(kw_only=True)\n",
    "class OutputState:\n",
    "    \"\"\"The response object for the end user.\n",
    "\n",
    "    This class defines the structure of the output that will be provided\n",
    "    to the user after the graph's execution is complete.\n",
    "    \"\"\"\n",
    "\n",
    "    info: dict[str, Any]\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "#------------------ Define the Configuration (Configuration.py) -----------------------\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
    "\n",
    "    max_search_queries: int = 3  # Max search queries per company\n",
    "    max_search_results: int = 3  # Max search results per query\n",
    "    max_reflection_steps: int = 0  # Max reflection steps\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})\n",
    "#-------------------------------------------------------------------------------\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from agent.configuration import Configuration\n",
    "\n",
    "builder = StateGraph(\n",
    "    OverallState,\n",
    "    input=InputState,\n",
    "    output=OutputState,\n",
    "    config_schema=Configuration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG: Directed Acyclic Graph \n",
    "    # Definition : A graph where nodes are connected in a linear, directional manner without forming closed loops .\n",
    "    # Use Case : Used by LangChain to represent workflows where tasks are executed in a non-repeating, linear sequence .\n",
    "        ''' Start → Node A → Node B → Node C → End '''\n",
    "            # No loops : Once a node is processed, it doesn’t revisit previous nodes.\n",
    "            # Linear flow : Tasks are executed in a strict sequence.\n",
    "            \n",
    "            \n",
    "# DCG: Directed Cyclic Graph --> used by LangGraph to represent the workflow of nodes and edges.\n",
    "    # Definition : A graph where nodes are connected in a directional manner and can form loops or cycles .\n",
    "    # Use Case : Used by LangGraph to represent workflows with complex patterns , including loops and conditional branching .\n",
    "        '''\n",
    "        Start → Node A → Node B → Node C\n",
    "                ↑              ↓\n",
    "                └──────────────┘\n",
    "        '''\n",
    "            # Loops allowed : Nodes can revisit previous nodes (e.g., for iterative tasks).\n",
    "            # Complex flow : Supports conditional edges, loops, and dynamic routing.\n",
    "\n",
    "\n",
    "# Edges:\n",
    "    # Simple Edge:\n",
    "        # A direct connection between two nodes in the graph. Used whrn the flow is fixed and uncontitional.\n",
    "        ''' Start → Node A → Node B → Node C → End '''\n",
    "    \n",
    "    # Conditional Edge:\n",
    "        # A connection between two nodes that is determined by a condition or decision function.\n",
    "        '''\n",
    "            Start → Node A\n",
    "                    ↓\n",
    "                ┌─────┴─────┐\n",
    "            Condition 1   Condition 2\n",
    "                ↓             ↓\n",
    "            Node B         Node C\n",
    "                ↓             ↓\n",
    "            Node D         Node E\n",
    "                └─────┬─────┘\n",
    "                    ↓\n",
    "                    End\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, Graph # Import the necessary classes\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Define the state as a Pydantic model\n",
    "class CustomerSupportState(BaseModel):\n",
    "    query: str = Field(..., description=\"The customer's query\")\n",
    "    response: str = Field(None, description=\"The response to the customer\")\n",
    "    issue_type: str = Field(None, description=\"The type of issue (FAQ, Escalation, Recommendation)\")\n",
    "    escalation_required: bool = Field(False, description=\"Whether the issue requires escalation\")\n",
    "    product_recommendation: str = Field(None, description=\"Product recommendation for the customer\")\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(CustomerSupportState)\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE WITHOUT LLM ---------------------------------------------\n",
    "# Node A: Classify the customer's query\n",
    "def classify_query(state: CustomerSupportState) -> dict:\n",
    "    query = state.query.lower()\n",
    "    if \"faq\" in query or \"how to\" in query or \"what is\" in query:\n",
    "        return {\"issue_type\": \"FAQ\"}\n",
    "    elif \"issue\" in query or \"problem\" in query or \"error\" in query:\n",
    "        return {\"issue_type\": \"Escalation\"}\n",
    "    elif \"recommend\" in query or \"suggest\" in query:\n",
    "        return {\"issue_type\": \"Recommendation\"}\n",
    "    else:\n",
    "        return {\"issue_type\": \"Unknown\"}\n",
    "\n",
    "#--------------------------------------------- TOOL NODE ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chrome\",\n",
    "    embedding=embeddings\n",
    "    \n",
    ")\n",
    "retriever=vectorstore.as_retriever()\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.You are a specialized assistant. Use the 'retriever_tool' **only** when the query explicitly relates to LangChain blog data. For all other queries, respond directly without using any tool. For simple queries like 'hi', 'hello', or 'how are you', provide a normal response.\",\n",
    "    )\n",
    "\n",
    "tools=[retriever_tool]\n",
    "retrieve=ToolNode([retriever_tool])\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM 1 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import add_messages\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "def ai_assistant(state:AgentState):\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state['messages']\n",
    "    \n",
    "    if len(messages)>1:\n",
    "        last_message = messages[-1]\n",
    "        question = last_message.content\n",
    "        prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a helpful assistant whatever question has been asked to find out that in the given question and answer.\n",
    "                        Here is the question:{question}\n",
    "                        \"\"\",\n",
    "                        input_variables=[\"question\"]\n",
    "                        )\n",
    "            \n",
    "        chain = prompt | llm\n",
    "    \n",
    "        response=chain.invoke({\"question\": question})\n",
    "        return {\"messages\": [response]}\n",
    "    else:\n",
    "        llm_with_tool = llm.bind_tools(tools)\n",
    "        response = llm_with_tool.invoke(messages)\n",
    "        #response=handle_query(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM 2 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "class grade(BaseModel):\n",
    "    binary_score:str=Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "def grade_documents(state:AgentState)->Literal[\"Output_Generator\", \"Query_Rewriter\"]:\n",
    "    llm_with_structure_op=llm.with_structured_output(grade)\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader deciding if a document is relevant to a user’s question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\n",
    "                    If the document talks about or contains information related to the user’s question, mark it as relevant. \n",
    "                    Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "                    )\n",
    "    chain = prompt | llm_with_structure_op\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generator\" #this should be a node name\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewriter\" #this should be a node name\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 1 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "\n",
    "# Define a REACT-based agent node\n",
    "def react_agent_node(state: CustomerSupportState):\n",
    "    tools = [retriever_tool]  # Add your tool(s) here\n",
    "    prompt_template = \"\"\"You are a reasoning and acting agent.\n",
    "    Use the tools available to gather or verify information as needed.\n",
    "    Respond directly if no tools are required.\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    react_agent = create_react_agent(\n",
    "        tools=tools,\n",
    "        prompt_template=prompt_template,\n",
    "        llm=llm,\n",
    "    )\n",
    "    # Execute the REACT agent\n",
    "    query = state.query\n",
    "    response = react_agent.invoke({\"query\": query})\n",
    "    state.response = response\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 2 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# more advanced node\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.agents import AgentType, initialize_agent\n",
    "\n",
    "def advanced_multi_tool_agent_node(state: CustomerSupportState):\n",
    "    \"\"\"\n",
    "    An advanced agent that uses multiple tools to handle queries.\n",
    "    \"\"\"\n",
    "    tools = [retriever_tool]  # Add more tools as needed\n",
    "\n",
    "    # Define the agent's prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You are an advanced agent with access to multiple tools. Your task is to resolve customer queries by:\n",
    "    1. Identifying the problem or request.\n",
    "    2. Using the tools provided to gather additional information if needed.\n",
    "    3. Synthesizing the information into a clear, concise response.\n",
    "\n",
    "    You can chain tools if required. If you are unsure, respond with 'I need more details.'\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        model=\"deepseek-chat\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    \n",
    "    # Initialize the agent\n",
    "    advanced_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\"prompt_template\": prompt_template},\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    # Execute the agent\n",
    "    query = state.query\n",
    "    try:\n",
    "        response = advanced_agent.invoke({\"query\": query})\n",
    "        state.response = response\n",
    "    except Exception as e:\n",
    "        state.response = f\"Error: {str(e)}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Custom Agent --------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.agents import BaseAgent\n",
    "from typing import Optional\n",
    "\n",
    "class AdvancedCustomAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Custom advanced agent with LLM, human-in-the-loop, and iterative reasoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, tools=None, max_iterations: int = 3):\n",
    "        self.llm = llm\n",
    "        self.tools = tools or []\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    async def run(self, query: str, human_review: bool = False, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Executes the custom agent's workflow.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query.\n",
    "            human_review (bool): If True, adds human-in-the-loop for review.\n",
    "        \n",
    "        Returns:\n",
    "            str: Final response.\n",
    "        \"\"\"\n",
    "        response = \"\"\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"--- Iteration {iteration}/{self.max_iterations} ---\")\n",
    "\n",
    "            # Generate a response using LLM\n",
    "            prompt = f\"\"\"\n",
    "            You are an advanced customer support agent. Use the tools provided to solve the query. \n",
    "            Tools: {', '.join([tool.name for tool in self.tools]) if self.tools else 'None'}\n",
    "\n",
    "            Query: {query}\n",
    "\n",
    "            If you need clarification or further details, request them from the user.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = await self.llm.apredict(prompt)\n",
    "                print(f\"Generated Response: {response}\")\n",
    "\n",
    "                # Check if human review is required\n",
    "                if human_review:\n",
    "                    review = input(\"Do you approve this response? (yes/no): \")\n",
    "                    if review.lower() == \"yes\":\n",
    "                        break\n",
    "                    else:\n",
    "                        query = input(\"Provide additional details or corrections: \")\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                response = f\"Error: {str(e)}\"\n",
    "                break\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# Define the custom agent node\n",
    "async def custom_agent_node(state: CustomerSupportState):\n",
    "    \"\"\"\n",
    "    Node with a custom advanced agent that uses LLM and human-in-the-loop.\n",
    "    \"\"\"\n",
    "    custom_agent = AdvancedCustomAgent(llm=llm, tools=[retriever_tool], max_iterations=3)\n",
    "    query = state.query\n",
    "\n",
    "    # Human-in-the-loop enabled for critical queries\n",
    "    response = await custom_agent.run(query, human_review=True)\n",
    "    state.response = response\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- LangGraph Workflow ---------------------------------------------   \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"Classify Query\", classify_query)\n",
    "workflow.add_node(\"End Conversation\", end_conversation)\n",
    "\n",
    "# Define edges between nodes\n",
    "workflow.add_edge(START, \"Classify Query\")\n",
    "workflow.add_edge(\"Classify Query\", \"Answer FAQ\")\n",
    "workflow.add_edge(\"Recommend Products\", \"End Conversation\")\n",
    "workflow.add_edge(\"End Conversation\", END)\n",
    "\n",
    "# Set entry and finish points\n",
    "workflow.set_entry_point(\"Classify Query\")  # Start the conversation. Use this only when START is not used.\n",
    "workflow.set_finish_point(\"End Conversation\")   # End the conversation. Use this only when END is not used.\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test the workflow with a sample query\n",
    "initial_state = CustomerSupportState(query=\"which do you recommend between product A and product B?\")\n",
    "result = app.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualize LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tNode_A(Node A)\n",
      "\tNode_B(Node B)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\tNode_A --> Node_B;\n",
      "\tNode_B --> __end__;\n",
      "\t__start__ --> Node_A;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "print(app.get_graph().draw_mermaid())       # Converting a Graph to a Mermaid Diagram\n",
    "\n",
    "\n",
    "#-------------------------Using Mermaid.Ink--------------------------------\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Using Mermaid + Pyppeteer--------------------------------\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            curve_style=CurveStyle.LINEAR,\n",
    "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
    "            wrap_label_n_words=9,\n",
    "            output_file_path=None,\n",
    "            draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "            background_color=\"white\",\n",
    "            padding=10,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Using Graphviz--------------------------------\n",
    "%pip install pygraphviz\n",
    "\n",
    "display(Image(app.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the langgraph.json file (see example)\n",
    "{\n",
    "    \"dockerfile_lines\": [],\n",
    "    \"graphs\": {\n",
    "        \"chat\": \"./backend/graph.py:graph\",\n",
    "        \"researcher\": \"./backend/graph.py:researcher\",\n",
    "        \"agent\": \"./backend/graph.py:agent\"\n",
    "    },\n",
    "    \"env\": [\n",
    "        \"OPENAI_API_KEY\",\n",
    "        \"WEAVIATE_API_KEY\",\n",
    "        \"WEAVIATE_URL\",\n",
    "        \"ANTHROPIC_API_KEY\",\n",
    "        \"ELASTIC_API_KEY\"\n",
    "    ],\n",
    "    \"python_version\": \"3.11\",\n",
    "    \"dependencies\": [\n",
    "        \".\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "!pip install \"langgraph-cli[inmem]==0.1.55\" # Install the langgraph-cli package\n",
    "\n",
    "# move to the directory containing the langgraph.json file\n",
    "\n",
    "langgraph dev # start a local development server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langsmith import Client, traceable, wrappers\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"my-project\" \n",
    "\n",
    "\n",
    "client = Client()\n",
    "# openai_client = wrappers.wrap_openai(openai.Client())\n",
    "\n",
    "#-------------------------------- Create Dataset --------------------------------\n",
    "# Create dataset for testing our AI agents\n",
    "dataset_input = [\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\"},\n",
    "    {\"input\": \"Who wrote the book '1984'?\",  \"output\": \"George Orwell\"},\n",
    "    {\"input\": \"What is the square root of 16?\",  \"output\": \"4\"},\n",
    "]\n",
    "\n",
    "dataset_name = \"my-dataset\"\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name = dataset_name, \n",
    "    description=\"A dataset for testing AI agents.\")\n",
    "\n",
    "for data in dataset_input:\n",
    "    try:\n",
    "        client.create_example(\n",
    "            inputs={\"question\": data['input']},  # Wrapping the input into a dictionary\n",
    "            outputs={\"answer\": data['output']},  # Wrapping the output into a dictionary\n",
    "            dataset_id=dataset.id  # Assuming dataset.id is already created\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create example for input: {data['input']}, Error: {e}\")\n",
    "    \n",
    "\n",
    "#-------------------------------- Create Targets --------------------------------\n",
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs: dict) -> dict:\n",
    "  response = openai_client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },\n",
    "          { \"role\": \"user\", \"content\": inputs[\"question\"] },\n",
    "      ],\n",
    "  )\n",
    "  return { \"response\": response.choices[0].message.content.strip() }\n",
    "\n",
    "\n",
    "#-------------------------------- Define the Evaluator --------------------------------\n",
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "  score: bool = Field(\n",
    "      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"\n",
    "  )\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "  response = openai_client.beta.chat.completions.parse(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "          { \"role\": \"system\", \"content\": instructions },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]}; \n",
    "              Student's Answer: {outputs[\"response\"]}\"\"\"\n",
    "          },\n",
    "      ],\n",
    "      response_format=Grade,\n",
    "  )\n",
    "  return response.choices[0].message.parsed.score\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- Run and View Results --------------------------------\n",
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "  target,\n",
    "  data=\"Sample dataset\",\n",
    "  evaluators=[\n",
    "      accuracy,\n",
    "      # can add multiple evaluators here\n",
    "  ],\n",
    "  experiment_prefix=\"first-eval-in-langsmith\",\n",
    "  max_concurrency=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `interrupt` function instead.\n",
    "\n",
    "#------------------------- Basic Human-in-the-Loop with Breakpoints--------------------------------\n",
    "# Compile graph with breakpoint\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory, \n",
    "    interrupt_before=[\"step_for_human_in_the_loop\"] # Add breakpoint\n",
    ")\n",
    "\n",
    "# Run graph up to breakpoint\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Perform human action (e.g., approve, edit, input)\n",
    "# Resume graph execution\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "\n",
    "#------------------------- Dynamic Breakpoints--------------------------------\n",
    "# can define some *condition* that must be met for a breakpoint to be triggered\n",
    "\n",
    "# Define a node with dynamic breakpoint\n",
    "def my_node(state: State) -> State:\n",
    "    if len(state['input']) > 5:  # Condition for breakpoint\n",
    "        raise NodeInterrupt(f\"Input too long: {state['input']}\")\n",
    "    return state\n",
    "\n",
    "# Resume after dynamic breakpoint\n",
    "graph.update_state(config=thread_config, values={\"input\": \"foo\"})  # Update state to pass condition\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Skip node entirely\n",
    "graph.update_state(config=thread_config, values=None, as_node=\"my_node\")  # Skip node\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    \n",
    "\n",
    "#------------------------- Input Pattern or Tool Call--------------------------------\n",
    "    \n",
    " # Compile graph with input breakpoint\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer, \n",
    "    interrupt_before=[\"human_input\"]  # Node for human input\n",
    ")\n",
    "\n",
    "# Run graph up to input breakpoint\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Add human input and resume\n",
    "graph.update_state(\n",
    "    thread_config, \n",
    "    {\"user_input\": \"human input\"},  # Provide human input or tool call\n",
    "    as_node=\"human_input\"  # Treat update as node\n",
    ")\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)   \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Agent Systems\n",
    "    # Definition : A system where multiple agents interact to achieve a common goal.\n",
    "    # Use Case : Used by LangChain to model complex workflows involving multiple agents with different capabilities.\n",
    "    # Example : A multi-agent system for customer support, where agents handle different types of customer queries (FAQs, escalations, recommendations).\n",
    "    \n",
    "    # Agent Supervisor Node : Routes queries to specific agents based on the query type.\n",
    "    # Hierarchical Agent Teams : Organizes agents into teams based on their expertise or function.\n",
    "    # Multi-Agent Collaboration : Enables agents to share information and coordinate tasks to achieve a common goal.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agent Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Supervisor Node\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "\n",
    "'''\n",
    "User\n",
    " ├── Supervisor\n",
    " │     ├── [direct] Agent 1\n",
    " │     ├── [conditional] Agent 2 (if condition A is met)\n",
    " │     └── [direct] Agent 3\n",
    " │           ├── [conditional] Sub-Agent 3.1 (if condition B is met)\n",
    " │           └── [direct] Sub-Agent 3.2\n",
    " │\n",
    " └── Feedback Loop (User <--> Supervisor)\n",
    "\n",
    "'''\n",
    "members = [\"researcher\", \"coder\"]\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        f\" following workers: {members}. Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    class Router(TypedDict):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options]\n",
    "\n",
    "    def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto)\n",
    "\n",
    "    return supervisor_node\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
    "\n",
    "def research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",  # Return to supervisor node\n",
    "    )\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "graph = builder.compile()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
