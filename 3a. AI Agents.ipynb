{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "AI Agent Architectures\n",
    "├── Profiling Module or Perception Module (The Eyes and Ears of the Agent)\n",
    "│   ├── Sensory expertise\n",
    "│   ├── Perceives and interprets the environment and communicated with other agents.\n",
    "│   ├──  --> the agent may collect and analyze information from its environment like how human senses work.\n",
    "│   ├──  --> helps it comprehend visual signals, recognize speech patterns, and sense tactile inputs..\n",
    "│   └── Example: Recognizing objects via sensors in self-driving cars\n",
    "│\n",
    "├── Memory Module\n",
    "│   ├── Stores data, rules, and patterns\n",
    "│   ├── Enables knowledge recall and decision-making\n",
    "│   └── Example: Chatbots recalling customer preferences\n",
    "│   \n",
    "├── Planning Module\n",
    "│   ├── Analyzes current situations\n",
    "│   ├── Strategizes actions to meet goals\n",
    "│   └── Example: Optimizing delivery routes\n",
    "│\n",
    "├── Action Module\n",
    "│   ├── Executes planned actions\n",
    "│   ├── Interfaces with external systems\n",
    "│   └── Example: Robotic arms assembling parts\n",
    "│\n",
    "├── Learning Module\n",
    "│   ├── Adapts and improves performance\n",
    "│   ├── Methods include:\n",
    "│   │   ├── Supervised Learning\n",
    "│   │   ├── Unsupervised Learning\n",
    "│   │   └── Reinforcement Learning\n",
    "│   └── Example: Agents learning optimal decisions from feedback\n",
    "│\n",
    "├── Data Structuring and Transformation Module\n",
    "│   ├── Organizes and preprocesses data both from the environment as well as from the memory module\n",
    "│   ├── Converts data into trainable formats\n",
    "│   └── Example: Formatting images for neural network training\n",
    "│\n",
    "├── Training Module\n",
    "│   ├── Performs training operations and updates\n",
    "│   ├── Use methods like:\n",
    "│   │   ├── Supervised, Unsupervised, and Reinforcement Learning\n",
    "│   │   ├── Computer Vision, LLM, Time series Learning\n",
    "│   │   ├── ANN, CNN, Transformers, GANs, GNNs, \n",
    "│   │   └── Tools like TensorFlow, PyTorch, Keras, Scikit-learn\n",
    "│   └── Example: AI training itself in virtual environments\n",
    "│\n",
    "└── Other Modules\n",
    "\n",
    " \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "langchain_community\n",
    "tiktoken\n",
    "langchainhub\n",
    "langchain\n",
    "chromadb\n",
    "langgraph\n",
    "tavily-python\n",
    "python-dotenv\n",
    "google-generativeai\n",
    "langchain_google_genai\n",
    "langchain-nomic\n",
    "langchain-text-splitters\n",
    "langchain_mistralai\n",
    "wikipedia\n",
    "langchain_huggingface\n",
    "google-search-results\n",
    "faiss-cpu\n",
    "sentence-transformers\n",
    "youtube-search\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AI Agents Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG (Retrieval Augmented Generation) \n",
    "    # LangChain: \n",
    "    # RAGFlow by infiniflow: https://github.com/infiniflow/ragflow\n",
    "    # Haystack by deepset-ai: https://github.com/deepset-ai/haystack\n",
    "    # txtai by neuml: https://github.com/neuml/txtai\n",
    "    # LLM-App by pathwaycom: https://github.com/pathwaycom/llm-app \n",
    "\n",
    "# Intelligent Document Processing\n",
    "    # LangChain\n",
    "    # docling: https://github.com/docling-project/docling\n",
    "    # marker: https://github.com/VikParuchuri/marker\n",
    "    # unstructured: https://github.com/Unstructured-IO/unstructured\n",
    "    # landingAI document extractor: https://github.com/landing-ai/agentic-doc\n",
    "    # Haystack by deepset-ai: https://github.com/deepset-ai/haystack\n",
    "    # Microsoft:\n",
    "        # Markitdown: https://github.com/microsoft/markitdown\n",
    "        # Markitdown mcp server: https://github.com/microsoft/markitdown/tree/main/packages/markitdown-mcp\n",
    "        # Azure Document Intelligence: https://learn.microsoft.com/en-us/azure/ai-services/document-intelligence/how-to-guides/create-document-intelligence-resource?view=doc-intel-4.0.0\n",
    "        \n",
    "# LLM Chatbots (Langchain and LangGraph with AI Agents)\n",
    "   [Good]# onyx: https://github.com/onyx-dot-app/onyx\n",
    "   # OpenWebUI:\n",
    "   # Open MCP Client: https://github.com/CopilotKit/open-mcp-client\n",
    "   # LangGraph Agent Chat: https://github.com/langchain-ai/agent-chat-ui\n",
    "   # dify: https://github.com/langgenius/dify\n",
    "   # anything-llm: https://github.com/Mintplex-Labs/anything-llm\n",
    "\n",
    "# AI-Powered Knowledge Bases\n",
    "\n",
    "\n",
    "# Customer Service Automation \n",
    "\n",
    "\n",
    "# AI Agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> n8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n8n interface\n",
    "\n",
    "# n8n is a workflow automation tool that enables you to connect your favorite apps, services, and devices.\n",
    "# It allows you to automate workflows and integrate your apps, services, and devices with each other.\n",
    "\n",
    "    # workflow: a sequence of connected steps that automate a process.\n",
    "    # node: a single step in a workflow.\n",
    "    # connection: a link between two nodes that passes data from one node to another.\n",
    "    # execution: a single run of a workflow.\n",
    "\n",
    "# Types of Nodes\n",
    "    # Trigger Node: The starting point of a workflow. It initiates the execution of a workflow.\n",
    "    # Regular Node: A node that performs a specific action or operation.\n",
    "    # Parameter Node: A node that stores and provides data to other nodes in the workflow.\n",
    "    # Sub-Workflow Node: A node that allows you to reuse a workflow within another workflow.\n",
    "    # Webhook Node: A node that receives data from an external service or application.\n",
    "    # Error Node: A node that handles errors that occur during the execution of a workflow.\n",
    "    # No-Operation Node: A node that does nothing. It is used for debugging and testing purposes.\n",
    "    \n",
    "    # OR\n",
    "    # Trigger Nodes: These nodes initiate the execution of a workflow. They are the starting points of a workflow.\n",
    "    # Data Transformation Nodes: These nodes perform operations on data. They transform, filter, or manipulate data in some way.\n",
    "    # Action Nodes: These nodes perform actions such as sending an email, making an API call, or updating a database.\n",
    "    # Logic Nodes: These nodes control the flow of a workflow. They make decisions based on conditions and determine the path a workflow should take.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "from langchain.agents import tool\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "print(f'Length of the word '{get_word_length.invoke(\"hello\")})\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "tools = [tool_1, tool_2, tool_3]\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(text: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(text)\n",
    "\n",
    "print(get_word_length.invoke(\"hello\"))\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Web Answer\",\n",
    "        func = google_search.run,\n",
    "        description=\"Get an intermediate answer to a question.\",\n",
    "        verbose = True\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "#-----------------------------------------Custom Tool from a LangChain Chain ----------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "@tool(\"query_param_checker\")\n",
    "def query_param_checker(user_query: str, generated_query_params: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool checks if the query parameters generated by query_param_generator are valid.\n",
    "    It uses an LLM to evaluate the parameters based on a simple prompt.\n",
    "    \"\"\"\n",
    "    # Define a simple prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"Check if the generated query parameters are valid for the user query.\"),\n",
    "            (\"human\", \"User Query: {user_query}\\nGenerated Query Parameters: {generated_query_params}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a chain with the prompt and LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Invoke the chain with the inputs\n",
    "    result = chain.invoke({\"user_query\": user_query, \"generated_query_params\": generated_query_params})\n",
    "\n",
    "    # Return the LLM's response\n",
    "    return result.content\n",
    "\n",
    "\n",
    "#-----------------------------Custom Tool from Class (RECOMMENDED 1) ------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define Input Schema\n",
    "class SearchToolInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The search query to look up.\")\n",
    "    max_results: Optional[int] = Field(default=10, description=\"The maximum number of search results to return.\")\n",
    "\n",
    "# Define the Tool\n",
    "class TavilySearchTool:\n",
    "    def __init__(self, max_results: int = 10):\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def search(self, query: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Perform a web search using the Tavily search engine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize the Tavily search tool with the configured max_results\n",
    "            search_tool = TavilySearchResults(max_results=self.max_results)\n",
    "\n",
    "            # Perform the search\n",
    "            result = search_tool.invoke({\"query\": query})\n",
    "\n",
    "            # Return the search results\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Create the LangChain Tool\n",
    "search_tool = Tool(\n",
    "    name=\"Tavily Search\",\n",
    "    func=TavilySearchTool().search,\n",
    "    description=\"Performs web searches using the Tavily search engine, providing accurate and trusted results for general queries.\",\n",
    "    args_schema=SearchToolInput\n",
    ")\n",
    "\n",
    "\n",
    "#-----------------------------Structured Tool from Class (BEST AND MOST RECOMMENDED) ------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "from langchain.tools.base import StructuredTool\n",
    "\n",
    "# Define Input Schema\n",
    "class SearchToolInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The search query to look up.\")\n",
    "    max_results: Optional[int] = Field(default=10, description=\"The maximum number of search results to return.\")\n",
    "\n",
    "# Define the Tool\n",
    "class TavilySearchTool:\n",
    "    def __init__(self, max_results: int = 10):\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def search(self, query: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Perform a web search using the Tavily search engine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize the Tavily search tool with the configured max_results\n",
    "            search_tool = TavilySearchResults(max_results=self.max_results)\n",
    "\n",
    "            # Perform the search\n",
    "            result = search_tool.invoke({\"query\": query})\n",
    "\n",
    "            # Return the search results\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Create the LangChain Tool\n",
    "search_tool = StructuredTool(\n",
    "    name=\"Tavily Search\",\n",
    "    func=TavilySearchTool().search,\n",
    "    description=\"Performs web searches using the Tavily search engine, providing accurate and trusted results for general queries.\",\n",
    "    args_schema=SearchToolInput\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------ Convert Tool to Structured Tool ------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "from langchain.tools.base import StructuredTool\n",
    "from langchain.agents import Tool, load_tools\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def convert_to_structured_tool(tool):\n",
    "    return StructuredTool.from_function(tool.func, name=tool.name, description=tool.description)\n",
    "\n",
    "tools = load_tools(['serpapi'])\n",
    "tools = [convert_to_structured_tool(tool) for tool in tools]\n",
    "\n",
    "\n",
    "#---------------------------------- Custom Tool (RECOMMENDED 2) -------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Union\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Define Input Schema: Use Pydantic to define input parameters and descriptions for your tool.\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str = Field(..., description=\"Description of param1.\")\n",
    "    param2: int = Field(default=10, description=\"Description of param2.\")\n",
    "    \n",
    "# Create the Tool: Use the @tool decorator to define a custom tool.\n",
    "@tool(\"my_tool_function\", args_schema=MyToolInput, return_direct=True)\n",
    "def my_tool_function(param1: str, param2: int = 10) -> Union[Dict, str]:\n",
    "    \"\"\"\n",
    "    Description of what the tool does.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = (\n",
    "            f'https://api.financialdatasets.ai/insider-transactions'\n",
    "            f'?ticker={param1}'\n",
    "            f'&limit={param2}'\n",
    "            )\n",
    "        # Perform the task (e.g., call an API, process data, etc.)\n",
    "        response = requests.get(url, headers={'X-API-Key': api_key})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "tools = [my_tool_function, annual_report_tool, get_word_length]\n",
    "\n",
    "\n",
    "#-----------------------------Custom Tool from a Custom Chain----------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "class AnnualReportChain(Chain):\n",
    "    chain: Chain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return list(self.chain.input_keys)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        # Queries the database to get the relevant documents for a given query\n",
    "        query = inputs.get(\"input_documents\", \"\")\n",
    "        docs = vectorstore.similarity_search(query, include_metadata=True)\n",
    "        output = chain.run(input_documents=docs, question=query)\n",
    "        return {'output': output}\n",
    "    \n",
    "    \n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize your custom Chain\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "chain = load_qa_chain(llm)\n",
    "annual_report_chain = AnnualReportChain(chain=chain)\n",
    "\n",
    "# Initialize your custom Tool\n",
    "annual_report_tool = Tool(\n",
    "    name=\"Annual Report\",\n",
    "    func=annual_report_chain.run,\n",
    "    description=\"\"\"\n",
    "    useful for when you need to answer questions about a company's income statement,\n",
    "    cash flow statement, or balance sheet. This tool can help you extract data points like\n",
    "    net income, revenue, free cash flow, and total debt, among other financial line items.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------- Creating a Node from Tools ----------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "builder = StateGraph(State)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)\n",
    "\n",
    "\n",
    "# ----------------------------------------- Vision Agent Tool -------------------------------------\n",
    "# -----------------------------------------------------------------------------------------------\n",
    "from vision_agent.agent import VisionAgentCoderV2\n",
    "from vision_agent.models import AgentMessage\n",
    "\n",
    "agent = VisionAgentCoderV2(verbose=True)\n",
    "code_context = agent.generate_code(\n",
    "    [\n",
    "        AgentMessage(\n",
    "            role=\"user\",\n",
    "            content=\"Count the number of people in this image\",\n",
    "            media=[\"people.png\"]\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "with open(\"generated_code.py\", \"w\") as f:\n",
    "    f.write(code_context.code + \"\\n\" + code_context.test)\n",
    "    \n",
    "# --------------Using the tools directly in the code---------------------\n",
    "import vision_agent.tools as T\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = T.load_image(\"people.png\")\n",
    "dets = T.countgd_object_detection(\"person\", image)\n",
    "# visualize the countgd bounding boxes on the image\n",
    "viz = T.overlay_bounding_boxes(image, dets)\n",
    "\n",
    "# save the visualization to a file\n",
    "T.save_image(viz, \"people_detected.png\")\n",
    "\n",
    "# display the visualization\n",
    "plt.imshow(viz)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# ------------------------------Using the tools on video files---------------------\n",
    "import vision_agent.tools as T\n",
    "\n",
    "frames_and_ts = T.extract_frames_and_timestamps(\"people.mp4\")\n",
    "# extract the frames from the frames_and_ts list\n",
    "frames = [f[\"frame\"] for f in frames_and_ts]\n",
    "\n",
    "# run the countgd tracking on the frames\n",
    "tracks = T.countgd_sam2_video_tracking(\"person\", frames)\n",
    "# visualize the countgd tracking results on the frames and save the video\n",
    "viz = T.overlay_segmentation_masks(frames, tracks)\n",
    "T.save_video(viz, \"people_detected.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain, LangGraph, and LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain \n",
    "    # Tools\n",
    "    # Agents\n",
    "    # Chains\n",
    "    # Multi-Agent Systems\n",
    "    # Plan and Execute\n",
    "    # Reflection and Learning\n",
    "    # Communication\n",
    "    # Perception\n",
    "\n",
    "# LangChain is a platform that enables developers to build, test, and deploy blockchain applications using multiple programming languages.\n",
    "# It provides a set of tools and libraries that simplify the development process and make it easier to create blockchain applications.\n",
    "\n",
    "\n",
    "\n",
    "# Types of LangChain Agents\n",
    "    # LangChain offers several agentic patterns, each tailored to specific needs. These include:\n",
    "\n",
    "    # Tool Calling Agents: Designed for straightforward tool usage.\n",
    "    # React Agents: Use reasoning and action mechanisms to dynamically decide the best steps.\n",
    "    # Structured Chat Agents: Parse inputs and outputs into structured formats like JSON.\n",
    "    # Self-Ask with Search: Handle queries by splitting them into smaller, manageable steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Tool Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents.tool_calling_agent import base\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI as LangchainChatDeepSeek\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults, TavilyAnswer\n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "import os\n",
    "\n",
    "# Load API key\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# or\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        # First put the history\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        # Then the new input\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Finally the scratchpad\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tools\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "tool_3 = TavilySearchResults(max_results=10)\n",
    "\n",
    "tools = [tool_1, tool_2, tool_3]\n",
    "\n",
    "# LLM\n",
    "llm = LangchainChatDeepSeek(\n",
    "            api_key=api_key,\n",
    "            model=\"deepseek-chat\",\n",
    "            base_url=\"https://api.deepseek.com\",\n",
    "        )\n",
    "\n",
    "# Agent\n",
    "\n",
    "# Create a tool-calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "# agent = base.create_tool_calling_agent()\n",
    "\n",
    "# Agent Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,  # Only final output. If True, returns all intermediate steps\n",
    "    handle_parsing_errors=True,  # Graceful parsing errors\n",
    ")\n",
    "        \n",
    "\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": [HumanMessage(content=query)]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The ReActAgent employs the ReAct (Reason+Act) framework, enabling the agent to perform both reasoning and actions within a \n",
    "# single framework. It integrates chain-of-thought reasoning with action execution, allowing the agent to handle complex, \n",
    "# multi-step tasks effectively.\n",
    "\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "from typing import Annotated\n",
    "\n",
    "template = '''Answer the following questions as best as you can. You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation sequence can be repeated N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Here is an example of how to use the tools:\n",
    "Question: Generate a chart of the Fibonacci sequence.\n",
    "Thought: I need to write Python code to generate the Fibonacci sequence and plot it.\n",
    "Action: python_repl_tool\n",
    "Action Input: \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fibonacci(n):\n",
    "    fib_sequence = [0, 1]\n",
    "    for i in range(2, n):\n",
    "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
    "    return fib_sequence\n",
    "\n",
    "n = 10  # Number of Fibonacci numbers to generate\n",
    "fib_sequence = fibonacci(n)\n",
    "plt.plot(fib_sequence)\n",
    "plt.title(\"Fibonacci Sequence\")\n",
    "plt.show()\n",
    "```\n",
    "Observation: The chart was successfully generated.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: The chart of the Fibonacci sequence has been generated.\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return result_str\n",
    "    \n",
    "prompt = PromptTemplate.from_template(template)\n",
    "search_agent = create_react_agent(llm, tools = [python_repl_tool], prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=search_agent, tools=[python_repl_tool], verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"create a visualization of some advanced dataset.\"})\n",
    "# agent_executor.invoke({\"input\": [HumanMessage(content=\"What is the capital of France?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "LangGraph's prebuilt create_react_agent does not take a prompt template directly as a parameter, but instead takes a prompt parameter. \n",
    "This modifies the graph state before the llm is called, and can be one of four values:\n",
    "\n",
    "    1. A SystemMessage, which is added to the beginning of the list of messages.\n",
    "    2. A string, which is converted to a SystemMessage and added to the beginning of the list of messages.\n",
    "    3. A Callable, which should take in full graph state. The output is then passed to the language model.\n",
    "    4. Or a Runnable, which should take in full graph state. The output is then passed to the language model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, AnyMessage, HumanMessage, AgentMessage, AgentMessageWithScratchpad\n",
    "\n",
    "# ----------------------------------------System Message-------------------\n",
    "system_message = \"You are a helpful assistant. Respond only in Spanish.\"\n",
    "# This could also be a SystemMessage object\n",
    "# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(model, tools, prompt=system_message)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------ChatPromptTemplate-------------------\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are a helpful assistant.\"),\n",
    "#         # First put the history\n",
    "#         (\"placeholder\", \"{chat_history}\"),\n",
    "#         # Then the new input\n",
    "#         (\"human\", \"{input}\"),\n",
    "#         # Finally the scratchpad\n",
    "#         (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "react_agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are an advanced AI assistant designed to solve complex tasks using a systematic, step-by-step approach. \n",
    "\n",
    "CORE AGENT INSTRUCTIONS:\n",
    "1. ALWAYS follow the React (Reasoning and Acting) paradigm\n",
    "2. For EACH task, you must:\n",
    "   a) REASON about the problem\n",
    "   b) DETERMINE which TOOL to use\n",
    "   c) Take ACTION using the selected tool\n",
    "   d) OBSERVE the results\n",
    "   e) REFLECT and decide next steps\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "{tools}\n",
    "\n",
    "TOOL USAGE PROTOCOL:\n",
    "- You have access to the following tools: {tool_names}\n",
    "- BEFORE using any tool, EXPLICITLY state:\n",
    "  1. WHY you are using this tool\n",
    "  2. WHAT specific information you hope to retrieve\n",
    "  3. HOW this information will help solve the task\n",
    "\n",
    "TOOL INTERACTION FORMAT:\n",
    "When using a tool, you MUST follow this strict format:\n",
    "Thought: [Your reasoning for using the tool]\n",
    "Action: [Exact tool name]\n",
    "Action Input: [Precise input for the tool]\n",
    "\n",
    "After receiving the observation, you will:\n",
    "Observation: [Tool's response]\n",
    "Reflection: [Analysis of the observation and next steps]\n",
    "\n",
    "FINAL OUTPUT EXPECTATIONS:\n",
    "- Provide a comprehensive, step-by-step solution\n",
    "- Cite sources and tools used\n",
    "- Explain your reasoning at each stage\n",
    "- Offer clear conclusions or recommendations\n",
    "\n",
    "Are you ready to solve the task systematically and intelligently?\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"human\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Partial the prompt with tools and tool names\n",
    "prompt = react_agent_prompt.partial(\n",
    "    tools=\"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools]),\n",
    "    tool_names=\", \".join([tool.name for tool in tools])\n",
    ")\n",
    "\n",
    "# Create the React agent\n",
    "agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "query = \"Calculate the total cost of 15 items priced at $24.50 each, including a 7% sales tax\"\n",
    "\n",
    "messages = agent.invoke({\"messages\": [(\"human\", query)]})\n",
    "\n",
    "\n",
    "# -------------------------------------- STREAM MODE --------------------------------------\n",
    "# Create the React agent\n",
    "langgraph_agent_executor = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "for step in langgraph_agent_executor.stream(\n",
    "    {\"messages\": [(\"human\", query)]}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(step)\n",
    "    \n",
    "    \n",
    "# --------------------------- FOR mAX ITERATION, USE RECURSION LIMIT ---------------------------\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "RECURSION_LIMIT = 2 * 3 + 1\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(model, tools=tools)\n",
    "\n",
    "try:\n",
    "    for chunk in langgraph_agent_executor.stream(\n",
    "        {\"messages\": [(\"human\", query)]},\n",
    "        {\"recursion_limit\": RECURSION_LIMIT},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        print(chunk[\"messages\"][-1])\n",
    "except GraphRecursionError:\n",
    "    print({\"input\": query, \"output\": \"Agent stopped due to max iterations.\"})\n",
    "    \n",
    "    \n",
    "\n",
    "# --------------------------- FOR early_stopping_method ---------------------------\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "RECURSION_LIMIT = 2 * 1 + 1\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(model, tools=tools)\n",
    "\n",
    "try:\n",
    "    for chunk in langgraph_agent_executor.stream(\n",
    "        {\"messages\": [(\"human\", query)]},\n",
    "        {\"recursion_limit\": RECURSION_LIMIT},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        print(chunk[\"messages\"][-1])\n",
    "except GraphRecursionError:\n",
    "    print({\"input\": query, \"output\": \"Agent stopped due to max iterations.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Self Ask with Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This agent incorporates a self-asking mechanism combined with search capabilities. It can autonomously formulate internal queries \n",
    "# to gather additional information necessary to answer user questions comprehensively\n",
    "\n",
    "from langchain.agents import create_self_ask_with_search_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Web Answer\",\n",
    "        func = google_search.run,\n",
    "        description=\"Get an intermediate answer to a question.\",\n",
    "        verbose = True\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "search_agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=search_agent, tools=tools, verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n",
    "# agent_executor.invoke({\"input\": [HumanMessage(content=\"What is the capital of France?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> StructuredChatAgent (can use multiple inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param format_instructions: str = 'Use a json blob to specify a tool by providing an action key (tool name) and an action_input \n",
    "# key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\n\\nProvide only ONE action per $JSON_BLOB, \n",
    "# as shown:\n",
    "# \\n\n",
    "# \\n```\n",
    "# \\n{{{{\n",
    "    # \\n  \"action\": $TOOL_NAME,\n",
    "    # \\n  \"action_input\": $INPUT\n",
    "    # \\n}}}}\n",
    "    # \\n```\n",
    "# \n",
    "# \\n\\nFollow this format:\n",
    "# \\n\\nQuestion: input question to answer\n",
    "# \\nThought: consider previous and subsequent steps\n",
    "# \\nAction:\\n```\n",
    "# \\n$JSON_BLOB\\n```\n",
    "# \\nObservation: action result\n",
    "# \\n... (repeat Thought/Action/Observation N times)\n",
    "# \\nThought: I know what to respond\\nAction:\n",
    "# \\n```\n",
    "# \\n{{{{\n",
    "    # \\n  \"action\": \"Final Answer\",\n",
    "    # \\n  \"action_input\": \"Final response to human\"\n",
    "    # \\n}}}}\n",
    "    # \\n```'\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that uses tools to answer the user's queries. Always respond with a JSON object that specifies the \n",
    "action to take. Use the following format:\n",
    "\n",
    "{\n",
    "    \"action\": \"ToolName\",\n",
    "    \"action_input\": \"Input for the tool\"\n",
    "}\n",
    "\n",
    "If the answer can be provided without using any tools, use the following format:\n",
    "{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"Your final answer to the user.\"\n",
    "}\n",
    "\n",
    "Available tools: {tools}\n",
    "Begin!\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "User: {input}\n",
    "Chat History: {agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define the tools the agent can use\n",
    "tools = [tool_1, tool_2]\n",
    "\n",
    "# Create the StructuredChatAgent\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    stop_sequence=True  # Ensures the agent stops at the defined stop token\n",
    ")\n",
    "\n",
    "# Create the AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Define the input\n",
    "input_data = {\n",
    "    \"input\": \"What is 15 multiplied by 7?\"\n",
    "}\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent_executor.invoke(input_data)\n",
    "\n",
    "# Print the structured output\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain BigTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langgraph-bigtool\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "\n",
    "import math\n",
    "import types\n",
    "import uuid\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.embeddings import init_embeddings\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "from langgraph_bigtool import create_agent\n",
    "from langgraph_bigtool.utils import (\n",
    "    convert_positional_only_function_to_tool\n",
    ")\n",
    "\n",
    "# Collect functions from `math` built-in\n",
    "all_tools = []\n",
    "for function_name in dir(math):\n",
    "    function = getattr(math, function_name)\n",
    "    if not isinstance(\n",
    "        function, types.BuiltinFunctionType\n",
    "    ):\n",
    "        continue\n",
    "    # This is an idiosyncrasy of the `math` library\n",
    "    if tool := convert_positional_only_function_to_tool(\n",
    "        function\n",
    "    ):\n",
    "        all_tools.append(tool)\n",
    "\n",
    "# Create registry of tools. This is a dict mapping\n",
    "# identifiers to tool instances.\n",
    "tool_registry = {\n",
    "    str(uuid.uuid4()): tool\n",
    "    for tool in all_tools\n",
    "}\n",
    "\n",
    "# Index tool names and descriptions in the LangGraph\n",
    "# Store. Here we use a simple in-memory store.\n",
    "embeddings = init_embeddings(\"openai:text-embedding-3-small\")\n",
    "\n",
    "store = InMemoryStore(\n",
    "    index={\n",
    "        \"embed\": embeddings,\n",
    "        \"dims\": 1536,\n",
    "        \"fields\": [\"description\"],\n",
    "    }\n",
    ")\n",
    "for tool_id, tool in tool_registry.items():\n",
    "    store.put(\n",
    "        (\"tools\",),\n",
    "        tool_id,\n",
    "        {\n",
    "            \"description\": f\"{tool.name}: {tool.description}\",\n",
    "        },\n",
    "    )\n",
    "\n",
    "# Initialize agent\n",
    "llm = init_chat_model(\"openai:gpt-4o-mini\")\n",
    "\n",
    "# # qwen2.5 - 14b (#30 on BFCL leaderboard)\n",
    "# local_llm = \"qwen2.5:14b\"\n",
    "# llm = ChatOllama(model=local_llm, temperature=0.0)\n",
    "\n",
    "\n",
    "builder = create_agent(llm, tool_registry)\n",
    "agent = builder.compile(store=store)\n",
    "agent\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------- Customizing Tool Retrieval --------------------------------\n",
    "from langgraph.prebuilt import InjectedStore\n",
    "from langgraph.store.base import BaseStore\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "\n",
    "def retrieve_tools(\n",
    "    query: str,\n",
    "    # Add custom arguments here...\n",
    "    *,\n",
    "    store: Annotated[BaseStore, InjectedStore],\n",
    ") -> list[str]:\n",
    "    \"\"\"Retrieve a tool to use, given a search query.\"\"\"\n",
    "    results = store.search((\"tools\",), query=query, limit=2)\n",
    "    tool_ids = [result.key for result in results]\n",
    "    # Insert your custom logic here...\n",
    "    return tool_ids\n",
    "\n",
    "builder = create_agent(\n",
    "    llm, tool_registry, retrieve_tools_function=retrieve_tools\n",
    ")\n",
    "agent = builder.compile(store=store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CrewAI Agents (Advanced Collaboration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "# Define agents\n",
    "researcher = Agent(\n",
    "    role='Senior Research Analyst',\n",
    "    goal='Uncover cutting-edge technologies and market trends',\n",
    "    backstory=\"You are an experienced technology analyst with a knack for identifying emerging trends.\",\n",
    "    max_iter=5,  # Limit reasoning steps\n",
    "    llm=ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.3),\n",
    "    verbose=True,\n",
    "    memory=True,  # Maintains conversation history\n",
    "    tools=[SerperDevTool()],  # Search tool\n",
    "    allow_delegation=True\n",
    ")\n",
    "\n",
    "# # Usage\n",
    "# research_result = research_agent.execute(\n",
    "#     \"Find recent breakthroughs in AI-driven drug discovery\"\n",
    "# )\n",
    "\n",
    "writer = Agent(\n",
    "    role='Tech Content Strategist',\n",
    "    goal='Create compelling content about technology trends',\n",
    "    backstory=\"You transform complex technical concepts into engaging content.\",\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "research_task = Task(\n",
    "    description=\"Research emerging AI technologies focusing on practical applications in healthcare\",\n",
    "    expected_output=\"A comprehensive report on emerging AI in healthcare with at least 5 specific technologies\",\n",
    "    agent=researcher\n",
    ")\n",
    "\n",
    "writing_task = Task(\n",
    "    description=\"Create an engaging blog post based on the research findings\",\n",
    "    expected_output=\"A 1000-word blog post with sections covering each major technology\",\n",
    "    agent=writer,\n",
    "    context=[research_task]\n",
    ")\n",
    "\n",
    "# Create the crew\n",
    "tech_crew = Crew(\n",
    "    agents=[researcher, writer],\n",
    "    tasks=[research_task, writing_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Execute the crew\n",
    "result = tech_crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AutoGen Conversational Agents (Microsoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, GroupChatManager\n",
    "\n",
    "# Create specialized agents\n",
    "data_scientist = ConversableAgent(\n",
    "    name=\"Data_Scientist\",\n",
    "    system_message=\"Expert in statistical analysis and ML modeling\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\"}]}\n",
    ")\n",
    "\n",
    "domain_expert = ConversableAgent(\n",
    "    name=\"Medical_Expert\",\n",
    "    system_message=\"Healthcare domain expert with clinical trial experience\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\"}]}\n",
    ")\n",
    "\n",
    "# Advanced group chat manager\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat_participants=[data_scientist, domain_expert],\n",
    "    max_round=10,\n",
    "    admin_name=\"Moderator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Google Vertex AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel, Part, Tool, ToolConfig, ToolUseBlock\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Define tools\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets the current weather for a given location.\"\"\"\n",
    "    # This would typically call a weather API\n",
    "    return f\"Sunny, 72°F in {location}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        function_declarations=[\n",
    "            {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Gets the current weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The location to get weather for, e.g. 'San Francisco, CA'\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the model with tool configuration\n",
    "model = GenerativeModel(\n",
    "    \"gemini-pro\",\n",
    "    tools=tools,\n",
    "    generation_config={\"temperature\": 0.2}\n",
    ")\n",
    "\n",
    "# Handle function execution\n",
    "def handle_tool_call(tool_call):\n",
    "    if tool_call.name == \"get_weather\":\n",
    "        location = tool_call.args[\"location\"]\n",
    "        response = get_weather(location)\n",
    "        return response\n",
    "    return \"Unknown tool\"\n",
    "\n",
    "# Generate content with tool use\n",
    "response = model.generate_content(\n",
    "    \"What's the weather like in Seattle right now?\",\n",
    ")\n",
    "\n",
    "# Process any tool calls in the response\n",
    "if hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        if isinstance(part, ToolUseBlock):\n",
    "            # Process and respond to the tool call\n",
    "            tool_result = handle_tool_call(part.function_call)\n",
    "            \n",
    "            # Continue the conversation with the tool result\n",
    "            follow_up = model.generate_content(\n",
    "                [\n",
    "                    Part.from_text(\"What's the weather like in Seattle right now?\"),\n",
    "                    response.candidates[0].content,\n",
    "                    Part.from_function_response(\n",
    "                        name=part.function_call.name,\n",
    "                        response=tool_result\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            print(follow_up.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PandasAI Data Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import SmartDataFrame\n",
    "from pandasai.llm import OpenAI\n",
    "\n",
    "# Initialize advanced data agent\n",
    "llm = OpenAI(api_token=\"sk-...\", model=\"gpt-4\")\n",
    "df = SmartDataFrame(\n",
    "    \"medical_data.csv\",\n",
    "    config={\n",
    "        \"llm\": llm,\n",
    "        \"enable_cache\": False,\n",
    "        \"max_retries\": 5,\n",
    "        \"custom_prompts\": {\n",
    "            \"clean_data\": \"Automatically clean and preprocess this dataset\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Execute complex analysis\n",
    "response = df.chat(\n",
    "    \"Predict which drug candidates have >80% efficacy probability \"\n",
    "    \"using Bayesian regression analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "class CustomAgent(BaseModel):\n",
    "    llm: BaseLanguageModel  # The LLM to use for decision-making\n",
    "    tools: List[BaseTool]  # List of tools the agent can use\n",
    "    max_loops: int = 5  # Maximum number of loops to prevent infinite execution\n",
    "    stop_pattern: List[str]  # Stop patterns for the LLM to avoid hallucinations\n",
    "\n",
    "    @property\n",
    "    def tool_by_names(self) -> Dict[str, BaseTool]:\n",
    "        \"\"\"Map tool names to tool objects.\"\"\"\n",
    "        return {tool.name: tool for tool in self.tools}\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        \"\"\"Run the agent to answer a question.\"\"\"\n",
    "        name_to_tool_map = self.tool_by_names\n",
    "        previous_responses = []\n",
    "        num_loops = 0\n",
    "\n",
    "        while num_loops < self.max_loops:\n",
    "            num_loops += 1\n",
    "\n",
    "            # Format the prompt with the current state\n",
    "            curr_prompt = PROMPT_TEMPLATE.format(\n",
    "                tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools]),\n",
    "                tool_names=\", \".join([tool.name for tool in self.tools]),\n",
    "                question=question,\n",
    "                previous_responses=\"\\n\".join(previous_responses),\n",
    "            )\n",
    "\n",
    "            # Get the next action from the LLM\n",
    "            output, tool, tool_input = self._get_next_action(curr_prompt)\n",
    "\n",
    "            # If the final answer is found, return it\n",
    "            if tool == \"Final Answer\":\n",
    "                return tool_input\n",
    "\n",
    "            # Execute the tool and get the result\n",
    "            tool_result = name_to_tool_map[tool].run(tool_input)\n",
    "            output += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
    "            print(output)  # Print the agent's reasoning\n",
    "            previous_responses.append(output)\n",
    "\n",
    "        return \"Max loops reached without finding a final answer.\"\n",
    "\n",
    "    def _get_next_action(self, prompt: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Get the next action from the LLM.\"\"\"\n",
    "        result = self.llm.generate([prompt], stop=self.stop_pattern)\n",
    "        output = result.generations[0][0].text  # Get the first generation\n",
    "\n",
    "        # Parse the output to extract the tool and input\n",
    "        tool, tool_input = self._get_tool_and_input(output)\n",
    "        return output, tool, tool_input\n",
    "\n",
    "    def _get_tool_and_input(self, generated: str) -> Tuple[str, str]:\n",
    "        \"\"\"Parse the LLM output to extract the tool and input.\"\"\"\n",
    "        if FINAL_ANSWER_TOKEN in generated:\n",
    "            return \"Final Answer\", generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
    "\n",
    "        # Use regex to extract the tool and input\n",
    "        regex = r\"Action: (.*?)\\nAction Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, generated, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Output of LLM is not parsable for next tool use: `{generated}`\")\n",
    "\n",
    "        tool = match.group(1).strip()\n",
    "        tool_input = match.group(2).strip(\" \").strip('\"')\n",
    "        return tool, tool_input\n",
    "    \n",
    "\n",
    "FINAL_ANSWER_TOKEN = \"Final Answer:\"\n",
    "OBSERVATION_TOKEN = \"Observation:\"\n",
    "THOUGHT_TOKEN = \"Thought:\"\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question as best as you can using the following tools: \n",
    "\n",
    "{tool_description}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: comment on what you want to do next\n",
    "Action: the action to take, exactly one element of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation repeats N times, use it until you are sure of the answer)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {question}\n",
    "Thought: {previous_responses}\n",
    "\"\"\"\n",
    "\n",
    "# The tool(s) that your Agent will use\n",
    "tools = [annual_report_tool]\n",
    "\n",
    "# The question that you will ask your Agent\n",
    "question = \"What was Meta's net income in 2022? What was net income the year before that?\"\n",
    "\n",
    "# The prompt that your Agent will use and update as it is \"reasoning\"\n",
    "prompt = PROMPT_TEMPLATE.format(\n",
    "  tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
    "  tool_names=\", \".join([tool.name for tool in tools]),\n",
    "  question=question,\n",
    "  previous_responses='{previous_responses}',\n",
    ")\n",
    "\n",
    "# The LLM that your Agent will use\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize your Agent\n",
    "agent = CustomAgent(\n",
    "  llm=llm, \n",
    "  tools=tools, \n",
    "  prompt=prompt, \n",
    "  stop_pattern=[f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}'],\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,  # Only final output. If True, returns all intermediate steps\n",
    "    handle_parsing_errors=True,  # Graceful parsing errors\n",
    ")\n",
    "# Run the Agent!\n",
    "result = agent.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use Langchain \"initialize_agent\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "tools = [tool_1,tool_2]  # Add more tools as needed\n",
    "\n",
    "# Define the agent's prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an advanced agent with access to multiple tools. Your task is to resolve customer queries by:\n",
    "1. Identifying the problem or request.\n",
    "2. Using the tools provided to gather additional information if needed.\n",
    "3. Synthesizing the information into a clear, concise response.\n",
    "\n",
    "You can chain tools if required. If you are unsure, respond with 'I need more details.'\n",
    "\n",
    "Query: {query}\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    model=\"deepseek-chat\",\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    streaming=True,\n",
    "    callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "\n",
    "# Initialize the agent\n",
    "advanced_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_kwargs={\"prompt_template\": prompt_template},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=advanced_agent, tools=tools, verbose=True, \n",
    "                            return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"query\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Key Concepts\n",
    "    # Graph : A workflow of nodes and edges.\n",
    "    # Nodes : Functions or agents that perform tasks.\n",
    "    # Edges : Connections between nodes that define the flow.\n",
    "    # State : A shared data structure passed between nodes.\n",
    "    # StateGraph : A graph that manages state transitions.\n",
    "\n",
    "# Draw a directory tree for the src directory for a LangChain project.\n",
    "\"\"\"\n",
    "src/\n",
    "├── agents/\n",
    "│   ├── __init__.py\n",
    "│   ├── agent.py\n",
    "│   ├── graph.py\n",
    "│   ├── tools.py\n",
    "│   ├── configuration.py\n",
    "│   ├── state.py\n",
    "│   ├── prompts.py\n",
    "│   └── utils.py\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, Sequence, List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "#------------------ Define the Memory Saver-----------------------\n",
    "memory = MemorySaver()\n",
    "\n",
    "#------------------ Define the State-----------------------     # You can write a custom state class by extending the TypedDict class.\n",
    "class AgentState(TypedDict):\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Selected Agents: Tracks which agents are active in the workflow\n",
    "    selected_analysts: List[str]\n",
    "    \n",
    "    # Current Agent Index: Tracks the progress through the selected agents\n",
    "    current_analyst_idx: int\n",
    "\n",
    "workflow = StateGraph(AgentState)   # Initialize the Graph\n",
    "\n",
    "#------------------ Create Nodes-----------------------\n",
    "def supervisor_router(state):\n",
    "    \"\"\"Route to appropriate analyst(s) based on the query\"\"\"\n",
    "    result = routing_chain.invoke(state)\n",
    "    selected_analysts = [a.strip() for a in result.content.strip().split(',')]\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [SystemMessage(content=f\"Routing query to: {', '.join(selected_analysts)}\", name=\"supervisor\")],\n",
    "        \"selected_analysts\": selected_analysts,\n",
    "        \"current_analyst_idx\": 0\n",
    "    }\n",
    "\n",
    "# or\n",
    "def agent_node(state: AgentState, agent, name: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Generic node function for an agent.\n",
    "    - `state`: The current state of the workflow.\n",
    "    - `agent`: The agent or function to process the state.\n",
    "    - `name`: The name of the agent (for logging or identification).\n",
    "    \"\"\"\n",
    "    # Invoke the agent with the current state\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # Update the state with the agent's output\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=result[\"messages\"][-1].content, name=name)],\n",
    "        \"selected_agents\": state[\"selected_agents\"],\n",
    "        \"current_agent_idx\": state[\"current_agent_idx\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "#--------------------- Wrap the agent in a node--------------------------\n",
    "\n",
    "# Create the analysts with their specific tools\n",
    "quant_strategist = create_react_agent(llm, tools=quant_strategist_tools)\n",
    "quant_strategist_node = functools.partial(agent_node, agent=quant_strategist, name=\"quant_strategist\")\n",
    "\n",
    "macro_analyst = create_react_agent(llm, tools=macro_analyst_tools)\n",
    "macro_analyst_node = functools.partial(agent_node, agent=macro_analyst, name=\"macro_analyst\")\n",
    "\n",
    "\n",
    "#------------------- Add Nodes to Graph-----------------------\n",
    "workflow = StateGraph(AgentState)   # Initialize the Graph\n",
    "workflow.add_node(\"supervisor\", supervisor_router)  # Add the supervisor node\n",
    "workflow.add_node(\"quant_strategist\", quant_strategist_node)    # Add the quant_strategist node\n",
    "workflow.add_node(\"macro_analyst\", macro_analyst_node)        # Add the macro_analyst node\n",
    "\n",
    "#------------------- Define the Prompt-----------------------\n",
    "class SupervisorPrompt(ChatPromptTemplate):\n",
    "    \"\"\"Prompt for the supervisor node\"\"\"\n",
    "    messages: MessagesPlaceholder\n",
    "    selected_analysts: List[str]\n",
    "    current_analyst_idx: int\n",
    "\n",
    "#------------------- Define Conditional Edge-----------------------\n",
    "def get_next_step(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next step in the workflow.\n",
    "    - If no agents are selected, go to the final summary.\n",
    "    - If all agents have processed, go to the final summary.\n",
    "    - Otherwise, go to the next agent.\n",
    "    \"\"\"\n",
    "    if not state[\"selected_agents\"]:\n",
    "        return \"final_summary\"\n",
    "    current_idx = state[\"current_agent_idx\"]\n",
    "    if current_idx >= len(state[\"selected_agents\"]):\n",
    "        return \"final_summary\"\n",
    "    return state[\"selected_agents\"][current_idx]\n",
    "\n",
    "\n",
    "# Add conditional edges:\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",  # Source node\n",
    "    get_next_step,  # Router node/Function to determine the next step\n",
    "    {\n",
    "        \"quant_strategist\": \"quant_strategist\",  # Route to quant_strategist node\n",
    "        \"macro_analyst\": \"macro_analyst\",        # Route to macro_analyst node\n",
    "        \"final_summary\": \"final_summary\"         # Route to final_summary node\n",
    "    }\n",
    ")\n",
    "\n",
    "#------------------ Add Final Edges ------------------------------------\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "workflow.add_edge(\"final_summary\", END)\n",
    "\n",
    "#-------------------- Compile the Graph --------------------------------\n",
    "graph = workflow.compile()\n",
    "# or\n",
    "graph = workflow.compile(checkpointer=memory)   # Compile the graph with memory\n",
    "# or\n",
    "graph = workflow.compile(checkpointer=memory, interrupt_before=[\"quant_strategist_node\"])  # Compile the graph with memory and interrupt before quant_strategist_node\n",
    "\n",
    "\n",
    "#------------------ Stream the Graph with Memory--------------------------------\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}   # add memory thread, we used thread_id = 2\n",
    "events = graph.stream({\"messages\": {\"Hi there, my name is Paul\"}}, config, stream_mode = \"values\")\n",
    "\n",
    "for event in events:    # Iterate over the events\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "memory.get(config)  # Retrieve the memory for a specific configuration or thread_id\n",
    "\n",
    "\n",
    "#-------------------- Accessing the Graph State --------------------------------\n",
    "graph = workflow.compile()\n",
    "graph.get_state(config).values  # get the state of the graph\n",
    "graph.get_state(config).values.get(\"messages\", \"\")  # get the messages from the state\n",
    "graph.update_state(config, {\"input\": \"Hello, World!\"})  # update the state of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nice way to execute the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------ATLERNATIVE WAY TO RUN THE GRAPH IN A BEAUTIFUL WAY------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Run the Graph------------------------------------\n",
    "#------------------------- Custom Function----------------------------------\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import re\n",
    "from langchain_core.messages import HumanMessage\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.rule import Rule\n",
    "\n",
    "#---------- Formatting Functions\n",
    "# Format Bold Text\n",
    "def format_bold_text(content: str) -> Text:\n",
    "    \"\"\"Convert **text** to rich Text with bold formatting.\"\"\"\n",
    "    text = Text()\n",
    "    pattern = r'\\*\\*(.*?)\\*\\*'\n",
    "    parts = re.split(pattern, content)\n",
    "    for i, part in enumerate(parts):\n",
    "        if i % 2 == 0:\n",
    "            text.append(part)\n",
    "        else:\n",
    "            text.append(part, style=\"bold\")\n",
    "    return text\n",
    "\n",
    "# Format Message Content\n",
    "def format_message_content(content: str) -> Union[str, Text]:\n",
    "    \"\"\"Format the message content, handling JSON and text with bold markers.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        return json.dumps(data, indent=2)\n",
    "    except:\n",
    "        if '**' in content:\n",
    "            return format_bold_text(content)\n",
    "        return content\n",
    "\n",
    "# Format Agent Message\n",
    "def format_agent_message(message: HumanMessage) -> Union[str, Text]:\n",
    "    \"\"\"Format a single agent message.\"\"\"\n",
    "    return format_message_content(message.content)\n",
    "\n",
    "# Get Agent Title\n",
    "def get_agent_title(agent: str, message: HumanMessage) -> str:\n",
    "    \"\"\"Get the title for the agent panel, with fallback handling.\"\"\"\n",
    "    base_title = agent.replace('_', ' ').title()\n",
    "    if hasattr(message, 'name') and message.name is not None:\n",
    "        try:\n",
    "            return message.name.replace('_', ' ').title()\n",
    "        except:\n",
    "            return base_title\n",
    "    return base_title\n",
    "\n",
    "# Print a Single Step\n",
    "def print_step(step: Dict[str, Any]) -> None:\n",
    "    \"\"\"Pretty print a single step of the agent execution.\"\"\"\n",
    "    console = Console()\n",
    "    for agent, data in step.items():\n",
    "        # Handle supervisor steps\n",
    "        if 'next' in data:\n",
    "            next_agent = data['next']\n",
    "            text = Text()\n",
    "            text.append(\"Portfolio Manager \", style=\"bold magenta\")\n",
    "            text.append(\"assigns next task to \", style=\"white\")\n",
    "            if next_agent == \"final_summary\":\n",
    "                text.append(\"FINAL SUMMARY\", style=\"bold yellow\")\n",
    "            elif next_agent == \"END\":\n",
    "                text.append(\"END\", style=\"bold red\")\n",
    "            else:\n",
    "                text.append(f\"{next_agent}\", style=\"bold green\")\n",
    "            console.print(Panel(\n",
    "                text,\n",
    "                title=\"[bold blue]Supervision Step\",\n",
    "                border_style=\"blue\"\n",
    "            ))\n",
    "        # Handle agent responses and final summary\n",
    "        if 'messages' in data:\n",
    "            message = data['messages'][0]\n",
    "            formatted_content = format_agent_message(message)\n",
    "            if agent == \"final_summary\":\n",
    "                # Final summary formatting\n",
    "                console.print(Rule(style=\"yellow\", title=\"Portfolio Analysis\"))\n",
    "                console.print(Panel(\n",
    "                    formatted_content,\n",
    "                    title=\"[bold yellow]Investment Summary and Recommendation\",\n",
    "                    border_style=\"yellow\",\n",
    "                    padding=(1, 2)\n",
    "                ))\n",
    "                console.print(Rule(style=\"yellow\"))\n",
    "            else:\n",
    "                # Regular analyst reports\n",
    "                title = get_agent_title(agent, message)\n",
    "                console.print(Panel(\n",
    "                    formatted_content,\n",
    "                    title=f\"[bold blue]{title} Report\",\n",
    "                    border_style=\"green\"\n",
    "                ))\n",
    "\n",
    "# Stream the Execution\n",
    "def stream_agent_execution(graph, input_data: Dict, config: Dict) -> None:\n",
    "    \"\"\"Stream and pretty print the agent execution.\"\"\"\n",
    "    console = Console()\n",
    "    console.print(\"\\n[bold blue]Starting Agent Execution...[/bold blue]\\n\")\n",
    "    for step in graph.stream(input_data, config):\n",
    "        if \"__end__\" not in step:\n",
    "            print_step(step)\n",
    "            console.print(\"\\n\")\n",
    "    console.print(\"[bold blue]Analysis Complete[/bold blue]\\n\")\n",
    "\n",
    "\n",
    "# Run the Graph\n",
    "# Define the input data\n",
    "input_data = {\n",
    "    \"messages\": [HumanMessage(content=\"What is AAPL's current price and latest revenue?\")]\n",
    "}\n",
    "\n",
    "# Define the configuration (e.g., recursion limit)\n",
    "config = {\"recursion_limit\": 10}\n",
    "\n",
    "# Stream the execution\n",
    "stream_agent_execution(graph, input_data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph State: --> Example\n",
    "# What is a LangGraph State?\n",
    "    # A LangGraph state is a data structure that holds the current state of the workflow. It is passed between nodes in the graph, \n",
    "    # and each node can modify the state as needed. The state typically contains all the information required for the workflow to function, \n",
    "    # such as inputs, intermediate results, and outputs.\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional, Annotated\n",
    "import operator\n",
    "from langgraph.graph import Graph, StateGraph, MessageGraph, MessagesState\n",
    "\n",
    " \n",
    "#------------------ Define the State (State.py) -----------------------\n",
    "DEFAULT_EXTRACTION_SCHEMA = {\n",
    "    \"title\": \"CompanyInfo\",\n",
    "    \"description\": \"Basic information about a company\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"company_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Official name of the company\",\n",
    "        },\n",
    "        \"founding_year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Year the company was founded\",\n",
    "        },\n",
    "        \"founder_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"Names of the founding team members\",\n",
    "        },\n",
    "        \"product_description\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Brief description of the company's main product or service\",\n",
    "        },\n",
    "        \"funding_summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Summary of the company's funding history\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"company_name\"],\n",
    "}\n",
    "\n",
    "class SampleState(MessagesState):   # this state will have both company and messages (since messages is already defined in the MessagesState)\n",
    "    \"\"\"A sample state class that extends the MessagesState.\"\"\"\n",
    "    company: str\n",
    "    \"\"\"Company to research provided by the user.\"\"\"\n",
    "    \n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class InputState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    company: str\n",
    "    \"Company to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: Optional[dict[str, Any]] = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class OverallState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    company: str\n",
    "    \"Company to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: str = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "    search_queries: list[str] = field(default=None)\n",
    "    \"List of generated search queries to find relevant information\"\n",
    "\n",
    "    completed_notes: Annotated[list, operator.add] = field(default_factory=list)\n",
    "    \"Notes from completed research related to the schema\"\n",
    "\n",
    "    info: dict[str, Any] = field(default=None)\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "    is_satisfactory: bool = field(default=None)\n",
    "    \"True if all required fields are well populated, False otherwise\"\n",
    "\n",
    "    reflection_steps_taken: int = field(default=0)\n",
    "    \"Number of times the reflection node has been executed\"\n",
    "\n",
    "    \n",
    "@dataclass(kw_only=True)\n",
    "class OutputState:\n",
    "    \"\"\"The response object for the end user.\n",
    "\n",
    "    This class defines the structure of the output that will be provided\n",
    "    to the user after the graph's execution is complete.\n",
    "    \"\"\"\n",
    "\n",
    "    info: dict[str, Any]\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "#------------------ Define the Configuration (Configuration.py) -----------------------\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
    "\n",
    "    max_search_queries: int = 3  # Max search queries per company\n",
    "    max_search_results: int = 3  # Max search results per query\n",
    "    max_reflection_steps: int = 0  # Max reflection steps\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})\n",
    "#-------------------------------------------------------------------------------\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from agent.configuration import Configuration\n",
    "\n",
    "builder = StateGraph(\n",
    "    OverallState,\n",
    "    input=InputState,\n",
    "    output=OutputState,\n",
    "    config_schema=Configuration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG: Directed Acyclic Graph \n",
    "    # Definition : A graph where nodes are connected in a linear, directional manner without forming closed loops .\n",
    "    # Use Case : Used by LangChain to represent workflows where tasks are executed in a non-repeating, linear sequence .\n",
    "        ''' Start → Node A → Node B → Node C → End '''\n",
    "            # No loops : Once a node is processed, it doesn’t revisit previous nodes.\n",
    "            # Linear flow : Tasks are executed in a strict sequence.\n",
    "            \n",
    "            \n",
    "# DCG: Directed Cyclic Graph --> used by LangGraph to represent the workflow of nodes and edges.\n",
    "    # Definition : A graph where nodes are connected in a directional manner and can form loops or cycles .\n",
    "    # Use Case : Used by LangGraph to represent workflows with complex patterns , including loops and conditional branching .\n",
    "        '''\n",
    "        Start → Node A → Node B → Node C\n",
    "                ↑              ↓\n",
    "                └──────────────┘\n",
    "        '''\n",
    "            # Loops allowed : Nodes can revisit previous nodes (e.g., for iterative tasks).\n",
    "            # Complex flow : Supports conditional edges, loops, and dynamic routing.\n",
    "\n",
    "\n",
    "# Edges:\n",
    "    # Simple Edge:\n",
    "        # A direct connection between two nodes in the graph. Used whrn the flow is fixed and uncontitional.\n",
    "        ''' Start → Node A → Node B → Node C → End '''\n",
    "    \n",
    "    # Conditional Edge:\n",
    "        # A connection between two nodes that is determined by a condition or decision function.\n",
    "        '''\n",
    "            Start → Node A\n",
    "                    ↓\n",
    "                ┌─────┴─────┐\n",
    "            Condition 1   Condition 2\n",
    "                ↓             ↓\n",
    "            Node B         Node C\n",
    "                ↓             ↓\n",
    "            Node D         Node E\n",
    "                └─────┬─────┘\n",
    "                    ↓\n",
    "                    End\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, Graph # Import the necessary classes\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Define the state as a Pydantic model\n",
    "class CustomerSupportState(BaseModel):\n",
    "    query: str = Field(..., description=\"The customer's query\")\n",
    "    response: str = Field(None, description=\"The response to the customer\")\n",
    "    issue_type: str = Field(None, description=\"The type of issue (FAQ, Escalation, Recommendation)\")\n",
    "    escalation_required: bool = Field(False, description=\"Whether the issue requires escalation\")\n",
    "    product_recommendation: str = Field(None, description=\"Product recommendation for the customer\")\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(CustomerSupportState)\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE WITHOUT LLM ---------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "# Node A: Classify the customer's query\n",
    "def classify_query(state: CustomerSupportState) -> dict:\n",
    "    query = state.query.lower()\n",
    "    if \"faq\" in query or \"how to\" in query or \"what is\" in query:\n",
    "        return {\"issue_type\": \"FAQ\"}\n",
    "    elif \"issue\" in query or \"problem\" in query or \"error\" in query:\n",
    "        return {\"issue_type\": \"Escalation\"}\n",
    "    elif \"recommend\" in query or \"suggest\" in query:\n",
    "        return {\"issue_type\": \"Recommendation\"}\n",
    "    else:\n",
    "        return {\"issue_type\": \"Unknown\"}\n",
    "\n",
    "#--------------------------------------------- TOOL NODE ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chrome\",\n",
    "    embedding=embeddings\n",
    "    \n",
    ")\n",
    "retriever=vectorstore.as_retriever()\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.You are a specialized assistant. Use the 'retriever_tool' **only** when the query explicitly relates to LangChain blog data. For all other queries, respond directly without using any tool. For simple queries like 'hi', 'hello', or 'how are you', provide a normal response.\",\n",
    "    )\n",
    "\n",
    "tools=[retriever_tool]\n",
    "retrieve=ToolNode([retriever_tool])\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- TOOL NODE WITH FALLBACK ---------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    \"\"\"\n",
    "    Function to handle errors that occur during tool execution.\n",
    "    Args:\n",
    "        state (dict): The current state of the AI agent, which includes messages and tool call details.\n",
    "    Returns:\n",
    "        dict: A dictionary containing error messages for each tool that encountered an issue.\n",
    "    \"\"\"\n",
    "    # Retrieve the error from the current state\n",
    "    error = state.get(\"error\")\n",
    "    # Access the tool calls from the last message in the state's message history\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    # Return a list of ToolMessages with error details, linked to each tool call ID\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",  # Format the error message for the user\n",
    "                tool_call_id=tc[\"id\"],  # Associate the error message with the corresponding tool call ID\n",
    "            )\n",
    "            for tc in tool_calls  # Iterate over each tool call to produce individual error messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create a tool node with fallback error handling.\n",
    "    Args:\n",
    "        tools (list): A list of tools to be included in the node.\n",
    "    Returns:\n",
    "        dict: A tool node that uses fallback behavior in case of errors.\n",
    "    \"\"\"\n",
    "    # Create a ToolNode with the provided tools and attach a fallback mechanism\n",
    "    # If an error occurs, it will invoke the handle_tool_error function to manage the error\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)],  # Use a lambda function to wrap the error handler\n",
    "        exception_key=\"error\"  # Specify that this fallback is for handling errors\n",
    "    )\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback([retriever_tool, tool_1, tool_2]))\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM 1 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import add_messages\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "def ai_assistant(state:AgentState):\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state['messages']\n",
    "    \n",
    "    if len(messages)>1:\n",
    "        last_message = messages[-1]\n",
    "        question = last_message.content\n",
    "        prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a helpful assistant whatever question has been asked to find out that in the given question and answer.\n",
    "                        Here is the question:{question}\n",
    "                        \"\"\",\n",
    "                        input_variables=[\"question\"]\n",
    "                        )\n",
    "            \n",
    "        chain = prompt | llm\n",
    "    \n",
    "        response=chain.invoke({\"question\": question})\n",
    "        return {\"messages\": [response]}\n",
    "    else:\n",
    "        llm_with_tool = llm.bind_tools(tools)\n",
    "        response = llm_with_tool.invoke(messages)\n",
    "        #response=handle_query(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM + structured output ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "class grade(BaseModel):\n",
    "    binary_score:str=Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "def grade_documents(state:AgentState)->Literal[\"Output_Generator\", \"Query_Rewriter\"]:\n",
    "    llm_with_structure_op=llm.with_structured_output(grade)\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader deciding if a document is relevant to a user’s question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\n",
    "                    If the document talks about or contains information related to the user’s question, mark it as relevant. \n",
    "                    Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "                    )\n",
    "    chain = prompt | llm_with_structure_op\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generator\" #this should be a node name\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewriter\" #this should be a node name\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM + structured output 2 ----------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "query_writer_instructions=\"\"\"Your goal is to generate targeted web search query.\n",
    "\n",
    "The query will gather information related to a specific topic.\n",
    "\n",
    "Topic:\n",
    "{research_topic}\n",
    "\n",
    "Return your query as a JSON object:\n",
    "{{\n",
    "    \"query\": \"string\",\n",
    "    \"aspect\": \"string\",\n",
    "    \"rationale\": \"string\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def generate_query(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\" Generate a query for web search \"\"\"\n",
    "    \n",
    "    # Format the prompt\n",
    "    query_writer_instructions_formatted = query_writer_instructions.format(research_topic=state.research_topic)\n",
    "\n",
    "    # Generate a query\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm_json_mode = ChatOllama(model=configurable.local_llm, temperature=0, format=\"json\")\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=query_writer_instructions_formatted),\n",
    "        HumanMessage(content=f\"Generate a query for web search:\")]\n",
    "    )   \n",
    "    query = json.loads(result.content)\n",
    "    \n",
    "    return {\"search_query\": query['query']}\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------ NODE with LLM + structured output 3 ----------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "RECOMMENDATION_PROMPT_2 = \"\"\"\n",
    "You are a specialized travel recommendation assistant. \n",
    "Generate at least 10 unique recommendations for the user. \n",
    "Output your response as a list of dictionaries, each containing the fields: \n",
    "  - \"key\": A short label, e.g. \"Crime Rate\"\n",
    "  - \"value\": A concise recommendation\n",
    "\n",
    "For example:\n",
    "[\n",
    "    {\"key\": \"Crime Rate\", \"value\": \"The city is generally safe but beware of pickpockets in tourist areas.\"},\n",
    "    {\"key\": \"Weather Advice\", \"value\": \"Spring is mild; pack light jackets and an umbrella.\"}\n",
    "]\n",
    "\n",
    "### User Query:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "def recommendations_node_2(state: OverallState) -> OverallState:\n",
    "    import openai\n",
    "    \n",
    "    # If you haven't set up your API key globally, do so here:\n",
    "    # openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "    \n",
    "    client = openai  # or adapt to your environment if needed\n",
    "\n",
    "    # Combine all user messages into a single query\n",
    "    all_messages = \"\\n\".join([message.content for message in state.messages])\n",
    "    preferences_text = \"\\n\".join([f\"{key}: {value}\" for key, value in state.user_preferences.items()])\n",
    "    query = f\"{all_messages}\\n\\nUser Preferences:\\n{preferences_text}\"\n",
    "\n",
    "    # Define the structured response format with a JSON Schema\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Replace with a valid model you have access to.\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": RECOMMENDATION_PROMPT_2},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        # The 'response_format' parameter needs 'json_schema' -> 'name' + 'schema'\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"recommendation_schema\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"recommendations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"key\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Short label for the recommendation\"\n",
    "                                    },\n",
    "                                    \"value\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Concise recommendation content\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"key\", \"value\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            },\n",
    "                            \"description\": \"A list of travel recommendations.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"recommendations\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Parse and return the generated structured output\n",
    "    try:\n",
    "        structured_output = completion.choices[0].message.content\n",
    "        parsed_output = json.loads(structured_output)\n",
    "        recommendation_list = parsed_output[\"recommendations\"]  # This is the list of dictionaries\n",
    "        transformed_list = [{item[\"key\"]: item[\"value\"]} for item in recommendation_list]\n",
    "        \n",
    "        state.recommendations = transformed_list\n",
    "        \n",
    "        return state \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return {\"error\": \"Failed to generate recommendations.\"}\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM + Blind Tools (Used for User call) ---------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "# from langchain_aws import ChatBedrock\n",
    "import boto3\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "@tool\n",
    "def compute_savings(monthly_cost: float) -> float:\n",
    "    \"\"\"\n",
    "    Tool to compute the potential savings when switching to solar energy based on the user's monthly electricity cost.\n",
    "    \n",
    "    Args:\n",
    "        monthly_cost (float): The user's current monthly electricity cost.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'number_of_panels': The estimated number of solar panels required.\n",
    "            - 'installation_cost': The estimated installation cost.\n",
    "            - 'net_savings_10_years': The net savings over 10 years after installation costs.\n",
    "    \"\"\"\n",
    "    def calculate_solar_savings(monthly_cost):\n",
    "        # Assumptions for the calculation\n",
    "        cost_per_kWh = 0.28  \n",
    "        cost_per_watt = 1.50  \n",
    "        sunlight_hours_per_day = 3.5  \n",
    "        panel_wattage = 350  \n",
    "        system_lifetime_years = 10  \n",
    "        # Monthly electricity consumption in kWh\n",
    "        monthly_consumption_kWh = monthly_cost / cost_per_kWh\n",
    "        \n",
    "        # Required system size in kW\n",
    "        daily_energy_production = monthly_consumption_kWh / 30\n",
    "        system_size_kW = daily_energy_production / sunlight_hours_per_day\n",
    "        \n",
    "        # Number of panels and installation cost\n",
    "        number_of_panels = system_size_kW * 1000 / panel_wattage\n",
    "        installation_cost = system_size_kW * 1000 * cost_per_watt\n",
    "        \n",
    "        # Annual and net savings\n",
    "        annual_savings = monthly_cost * 12\n",
    "        total_savings_10_years = annual_savings * system_lifetime_years\n",
    "        net_savings = total_savings_10_years - installation_cost\n",
    "        \n",
    "        return {\n",
    "            \"number_of_panels\": round(number_of_panels),\n",
    "            \"installation_cost\": round(installation_cost, 2),\n",
    "            \"net_savings_10_years\": round(net_savings, 2)\n",
    "        }\n",
    "    # Return calculated solar savings\n",
    "    return calculate_solar_savings(monthly_cost)\n",
    "\n",
    "# Define the state for the workflow\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# Define the assistant class used for invoking the runnable\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        # Initialize with the runnable that defines the process for interacting with the tools\n",
    "        self.runnable = runnable\n",
    "    def __call__(self, state: State):\n",
    "        while True:\n",
    "            # Invoke the runnable with the current state (messages and context)\n",
    "            result = self.runnable.invoke(state)\n",
    "            \n",
    "            # If the tool fails to return valid output, re-prompt the user to clarify or retry\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                # Add a message to request a valid response\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                # Break the loop when valid output is obtained\n",
    "                break\n",
    "        # Return the final state after processing the runnable\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        model=\"deepseek-chat\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful customer support assistant for Solar Panels Belgium.\n",
    "            You should get the following information from them:\n",
    "            - monthly electricity cost\n",
    "            If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "            After you are able to discern all the information, call the relevant tool.\n",
    "            ''',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the tools the assistant will use\n",
    "part_1_tools = [\n",
    "    compute_savings\n",
    "]\n",
    "\n",
    "# Bind the tools to the assistant's workflow\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools, tool_choice=\"any\")\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------- NODE with LLM + Tools with multiple parameters ----------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def accommodation_finder_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    This node extracts accommodation details from the user's query in state.messages\n",
    "    and returns a structured output that can be passed to the booking tool.\n",
    "    \"\"\"\n",
    "\n",
    "    class AccommodationOutput(BaseModel):\n",
    "        location: str = Field(..., description=\"The exact location or neighborhood where the traveler wants to stay (e.g., 'Brooklyn').\")\n",
    "        checkin_date: str = Field(..., description=\"The check-in date in YYYY-MM-DD format.\")\n",
    "        checkout_date: str = Field(..., description=\"The check-out date in YYYY-MM-DD format.\")\n",
    "        adults: int = Field(default=2, description=\"The number of adult guests.\")\n",
    "        rooms: int = Field(default=1, description=\"The number of rooms.\")\n",
    "        currency: str = Field(default=\"USD\", description=\"The currency for the prices.\")\n",
    "        \n",
    "    # Create a new LLM with structured output\n",
    "    llm_with_structure = llm.with_structured_output(AccommodationOutput)\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an advanced travel planner assistant. Your task is to extract accommodation details\n",
    "        from the traveler's query. Use the following information to generate a structured output for\n",
    "        booking accommodations:\n",
    "\n",
    "        ### Traveler Query:\n",
    "        {query}\n",
    "\n",
    "        ### Instructions:\n",
    "        1. Extract the exact location or neighborhood where the traveler wants to stay (e.g., \"Brooklyn\").\n",
    "           - If the traveler does not specify a location, use the city or city code provided in the state.\n",
    "        2. Extract the check-in and check-out dates from the query.\n",
    "           - If the dates are not explicitly mentioned, use the default dates from the state.\n",
    "        3. Extract the number of adults and rooms from the query.\n",
    "           - If not specified, use the default values: 1 adult and 1 room.\n",
    "        4. Use the default currency 'USD' unless specified otherwise.\n",
    "        5. Return the structured output in the following format:\n",
    "           - location: The exact location or neighborhood.\n",
    "           - checkin_date: The check-in date in YYYY-MM-DD format.\n",
    "           - checkout_date: The check-out date in YYYY-MM-DD format.\n",
    "           - adults: The number of adult guests.\n",
    "           - rooms: The number of rooms.\n",
    "           - currency: The currency for the prices.\n",
    "\n",
    "        ### Example Output:\n",
    "        - location: \"Brooklyn\"\n",
    "        - checkin_date: \"2023-12-01\"\n",
    "        - checkout_date: \"2023-12-10\"\n",
    "        - adults: 2\n",
    "        - rooms: 1\n",
    "        - currency: \"USD\"\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"]\n",
    "    )\n",
    "\n",
    "    # Create the chain\n",
    "    chain = prompt | llm_with_structure\n",
    "\n",
    "    # Extract the user's query from state.messages\n",
    "    query = state.messages[-1].content  # Assuming the last message is the user's query\n",
    "\n",
    "    # Invoke the chain to generate the structured output\n",
    "    structured_output = chain.invoke({\"query\": query})\n",
    "\n",
    "    # Call Google Flights Search Tool        \n",
    "    booking_search_input = BookingSearchInput(\n",
    "        location=structured_output.location,\n",
    "        checkin_date=structured_output.checkin_date,\n",
    "        checkout_date=structured_output.checkout_date,\n",
    "        adults=structured_output.adults,\n",
    "        rooms=structured_output.rooms,\n",
    "        currency=structured_output.currency,\n",
    "    )\n",
    "\n",
    "    booking_results = booking_tool.func(booking_search_input)\n",
    "    \n",
    "    # Update the state with the structured output\n",
    "    state.accommodation = booking_results\n",
    "\n",
    "    # Return the updated state\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 1 ---------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "\n",
    "# Define a REACT-based agent node\n",
    "def react_agent_node(state: CustomerSupportState):\n",
    "    tools = [retriever_tool]  # Add your tool(s) here\n",
    "    prompt_template = \"\"\"You are a reasoning and acting agent.\n",
    "    Use the tools available to gather or verify information as needed.\n",
    "    Respond directly if no tools are required.\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    react_agent = create_react_agent(\n",
    "        tools=tools,\n",
    "        prompt_template=prompt_template,\n",
    "        llm=llm,\n",
    "    )\n",
    "    \n",
    "    agent_executor = AgentExecutor(agent=react_agent, tools=tools, verbose=True)\n",
    "    # Execute the REACT agent\n",
    "    query = state.query\n",
    "    response = agent_executor.invoke({\"query\": query})\n",
    "    state.response = response\n",
    "    return state\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 2 ---------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "import functools\n",
    "\n",
    "def agent_node(state: OverallState, agent, name: str) -> OverallState:\n",
    "    \"\"\"\n",
    "    Generic node function for an agent.\n",
    "    - `state`: The current state of the workflow.\n",
    "    - `agent`: The agent or function to process the state.\n",
    "    - `name`: The name of the agent (for logging or identification).\n",
    "    \"\"\"\n",
    "    # Invoke the agent with the current state\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # Update the state with the agent's output\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=result[\"messages\"][-1].content, name=name)],\n",
    "        \"selected_agents\": state[\"selected_agents\"],\n",
    "        \"current_agent_idx\": state[\"current_agent_idx\"] + 1\n",
    "    }\n",
    "\n",
    "# wrap the agent in a node\n",
    "def query_param_generator_node(agent_node):\n",
    "\n",
    "    query_param_generator_agent = create_react_agent(llm, tools=[retriever_tool], prompt=prompt)\n",
    "    query_param_node = functools.partial(agent_node, agent=query_param_generator_agent, name=\"query_param_generator\")\n",
    "    return query_param_node\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 3 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# more advanced node\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "\n",
    "\n",
    "def advanced_multi_tool_agent_node(state: CustomerSupportState):\n",
    "    \"\"\"\n",
    "    An advanced agent that uses multiple tools to handle queries.\n",
    "    \"\"\"\n",
    "    tools = [retriever_tool]  # Add more tools as needed\n",
    "\n",
    "    # Define the agent's prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You are an advanced agent with access to multiple tools. Your task is to resolve customer queries by:\n",
    "    1. Identifying the problem or request.\n",
    "    2. Using the tools provided to gather additional information if needed.\n",
    "    3. Synthesizing the information into a clear, concise response.\n",
    "\n",
    "    You can chain tools if required. If you are unsure, respond with 'I need more details.'\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        model=\"deepseek-chat\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    \n",
    "    # Initialize the agent\n",
    "    advanced_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\"prompt_template\": prompt_template},\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    agent_executor = AgentExecutor(agent=advanced_agent, tools=[tool_1, tool_2], verbose=True, \n",
    "                               return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "\n",
    "    # Execute the agent\n",
    "    query = state.query\n",
    "    try:\n",
    "        response = agent_executor.invoke({\"query\": query})\n",
    "        state.response = response\n",
    "    except Exception as e:\n",
    "        state.response = f\"Error: {str(e)}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Custom Agent --------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.agents import BaseAgent\n",
    "from typing import Optional\n",
    "\n",
    "class AdvancedCustomAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Custom advanced agent with LLM, human-in-the-loop, and iterative reasoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, tools=None, max_iterations: int = 3):\n",
    "        self.llm = llm\n",
    "        self.tools = tools or []\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    async def run(self, query: str, human_review: bool = False, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Executes the custom agent's workflow.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query.\n",
    "            human_review (bool): If True, adds human-in-the-loop for review.\n",
    "        \n",
    "        Returns:\n",
    "            str: Final response.\n",
    "        \"\"\"\n",
    "        response = \"\"\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"--- Iteration {iteration}/{self.max_iterations} ---\")\n",
    "\n",
    "            # Generate a response using LLM\n",
    "            prompt = f\"\"\"\n",
    "            You are an advanced customer support agent. Use the tools provided to solve the query. \n",
    "            Tools: {', '.join([tool.name for tool in self.tools]) if self.tools else 'None'}\n",
    "\n",
    "            Query: {query}\n",
    "\n",
    "            If you need clarification or further details, request them from the user.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = await self.llm.apredict(prompt)\n",
    "                print(f\"Generated Response: {response}\")\n",
    "\n",
    "                # Check if human review is required\n",
    "                if human_review:\n",
    "                    review = input(\"Do you approve this response? (yes/no): \")\n",
    "                    if review.lower() == \"yes\":\n",
    "                        break\n",
    "                    else:\n",
    "                        query = input(\"Provide additional details or corrections: \")\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                response = f\"Error: {str(e)}\"\n",
    "                break\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# Define the custom agent node\n",
    "async def custom_agent_node(state: CustomerSupportState):\n",
    "    \"\"\"\n",
    "    Node with a custom advanced agent that uses LLM and human-in-the-loop.\n",
    "    \"\"\"\n",
    "    custom_agent = AdvancedCustomAgent(llm=llm, tools=[retriever_tool], max_iterations=3)\n",
    "    query = state.query\n",
    "\n",
    "    # Human-in-the-loop enabled for critical queries\n",
    "    response = await custom_agent.run(query, human_review=True)\n",
    "    state.response = response\n",
    "    return state\n",
    "\n",
    "\n",
    "#--------------------------------------------- Supervisor NODE with Router + Command ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "# Agent Supervisor Node\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "\n",
    "'''\n",
    "User\n",
    " ├── Supervisor\n",
    " │     ├── [direct] Agent 1\n",
    " │     ├── [conditional] Agent 2 (if condition A is met)\n",
    " │     └── [direct] Agent 3\n",
    " │           ├── [conditional] Sub-Agent 3.1 (if condition B is met)\n",
    " │           └── [direct] Sub-Agent 3.2\n",
    " │\n",
    " └── Feedback Loop (User <--> Supervisor)\n",
    "\n",
    "'''\n",
    "members = [\"researcher\", \"coder\"]\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        f\" following workers: {members}. Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    class Router(TypedDict):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options]\n",
    "\n",
    "    def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto)\n",
    "\n",
    "    return supervisor_node\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "\n",
    "\n",
    "#--------------------------------------------- More Advanced Supervisor NODE -------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[tool_1, tool_2],\n",
    "    name=\"math_expert\",\n",
    "    prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[tool_1, tool_2],\n",
    "    name=\"research_expert\",\n",
    "    prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "# Create supervisor workflow\n",
    "workflow = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    output_mode = \"last_message\",    # what we pass back from agent to supervisor. we dont need to always state this\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what's the combined headcount of the FAANG companies in 2024?\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "#--------------------------------------------- Supervisor managing other Supervisors------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "math_supervisor = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    output_mode = \"last_message\",    # what we pass back from agent to supervisor. we dont need to always state this\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ").compile(name=\"math_supervisor\")\n",
    "\n",
    "\n",
    "research_supervisor = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    output_mode = \"last_message\",    # what we pass back from agent to supervisor. we dont need to always state this\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ").compile(name=\"research_supervisor\")\n",
    "\n",
    "workflow = create_supervisor(\n",
    "    [math_supervisor, research_supervisor],\n",
    "    supervisor_name = \"top_level_supervisor\",\n",
    "    model=model,\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ").compile()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Command (used with tool node) ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "llm = llm.bind_tools([retriever_tool, tool_1, tool_2])\n",
    "\n",
    "def call_model(state:OverallState) -> Command[Literal['tools', END]]:\n",
    "    message = state.messages[-1].content\n",
    "    response = llm.invoke(message)\n",
    "    if len(response.tool_calls) > 0:\n",
    "        next_node = \"tools\"\n",
    "    else:\n",
    "        next_node = END\n",
    "    return Command(goto=next_node, update={\"messages\": response})   # update helps to update the state with the response from the model. You dont need always need to do this\n",
    "\n",
    "workflow = StateGraph(OverallState)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_node(\"tools\", create_tool_node_with_fallback([retriever_tool, tool_1, tool_2]))\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", \"tools\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "\n",
    "# If you are using subgraphs, you might want to navigate from a node a subgraph to a different subgraph \n",
    "# (i.e. a different node in the parent graph).\n",
    "def my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n",
    "    return Command(\n",
    "        update={\"foo\": \"bar\"},\n",
    "        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n",
    "        graph=Command.PARENT\n",
    "    )\n",
    "\n",
    "#--------------------------------------------- LangGraph Workflow ---------------------------------------------   \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"Classify Query\", classify_query)\n",
    "workflow.add_node(\"End Conversation\", end_conversation)\n",
    "\n",
    "# Define edges between nodes\n",
    "workflow.add_edge(START, \"Classify Query\")\n",
    "workflow.add_edge(\"Classify Query\", \"Answer FAQ\")\n",
    "workflow.add_edge(\"Recommend Products\", \"End Conversation\")\n",
    "workflow.add_edge(\"End Conversation\", END)\n",
    "\n",
    "# Set entry and finish points\n",
    "workflow.set_entry_point(\"Classify Query\")  # Start the conversation. Use this only when START is not used.\n",
    "workflow.set_finish_point(\"End Conversation\")   # End the conversation. Use this only when END is not used.\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test the workflow with a sample query\n",
    "initial_state = CustomerSupportState(query=\"which do you recommend between product A and product B?\")\n",
    "result = app.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> langgraph-swarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation and Setup\n",
    "pip install langgraph-swarm langchain-openai\n",
    "export OPENAI_API_KEY=<your_api_key>  # Or use a .env file\n",
    "\n",
    "\n",
    "# Core Components and Code\n",
    "    from langgraph_swarm import create_handoff_tool, create_swarm, add_active_agent_router\n",
    "    from langgraph.checkpoint.memory import InMemorySaver\n",
    "    from langgraph_swarm.swarm import SwarmState\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    \n",
    "    # langgraph_swarm:  The main library.\n",
    "    # create_handoff_tool(agent_name: str, description: str | None = None) -> BaseTool:  \n",
    "        # This is the crucial function. It creates a LangChain Tool that, when invoked by an agent, triggers a handoff to another agent.\n",
    "            # agent_name: The name of the agent to transfer control to (must match the node name in the graph).\n",
    "            # description: A description of when this tool should be used. This is important for the LLM to understand the tool's purpose.\n",
    "\n",
    "        from langgraph_swarm import create_handoff_tool\n",
    "        from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "        transfer_to_bob = create_handoff_tool(\n",
    "            agent_name=\"Bob\",\n",
    "            description=\"Ask Bob for help with pirate speak.\"\n",
    "        )\n",
    "\n",
    "        alice = create_react_agent(\n",
    "            model=ChatOpenAI(model=\"gpt-4\"),\n",
    "            tools=[some_tool, transfer_to_bob],\n",
    "            prompt=\"You are Alice, an expert at math. If you can't help, hand off to Bob.\",\n",
    "            name=\"Alice\",\n",
    "        )\n",
    "\n",
    "\n",
    "    # create_swarm(agents: list[CompiledStateGraph], *, default_active_agent: str, state_schema: StateSchemaType = SwarmState) -> StateGraph: \n",
    "        # This function builds the overall multi-agent graph (the \"swarm\").\n",
    "            # agents: A list of pre-compiled LangGraph agents (each is a CompiledStateGraph). These are your individual, specialized agents.\n",
    "            # default_active_agent: The agent that starts the conversation.\n",
    "            # state_schema: (Optional, but important) Defines the structure of the state that's passed between agents and within the swarm. \n",
    "                # The default SwarmState includes messages (conversation history) and active_agent (who's currently in control).\n",
    "\n",
    "            workflow = create_swarm(\n",
    "                agents=[agent1, agent2, ...],\n",
    "                default_active_agent=\"Alice\",  # or whichever agent to start with\n",
    "                state_schema=MySwarmState      # By default uses SwarmState\n",
    "            )\n",
    "\n",
    "    # add_active_agent_router(builder: StateGraph, *, route_to: list[str], default_active_agent: str) -> StateGraph: \n",
    "        # Adds routing logic to the graph to switch between agents based on the active_agent in the state.\n",
    "            # builder: The StateGraph instance being built.\n",
    "            # route_to: A list of valid agent names (node names) that can be routed to.\n",
    "            # default_active_agent: The agent to start with if no agent is active.\n",
    "\n",
    "    # InMemorySaver():  This is a checkpointer. It's responsible for storing and retrieving the conversation state (including messages and \n",
    "        # active_agent).  InMemorySaver keeps it in memory (good for testing, not for production).  LangGraph also supports other checkpointers \n",
    "        # (e.g., Redis, databases).\n",
    "\n",
    "        from langgraph.checkpoint.memory import InMemorySaver\n",
    "        from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "        checkpointer = InMemorySaver()  # short-term memory\n",
    "        store = InMemoryStore()         # optional long-term store\n",
    "\n",
    "        app = swarm_workflow.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            store=store\n",
    "        )\n",
    "\n",
    "    # SwarmState:  A TypedDict (from typing_extensions) that defines the structure of the state.  By default, it includes:\n",
    "        # messages: Annotated[list[AnyMessage], add_messages] - The conversation history, using LangChain's AnyMessage type. The add_messages \n",
    "            # annotation is important for LangGraph to know how to update this list.\n",
    "        # active_agent:str The current active agent.\n",
    "        from langgraph_swarm.swarm import SwarmState\n",
    "\n",
    "        # Minimal example: SwarmState is a typed dictionary that includes\n",
    "        # \"messages\" and \"active_agent\" by default.\n",
    "        class MySwarmState(SwarmState):\n",
    "            # Optionally, you can add any other keys here if needed\n",
    "            pass\n",
    "\n",
    "\n",
    "# --------------------------------------------- Customizing Handoff Tools ----------------------------------------------------\n",
    "from typing import Annotated\n",
    "from langchain_core.tools import tool, BaseTool, InjectedToolCallId\n",
    "from langgraph.prebuilt import InjectedState\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langgraph.types import Command\n",
    "\n",
    "def create_custom_handoff_tool(agent_name: str) -> BaseTool:\n",
    "    @tool(name=\"custom_transfer\", description=f\"Custom Transfer to {agent_name}\")\n",
    "    def handoff(\n",
    "        extra_context: Annotated[str, \"Additional context for the next agent.\"],\n",
    "        state: Annotated[dict, InjectedState],\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ):\n",
    "        tool_msg = ToolMessage(\n",
    "            content=f\"Transferred with extra context: {extra_context}\",\n",
    "            name=\"custom_transfer\",\n",
    "            tool_call_id=tool_call_id,\n",
    "        )\n",
    "        return Command(\n",
    "            goto=agent_name,\n",
    "            graph=Command.PARENT,\n",
    "            update={\n",
    "                \"messages\": state[\"messages\"] + [tool_msg],\n",
    "                \"active_agent\": agent_name,\n",
    "                \"some_extra_field\": extra_context,\n",
    "            },\n",
    "        )\n",
    "    return handoff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "from collections import defaultdict\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langgraph_swarm import create_handoff_tool, create_swarm\n",
    "import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "# --- Mock Data (Replace with real data/APIs) ---\n",
    "reservations = defaultdict(lambda: {\"flights\": [], \"hotels\": []})\n",
    "tomorrow = (datetime.date.today() + datetime.timedelta(days=1)).isoformat()\n",
    "\n",
    "flights_data = [\n",
    "  {\"id\": \"F123\", \"from\": \"BOS\", \"to\": \"JFK\", \"date\": tomorrow, \"airline\": \"JetBlue\"},\n",
    "  {\"id\": \"F456\", \"from\": \"LAX\", \"to\": \"SFO\", \"date\": tomorrow, \"airline\": \"United\"},\n",
    "]\n",
    "\n",
    "hotels_data = [\n",
    "  {\"id\": \"H123\", \"city\": \"New York\", \"name\": \"The Plaza\", \"stars\": 5},\n",
    "  {\"id\": \"H456\", \"city\": \"Los Angeles\", \"name\": \"The Beverly Hills Hotel\", \"stars\": 5},\n",
    "]\n",
    "\n",
    "# --- Flight Agent Tools ---\n",
    "def search_flights(departure_airport: str, arrival_airport: str, date: str) -> list[dict]:\n",
    "    \"\"\"Searches for flights based on departure, arrival, and date.\"\"\"\n",
    "    # In a real application, this would query a database or API.\n",
    "    results = []\n",
    "    for flight in flights_data:\n",
    "        if (flight[\"from\"] == departure_airport and\n",
    "            flight[\"to\"] == arrival_airport and\n",
    "            flight[\"date\"] == date):\n",
    "            results.append(flight)\n",
    "    return results\n",
    "\n",
    "def book_flight(flight_id: str, config:dict) -> str:\n",
    "    \"\"\"Books a flight given its ID.\"\"\"\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    for flight in flights_data:\n",
    "      if flight[\"id\"] == flight_id:\n",
    "          reservations[user_id][\"flights\"].append(flight)\n",
    "          return f\"Booked flight {flight_id}.\"\n",
    "    return f\"Could not book flight {flight_id}.\"\n",
    "\n",
    "# --- Hotel Agent Tools ---\n",
    "def search_hotels(city: str) -> list[dict]:\n",
    "    \"\"\"Searches for hotels in a given city.\"\"\"\n",
    "    # In a real application, this would query a database or API.\n",
    "    results = []\n",
    "    for hotel in hotels_data:\n",
    "      if hotel[\"city\"] == city:\n",
    "          results.append(hotel)\n",
    "    return results\n",
    "\n",
    "def book_hotel(hotel_id: str, config: dict) -> str:\n",
    "    \"\"\"Books a hotel given its ID.\"\"\"\n",
    "    user_id = config[\"configurable\"].get(\"user_id\")\n",
    "    for hotel in hotels_data:\n",
    "        if hotel[\"id\"] == hotel_id:\n",
    "            reservations[user_id][\"hotels\"].append(hotel)\n",
    "            return f\"Booked hotel {hotel_id}.\"\n",
    "    return f\"Could not book hotel {hotel_id}.\"\n",
    "\n",
    "# --- Create Handoff Tools ---\n",
    "transfer_to_hotel = create_handoff_tool(\n",
    "    agent_name=\"HotelAgent\",\n",
    "    description=\"Transfer to the hotel booking assistant for help with finding and booking hotels.\",\n",
    ")\n",
    "\n",
    "transfer_to_flight = create_handoff_tool(\n",
    "    agent_name=\"FlightAgent\",\n",
    "    description=\"Transfer to the flight booking assistant for help with finding and booking flights.\",\n",
    ")\n",
    "\n",
    "# --- Create Agents ---\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")  # Or any other suitable model\n",
    "\n",
    "flight_agent = create_react_agent(\n",
    "    llm,\n",
    "    [search_flights, book_flight, transfer_to_hotel],\n",
    "    prompt=\"You are a helpful flight booking assistant. Help users find and book flights.\",\n",
    "    name=\"FlightAgent\",\n",
    ")\n",
    "\n",
    "hotel_agent = create_react_agent(\n",
    "    llm,\n",
    "    [search_hotels, book_hotel, transfer_to_flight],\n",
    "    prompt=\"You are a helpful hotel booking assistant. Help users find and book hotels.\",\n",
    "    name=\"HotelAgent\",\n",
    ")\n",
    "\n",
    "# --- Create Swarm ---\n",
    "checkpointer = InMemorySaver()\n",
    "workflow = create_swarm(\n",
    "    [flight_agent, hotel_agent],\n",
    "    default_active_agent=\"FlightAgent\"\n",
    ")\n",
    "\n",
    "app = workflow.compile(checkpointer=checkpointer)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------ Run the swarm ------------------------------------\n",
    "# --- Example Interaction ---\n",
    "config = {\"configurable\": {\"thread_id\": \"user123\", \"user_id\": \"user123\"}}  # Use a consistent thread ID!\n",
    "result = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"I need a flight from Boston to NYC tomorrow.\"}]}, config)\n",
    "print(result)\n",
    "\n",
    "result = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"And a 5-star hotel.\"}]}, config) # Keep the same config\n",
    "print(result)\n",
    "\n",
    "result = app.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What's my reservation?\"}]}, config) # Keep the same config\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph CodeAct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langgraph-codeact\n",
    "# also install langchain-openai\n",
    "pip install langchain langchain-anthropic\n",
    "\n",
    "\n",
    "# --------------------------------------------- Tools Setup ------------------------------------------------\n",
    "\n",
    "# Define your tools (functions the LLM can use)\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def search_database(query: str) -> list:\n",
    "    \"\"\"Search the database with the given query.\"\"\"\n",
    "    # Implementation\n",
    "    return results\n",
    "\n",
    "# Create a list of all tools\n",
    "tools = [search_database, other_tool, ...]\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------- Setting Up the CodeAct Graph ------------------------------------------------\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langgraph_codeact import create_codeact\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize LLM\n",
    "model = init_chat_model(\"claude-3-7-sonnet-latest\", model_provider=\"anthropic\")\n",
    "\n",
    "# Create CodeAct instance\n",
    "code_act = create_codeact(model, tools, sandbox_function)\n",
    "\n",
    "# Compile with checkpointer to maintain state between interactions\n",
    "agent = code_act.compile(checkpointer=MemorySaver())\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------- Running the CodeAct Graph ------------------------------------------------\n",
    "# For final output only\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"Your query here\"}]\n",
    "})\n",
    "\n",
    "# For streaming output\n",
    "for typ, chunk in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"Your query here\"}]},\n",
    "    stream_mode=[\"values\", \"messages\"],\n",
    "    config={\"configurable\": {\"thread_id\": 1}}\n",
    "):\n",
    "    if typ == \"messages\":\n",
    "        print(chunk[0].content, end=\"\")\n",
    "    elif typ == \"values\":\n",
    "        print(\"\\nFinal result:\", chunk)\n",
    "\n",
    "\n",
    "# ------------------------------------------ Code Sandbox Implementation ------------------------------------------------\n",
    "\n",
    "# Basic sandbox function (DO MOT USE IN PRODUCTION)\n",
    "    # DO NOT USE THIS IN PRODUCTION - EXAMPLE ONLY\n",
    "    def unsafe_eval(code: str, _locals: dict) -> tuple[str, dict]:\n",
    "        # This is NOT safe for production use\n",
    "        import io, contextlib\n",
    "        original_keys = set(_locals.keys())\n",
    "        try:\n",
    "            with contextlib.redirect_stdout(io.StringIO()) as f:\n",
    "                exec(code, {}, _locals)\n",
    "            result = f.getvalue() or \"<code ran, no output>\"\n",
    "        except Exception as e:\n",
    "            result = f\"Error: {repr(e)}\"\n",
    "        \n",
    "        # Track new variables created during execution\n",
    "        new_keys = set(_locals.keys()) - original_keys\n",
    "        new_vars = {key: _locals[key] for key in new_keys}\n",
    "        return result, new_vars\n",
    "\n",
    "\n",
    "\n",
    "# RestrictedPython\n",
    "from RestrictedPython import compile_restricted, safe_globals\n",
    "\n",
    "def secure_sandbox(code: str, _locals: dict) -> tuple[str, dict]:\n",
    "    try:\n",
    "        byte_code = compile_restricted(code, filename=\"<inline>\", mode=\"exec\")\n",
    "        exec_globals = safe_globals.copy()\n",
    "        exec_globals.update({\n",
    "            \"_getattr_\": getattr,\n",
    "            \"_write_\": lambda x: None,  # Replace with output capture\n",
    "            \"__builtins__\": {\"__import__\": lambda name: None}  # Block imports\n",
    "        })\n",
    "        \n",
    "        # Add your tools to the globals\n",
    "        for tool_name, tool_func in _locals.items():\n",
    "            if callable(tool_func):\n",
    "                exec_globals[tool_name] = tool_func\n",
    "                \n",
    "        # Execute the code\n",
    "        exec(byte_code, exec_globals)\n",
    "        \n",
    "        # Capture new variables\n",
    "        new_vars = {k: v for k, v in exec_globals.items() \n",
    "                   if k not in safe_globals and not k.startswith('_')}\n",
    "        \n",
    "        return \"Code executed successfully\", new_vars\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", {}\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "# Docker-based Sandbox\n",
    "import docker\n",
    "import uuid\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def docker_sandbox(code: str, _locals: dict) -> tuple[str, dict]:\n",
    "    client = docker.from_env()\n",
    "    \n",
    "    # Create a temporary file with the code\n",
    "    run_id = str(uuid.uuid4())\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    code_file = os.path.join(temp_dir, f\"code_{run_id}.py\")\n",
    "    \n",
    "    # Serialize tools and variables to be imported in the container\n",
    "    tool_code = \"# Tool definitions\\n\"\n",
    "    for name, func in _locals.items():\n",
    "        if callable(func):\n",
    "            tool_code += f\"# Tool: {name}\\n\"\n",
    "    \n",
    "    with open(code_file, 'w') as f:\n",
    "        f.write(tool_code + \"\\n\" + code)\n",
    "    \n",
    "    # Run in container with strict limitations\n",
    "    try:\n",
    "        container = client.containers.run(\n",
    "            \"python:3.10-slim\",\n",
    "            command=f\"python /code/code_{run_id}.py\",\n",
    "            volumes={temp_dir: {'bind': '/code', 'mode': 'ro'}},\n",
    "            mem_limit=\"50m\",\n",
    "            cpu_quota=10000,  # 10% of CPU\n",
    "            network_mode=\"none\",  # No network access\n",
    "            detach=True\n",
    "        )\n",
    "        \n",
    "        # Wait for execution with timeout\n",
    "        result = container.wait(timeout=5)\n",
    "        output = container.logs().decode('utf-8')\n",
    "        container.remove()\n",
    "        \n",
    "        # Placeholder for returning variables (needs additional implementation)\n",
    "        # In practice, you would need to serialize output variables from container\n",
    "        return output, {}\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", {}\n",
    "    finally:\n",
    "        # Clean up\n",
    "        os.remove(code_file)\n",
    "        os.rmdir(temp_dir)\n",
    "\n",
    "\n",
    "\n",
    "# PyPy Sandbox\n",
    "from pypy.interpreter.gateway import unwrap_spec\n",
    "from pypy.translator.sandbox.sandlib import SandboxedProc\n",
    "\n",
    "def pypy_sandbox(code: str, _locals: dict) -> tuple[str, dict]:\n",
    "    # Implementation details would depend on PyPy setup\n",
    "    # This is a simplified skeleton\n",
    "    \n",
    "    proc = SandboxedProc(['pypy'])\n",
    "    # Setup sandbox with limited resources and capabilities\n",
    "    \n",
    "    # Execute the code\n",
    "    try:\n",
    "        result = proc.interact(code)\n",
    "        # Handle capturing new variables\n",
    "        return result, {}\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", {}\n",
    "    \n",
    "\n",
    "# Pysandbox (Using separate process)\n",
    "import subprocess\n",
    "import json\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "def process_sandbox(code: str, _locals: dict) -> tuple[str, dict]:\n",
    "    # Create a file with the code\n",
    "    with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as temp:\n",
    "        # Prepare tool definitions to be available\n",
    "        tools_code = \"# Tool definitions\\n\"\n",
    "        for name, func in _locals.items():\n",
    "            if callable(func):\n",
    "                tools_code += f\"# {name} is available as a tool\\n\"\n",
    "        \n",
    "        # Write the code to the file\n",
    "        temp.write(tools_code + \"\\n\" + code)\n",
    "        temp_name = temp.name\n",
    "    \n",
    "    try:\n",
    "        # Execute in a separate process with resource limits\n",
    "        result = subprocess.run(\n",
    "            [\"python\", temp_name],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=5  # 5 second timeout\n",
    "        )\n",
    "        \n",
    "        output = result.stdout\n",
    "        if result.stderr:\n",
    "            output += f\"\\nErrors: {result.stderr}\"\n",
    "            \n",
    "        # In a real implementation, you'd need to serialize variables\n",
    "        # from the subprocess back to the main process\n",
    "        return output, {}\n",
    "    \n",
    "    except subprocess.TimeoutExpired:\n",
    "        return \"Execution timed out\", {}\n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", {}\n",
    "    finally:\n",
    "        os.unlink(temp_name)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Graph Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage  # Import AIMessage for assistant responses\n",
    "\n",
    "#----------------------------------------------------- Graoh Invoke-----------------------------------\n",
    "\n",
    "# **Input Collection**\n",
    "user_input = \"I want to travel from New York to Paris on 2023-12-15 and return on 2023-12-22. There are 2 adults and 1 child. My budget is $5000. I need 1 room. I prefer a hotel with free breakfast and a swimming pool. I also want to visit the museums and enjoy local cuisine, and go to the club at night. I might also want a massage.\"    \n",
    "\n",
    "# **Input State**\n",
    "input_state = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "# **Graph Invocation**\n",
    "output = graph.invoke(input_state, {\"recursion_limit\": 300})\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------- Graph Stream-----------------------------------\n",
    "# Stream the Graph\n",
    "# Define the input data\n",
    "input_data = {\n",
    "    \"messages\": [HumanMessage(content=\"I want to travel from New York to Paris on 2023-12-15 and return on 2023-12-22. There are 2 adults and 1 child. My budget is $5000. I need 1 room. I prefer a hotel with free breakfast and a swimming pool. I also want to visit the museums and enjoy local cuisine, and go to the club at night. I might also want a massage.\")]\n",
    "}\n",
    "\n",
    "# Define the configuration (e.g., recursion limit)\n",
    "config = {\"recursion_limit\": 10}\n",
    "\n",
    "# Stream the execution\n",
    "events = graph.stream(input_data, config)\n",
    "for event in events:\n",
    "    print(event)\n",
    "    \n",
    "#--------------------------------------------------------- Graph Stream with Memory-----------------------------------\n",
    "\n",
    "\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "try:\n",
    "    for event in graph.stream(input_state, thread_config, stream_mode = \"values\", ):\n",
    "        messages = event['messages'][-1]\n",
    "        # Filter and print only the AIMessage content\n",
    "        if isinstance(messages, AIMessage):\n",
    "            print(messages.content)\n",
    "\n",
    "except GraphRecursionError:\n",
    "    print(\"Recursion Error\")\n",
    "\n",
    "\n",
    "#--------------------------------------------------------- Graph Stream with pretty print-----------------------------------\n",
    "\n",
    "from langgraph.pregel.remote import RemoteGraph\n",
    "from langchain_core.messages import convert_to_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "# Connect to the deployment\n",
    "remote_graph = RemoteGraph(graph_name, url=local_deployment_url)\n",
    "\n",
    "user_input = \"Hi I'm Lance. I live in San Francisco with my wife and have a 1 year old.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Test-Deployment-User\"}}\n",
    "for chunk in remote_graph.stream({\"messages\": [HumanMessage(content=user_input)]}, stream_mode=\"values\", config=config):\n",
    "    convert_to_messages(chunk[\"messages\"])[-1].pretty_print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------- Tool with Input Class and Tool Class ----------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Define the input class\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str\n",
    "    param2: int\n",
    "\n",
    "# Define the tool class\n",
    "class MyTool:\n",
    "    def __call__(self, input: MyToolInput) -> str:\n",
    "        # Tool logic here\n",
    "        return f\"Processed: {input.param1}, {input.param2}\"\n",
    "\n",
    "my_tool = Tool(\n",
    "    name=\"my_tool\",\n",
    "    func=MyTool(),\n",
    "    description=\"Tool description.\",\n",
    "    args_schema=MyToolInput\n",
    ")\n",
    "\n",
    "# Call the tool\n",
    "result = my_tool.func(MyToolInput(param1=\"value1\", param2=42))\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------- Tool Using @tool Decorator ----------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def my_tool(param1: str, param2: int) -> str:\n",
    "    \"\"\"Tool description.\"\"\"\n",
    "    return f\"Processed: {param1}, {param2}\"\n",
    "\n",
    "# Call the tool\n",
    "result = my_tool({\"param1\": \"value1\", \"param2\": 42})\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- Tool with Structured Inputs Using \"BaseTool\" ----------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str\n",
    "    param2: int\n",
    "\n",
    "class MyTool(BaseTool):\n",
    "    name: str = \"my_tool\"\n",
    "    description: str = \"Tool description.\"\n",
    "\n",
    "    def _run(self, param1: str, param2: int) -> str:\n",
    "        \"\"\"Tool logic.\"\"\"\n",
    "        return f\"Processed: {param1}, {param2}\"\n",
    "\n",
    "# Create an instance of the tool\n",
    "my_tool = MyTool()\n",
    "\n",
    "# Call the tool\n",
    "result = my_tool.run({\"param1\": \"value1\", \"param2\": 42})\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------  Tool call with Agent ----------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def my_tool_func(param1: str, param2: int) -> str:\n",
    "    \"\"\"Tool logic.\"\"\"\n",
    "    return f\"Processed: {param1}, {param2}\"\n",
    "\n",
    "# Create the tool\n",
    "my_tool = Tool(\n",
    "    name=\"my_tool\",\n",
    "    func=my_tool_func,\n",
    "    description=\"Tool description.\"\n",
    ")\n",
    "\n",
    "# Add the tool to an agent\n",
    "tools = [my_tool]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\")   # you can also use react agent here or any agent\n",
    "\n",
    "# Call the tool via the agent\n",
    "result = agent.invoke(\"Call my_tool with param1='value1' and param2=42\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#--------------------------------------------- Style 1 ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "query_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, output a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", query_gen_system),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#--------------------------------------------- Style 2 ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful customer support assistant for Solar Panels Belgium.\n",
    "            You should get the following information from them:\n",
    "            - monthly electricity cost\n",
    "            If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "            After you are able to discern all the information, call the relevant tool.\n",
    "            ''',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#--------------------------------------------- Style 3 ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "template = ''' \n",
    "You are a travel suggestion agent. Answer the user's questions based on their travel preferences. \n",
    "If you need to find information about a specific destination, use the search_tool. Understand that the information was retrieved from the web,\n",
    "interpret it, and generate a response accordingly.\n",
    "\n",
    "Answer the following questions as best as you can. You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "\"Question\": the input question you must answer\n",
    "\"Thought\": your reasoning about what to do next\n",
    "\"Action\": the action you should take, one of [{tool_names}] (if no action is needed, write \"None\")\n",
    "\"Action Input\": the input to the action (if no action is needed, write \"None\")\n",
    "\"Observation\": the result of the action (if no action is needed, write \"None\")\n",
    "\"Thought\": your reasoning after observing the action\n",
    "\"Final Answer\": the final answer to the original input question\n",
    "\n",
    "Ensure every Thought is followed by an Action, Action Input, and Observation. If no tool is needed, explicitly write \"None\" for Action, Action Input, and Observation.\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=search_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": latest_query,\n",
    "    \"agent_scratchpad\": \"\"  # Initialize with an empty scratchpad\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Display or Visualize LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tNode_A(Node A)\n",
      "\tNode_B(Node B)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\tNode_A --> Node_B;\n",
      "\tNode_B --> __end__;\n",
      "\t__start__ --> Node_A;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "print(app.get_graph().draw_mermaid())       # Converting a Graph to a Mermaid Diagram\n",
    "\n",
    "\n",
    "#-------------------------Using Mermaid.Ink--------------------------------\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Using Mermaid + Pyppeteer--------------------------------\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            curve_style=CurveStyle.LINEAR,\n",
    "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
    "            wrap_label_n_words=9,\n",
    "            output_file_path=None,\n",
    "            draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "            background_color=\"white\",\n",
    "            padding=10,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Using Graphviz--------------------------------\n",
    "%pip install pygraphviz\n",
    "\n",
    "display(Image(app.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "\n",
    "# requirements.txt\n",
    "langgraph==0.1.0\n",
    "langchain_core==0.1.0\n",
    "\n",
    "\n",
    "# Langgraph.json\n",
    "{\n",
    "  \"name\": \"todo_agent\",\n",
    "  \"description\": \"A simple ToDo list agent\",\n",
    "  \"graphs\": {\n",
    "    \"todo_agent\": {\n",
    "      \"entrypoint\": \"agent\",\n",
    "      \"file\": \"agent.py\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY . .\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "CMD [\"langgraph\", \"serve\", \"--host\", \"0.0.0.0\", \"--port\", \"8123\"]\n",
    "\n",
    "\n",
    "\n",
    "# or\n",
    "# Use docker-compose.yml to create containers for Redis, PostgreSQL, and the LangGraph API.\n",
    "$ cd module-6/deployment\n",
    "$ docker compose up\n",
    "\n",
    "# Building Docker Image\n",
    "$ langgraph build -t todo_agent\n",
    "\n",
    "# Run the Docker Image\n",
    "docker run -p 8123:8123 todo_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Deployment Setup--------------------------------\n",
    "# Use docker-compose.yml to create containers for Redis, PostgreSQL, and the LangGraph API.\n",
    "$ cd module-6/deployment\n",
    "$ docker compose up\n",
    "\n",
    "# Building Docker Image\n",
    "$ langgraph build -t todo_agent\n",
    "\n",
    "# Run the Docker Image\n",
    "docker run -p 8123:8123 todo_agent\n",
    "\n",
    "#-------------------------Assistants-------------------------------------\n",
    "#---------------------\n",
    "# Creating Assistants (Connect to the Deployment)\n",
    "#---------------------\n",
    "from langgraph_sdk import get_client\n",
    "client = get_client(url=\"http://localhost:8123\")\n",
    "\n",
    "# Create a personal assistant\n",
    "personal_assistant = await client.assistants.create(\n",
    "    \"task_maistro\",\n",
    "    config={\"configurable\": {\"todo_category\": \"personal\"}}\n",
    ")\n",
    "\n",
    "#---------------------\n",
    "# Updating Assistants\n",
    "#---------------------\n",
    "personal_assistant = await client.assistants.update(\n",
    "    personal_assistant[\"assistant_id\"],\n",
    "    config={\"configurable\": {\"todo_category\": \"personal\", \"user_id\": \"lance\"}}\n",
    ")\n",
    "\n",
    "#---------------------\n",
    "# Searching and Deleting Assistants\n",
    "#---------------------\n",
    "assistants = await client.assistants.search()\n",
    "for assistant in assistants:\n",
    "    print(assistant['assistant_id'], assistant['config'])\n",
    "    \n",
    "\n",
    "await client.assistants.delete(\"assistant_id\")  # Delete an assistant\n",
    "\n",
    "\n",
    "#-------------------------Threads and Runs--------------------------------\n",
    "\n",
    "#---------------------\n",
    "# Creating Threads\n",
    "#---------------------\n",
    "thread = await client.threads.create()\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Running a Graph\n",
    "#---------------------\n",
    "run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"Add a ToDo\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"Test\"}}\n",
    ")\n",
    "\n",
    "#---------------------\n",
    "# Streaming Runs\n",
    "#---------------------\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"What ToDo should I focus on?\")]},\n",
    "    stream_mode=\"messages-tuple\"\n",
    "):\n",
    "    if chunk.event == \"messages\":\n",
    "        print(chunk.data)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Background Runs\n",
    "#---------------------\n",
    "run = await client.runs.create(thread[\"thread_id\"], \"task_maistro\", input={\"messages\": [...]})\n",
    "print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))\n",
    "\n",
    "\n",
    "#-------------------------Double Texting Strategies--------------------------------\n",
    "\n",
    "#---------------------\n",
    "# Reject\n",
    "#---------------------\n",
    "await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"New ToDo\")]},\n",
    "    multitask_strategy=\"reject\" # Reject the current task if another task is already in progress ()\n",
    ")\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Enqueue\n",
    "#---------------------\n",
    "await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"New ToDo\")]},\n",
    "    multitask_strategy=\"enqueue\"    # Enqueue new runs (or Interrupt, Rollback etc.)\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Human-in-the-Loop--------------------------------\n",
    "\n",
    "#---------------------\n",
    "# Forking Threads\n",
    "#---------------------\n",
    "copied_thread = await client.threads.copy(thread[\"thread_id\"])\n",
    "\n",
    "#---------------------\n",
    "# Editing State\n",
    "#---------------------\n",
    "forked_input = {\"messages\": HumanMessage(content=\"Updated ToDo\", id=message_id)}\n",
    "await client.threads.update_state(\n",
    "    thread[\"thread_id\"],\n",
    "    forked_input,\n",
    "    checkpoint_id=checkpoint_id\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Functional API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The Functional API is designed to simplify the creation of AI workflows by leveraging traditional programming constructs like loops \n",
    "# and conditionals, while still providing access to LangGraph's powerful features such as human-in-the-loop , persistence/memory , \n",
    "# and streaming .\n",
    "\n",
    "\n",
    "# 1. Key Concepts\n",
    "    # 1.1 Entrypoint\n",
    "        # Definition : The starting point of a workflow. It encapsulates the workflow logic and manages the execution flow, \n",
    "        # including handling long-running tasks and interrupts.\n",
    "        \n",
    "        # Key Features :\n",
    "            # Manages the lifecycle of the workflow.\n",
    "            # Supports interrupts for human-in-the-loop interactions.\n",
    "            # Can be configured with a checkpointer for persistence.\n",
    "\n",
    "            from langgraph.func import entrypoint\n",
    "            from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "            @entrypoint(checkpointer=MemorySaver(), store=InMemoryStore())\n",
    "            def workflow(input_data: dict, *, previous: Any = None, store: BaseStore):\n",
    "                \"\"\"Main workflow entry point\"\"\"\n",
    "                # Workflow logic\n",
    "                return result\n",
    "            # Parameters:\n",
    "                # checkpointer: Persistence layer (MemorySaver, RedisCheckpointer, etc.)\n",
    "                # store: Long-term storage interface\n",
    "                # stream_mode: Configure streaming behavior (updates/messages/custom)\n",
    "\n",
    "    # 1.2 Task\n",
    "        # Definition : A discrete unit of work, such as an API call or data processing step, that can be executed asynchronously.\n",
    "        # Key Features :\n",
    "            # Returns a future-like object that can be awaited for results.\n",
    "            # Can be used within an entrypoint to perform specific actions.\n",
    "\n",
    "            from langgraph.func import task\n",
    "\n",
    "            @task\n",
    "            def process_data(input_data: dict) -> dict:\n",
    "                \"\"\"Long-running or complex processing\"\"\"\n",
    "                time.sleep(2)  # Simulate long operation\n",
    "                return {\"processed\": True}\n",
    "\n",
    "    # 1.3 Human-in-the-Loop\n",
    "        # Definition : A workflow that pauses for human input at critical stages, allowing for review, validation, or corrections.\n",
    "        # Key Features :\n",
    "            # Uses the interrupt function to pause the workflow indefinitely.\n",
    "            # Resumes with the Command primitive, skipping previously completed tasks.\n",
    "\n",
    "            from langgraph.types import interrupt\n",
    "\n",
    "            @task\n",
    "            def generate_draft(topic: str) -> str:\n",
    "                return f\"An essay about {topic}\"\n",
    "\n",
    "            @entrypoint(checkpointer=MemorySaver())\n",
    "            def document_approval_flow(content: str):\n",
    "                draft = generate_draft(content).result()\n",
    "                approval = interrupt({\n",
    "                    \"draft\": draft,\n",
    "                    \"action\": \"Approve/Reject with comments\",\n",
    "                    \"deadline\": \"2024-03-01\"\n",
    "                })\n",
    "                \n",
    "                if approval.get(\"status\") == \"approved\":\n",
    "                    return publish_draft(draft)\n",
    "                else:\n",
    "                    return revise_draft(draft, approval[\"comments\"])\n",
    "                \n",
    "    # 1.4 Persistence/Memory\n",
    "        # Definition : The ability to store and retrieve data across different interactions or workflow executions.\n",
    "        # Key Features :\n",
    "            # Short-term memory : Maintains conversation history or state within a single workflow execution.\n",
    "            # Long-term memory : Stores user preferences or other persistent data across multiple interactions.\n",
    "\n",
    "            # Short-term Memory (Conversation History):\n",
    "            @entrypoint(checkpointer=MemorySaver())\n",
    "            def chat_agent(query: str, *, previous: list = None):\n",
    "                history = previous or []\n",
    "                history.append(HumanMessage(content=query))\n",
    "                response = llm.invoke(history)\n",
    "                history.append(AIMessage(content=response))\n",
    "                return entrypoint.final(value=response, save=history)\n",
    "\n",
    "            # Long-term Memory (User Preferences):\n",
    "            @entrypoint(checkpointer=MemorySaver(), store=RedisStore())\n",
    "            def personalized_recommendation(user_id: str, query: str, *, store: BaseStore):\n",
    "                preferences = store.get(f\"user:{user_id}:preferences\") or {}\n",
    "                recommendations = generate_recs(query, preferences)\n",
    "                store.set(f\"user:{user_id}:preferences\", update_prefs(preferences, query))\n",
    "                return recommendations\n",
    "\n",
    "    # 1.5 Streaming\n",
    "        # Definition : Real-time updates sent to the client as the workflow progresses.\n",
    "        # Key Features :\n",
    "            # Supports streaming of workflow progress , LLM tokens , and custom updates .\n",
    "            # Uses the stream method to send real-time data.\n",
    "            \n",
    "            @entrypoint(checkpointer=MemorySaver())\n",
    "            def real_time_analysis(input_data: dict, writer: StreamWriter):\n",
    "                writer(\"Starting analysis...\", stream=\"custom\")\n",
    "                for chunk in data_processor.stream(input_data):\n",
    "                    writer(chunk, stream=\"updates\")\n",
    "                    processed = transform_data(chunk)\n",
    "                    writer(processed, stream=\"messages\")\n",
    "                return {\"status\": \"complete\"}\n",
    "\n",
    "            # Client-side consumption\n",
    "            for chunk in workflow.stream(inputs, stream_mode=[\"custom\", \"updates\", \"messages\"]):\n",
    "                handle_stream_chunk(chunk)\n",
    "\n",
    "\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Advanced Patterns\n",
    "    # Multi-stage Approval Workflow\n",
    "    @entrypoint(checkpointer=RedisCheckpointer())\n",
    "    def content_creation_pipeline(topic: str):\n",
    "        draft = research_topic(topic).result()\n",
    "        editor_review = interrupt({\"draft\": draft}, role=\"editor\")\n",
    "        revised = incorporate_feedback(draft, editor_review))\n",
    "        \n",
    "        legal_check = legal_review(revised).result()\n",
    "        if legal_check[\"approved\"]:\n",
    "            publish(revised)\n",
    "        else:\n",
    "            return {\"status\": \"legal_blocked\", \"reasons\": legal_check[\"reasons\"]}\n",
    "        \n",
    "        \n",
    "    # Context-Aware Processing\n",
    "    @entrypoint(checkpointer=MemorySaver(), store=PostgresStore())\n",
    "    def contextual_processing(user_id: str, query: str):\n",
    "        context = store.get(f\"user:{user_id}:context\") or {}\n",
    "        enhanced_query = enrich_query(query, context)\n",
    "        \n",
    "        result = process_query(enhanced_query).result()\n",
    "        update_context(user_id, result, store)\n",
    "        \n",
    "        return format_response(result, context)\n",
    "\n",
    "\n",
    "# Observability & Debugging\n",
    "    # Enable LangSmith tracing\n",
    "    import os\n",
    "    os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "    os.environ[\"LANGCHAIN_PROJECT\"] = \"functional-api-workflows\"\n",
    "\n",
    "    @task\n",
    "    def monitored_task(input_data):\n",
    "        \"\"\"Auto-logged task execution\"\"\"\n",
    "        return process(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Use Case\n",
    "\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "import datetime\n",
    "\n",
    "from langchain_community.chat_models import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.func import entrypoint, task\n",
    "from langgraph.types import StreamWriter, interrupt\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup LLM and Tools\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0.7)\n",
    "search_tool = TavilySearchResults()\n",
    "\n",
    "# Custom Tools\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Retrieves the current weather for a given location.\"\"\"\n",
    "    api_key = os.getenv(\"WEATHER_API_KEY\") # Replace with your weather api key.\n",
    "    if not api_key:\n",
    "        return \"Weather API key not found. Please set WEATHER_API_KEY in .env\"\n",
    "    \n",
    "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={api_key}&units=metric\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        data = response.json()\n",
    "        if data[\"cod\"] != \"404\":\n",
    "            main_data = data[\"main\"]\n",
    "            weather_data = data[\"weather\"][0]\n",
    "            temperature = main_data[\"temp\"]\n",
    "            description = weather_data[\"description\"]\n",
    "            return f\"Current weather in {location}: {temperature}°C, {description}\"\n",
    "        else:\n",
    "            return \"Location not found.\"\n",
    "    except Exception as e:\n",
    "        return f\"Error fetching weather: {e}\"\n",
    "\n",
    "def find_activities(location: str, interests: str) -> List[str]:\n",
    "    \"\"\"Finds activities based on location and interests.\"\"\"\n",
    "    query = f\"Activities in {location} related to {interests}\"\n",
    "    results = search_tool.run(query)\n",
    "    # Handle the case where 'url' key might be missing\n",
    "    processed_results = []\n",
    "    for result in results:\n",
    "        if isinstance(result, dict) and 'content' in result:\n",
    "            processed_results.append(result['content'])\n",
    "        elif isinstance(result, str):\n",
    "            processed_results.append(result)\n",
    "        else:\n",
    "            processed_results.append(\"No content found.\")\n",
    "    return processed_results\n",
    "\n",
    "def book_activity(activity: str, date: str, time: str) -> str:\n",
    "    \"\"\"Simulates booking an activity.\"\"\"\n",
    "    return f\"Booking confirmed for {activity} on {date} at {time}.\"\n",
    "\n",
    "# Define Tasks\n",
    "@task\n",
    "def collect_user_profile(user_message: str, *, previous: List[Dict[str, str]] = None) -> List[Dict[str, str]]:\n",
    "    \"\"\"Collects detailed user profile.\"\"\"\n",
    "    messages = previous or []\n",
    "    messages.append({\"role\": \"user\", \"content\": user_message})\n",
    "    response = llm.invoke(messages).content\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return messages\n",
    "\n",
    "@task\n",
    "def generate_itinerary(profile: List[Dict[str, str]], location: str) -> str:\n",
    "    \"\"\"Generates a dynamic itinerary.\"\"\"\n",
    "    interests = [msg['content'] for msg in profile if \"interests\" in msg['content'].lower()]\n",
    "    interests = interests[0] if interests else \"general interests\"\n",
    "\n",
    "    # Invoke the tools directly\n",
    "    activities = find_activities(location, interests)\n",
    "    weather = get_weather(location)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an adventure planner. Create a personalized itinerary.\"),\n",
    "        (\"user\", f\"Profile: {profile[-1]['content']}. Location: {location}. Activities: {activities}. Weather: {weather}. Generate an itinerary.\"),\n",
    "    ])\n",
    "    response = llm.invoke(prompt.format_messages(profile=profile[-1]['content'], location=location, activities=activities, weather=weather)).content\n",
    "    return response\n",
    "\n",
    "@task\n",
    "def update_itinerary(itinerary: str, feedback: str) -> str:\n",
    "    \"\"\"Updates the itinerary based on user feedback.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are an adventure planner. Update the itinerary based on feedback.\"),\n",
    "        (\"user\", f\"Itinerary: {itinerary}. Feedback: {feedback}. Update the itinerary.\"),\n",
    "    ])\n",
    "    response = llm.invoke(prompt.format_messages(itinerary=itinerary, feedback=feedback)).content\n",
    "    return response\n",
    "\n",
    "# Define Entrypoint\n",
    "@entrypoint(checkpointer=MemorySaver())\n",
    "def adventure_curator(user_message: str, writer: StreamWriter) -> Dict[str, Any]:\n",
    "    \"\"\"Main adventure curation workflow.\"\"\"\n",
    "    writer(\"Collecting user profile...\")\n",
    "    profile = collect_user_profile(user_message).result()\n",
    "    print(f'User Profile: {profile}')\n",
    "    location = \"Paris\" #example, can be gathered from user.\n",
    "    writer(\"Generating itinerary...\")\n",
    "    itinerary = generate_itinerary(profile, location).result()\n",
    "    writer(\"Itinerary generated!\")\n",
    "    print(f'Generated Itinerary: {itinerary}')\n",
    "    approval = interrupt({\n",
    "        \"itinerary\": itinerary,\n",
    "        \"action\": \"Do you approve this itinerary? (yes/no)\",\n",
    "    })\n",
    "    \n",
    "    if approval.lower() == \"yes\":\n",
    "        writer(\"Itinerary approved!\")\n",
    "        return {\"itinerary\": itinerary, \"status\": \"approved\"}\n",
    "    else:\n",
    "        writer(\"Itinerary modified. Please provide feedback.\")\n",
    "        feedback = interrupt({\n",
    "            \"itinerary\": itinerary,\n",
    "            \"action\": \"Please provide feedback to modify the itinerary.\",\n",
    "        })\n",
    "        writer(\"Generating revised itinerary...\")\n",
    "        revised_itinerary = update_itinerary(itinerary, feedback).result()\n",
    "        return {\"itinerary\": revised_itinerary, \"status\": \"modified\"}\n",
    "\n",
    "# Run the Workflow\n",
    "user_input = \"I love hiking and historical sites. I prefer moderate activity levels and have a budget of $1000.\"\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for chunk in adventure_curator.stream(user_input, stream_mode=[\"custom\"], config=config):\n",
    "    print(chunk)\n",
    "\n",
    "# Simulate User Approval\n",
    "approval_input = \"no, make it more hiking focused\"\n",
    "\n",
    "if approval_input.lower() != \"yes\":\n",
    "    for chunk in adventure_curator.stream(approval_input, stream_mode=[\"custom\"], config=config):\n",
    "        print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain MCP Adapters Cheat Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LangChain MCP Adapters library, which enables seamless integration of Anthropic Model Context Protocol (MCP) tools with LangChain and \n",
    "# LangGraph. MCP offers a standardized way for AI models and external systems to interact.\n",
    "\n",
    "\n",
    "# Key Concepts:\n",
    "    # Model Context Protocol (MCP): A protocol that defines how AI models can interact with tools and external systems. \n",
    "        # It promotes interoperability and standardized tool usage.\n",
    "    # LangChain MCP Adapters: A library that wraps MCP tools, making them compatible with LangChain's tool interface and LangGraph agents.\n",
    "    # MCP Servers: Implementations of MCP that expose tools. Servers can be written in various languages (e.g., Python, JavaScript).\n",
    "    # MCP Clients: Applications that connect to MCP servers, discover available tools, and invoke them.\n",
    "    \n",
    "\n",
    "# Core Functionality:\n",
    "\n",
    "# Installing the Library:\n",
    "    pip install mcp\n",
    "    pip install langchain-mcp-adapters\n",
    "\n",
    "# MCP Servers:\n",
    "    https://github.com/modelcontextprotocol/servers\n",
    "    \n",
    "# MCP Servers:\n",
    "    # MCP Servers can be implemented using libraries like mcp.server.fastmcp (Python).\n",
    "    # Servers define \"tools\" using decorators (e.g., @mcp.tool()).\n",
    "    # Servers can use different transports (e.g., stdio, sse).\n",
    "    # Example (Python - MCP Server):\n",
    "    \n",
    "    from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "    mcp = FastMCP(\"Math\")\n",
    "\n",
    "    @mcp.tool()\n",
    "    def add(a: int, b: int) -> int:\n",
    "        \"\"\"Add two numbers\"\"\"\n",
    "        return a + b\n",
    "\n",
    "    @mcp.tool()\n",
    "    async def multiply(a: int, b: int) -> int:\n",
    "        \"\"\"Multiply two numbers\"\"\"\n",
    "        return a * b\n",
    "\n",
    "    if __name__ == \"__main__\":\n",
    "        mcp.run(transport=\"stdio\")\n",
    "        \n",
    "# MCP Clients:\n",
    "    # LangChain MCP Adapters provide client implementations to connect to MCP servers.\n",
    "    # Clients can connect to single or multiple servers.\n",
    "    # Clients can load MCP tools and use them within LangChain or LangGraph.\n",
    "    \n",
    "# Connecting to MCP Servers:\n",
    "    # langchain_mcp_adapters.client.MultiServerMCPClient: Connects to multiple servers with different transports.\n",
    "    # mcp.client.stdio.stdio_client: Connects to a server using standard input/output.\n",
    "    # mcp.client.sse.sse_client: Connects to a server using Server-Sent Events (SSE).\n",
    "    \n",
    "    # Example (Python - MCP Client - Single Server):\n",
    "    from mcp import ClientSession, StdioServerParameters\n",
    "    from mcp.client.stdio import stdio_client\n",
    "    from langchain_mcp_adapters.tools import load_mcp_tools\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[\"/path/to/math_server.py\"],  # Update with the path to your server\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read, write):\n",
    "        async with ClientSession(read, write) as session:\n",
    "            await session.initialize()\n",
    "            tools = await load_mcp_tools(session)\n",
    "            agent = create_react_agent(model, tools)\n",
    "            agent_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "            print(agent_response)\n",
    "        \n",
    "        \n",
    "    # Example (Python - MCP Client - Multiple Servers):\n",
    "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "    from langchain_openai import ChatOpenAI\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    async with MultiServerMCPClient(\n",
    "        {\n",
    "            \"math\": {\n",
    "                \"command\": \"python\",\n",
    "                \"args\": [\"/path/to/math_server.py\"],  # Update with the path\n",
    "                \"transport\": \"stdio\",\n",
    "            },\n",
    "            \"weather\": {\n",
    "                \"url\": \"http://localhost:8000/sse\",\n",
    "                \"transport\": \"sse\",\n",
    "            }\n",
    "        }\n",
    "    ) as client:\n",
    "        agent = create_react_agent(model, client.get_tools())\n",
    "        math_response = await agent.ainvoke({\"messages\": \"what's (3 + 5) x 12?\"})\n",
    "        weather_response = await agent.ainvoke({\"messages\": \"what is the weather in nyc?\"})\n",
    "        print(math_response)\n",
    "        print(weather_response)\n",
    "\n",
    "\n",
    "# Loading MCP Tools:\n",
    "    # langchain_mcp_adapters.tools.load_mcp_tools: Asynchronously loads MCP tools from a ClientSession.\n",
    "    # MCP tools are converted into LangChain BaseTool instances.\n",
    "    # LangChain tools can then be used with LangChain agents or LangGraph workflows.\n",
    "\n",
    "# Using MCP Tools:\n",
    "    # LangChain tools created from MCP tools can be used like any other LangChain tool.\n",
    "    # They can be passed to LangChain agents (e.g., create_react_agent).\n",
    "    # They can be used within LangGraph nodes.\n",
    "\n",
    "\n",
    "# Key Classes and Functions:\n",
    "    # langchain_mcp_adapters.client.MultiServerMCPClient: Manages connections to multiple MCP servers.\n",
    "        # __init__(connections: dict[str, StdioConnection | SSEConnection] = None): Initializes the client.\n",
    "        # connect_to_server(server_name: str, *, transport: Literal[\"stdio\", \"sse\"] = \"stdio\", **kwargs) -> None: Connects to a server.\n",
    "        # connect_to_server_via_stdio(server_name: str, *, command: str, args: list[str], env: dict[str, str] | None = None, encoding: str = DEFAULT_ENCODING, encoding_error_handler: Literal[\"strict\", \"ignore\", \"replace\"] = DEFAULT_ENCODING_ERROR_HANDLER) -> None: Connects to a server via stdio.\n",
    "        # connect_to_server_via_sse(server_name: str, *, url: str) -> None: Connects to a server via SSE.\n",
    "        # get_tools() -> list[BaseTool]: Retrieves all tools from connected servers.\n",
    "    \n",
    "    # langchain_mcp_adapters.tools.load_mcp_tools(session: ClientSession) -> list[BaseTool]: Loads MCP tools from a client session.\n",
    "    \n",
    "    # mcp.server.fastmcp.FastMCP: (MCP Server) A class for creating MCP servers in Python.\n",
    "        # @mcp.tool(): Decorator to define tools.\n",
    "        # run(transport: str): Starts the MCP server.\n",
    "    \n",
    "    # mcp.client.stdio.stdio_client: (MCP Client) Context manager for connecting to MCP servers via stdio.\n",
    "    \n",
    "    # mcp.client.sse.sse_client: (MCP Client) Context manager for connecting to MCP servers via SSE.\n",
    "    \n",
    "\n",
    "# Use Cases:\n",
    "    # Extending LangChain Agents: Easily integrate tools from various MCP servers into LangChain agents.\n",
    "    # Building Modular AI Systems: Create modular AI systems where different components (servers) provide specific functionalities (tools).\n",
    "    # Integrating with Anthropic Models: Use MCP tools with Anthropic models (e.g., Claude) to enhance their capabilities.\n",
    "    # Creating LangGraph Workflows with External Tools: Incorporate MCP tools into LangGraph workflows for complex AI applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example application\n",
    "\n",
    "import asyncio\n",
    "import nest_asyncio\n",
    "import requests\n",
    "from mcp.server.fastmcp import FastMCP\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import threading\n",
    "import time\n",
    "import io\n",
    "import sys\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- MCP Servers ---\n",
    "\n",
    "def run_math_server():\n",
    "    try:\n",
    "        math_mcp = FastMCP(\"Math\")\n",
    "\n",
    "        @math_mcp.tool()\n",
    "        def add(a: float, b: float) -> float:\n",
    "            \"\"\"Adds two numbers.\"\"\"\n",
    "            return a + b\n",
    "\n",
    "        @math_mcp.tool()\n",
    "        def multiply(a: float, b: float) -> float:\n",
    "            \"\"\"Multiplies two numbers.\"\"\"\n",
    "            return a * b\n",
    "\n",
    "        math_mcp.run(transport=\"stdio\")\n",
    "    except Exception as e:\n",
    "        print(f\"Math server error: {e}\")\n",
    "\n",
    "def run_weather_server():\n",
    "    try:\n",
    "        weather_mcp = FastMCP(\"Weather\")\n",
    "\n",
    "        @weather_mcp.tool()\n",
    "        def get_weather(city: str) -> str:\n",
    "            \"\"\"Gets the current weather for a city.\"\"\"\n",
    "            api_key = \"YOUR_OPENWEATHERMAP_API_KEY\"  # Replace with your API key\n",
    "            url = f\"http://api.openweathermap.org/data/2.5/weather?q={city}&appid={api_key}&units=metric\"\n",
    "            response = requests.get(url)\n",
    "            data = response.json()\n",
    "            if data[\"cod\"] == \"404\":\n",
    "                return \"City not found.\"\n",
    "            else:\n",
    "                description = data[\"weather\"][0][\"description\"]\n",
    "                temperature = data[\"main\"][\"temp\"]\n",
    "                return f\"The weather in {city} is {description} with a temperature of {temperature}°C.\"\n",
    "\n",
    "        weather_mcp.run(transport=\"stdio\")\n",
    "    except Exception as e:\n",
    "        print(f\"Weather server error: {e}\")\n",
    "\n",
    "# --- LangChain Agent ---\n",
    "\n",
    "async def main():\n",
    "    model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "    # Start servers in separate threads\n",
    "    math_thread = threading.Thread(target=run_math_server)\n",
    "    weather_thread = threading.Thread(target=run_weather_server)\n",
    "\n",
    "    math_thread.daemon = True\n",
    "    weather_thread.daemon = True\n",
    "\n",
    "    math_thread.start()\n",
    "    weather_thread.start()\n",
    "    #Give the threads time to start.\n",
    "    time.sleep(1)\n",
    "\n",
    "    try:\n",
    "      async with MultiServerMCPClient(\n",
    "          {\n",
    "              \"math\": {\n",
    "                  \"command\": \"python\",\n",
    "                  \"args\": [\"-c\", \"pass\"], #dummy command since server is already running.\n",
    "                  \"transport\": \"stdio\",\n",
    "              },\n",
    "              \"weather\": {\n",
    "                  \"command\": \"python\",\n",
    "                  \"args\": [\"-c\", \"pass\"], #dummy command since server is already running.\n",
    "                  \"transport\": \"stdio\",\n",
    "              },\n",
    "          }\n",
    "      ) as client:\n",
    "          tools = client.get_tools()\n",
    "          agent = create_react_agent(model, tools)\n",
    "\n",
    "          math_response = await agent.ainvoke({\"messages\": \"What is 12 multiplied by 7?\"})\n",
    "          print(\"Math Response:\", math_response)\n",
    "\n",
    "          weather_response = await agent.ainvoke({\"messages\": \"What is the weather in London?\"})\n",
    "          print(\"Weather Response:\", weather_response)\n",
    "    except Exception as e:\n",
    "        print(f\"Main error: {e}\")\n",
    "\n",
    "    #Threads will end when the main program ends.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model Context Protocol (MCP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. What Is MCP and Why Use It?\n",
    "    # Model Context Protocol (MCP) is a standard, open protocol that allows large language model (LLM) applications to securely integrate data, \n",
    "    # code, and external tools. You can think of MCP as a “universal interface” that both:\n",
    "        # Servers implement to expose resources, prompts, and tools.\n",
    "        # Clients use to discover and interact with these capabilities.\n",
    "    # By adopting MCP, you can:\n",
    "        # Integrate with many different backends or 3rd-party servers using the same protocol.\n",
    "        # Expose your own custom logic (“tools”), data (“resources”), and prompts so that any LLM-based application—like Claude, ChatGPT, or a custom agent—can readily consume them.\n",
    "        # Securely manage how and when LLMs see data or call external code.\n",
    "\n",
    "\n",
    "# 2. Core MCP Concepts\n",
    "    # MCP servers can provide three main categories of capabilities:\n",
    "        # Resources – Data your server can provide. For example, database records, file contents, the output of an API call, or any other read-only data you want to pass to an LLM.\n",
    "        # Tools – Functions or “actions” that an LLM can invoke with arguments. Tools can be used to write to a database, call an external API, or otherwise perform an operation.\n",
    "        # Prompts – Reusable prompt templates and conversation setups that LLM-based applications can request.\n",
    "    # Additionally, you’ll see advanced concepts like:\n",
    "        # Images: Special resource or tool outputs that handle image data.\n",
    "        # Sampling: Letting an MCP server itself call out to an LLM. (Often used in multi-LLM or “self-call” scenarios.)\n",
    "        # Context: The idea that the LLM can load context from resources or prompt templates to handle a user request more effectively.\n",
    "\n",
    "\n",
    "# 3. High-Level Workflow\n",
    "    # When you connect an MCP client (like Claude for Desktop, or your own custom client) to an MCP server:\n",
    "        #1. Initialization: The client and server exchange capabilities (e.g., “I support listing tools,” “I support subscribing to resource updates,” “I can do logging,” etc.).\n",
    "        #2. Discovery: The client calls list_tools, list_resources, or list_prompts to find out what the server can do or provide.\n",
    "        #3. Invocation:\n",
    "            # If the LLM wants to read data, the client issues a read_resource.\n",
    "            # If the LLM wants to do something (e.g., “create a new user”), the client calls tools/call.\n",
    "            # If the LLM wants a prebuilt prompt template, the client calls prompts/get.\n",
    "        #4. Response: The server returns results (resource data, tool outputs, prompt messages) back to the client, which then hands the content to the LLM in a structured way.\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "# 4. Building & Running a Server\n",
    "    # Below is a minimal example of building your own Python-based MCP server using the mcp.server.fastmcp.FastMCP class. \n",
    "    # This approach is suitable when you want your code to run locally or be easily launched in a container/VM.\n",
    "\n",
    "    # 4.1 Install MCP\n",
    "        # Use uv or pip:\n",
    "        uv add \"mcp[cli]\"\n",
    "        # or\n",
    "        pip install \"mcp[cli]\"\n",
    "\n",
    "    # 4.1b Create a new environment\n",
    "        conda create --name .mcp_server python=3.10\n",
    "        conda activate .mcp_server\n",
    "        # or\n",
    "        python -m venv .venv\n",
    "        .venv\\Scripts\\activate\n",
    "        \n",
    "    # 4.2 Example: Simple Calculator Server\n",
    "        # file: calculator_server.py\n",
    "        from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "        # Create an MCP server named \"Calculator\"\n",
    "        mcp = FastMCP(\"Calculator\")\n",
    "\n",
    "        # # Tools can be normal Python functions with docstrings.\n",
    "        @mcp.tool()\n",
    "        def add(a: int, b: int) -> int:\n",
    "            \"\"\"\n",
    "            Add two numbers\n",
    "            \"\"\"\n",
    "            return a + b\n",
    "\n",
    "        @mcp.tool()\n",
    "        def multiply(a: int, b: int) -> int:\n",
    "            \"\"\"\n",
    "            Multiply two numbers\n",
    "            \"\"\"\n",
    "            return a * b\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            # By default, run with stdio transport (where MCP messages flow over stdin/stdout)\n",
    "            mcp.run()   # default: transport=\"stdio\" --> mcp.run(transport=\"stdio\") or mcp.run(transport=\"sse\")\n",
    "        \n",
    "    # How to run it in dev mode:\n",
    "        # Launch an interactive inspector UI:\n",
    "        mcp dev calculator_server.py\n",
    "    \n",
    "    # or test\n",
    "        npx @modelcontextprotocol/inspector \n",
    "\n",
    "    # How to run it “directly”:\n",
    "        # # Just run the server, so it reads/writes JSON over stdin/stdout\n",
    "        mcp run calculator_server.py\n",
    "\n",
    "    # How to integrate with Claude Desktop:\n",
    "    mcp install calculator_server.py --name \"My Calculator\"\n",
    "    # or edit the connection file directly: --> %APPDATA%\\Claude\\claude_desktop_config.json\n",
    "    \n",
    "    {\n",
    "    \"mcpServers\": {\n",
    "        \"NBA_server\": {\n",
    "        \"command\": \"C:\\\\Users\\\\pault\\\\MCP_SERVERS\\\\.venv\\\\Scripts\\\\python.exe\", #or \"C:\\\\Users\\\\pault\\\\MCP_SERVERS\\\\.venv\\\\bin\\\\python\" if it exists,\n",
    "        \"args\": [\n",
    "            \"C:\\\\Users\\\\pault\\\\MCP_SERVERS\\\\nba_server.py\"\n",
    "            ],\n",
    "        \"env\": {\n",
    "            \"OPENAI_API_KEY\": \"<your-openai-api-key>\"\n",
    "            },\n",
    "        \"host\": \"127.0.0.1\",\n",
    "        \"port\": 8080,\n",
    "        \"timeout\": 3000\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # update these paths below with the above code for the client side\n",
    "    \"\"\" \n",
    "    **Cursor** \n",
    "    `~/.cursor/mcp.json` \n",
    "\n",
    "    **Windsurf**\n",
    "    `~/.codeium/windsurf/mcp_config.json`\n",
    "\n",
    "    **Claude Desktop**\n",
    "    `~/Library/Application\\ Support/Claude/claude_desktop_config.json`\n",
    "\n",
    "    **Claude Code**\n",
    "    `~/.claude.json`\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # This will update claude_desktop_config.json so Claude can see it as a new MCP server. Once it’s installed, open Claude Desktop, \n",
    "    # look for your server, and your two tools (add and multiply) will appear.\n",
    "\n",
    "\n",
    "    # 4.3 Adding Resources\n",
    "        # Let’s say you want to store some helpful text that LLMs might load. You can define a resource:\n",
    "        @mcp.resource(\"readme://instructions\")\n",
    "        def instructions() -> str:\n",
    "            \"\"\"\n",
    "            Basic usage instructions for the calculator\n",
    "            \"\"\"\n",
    "            return \"Use add(a,b) or multiply(a,b) to do arithmetic. Provide integer args!\"\n",
    "        # Now the LLM can request resources/read with the URI readme://instructions. Tools remain the same, but the LLM can load extra context \n",
    "        # from that resource.\n",
    "\n",
    "    # 4.4 Adding a Prompt\n",
    "        # Sometimes you want a prebuilt system prompt or conversation structure. You can define a prompt in code:\n",
    "        @mcp.prompt()\n",
    "        def troubleshoot_equation(equation: str) -> str:\n",
    "            \"\"\"Help debug a broken or invalid equation input.\"\"\"\n",
    "            return f\"User is having trouble evaluating {equation}. Provide step-by-step help.\"\n",
    "        # The client can discover this prompt by calling prompts/list, and then retrieve it with prompts/get.\n",
    "\n",
    "\n",
    "# 5. Using a Third-Party MCP Server\n",
    "    # You do not need to rewrite a remote server’s code if it already speaks MCP. You can simply spin up (or connect to) the third-party \n",
    "    # server, then point your client(s) to it. For example:\n",
    "\n",
    "        # If you install a third-party “SQLite Explorer” server from GitHub, just run it:\n",
    "        python sqlite_explorer_server.py\n",
    "        # Then, from your client’s config or code, connect to that server over stdio or SSE.\n",
    "        # Once connected, do a list_tools, see the tool named query_db, and call it with your SQL.\n",
    "\n",
    "    # Integration example in code:\n",
    "        from mcp.client.stdio import stdio_client, StdioServerParameters\n",
    "        from mcp import ClientSession\n",
    "\n",
    "        async def main():\n",
    "            # Suppose the third-party server is a local python script\n",
    "            # that we want to run with arguments...\n",
    "            server_params = StdioServerParameters(\n",
    "                command=\"python\",\n",
    "                args=[\"sqlite_explorer_server.py\", \"--db\", \"/path/to/data.db\"]\n",
    "            )\n",
    "\n",
    "            # Establish a stdio-based connection:\n",
    "            async with stdio_client(server_params) as (read_stream, write_stream):\n",
    "                # Create a client session:\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    # Initialize handshake\n",
    "                    await session.initialize()\n",
    "\n",
    "                    # List tools:\n",
    "                    tools_response = await session.list_tools()\n",
    "                    print(\"Tools:\", tools_response.tools)\n",
    "\n",
    "                    # For example, call the `query_db` tool\n",
    "                    result = await session.call_tool(\"query_db\", arguments={\"sql\": \"SELECT * FROM foo\"})\n",
    "                    print(\"Query Results:\", result)\n",
    "\n",
    "        # Then run your async code\n",
    "        import asyncio\n",
    "        asyncio.run(main())\n",
    "\n",
    "    # As soon as the third-party server is running, your client can discover and call all exposed Tools/Resources/Prompts. \n",
    "    # No special “hand-coded integration” is needed—just follow the MCP calls.\n",
    "\n",
    "\n",
    "# 6. Writing an MCP Client\n",
    "    # Most often, you’ll use an existing client, e.g. Claude for Desktop or your own agent framework, to talk to the server. \n",
    "    # If you want to code your own from scratch or from the Python SDK, here’s a quick snippet showing a typical usage pattern:\n",
    "\n",
    "        # file: my_client.py\n",
    "        import asyncio\n",
    "        from mcp import ClientSession, StdioServerParameters\n",
    "        from mcp.client.stdio import stdio_client\n",
    "\n",
    "        async def main():\n",
    "            # 1. Describe how to launch the server\n",
    "            server_params = StdioServerParameters(\n",
    "                command=\"python\",\n",
    "                args=[\"calculator_server.py\"],  # Our earlier example\n",
    "                env={\"SOME_ENV_VAR\": \"123\"}     # if needed\n",
    "            )\n",
    "\n",
    "            # 2. Connect via stdio\n",
    "            async with stdio_client(server_params) as (read_stream, write_stream):\n",
    "                # 3. Create a high-level ClientSession\n",
    "                async with ClientSession(read_stream, write_stream) as session:\n",
    "                    # 4. Initialize the connection\n",
    "                    await session.initialize()\n",
    "\n",
    "                    # 5. List available tools\n",
    "                    tools_result = await session.list_tools()\n",
    "                    for tool in tools_result.tools:\n",
    "                        print(\"Tool discovered:\", tool.name, tool.description)\n",
    "\n",
    "                    # 6. Actually call a tool\n",
    "                    sum_result = await session.call_tool(\"add\", {\"a\": 4, \"b\": 5})\n",
    "                    print(\"Sum result\", sum_result)\n",
    "\n",
    "                    # 7. Optionally read a resource\n",
    "                    # read_result = await session.read_resource(\"readme://instructions\")\n",
    "                    # print(\"Resource contents:\", read_result)\n",
    "\n",
    "        # Run the client\n",
    "        if __name__ == \"__main__\":\n",
    "            asyncio.run(main())\n",
    "\n",
    "    # What is happening behind the scenes?\n",
    "        # We define how to run the server (in this case, “python calculator_server.py”).\n",
    "        # We open a stdio connection via stdio_client(...).\n",
    "        # We create ClientSession which handles the JSON-RPC handshake and method calls.\n",
    "        # We do session.initialize(), which starts the “initialize” handshake with the server.\n",
    "        # We discover tools/call and resources/read by listing them.\n",
    "        # We call the add tool with some arguments. The server returns the sum as text.\n",
    "        # We optionally do a resource read if needed.\n",
    "\n",
    "\n",
    "# 7. Running the Entire System\n",
    "    # Let’s suppose you want to see everything running end-to-end locally:\n",
    "\n",
    "    # Terminal #1 – Launch your server:\n",
    "        # For dev + interactive inspector\n",
    "        mcp dev calculator_server.py\n",
    "\n",
    "        # Or normal run in production mode\n",
    "        mcp run calculator_server.py\n",
    "        \n",
    "    # Terminal #2 – Launch your custom client:\n",
    "        python my_client.py\n",
    "\n",
    "    # You’ll see logs from both sides. The client prints out discovered tools, calls them, and prints results. \n",
    "    # The server logs any invocation or resource read requests.\n",
    "\n",
    "\n",
    "    # If you want to let Claude for Desktop manage the server, you’d do:\n",
    "    mcp install calculator_server.py --name \"My Calculator\"\n",
    "    # Then open Claude Desktop, confirm the new server appears, and try using the “tools panel” in Claude.\n",
    "\n",
    "\n",
    "# 8. Examples & Advanced Usage\n",
    "    # 8.1 Echo Server\n",
    "    # A minimal server that echoes requests. Typically used for debugging. You can see the snippet below:\n",
    "        from mcp.server.fastmcp import FastMCP\n",
    "\n",
    "        mcp = FastMCP(\"Echo\")\n",
    "\n",
    "        PATH = \"C:/path/to/your/files/\"\n",
    "        \n",
    "        @mcp.resource(\"docs://langgraph/full\")\n",
    "        def get_all_langgraph_docs() -> str:\n",
    "            \"\"\"\n",
    "            Get all the LangGraph documentation. Returns the contents of the file llms_full.txt,\n",
    "            which contains a curated set of LangGraph documentation (~300k tokens). This is useful\n",
    "            for a comprehensive response to questions about LangGraph.\n",
    "\n",
    "            Args: None\n",
    "\n",
    "            Returns:\n",
    "                str: The contents of the LangGraph documentation\n",
    "            \"\"\"\n",
    "\n",
    "            # Local path to the LangGraph documentation\n",
    "            doc_path = PATH + \"llms_full.txt\"\n",
    "            try:\n",
    "                with open(doc_path, 'r') as file:\n",
    "                    return file.read()\n",
    "            except Exception as e:\n",
    "                return f\"Error reading log file: {str(e)}\"\n",
    "\n",
    "        @mcp.tool()\n",
    "        def echo_tool(msg: str) -> str:\n",
    "            return f\"Tool echo: {msg}\"\n",
    "\n",
    "        @mcp.prompt()\n",
    "        def echo_prompt(message: str) -> str:\n",
    "            return f\"Please consider this message: {message}\"\n",
    "\n",
    "        if __name__ == \"__main__\":\n",
    "            mcp.run()\n",
    "\n",
    "    # 8.2 SQLite Explorer\n",
    "        # See the python-sdk examples folder. It shows how to connect to a real DB, define resources for table schemas, and tools for queries.\n",
    "\n",
    "    # 8.3 Using SSE Instead of Stdio\n",
    "        # Sometimes you might prefer SSE (Server-Sent Events) over stdio. The MCP Python SDK includes an SSE transport, typically used within \n",
    "        # a Starlette or Uvicorn environment. You’d do:\n",
    "        \n",
    "        # inside a starlette route:\n",
    "        from starlette.applications import Starlette\n",
    "        from starlette.routing import Route, Mount\n",
    "        from mcp.server.sse import SseServerTransport\n",
    "        from mcp.server.lowlevel import Server\n",
    "\n",
    "        server = Server(\"my-sse-server\")\n",
    "        sse_transport = SseServerTransport(\"/my_sse_endpoint/\")\n",
    "\n",
    "        async def sse_entrypoint(request):\n",
    "            async with sse_transport.connect_sse(request.scope, request.receive, request._send) as (rs, ws):\n",
    "                await server.run(rs, ws, server.create_initialization_options())\n",
    "\n",
    "        star_app = Starlette(\n",
    "            debug=True,\n",
    "            routes=[\n",
    "                Route(\"/sse\", endpoint=sse_entrypoint),\n",
    "                Mount(\"/my_sse_endpoint/\", app=sse_transport.handle_post_message)\n",
    "            ]\n",
    "        )\n",
    "        # Then in your client code, you’d specify an SSE-based connection. This is more advanced usage.\n",
    "\n",
    "\n",
    "# 9. Key Tips & Best Practices\n",
    "    # Security: Tools can be extremely powerful. By default, anything the server implements is exposed to the LLM if your client permits it. \n",
    "        # Be mindful about restricting or gating certain calls or arguments.\n",
    "    # Documentation: Provide docstrings and type hints in Python. The MCP Python SDK automatically extracts these to help LLMs understand \n",
    "        # your tools.\n",
    "    # Testing: Use mcp dev server.py and the official MCP Inspector to check the list of tools, resources, and prompts. This interactive \n",
    "        # tool helps debug issues.\n",
    "    # Environments: You can specify environment variables in mcp install server.py --env-file .env. That way, your secrets (API keys, \n",
    "        # DB credentials) remain separate from code.\n",
    "    # Performance: For large or computationally heavy tasks, consider using progress notifications (ctx.report_progress(...)) so that the \n",
    "        # LLM or user gets partial updates.\n",
    "    # Claude Desktop: Make sure you have the newest version. Use claude_desktop_config.json to define multiple servers. This is how you can \n",
    "        # run many local or remote servers at once, each focusing on different data or functionality.\n",
    "\n",
    "\n",
    "# 10. Putting It All Together\n",
    "    # 10.1 End-to-End Example\n",
    "        # server.py (the server code we described, e.g. “Calculator”)\n",
    "        # my_client.py (your custom client code, if you want it)\n",
    "        # Claude for Desktop (another standard client)\n",
    "\n",
    "    # Run the server:\n",
    "    mcp run server.py\n",
    "\n",
    "    # Install in Claude (optional but recommended if you use Claude Desktop):\n",
    "    mcp install server.py --name \"LocalCalculator\"\n",
    "\n",
    "    # Or run your custom client:\n",
    "    python my_client.py\n",
    "\n",
    "    # Interact:\n",
    "        # If using the “Claude for Desktop” approach, you can open Claude, see “LocalCalculator,” and see a list of Tools. \n",
    "            # You might type “Please add 2 and 3,” and the LLM calls your add tool behind the scenes.\n",
    "        # If using “my_client.py,” it will directly orchestrate calls to add, multiply, or resource reads.\n",
    "\n",
    "    # You’ve now got a working MCP system where:\n",
    "        # Your server defines custom logic and data (in Python).\n",
    "        # The LLM client can discover and call that logic safely.\n",
    "        # Tools or resources can easily be extended over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Deploying Python MCP Server to Cloudflare with SSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to this video for more information: https://www.youtube.com/watch?v=cbeOWKANtj8\n",
    "# refer to this second video: https://www.youtube.com/watch?v=H7Qe96fqg1M \n",
    "# refer to this documentation for more information: https://developers.cloudflare.com/agents/guides/remote-mcp-server\n",
    "# refer to this github repository for more information: https://github.com/cloudflare/workers-mcp/tree/main\n",
    "# reddit help: https://www.reddit.com/r/mcp/comments/1jjbgwu/hosting_mcp_on_the_cloud/ \n",
    "\n",
    "# Usage\n",
    "    # Step 1: Generate a new Worker\n",
    "        # Use \"create-cloudflare? to generate a new Worker.\n",
    "        \"\"\"\n",
    "        npx create-cloudflare@latest my-new-worker\n",
    "        \"\"\"\n",
    "            # Use Hello World Worker\n",
    "            # Use Typescript\n",
    "            # Select \"Yes\" to use git for version control\n",
    "            # Select \"Yes\" to deploy the application\n",
    "            # When it starts to perform DNS propagation, use Ctrl+C to stop the process \n",
    "            \n",
    "\n",
    "    # Step 2: Install workers-mcp\n",
    "        \"\"\" \n",
    "        cd my-new-worker \n",
    "        npm install workers-mcp\n",
    "\n",
    "        \"\"\"\n",
    "    # Step 3: Run the setup command (after deploying, this will also configure your claude desktop automatically)\n",
    "        \"\"\" \n",
    "        npx workers-mcp setup\n",
    "        \"\"\"\n",
    "            # Select \"Yes\" to deploy\n",
    "            # Select \"Yes\" tp replace index.ts with the above code\n",
    "            # You can change the name or use the same name for claude desktop\n",
    "    \n",
    "        # Error step:\n",
    "            # If you get an error (■  ERROR spawn npx ENOENT), you can run the following command to fix it\n",
    "                npx wrangler secret put SHARED_SECRET   #Then paste the value from your .dev.vars file when prompted.\n",
    "            \n",
    "            # You can also manually deploy it by running the following command:\n",
    "                npx wrangler deploy\n",
    "                \n",
    "\n",
    "    # Step 4: You can then edit the index.ts file to include your server code\n",
    "        # The code should include the server code as shown above\n",
    "        \n",
    "        # If you need to add environment variables, then edit the worker-configuration.d.ts and wrangler.toml files\n",
    "            # worker-configuration.d.ts\n",
    "                \"\"\"\n",
    "                interface Env {\n",
    "                    // Tavily API key for search functionality\n",
    "                    SHARED_SECRET: string;\n",
    "                    TAVILY_API_KEY: string; # Add this line\n",
    "                }\n",
    "                \"\"\"\n",
    "            # wrangler.toml\n",
    "                # option 1 (Using secrets (recommended for API keys)):\n",
    "                    \"\"\"\n",
    "                    npx wrangler secret put TAVILY_API_KEY\n",
    "                    \"\"\"\n",
    "                \n",
    "                # option 2 (Using environment variables):\n",
    "                    # Add the following to the wrangler.toml file\n",
    "                    \"\"\"\n",
    "                    [vars]\n",
    "                    TAVILY_API_KEY = \"your-api-key\"\n",
    "                    \n",
    "                    # You can then access the environment variable in your code as shown below\n",
    "                    const tavily = new TavilyClient(env.TAVILY_API_KEY);\n",
    "                    \"\"\"\n",
    "                    \n",
    "        # Save the file and run the deploy command\n",
    "        \"\"\"\n",
    "        npm run deploy\n",
    "        \"\"\"\n",
    "    \n",
    "    # Step 5: You can now interact with your server using Claude Desktop or your custom client\n",
    "    npm install -g mcp-remote\n",
    "    npm install mcp-remote\n",
    "\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"tavily-search\": {\n",
    "      \"command\": \"C:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\cloudflare_wrangler\\\\tavily\\\\tavily-search\\\\node_modules\\\\.bin\\\\mcp-remote\",\n",
    "      \"args\": [\n",
    "        \"https://tavily-search.tavily-search-mcp.workers.dev/sse\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "{\n",
    "  \"mcpServers\": {\n",
    "    \"tavily-search\": {\n",
    "      \"command\": \"node\",\n",
    "      \"args\": [\n",
    "        \"C:\\\\Users\\\\pault\\\\Documents\\\\3. AI and Machine Learning\\\\2. Deep Learning\\\\1c. App\\\\Projects\\\\cloudflare_wrangler\\\\tavily\\\\tavily-search\\\\node_modules\\\\mcp-remote\\\\dist\\\\index.js\",\n",
    "        \"https://tavily-search.tavily-search-mcp.workers.dev/sse\"\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "    {\n",
    "    \"mcpServers\": {\n",
    "        \"tavily-search\": {\n",
    "            \"url\": \"https://tavily-search.tavily-search-mcp.workers.dev/sse\"\n",
    "            }\n",
    "    }\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Deploying Python MCP Server to Cloudflare with SSE\n",
    "# \n",
    "# ## 1. Using mcp-proxy to Expose Your Python Server via SSE\n",
    "    # # This is the simplest approach to make your existing Python MCP server available over SSE\n",
    "\n",
    "    # Install mcp-proxy\n",
    "    uv tool install mcp-proxy\n",
    "    # OR\n",
    "    pipx install mcp-proxy\n",
    "\n",
    "    # Run your existing Python MCP server behind the proxy with SSE support\n",
    "    mcp-proxy --sse-port=8080 python sample.py\n",
    "\n",
    "    # For public access (accessible outside localhost)\n",
    "    mcp-proxy --sse-host=0.0.0.0 --sse-port=8080 python sample.py\n",
    "\n",
    "    # Step 3: Configure your MCP client to connect\n",
    "        # Your server will now be accessible at http://your-ip:8080/sse. If you're running this on a cloud VM or server \n",
    "        # with a public IP, you'll be able to connect to it from anywhere.\n",
    "\n",
    "        {\n",
    "        \"mcpServers\": {\n",
    "            \"my-remote-server\": {\n",
    "            \"command\": \"mcp-proxy\",\n",
    "            \"args\": [\"http://your-server-ip:8080/sse\"],\n",
    "            \"env\": {}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "# ## 2. Deploying to Cloudflare\n",
    "    # # Based on https://developers.cloudflare.com/agents/guides/remote-mcp-server/\n",
    "    # # Cloudflare doesn't directly support Python MCP servers yet, but offers proxy solutions\n",
    "\n",
    "    # Install Cloudflare's Wrangler CLI\n",
    "    npm install -g wrangler\n",
    "\n",
    "    # Login to your Cloudflare account\n",
    "    wrangler login\n",
    "\n",
    "    # Create a new Cloudflare Worker project that will act as proxy\n",
    "    npm create cloudflare@latest my-mcp-proxy -- --type=\"hello-world\"\n",
    "    cd my-mcp-proxy\n",
    "\n",
    "\n",
    "\n",
    "# ## 3. Setting Up the Cloudflare Worker as a Proxy\n",
    "    # # Create a Worker that proxies requests to your Python MCP server\n",
    "\n",
    "    # Configure wrangler.toml - ensure you have these settings:\n",
    "    # name = \"my-mcp-proxy\"\n",
    "    # main = \"src/index.js\"\n",
    "    # compatibility_date = \"2024-03-22\"\n",
    "    # workers_dev = true\n",
    "\n",
    "    # Deploy your Worker\n",
    "    wrangler deploy\n",
    "\n",
    "# ## 4. Configure MCP Clients to Connect to Your Remote Server\n",
    "    # # Update your MCP client configuration to connect via mcp-proxy\n",
    "\n",
    "    # For Claude Desktop (edit claude_desktop_config.json):\n",
    "    # {\n",
    "    #   \"mcpServers\": {\n",
    "    #     \"my-remote-server\": {\n",
    "    #       \"command\": \"mcp-proxy\",\n",
    "    #       \"args\": [\"https://my-mcp-proxy.your-account.workers.dev/sse\"],\n",
    "    #       \"env\": {}\n",
    "    #     }\n",
    "    #   }\n",
    "    # }\n",
    "\n",
    "# ## 5. Authentication for Remote MCP Server (Optional)\n",
    "    # # From https://developers.cloudflare.com/agents/examples/build-mcp-server/\n",
    "\n",
    "    # Create a new OAuth-enabled MCP server\n",
    "    npm create cloudflare@latest -- my-mcp-server-github-auth --template=cloudflare/ai/demos/remote-mcp-github-oauth\n",
    "\n",
    "    # Deploy the OAuth-enabled MCP server\n",
    "    cd my-mcp-server-github-auth\n",
    "    npm install\n",
    "    npm run deploy\n",
    "\n",
    "# ## 6. Docker Deployment Alternative\n",
    "    # # If you prefer using Docker to host your Python MCP server with SSE\n",
    "\n",
    "    # Create a Dockerfile for your Python MCP server with mcp-proxy\n",
    "    # FROM python:3.10-slim\n",
    "    # \n",
    "    # RUN pip install --no-cache-dir mcp-proxy\n",
    "    # \n",
    "    # WORKDIR /app\n",
    "    # COPY sample.py .\n",
    "    # \n",
    "    # EXPOSE 8080\n",
    "    # \n",
    "    # CMD [\"mcp-proxy\", \"--sse-host=0.0.0.0\", \"--sse-port=8080\", \"python\", \"sample.py\"]\n",
    "\n",
    "    # Build and run the Docker container\n",
    "    # docker build -t mcp-server .\n",
    "    # docker run -p 8080:8080 mcp-server\n",
    "\n",
    "\n",
    "# ## 7. Testing the Deployment\n",
    "    # # Verify your SSE endpoint is working correctly\n",
    "\n",
    "    # Use MCP Inspector to test a remote SSE endpoint\n",
    "    npx @modelcontextprotocol/inspector\n",
    "    # Then input your SSE URL (http://your-server-ip:8080/sse or https://your-worker.workers.dev/sse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MCP example (travel_server.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Advanced MCP Server: Travel Planner\n",
    "\n",
    "Features:\n",
    "- Tools to search flights/hotels and build itineraries\n",
    "- Resources that return JSON or text\n",
    "- A reusable prompt template\n",
    "- Lifecycle (lifespan) with DB initialization\n",
    "- Uses progress notifications for a tool\n",
    "\n",
    "Run:\n",
    "    python travel_server.py\n",
    "\"\"\"\n",
    "\n",
    "import anyio\n",
    "import httpx\n",
    "import math\n",
    "from typing import AsyncIterator\n",
    "from dataclasses import dataclass, field\n",
    "from contextlib import asynccontextmanager\n",
    "\n",
    "# MCP imports\n",
    "from mcp.server.fastmcp import FastMCP, Context\n",
    "from mcp.server.fastmcp.utilities.logging import get_logger\n",
    "from mcp.server.fastmcp.utilities.types import Image\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# LOGGING\n",
    "logger = get_logger(\"travel_planner\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# LIFESPAN CONTEXT (Pretend DB or external resources)\n",
    "@dataclass\n",
    "class TravelData:\n",
    "    \"\"\"In-memory store for flight/hotel info, could be replaced by real DB.\"\"\"\n",
    "    flight_db: dict = field(default_factory=dict)\n",
    "    hotel_db: dict = field(default_factory=dict)\n",
    "\n",
    "@asynccontextmanager\n",
    "async def lifespan_ctx(server: FastMCP) -> AsyncIterator[TravelData]:\n",
    "    \"\"\"\n",
    "    This lifespan is called once when the server starts and once when it stops.\n",
    "    Use it to initialize DB connections, caches, etc.\n",
    "    \"\"\"\n",
    "    logger.info(\"Travel Planner server is starting up...\")\n",
    "    # For demonstration, we just fill an in-memory dictionary.\n",
    "    data = TravelData(\n",
    "        flight_db={\n",
    "            # flight_db is keyed by (origin, destination)\n",
    "            (\"SFO\", \"LAX\"): [\n",
    "                {\"flight_no\": \"UA100\", \"price\": 120, \"duration\": \"1h25m\"},\n",
    "                {\"flight_no\": \"DL223\", \"price\": 150, \"duration\": \"1h30m\"}\n",
    "            ],\n",
    "            (\"SFO\", \"NYC\"): [\n",
    "                {\"flight_no\": \"UA300\", \"price\": 300, \"duration\": \"5h50m\"},\n",
    "                {\"flight_no\": \"AA777\", \"price\": 320, \"duration\": \"5h45m\"}\n",
    "            ],\n",
    "        },\n",
    "        hotel_db={\n",
    "            # hotel_db is keyed by city\n",
    "            \"Los Angeles\": [\n",
    "                {\"name\": \"LA Grand Hotel\", \"stars\": 5, \"price_per_night\": 250},\n",
    "                {\"name\": \"Sunset Motel\", \"stars\": 3, \"price_per_night\": 90}\n",
    "            ],\n",
    "            \"New York\": [\n",
    "                {\"name\": \"NY Midtown Luxury\", \"stars\": 5, \"price_per_night\": 350},\n",
    "                {\"name\": \"Queens Budget Inn\", \"stars\": 2, \"price_per_night\": 70}\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    "    try:\n",
    "        yield data\n",
    "    finally:\n",
    "        logger.info(\"Travel Planner server is shutting down...\")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# CREATE MCP SERVER\n",
    "mcp = FastMCP(\n",
    "    name=\"Travel Planner\",\n",
    "    lifespan=lifespan_ctx,\n",
    "    # Optionally declare dependencies that we want installed or recognized\n",
    "    dependencies=[\"httpx\", \"anyio\"]\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# RESOURCES\n",
    "@mcp.resource(\"travel://top-destinations\")\n",
    "def get_top_destinations() -> str:\n",
    "    \"\"\"\n",
    "    Returns a JSON string listing top travel destinations.\n",
    "    For demonstration, it’s static. Typically you'd fetch from DB or API.\n",
    "    \"\"\"\n",
    "    # Could also return a JSON string, or we could return python and let the server\n",
    "    # convert, but here we'll just do direct JSON in a string\n",
    "    return \"\"\"\n",
    "    {\n",
    "      \"destinations\": [\n",
    "        { \"city\": \"Paris\", \"country\": \"France\", \"popularity\": 9.9 },\n",
    "        { \"city\": \"Tokyo\", \"country\": \"Japan\", \"popularity\": 9.8 },\n",
    "        { \"city\": \"New York\", \"country\": \"USA\", \"popularity\": 9.6 }\n",
    "      ]\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "@mcp.resource(\"travel://tips\")\n",
    "def get_general_travel_tips() -> str:\n",
    "    \"\"\"\n",
    "    A plain text resource giving general travel tips.\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"Always check the weather in your destination.\\n\"\n",
    "        \"Book flights & hotels in advance for better rates.\\n\"\n",
    "        \"Carry digital and physical copies of important documents.\\n\"\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# TOOLS\n",
    "\n",
    "@mcp.tool()\n",
    "async def search_flights(origin: str, destination: str, ctx: Context) -> str:\n",
    "    \"\"\"\n",
    "    Search flight info from origin to destination.\n",
    "    Demonstrates usage of in-memory \"flight_db\" from lifespan context + progress.\n",
    "    \"\"\"\n",
    "    data: TravelData = ctx.request_context.lifespan_context  # typed from our lifespan\n",
    "\n",
    "    # We'll do a short loop with progress updates to simulate a longer process:\n",
    "    for i in range(3):\n",
    "        # \"i\" is progress index, 3 is total\n",
    "        await ctx.report_progress(progress=i, total=3)\n",
    "        await anyio.sleep(0.4)  # simulate network or DB query delay\n",
    "\n",
    "    results = data.flight_db.get((origin.upper(), destination.upper()), [])\n",
    "    if not results:\n",
    "        return f\"No flights found from {origin} to {destination}.\"\n",
    "    msg = f\"Flights from {origin} to {destination}:\\n\"\n",
    "    for r in results:\n",
    "        msg += (\n",
    "            f\"- Flight {r['flight_no']}, Price ${r['price']}, \"\n",
    "            f\"Duration {r['duration']}\\n\"\n",
    "        )\n",
    "    return msg\n",
    "\n",
    "@mcp.tool()\n",
    "async def search_hotels(city: str, ctx: Context) -> str:\n",
    "    \"\"\"\n",
    "    Search hotels in a given city. Also demonstrates usage of lifespan context.\n",
    "    \"\"\"\n",
    "    data: TravelData = ctx.request_context.lifespan_context\n",
    "    city_title = city.title()\n",
    "    hotels = data.hotel_db.get(city_title, [])\n",
    "    if not hotels:\n",
    "        return f\"No hotel data found for city: {city_title}\"\n",
    "    resp = f\"Hotels in {city_title}:\\n\"\n",
    "    for h in hotels:\n",
    "        resp += (\n",
    "            f\"- {h['name']} | {h['stars']} star(s) | \"\n",
    "            f\"${h['price_per_night']} per night\\n\"\n",
    "        )\n",
    "    return resp\n",
    "\n",
    "@mcp.tool()\n",
    "def build_itinerary(city: str, days: int, budget: float) -> str:\n",
    "    \"\"\"\n",
    "    Build a simple itinerary for a given city, length of stay, and budget.\n",
    "    This is a purely local function that doesn't call external APIs,\n",
    "    but you'd typically combine flight/hotel costs or local events, etc.\n",
    "    \"\"\"\n",
    "    # We'll do a naive approach: assume daily cost is 80% of budget/days\n",
    "    daily_alloc = (budget / days) * 0.8\n",
    "    rec = (\n",
    "        f\"Proposed itinerary for {days} days in {city.title()}:\\n\"\n",
    "        f\"- Daily budget (approx): ${daily_alloc:.2f}\\n\"\n",
    "        f\"- Activities: We'll schedule tours, local cuisine, etc.\\n\"\n",
    "        f\"- Suggestions: Book hotels earlier, keep some buffer for unexpected costs.\\n\"\n",
    "    )\n",
    "    return rec\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# PROMPTS\n",
    "@mcp.prompt()\n",
    "def plan_prompt(destination: str, additional_notes: str) -> str:\n",
    "    \"\"\"\n",
    "    A prompt template that can be used by the LLM to refine a user's travel plan.\n",
    "\n",
    "    Arguments:\n",
    "        destination: The city or region the user wants to visit\n",
    "        additional_notes: Additional user preferences or notes\n",
    "    \"\"\"\n",
    "    return (\n",
    "        \"You are a travel-planning assistant. The user wants to visit \"\n",
    "        f\"{destination.title()}. The user says:\\n{additional_notes}\\n\\n\"\n",
    "        \"Please propose a step-by-step plan or suggestions for them.\"\n",
    "    )\n",
    "\n",
    "# --------------------------------------------------------------------------\n",
    "# SERVER RUN\n",
    "if __name__ == \"__main__\":\n",
    "    mcp.run()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MCP example (travel_client.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example MCP Client for the Travel Planner server.\n",
    "\n",
    "Connects via stdio to 'travel_server.py', initializes,\n",
    "lists available tools/prompts/resources, and calls them.\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "\n",
    "# From MCP Python SDK\n",
    "from mcp import ClientSession, StdioServerParameters\n",
    "from mcp.client.stdio import stdio_client\n",
    "\n",
    "async def main():\n",
    "    # 1) Define how to launch the server:\n",
    "    server_params = StdioServerParameters(\n",
    "        command=\"python\",\n",
    "        args=[\"travel_server.py\"],\n",
    "        env=None,  # Or pass in environment variables if needed\n",
    "    )\n",
    "\n",
    "    async with stdio_client(server_params) as (read_stream, write_stream):\n",
    "        # 2) Create a high-level client session\n",
    "        async with ClientSession(read_stream, write_stream) as session:\n",
    "            # 3) Initialize handshake\n",
    "            await session.initialize()\n",
    "\n",
    "            print(\"\\n--- MCP Client Initialized ---\")\n",
    "\n",
    "            # 4) List Tools\n",
    "            tool_result = await session.list_tools()\n",
    "            print(\"\\nTools discovered:\")\n",
    "            for t in tool_result.tools:\n",
    "                print(f\" • {t.name}: {t.description}\")\n",
    "\n",
    "            # 5) Call a Tool, e.g. search_flights\n",
    "            print(\"\\n--- Calling search_flights Tool ---\")\n",
    "            # This tool is asynchronous; we can gather partial progress updates if we want\n",
    "            # (the official sampling callback can handle progress notifications, but let's just do a single call here).\n",
    "            flight_res = await session.call_tool(\"search_flights\", arguments={\n",
    "                \"origin\": \"SFO\",\n",
    "                \"destination\": \"LAX\"\n",
    "            })\n",
    "            print(\"Flight search result:\\n\", flight_res)\n",
    "\n",
    "            # 6) List Resources\n",
    "            resources_result = await session.list_resources()\n",
    "            print(\"\\nResources discovered:\")\n",
    "            for r in resources_result.resources:\n",
    "                print(f\" • {r.uri} - {r.description}\")\n",
    "\n",
    "            # 7) Read Resource: travel://tips\n",
    "            tip_content, tip_mime = await session.read_resource(\"travel://tips\")\n",
    "            print(\"\\nTravel Tips Resource Content:\\n\", tip_content.decode(\"utf-8\"))\n",
    "\n",
    "            # 8) Use a prompt\n",
    "            # Let's see what prompts exist\n",
    "            prompts_resp = await session.list_prompts()\n",
    "            print(\"\\nPrompts discovered:\")\n",
    "            for p in prompts_resp.prompts:\n",
    "                print(f\" • {p.name}: {p.description}\")\n",
    "\n",
    "            # 9) Get the plan_prompt with arguments\n",
    "            plan_prompt_resp = await session.get_prompt(\n",
    "                name=\"plan_prompt\",\n",
    "                arguments={\n",
    "                    \"destination\": \"paris\",\n",
    "                    \"additional_notes\": \"I want to focus on art, museums, and budget-friendly options.\"\n",
    "                }\n",
    "            )\n",
    "            # The server responded with a set of messages\n",
    "            print(\"\\nPlan Prompt Messages:\\n\")\n",
    "            for msg in plan_prompt_resp.messages:\n",
    "                print(f\"Role: {msg.role}, Content:\\n{msg.content}\")\n",
    "\n",
    "            # 10) Optionally call 'build_itinerary' to see final\n",
    "            itinerary_res = await session.call_tool(\"build_itinerary\", arguments={\n",
    "                \"city\": \"Tokyo\",\n",
    "                \"days\": 5,\n",
    "                \"budget\": 1200\n",
    "            })\n",
    "            print(\"\\nSuggested itinerary:\\n\", itinerary_res)\n",
    "\n",
    "            print(\"\\n--- Done! ---\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> MCP example 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Personalized Recipe Recommendation System using MCP\n",
    "    # This server will:\n",
    "        # Provide access to a recipe database (resource).\n",
    "        # Offer tools for searching, filtering, and generating personalized recipes.\n",
    "        # Include prompts for interacting with the recipe recommendation system.\n",
    "\n",
    "\n",
    "\n",
    "# Project Structure:\n",
    "recipe_server/\n",
    "├── server.py\n",
    "├── recipes.json\n",
    "├── requirements.txt\n",
    "\n",
    "\n",
    "\n",
    "# recipes.json # (Recipe Database):\n",
    "\n",
    "[\n",
    "  {\n",
    "    \"id\": 1,\n",
    "    \"name\": \"Spaghetti Carbonara\",\n",
    "    \"ingredients\": [\"spaghetti\", \"eggs\", \"guanciale\", \"pecorino romano\", \"black pepper\"],\n",
    "    \"cuisine\": \"Italian\",\n",
    "    \"dietary_restrictions\": [],\n",
    "    \"instructions\": \"Cook spaghetti. Fry guanciale. Mix eggs and cheese. Combine everything. Serve with black pepper.\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": 2,\n",
    "    \"name\": \"Vegetarian Chili\",\n",
    "    \"ingredients\": [\"kidney beans\", \"black beans\", \"tomatoes\", \"onions\", \"bell peppers\", \"chili powder\"],\n",
    "    \"cuisine\": \"Mexican\",\n",
    "    \"dietary_restrictions\": [\"vegetarian\"],\n",
    "    \"instructions\": \"Sauté onions and peppers. Add beans and tomatoes. Season with chili powder. Simmer until thick.\"\n",
    "  },\n",
    "    {\n",
    "    \"id\": 3,\n",
    "    \"name\": \"Chicken Stir-Fry\",\n",
    "    \"ingredients\": [\"chicken breast\", \"broccoli\", \"carrots\", \"soy sauce\", \"ginger\", \"garlic\"],\n",
    "    \"cuisine\": \"Asian\",\n",
    "    \"dietary_restrictions\": [],\n",
    "    \"instructions\": \"Cut chicken and vegetables. Stir-fry chicken. Add vegetables, soy sauce, ginger, and garlic. Cook until chicken is done.\"\n",
    "  },\n",
    "    {\n",
    "    \"id\": 4,\n",
    "    \"name\": \"Vegan Chocolate Cake\",\n",
    "    \"ingredients\": [\"flour\", \"sugar\", \"cocoa powder\", \"baking soda\", \"almond milk\", \"vegetable oil\"],\n",
    "    \"cuisine\": \"Dessert\",\n",
    "    \"dietary_restrictions\": [\"vegan\"],\n",
    "    \"instructions\": \"Mix dry ingredients. Add almond milk and oil. Bake until done. Frost as desired.\"\n",
    "  },\n",
    "    {\n",
    "    \"id\": 5,\n",
    "    \"name\": \"Salmon with Lemon Dill Sauce\",\n",
    "    \"ingredients\": [\"salmon fillets\", \"lemon\", \"dill\", \"butter\", \"white wine\"],\n",
    "    \"cuisine\": \"Seafood\",\n",
    "    \"dietary_restrictions\": [],\n",
    "    \"instructions\": \"Bake salmon fillets. Prepare lemon dill sauce with butter, lemon, dill, and white wine. Serve sauce over salmon.\"\n",
    "  }\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# server.py     # (MCP Server):\n",
    "    import json\n",
    "    from typing import List, Optional\n",
    "\n",
    "    from mcp.server.fastmcp import FastMCP, Context\n",
    "    from pydantic import BaseModel\n",
    "\n",
    "    mcp = FastMCP(\"Recipe Recommendation Server\")\n",
    "\n",
    "    # Load recipes from JSON\n",
    "    with open(\"recipes.json\", \"r\") as f:\n",
    "        recipes = json.load(f)\n",
    "\n",
    "    class Recipe(BaseModel):\n",
    "        id: int\n",
    "        name: str\n",
    "        ingredients: List[str]\n",
    "        cuisine: str\n",
    "        dietary_restrictions: List[str]\n",
    "        instructions: str\n",
    "\n",
    "    class RecipeFilter(BaseModel):\n",
    "        cuisine: Optional[str] = None\n",
    "        dietary_restrictions: Optional[str] = None\n",
    "        ingredients: Optional[List[str]] = None\n",
    "\n",
    "    @mcp.resource(\"recipes://all\")\n",
    "    def get_all_recipes() -> List[Recipe]:\n",
    "        \"\"\"Returns all recipes.\"\"\"\n",
    "        return [Recipe(**recipe) for recipe in recipes]\n",
    "\n",
    "    @mcp.tool()\n",
    "    def search_recipes(query: str, ctx: Context) -> List[Recipe]:\n",
    "        \"\"\"Searches recipes by name or ingredients.\"\"\"\n",
    "        query = query.lower()\n",
    "        results = [\n",
    "            Recipe(**recipe)\n",
    "            for recipe in recipes\n",
    "            if query in recipe[\"name\"].lower() or any(query in ingredient.lower() for ingredient in recipe[\"ingredients\"])\n",
    "        ]\n",
    "        ctx.info(f\"Found {len(results)} recipes matching '{query}'\")\n",
    "        return results\n",
    "\n",
    "    @mcp.tool()\n",
    "    def filter_recipes(filters: RecipeFilter, ctx: Context) -> List[Recipe]:\n",
    "        \"\"\"Filters recipes by cuisine, dietary restrictions, or ingredients.\"\"\"\n",
    "        filtered_recipes = recipes[:]\n",
    "        if filters.cuisine:\n",
    "            filtered_recipes = [r for r in filtered_recipes if filters.cuisine.lower() == r[\"cuisine\"].lower()]\n",
    "        if filters.dietary_restrictions:\n",
    "            filtered_recipes = [r for r in filtered_recipes if filters.dietary_restrictions.lower() in [d.lower() for d in r[\"dietary_restrictions\"]]]\n",
    "        if filters.ingredients:\n",
    "            filtered_recipes = [r for r in filtered_recipes if all(ingredient.lower() in [i.lower() for i in r[\"ingredients\"]] for ingredient in filters.ingredients)]\n",
    "\n",
    "        ctx.info(f\"Filtered to {len(filtered_recipes)} recipes.\")\n",
    "        return [Recipe(**recipe) for recipe in filtered_recipes]\n",
    "\n",
    "    @mcp.tool()\n",
    "    def generate_personalized_recipe(ingredients: List[str], ctx: Context) -> Recipe:\n",
    "        \"\"\"Generates a personalized recipe based on available ingredients.\"\"\"\n",
    "        # Simple logic: finds a recipe that uses the most provided ingredients.\n",
    "        best_recipe = None\n",
    "        max_matches = 0\n",
    "        for recipe in recipes:\n",
    "            matches = sum(1 for ingredient in ingredients if ingredient.lower() in [i.lower() for i in recipe[\"ingredients\"]])\n",
    "            if matches > max_matches:\n",
    "                max_matches = matches\n",
    "                best_recipe = recipe\n",
    "\n",
    "        if best_recipe:\n",
    "            ctx.info(f\"Generated personalized recipe: {best_recipe['name']}\")\n",
    "            return Recipe(**best_recipe)\n",
    "        else:\n",
    "            ctx.info(\"No matching recipe found.\")\n",
    "            return None\n",
    "\n",
    "    @mcp.prompt()\n",
    "    def recipe_recommendation_prompt(user_preferences: str, ctx: Context) -> str:\n",
    "        \"\"\"Generates a prompt for recipe recommendations.\"\"\"\n",
    "        return f\"Based on the user's preferences: '{user_preferences}', recommend a recipe.\"\n",
    "\n",
    "\n",
    "# requirements.txt:\n",
    "fastapi\n",
    "uvicorn\n",
    "pydantic\n",
    "mcp\n",
    "\n",
    "\n",
    "# To run the server:\n",
    "    # Install dependencies: pip install -r requirements.txt\n",
    "    # Run the server: mcp dev server.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCP Client (Example using Python SDK):\n",
    "\n",
    "\n",
    "from mcp.client.client import MCPClient\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    client = MCPClient(\"http://127.0.0.1:8000\") # default dev server address\n",
    "    await client.connect()\n",
    "\n",
    "    # Get all recipes\n",
    "    all_recipes = await client.get_resource(\"recipes://all\")\n",
    "    print(\"All Recipes:\", all_recipes)\n",
    "\n",
    "    # Search for recipes\n",
    "    search_results = await client.call_tool(\"search_recipes\", query=\"chicken\")\n",
    "    print(\"\\nSearch Results:\", search_results)\n",
    "\n",
    "    # Filter recipes\n",
    "    filter_results = await client.call_tool(\"filter_recipes\", filters={\"dietary_restrictions\": \"vegetarian\"})\n",
    "    print(\"\\nFiltered Recipes:\", filter_results)\n",
    "\n",
    "    # Generate personalized recipe\n",
    "    personalized_recipe = await client.call_tool(\"generate_personalized_recipe\", ingredients=[\"tomatoes\", \"onions\", \"beans\"])\n",
    "    print(\"\\nPersonalized Recipe:\", personalized_recipe)\n",
    "\n",
    "    # Use a prompt\n",
    "    prompt_result = await client.call_prompt(\"recipe_recommendation_prompt\", user_preferences=\"I want a quick and healthy dinner.\")\n",
    "    print(\"\\nPrompt Result:\", prompt_result)\n",
    "\n",
    "    await client.disconnect()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the langgraph.json file (see example)\n",
    "{\n",
    "    \"dockerfile_lines\": [], \n",
    "    \"graphs\": {\n",
    "        \"chat\": \"./src/react_agent/graph.py:graph\",\n",
    "        \"researcher\": \"./src/react_agent/graph.py:researcher\",\n",
    "        \"agent\": \"./src/react_agent/graph.py:agent\",\n",
    "    },\n",
    "    \"env\": [\n",
    "        \"OPENAI_API_KEY\",\n",
    "        \"WEAVIATE_API_KEY\",\n",
    "        \"WEAVIATE_URL\",\n",
    "        \"ANTHROPIC_API_KEY\",\n",
    "        \"ELASTIC_API_KEY\"\n",
    "    ],\n",
    "    # or\n",
    "    \"env\": \"./.env\",\n",
    "    \"python_version\": \"3.11\",\n",
    "    \"dependencies\": [\n",
    "        \".\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Step 2: Run the langgraph-cli command\n",
    "!pip install \"langgraph-cli[inmem]==0.1.55\" # Install the langgraph-cli package\n",
    "\n",
    "# Step 3: Move to the directory containing the langgraph.json file\n",
    "\n",
    "# Step 4: Install the dependencies\n",
    "    # If you are using requirements.txt:\n",
    "    python -m pip install -r requirements.txt\n",
    "\n",
    "    # If you are using pyproject.toml or setuptools:\n",
    "    python -m pip install -e .\n",
    "\n",
    "# Step 5: Run the LangGraph server\n",
    "langgraph dev # start a local development server\n",
    "# or\n",
    "uvx --refresh --from \"langgraph-cli[inmem]\" --with-editable . --python 3.11 langgraph dev\n",
    "        # uvx: python environment manager (like pyenv, conda, etc.). Creates isolated environments for Python applications.\n",
    "        # --refresh: This flag tells uvx to refresh or recreate the environment\n",
    "        # --from \"langgraph-cli[inmem]\": Specifies the source of the environment. In this case, it's from the langgraph-cli package.\n",
    "        # --with-editable .: installs the current directory (your LangGraph application) in \"editable\" mode\n",
    "        # --python 3.11: Specifies the Python version to use for the environment.\n",
    "        # langgraph dev: This command starts the LangGraph server in n-memory mode.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Basic Conditional Edge-------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    value: int\n",
    "    query: str\n",
    "    response: str\n",
    "    \n",
    "# Define the conditional function\n",
    "def conditional_edge(state: State) -> str:\n",
    "    if state[\"value\"] > 10:\n",
    "        return \"node_b\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_a\", lambda state: {\"value\": state[\"value\"] + 1})\n",
    "builder.add_node(\"node_b\", lambda state: {\"value\": state[\"value\"] - 1})\n",
    "builder.add_edge(START, \"node_a\")\n",
    "builder.add_conditional_edges(\"node_a\", conditional_edge)\n",
    "builder.add_edge(\"node_b\", \"node_a\")\n",
    "graph = builder.compile()\n",
    "\n",
    "# Test the graph\n",
    "initial_state = {\"value\": 5}\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "\n",
    "#------------------------------------Router with Multiple Conditions-------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "members = [\"researcher\", \"coder\"]\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        f\" following workers: {members}. Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    class Router(TypedDict):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options]\n",
    "\n",
    "    def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto)\n",
    "\n",
    "    return supervisor_node\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"search\", search_node)\n",
    "builder.add_node(\"web_scraper\", web_scraper_node)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "#------------------------------------Using Tool Conditions-------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", ToolNode([retriever_tool]))\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = MemorySaver()\n",
    "part_1_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------Using Tool conditions from Scratch-------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def tools_condition(\n",
    "    state: Union[list[AnyMessage], dict[str, Any], BaseModel],\n",
    "    messages_key: str = \"messages\",\n",
    ") -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n",
    "\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "\n",
    "    Args:\n",
    "        state (Union[list[AnyMessage], dict[str, Any], BaseModel]): The state to check for\n",
    "            tool calls. Must have a list of messages (MessageGraph) or have the\n",
    "            \"messages\" key (StateGraph).\n",
    "\n",
    "    Returns:\n",
    "        The next node to route to.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):\n",
    "        ai_message = messages[-1]\n",
    "    elif messages := getattr(state, messages_key, []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"    # you can change this to any other node name instead of \"__end__\"\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,  # decides if the agent is calling a tool or finishing\n",
    "    {\n",
    "        \"tools\": \"retrieve\",        # the dictionary is helpful if we named the nodes differently from the default tool condition function\n",
    "        END: END,  # if the agent does not call any tool, we end the graph\n",
    "    },\n",
    ")\n",
    "\n",
    "#------------------------------------Custom Condition functions-------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "from typing import Literal, Union, List, Dict, Any\n",
    "from langchain_core.messages import AnyMessage, HumanMessage\n",
    "\n",
    "def data_api_condition(\n",
    "    state: Union[List[AnyMessage], Dict[str, Any]],\n",
    "    messages_key: str = \"messages\",\n",
    ") -> Literal[\"data_api_node\", \"assistant_node\"]:\n",
    "    \"\"\"\n",
    "    Route to the Data API Node if the query involves fetching information.\n",
    "    Otherwise, route to the Assistant Node.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        user_message = state[-1]\n",
    "    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):\n",
    "        user_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state: {state}\")\n",
    "    \n",
    "    # Check if the query involves fetching information\n",
    "    if isinstance(user_message, HumanMessage) and any(keyword in user_message.content.lower() for keyword in [\"weather\", \"stock\", \"price\"]):\n",
    "        return \"data_api_node\"\n",
    "    return \"assistant_node\"\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------Custom Conditional Edges 2---------------------------------\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"agent\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    if not messages:\n",
    "        return \"agent\"  # Start the conversation\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message is from a tool\n",
    "    if isinstance(last_message, ToolMessage):\n",
    "        if last_message.content == \"File added successfully\":\n",
    "            state[\"file_added\"] = True\n",
    "            print(\"📌 File addition confirmed\")\n",
    "            return \"agent\"\n",
    "        print(\"🏁 Search complete, ending workflow\")\n",
    "        return END\n",
    "    \n",
    "    # If the last message is from the AI\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        # If the file is added but not indexed, wait\n",
    "        if state.get(\"file_added\") and not state.get(\"indexed\"):\n",
    "            print(\"⏳ Waiting for indexing to complete...\")\n",
    "            return \"agent\"\n",
    "        \n",
    "        # If the AI asks to call a tool\n",
    "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "            print(\"🛠️ Executing tool calls...\")\n",
    "            return \"tools\"\n",
    "    \n",
    "    return \"agent\"\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", ToolNode([tool_1, tool_2]))\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",  # Route to tools if tool calls are detected\n",
    "        \"agent\": \"agent\",  # Continue with the agent if no tool calls\n",
    "        END: END,          # End the workflow if conditions are met\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain Messages (HumanMessage, AIMessage, SystemMessage, BaseMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ChatMessage, ToolMessage, RemoveMessage\n",
    "\n",
    "# BaseMessage\n",
    "    # The base class for all message types. Inherited by HumanMessage, AIMessage, and SystemMessage\n",
    "    class CustomMessage(BaseMessage):\n",
    "        content: str\n",
    "        role: str  # e.g., \"user\", \"assistant\", \"system\"\n",
    "\n",
    "    custom_msg = CustomMessage(content=\"Hello, world!\", role=\"user\")\n",
    "\n",
    "#------------------ Message Types -------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# HumanMessage\n",
    "    # Represents a message from a human user.\n",
    "    human_msg = HumanMessage(content=\"My name is John Doe.\", name = \"Paul Okafor\")  # you can use the name of the node or agent\n",
    "    \n",
    "    # or\n",
    "    # Represents a message from a human user.\n",
    "    class HumanMessage(BaseMessage):\n",
    "        content: str\n",
    "\n",
    "    human_msg = HumanMessage(content=\"My name is John Doe.\")\n",
    "\n",
    "# AIMessage\n",
    "    # Represents a message generated by an AI agent.\n",
    "    ai_msg = AIMessage(content=\"I am a helpful assistant.\")\n",
    "\n",
    "# SystemMessage\n",
    "    # Represents a system message or prompt.\n",
    "    system_msg = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "# ChatMessage\n",
    "    # Represents a message in a chat conversation.\n",
    "    chat_msg = ChatMessage(role=\"custom_role\", content=\"This is a custom message.\")\n",
    "\n",
    "# ToolMessage\n",
    "    # Represents a message generated by a tool.\n",
    "    tool_msg = ToolMessage(content=\"This is a tool message.\", tool_call_id=\"123\", tool_name=\"GradeMaster\", id=\"123\")\n",
    "\n",
    "# RemoveMessage\n",
    "    # Represents a message to remove a message from the conversation.\n",
    "    remove_msg = [RemoveMessage(id=m.id) for m in state['messages'][:-2]]\n",
    "    \n",
    "#---------------------------------- When to use it----------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Example 1:\n",
    "messages = [SystemMessage(content=\"Welcome! Please provide your name.\")]\n",
    "user_input = \"My name is John Doe.\"\n",
    "messages.append(HumanMessage(content=user_input))\n",
    "messages.append(AIMessage(content=responses))\n",
    "\n",
    "# Example 2:\n",
    "messages = [\n",
    "    HumanMessage(content=\"My name is John Doe.\"),\n",
    "    AIMessage(content=\"Hello, John Doe! How can I assist you today?\"),\n",
    "]\n",
    "\n",
    "# Example 3: Prompt Template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Welcome! Please provide your name.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.invoke({\"messages\": [HumanMessage(content=\"John Doe\")]})\n",
    "chat_template.messages\n",
    "\n",
    "# Example 4: Agent Invocation\n",
    "agent = create_react_agent(tools=tools, llm=llm)\n",
    "messages = [    # Simulate a conversation\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Search for LangChain documentation.\"),\n",
    "]\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "\n",
    "\n",
    "# Example 5: Node Example\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    messages: Sequence[BaseMessage]\n",
    "\n",
    "def user_node(state: OverallState) -> OverallState:\n",
    "    try:\n",
    "        # Initialize the conversation if no messages exist\n",
    "        if not state.messages:\n",
    "            state.messages = [SystemMessage(content=\"Welcome! Please provide your name.\")]\n",
    "        \n",
    "        # Check if the last message is from the user (HumanMessage)\n",
    "        if state.messages and isinstance(state.messages[-1], HumanMessage):\n",
    "            # Invoke the LLM with the current state\n",
    "            response = llm.invoke(state.messages)\n",
    "            \n",
    "            # Append the LLM's response as an Assistant Message (AIMessage)\n",
    "            state.messages.append(AIMessage(content=response.content))\n",
    "        \n",
    "        return state\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        state.messages = state.messages + [SystemMessage(content=f\"An error occurred: {str(e)}\")]\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseAgent : The base class for all agent nodes. Provides a common interface for all agents..\n",
    "# LandGraphNode : A generic template for any node in the LandGraph.\n",
    "# Workflow : Compiles and executes the workflow by connecting all nodes.\n",
    "\n",
    "\n",
    "#--------------------------------------------- BaseAgent ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "from .utils.views import print_agent_output\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"\n",
    "    Base class for all agents. Provides common functionality and a standardized interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, websocket=None, stream_output=None, headers=None, tools: Optional[List[Any]] = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        Args:\n",
    "            websocket: WebSocket connection for real-time communication.\n",
    "            stream_output: Function to stream output to the client.\n",
    "            headers: Additional headers or metadata for the agent.\n",
    "            tools: A list of tools (functions or objects) that the agent can use.\n",
    "            **kwargs: Additional configuration for the agent.\n",
    "        \"\"\"\n",
    "        self.websocket = websocket\n",
    "        self.stream_output = stream_output\n",
    "        self.headers = headers or {}\n",
    "        self.tools = tools or []\n",
    "        self.config = kwargs\n",
    "\n",
    "    async def log(self, message: str, agent_name: str = \"AGENT\"):\n",
    "        \"\"\"\n",
    "        Log messages to the console or stream them via WebSocket.\n",
    "        Args:\n",
    "            message: The message to log.\n",
    "            agent_name: The name of the agent (for logging purposes).\n",
    "        \"\"\"\n",
    "        if self.websocket and self.stream_output:\n",
    "            await self.stream_output(\"logs\", agent_name.lower(), message, self.websocket)\n",
    "        else:\n",
    "            print_agent_output(message, agent_name)\n",
    "\n",
    "    async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the agent's task. Subclasses must implement this method.\n",
    "        Args:\n",
    "            state: The current state of the workflow.\n",
    "        Returns:\n",
    "            Updated state after the agent's execution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the `run` method.\")\n",
    "\n",
    "    def add_tool(self, tool: Any):\n",
    "        \"\"\"\n",
    "        Add a tool to the agent's toolkit.\n",
    "        Args:\n",
    "            tool: A function or object that the agent can use.\n",
    "        \"\"\"\n",
    "        self.tools.append(tool)\n",
    "\n",
    "    def get_tool(self, tool_name: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Retrieve a tool by name or identifier.\n",
    "        Args:\n",
    "            tool_name: The name or identifier of the tool.\n",
    "        Returns:\n",
    "            The tool if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for tool in self.tools:\n",
    "            if hasattr(tool, \"__name__\") and tool.__name__ == tool_name:\n",
    "                return tool\n",
    "            if hasattr(tool, \"name\") and tool.name == tool_name:\n",
    "                return tool\n",
    "        return None\n",
    "    \n",
    "\n",
    "#--------------------------------------------- LandGraphNode ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class LandGraphNode(BaseAgent):\n",
    "    \"\"\"\n",
    "    A generic node template for the LandGraph. Can be customized for any task.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the node.\n",
    "        Args:\n",
    "            node_name: The name of the node (for identification and logging).\n",
    "            **kwargs: Additional configuration for the node.\n",
    "        \"\"\"\n",
    "        super().__init__(node_name=node_name,**kwargs)\n",
    "        self.add_tool([tool_1, tool_2])\n",
    "\n",
    "    async def process(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process the input state and return the updated state.\n",
    "        Args:\n",
    "            state: The current state of the workflow.\n",
    "        Returns:\n",
    "            Updated state after processing.\n",
    "        \"\"\"\n",
    "        query = state.get(\"task\", {}).get(\"query\", \"\")\n",
    "        await self.log(f\"Processing task in node: {query}\", self.node_name.upper())\n",
    "\n",
    "        # Use the tabular search tool\n",
    "        tool = self.get_tool(\"tabular_search_tool\")\n",
    "        if tool:\n",
    "            results = await tool(query, self.table)\n",
    "            return {\"node_name\": self.node_name, \"results\": results}\n",
    "        else:\n",
    "            return {\"node_name\": self.node_name, \"error\": \"Tool not found\"}\n",
    "\n",
    "    async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the node's task.\n",
    "        Args:\n",
    "            state: The current state of the workflow.\n",
    "        Returns:\n",
    "            Updated state after the node's execution.\n",
    "        \"\"\"\n",
    "        return await self.process(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Workflow ----------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# This class is responsible for:\n",
    "    # Initializing all nodes.\n",
    "    # Defining the workflow graph.\n",
    "    # Compiling and executing the workflow.\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "import time\n",
    "\n",
    "class Workflow:\n",
    "    \"\"\"\n",
    "    The Workflow class compiles all nodes into a LandGraph and executes the workflow.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: Dict[str, Any], websocket=None, stream_output=None, headers=None):\n",
    "        \"\"\"\n",
    "        Initialize the workflow.\n",
    "        Args:\n",
    "            task: The task to execute. Must include a \"query\" and can include additional metadata.\n",
    "            websocket: WebSocket connection for real-time communication.\n",
    "            stream_output: Function to stream output to the client.\n",
    "            headers: Additional headers or metadata.\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.websocket = websocket\n",
    "        self.stream_output = stream_output\n",
    "        self.headers = headers or {}\n",
    "        self.task_id = self._generate_task_id()\n",
    "        self.nodes = self._initialize_nodes()\n",
    "\n",
    "    def _generate_task_id(self) -> int:\n",
    "        \"\"\"Generate a unique task ID.\"\"\"\n",
    "        return int(time.time())\n",
    "\n",
    "    def _initialize_nodes(self) -> Dict[str, LandGraphNode]:\n",
    "        \"\"\"\n",
    "        Initialize all nodes for the workflow.\n",
    "        Returns:\n",
    "            A dictionary of nodes, keyed by their names.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"node_1\": LandGraphNode(node_name=\"node_1\", websocket=self.websocket, stream_output=self.stream_output, headers=self.headers),\n",
    "            \"node_2\": LandGraphNode(node_name=\"node_2\", websocket=self.websocket, stream_output=self.stream_output, headers=self.headers),\n",
    "            \"node_3\": LandGraphNode(node_name=\"node_3\", websocket=self.websocket, stream_output=self.stream_output, headers=self.headers),\n",
    "        }\n",
    "\n",
    "    def _create_workflow_graph(self) -> StateGraph:\n",
    "        \"\"\"\n",
    "        Create the workflow graph using the initialized nodes.\n",
    "        Returns:\n",
    "            The compiled workflow graph.\n",
    "        \"\"\"\n",
    "        workflow = StateGraph(ResearchState)\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        for node_name, node in self.nodes.items():\n",
    "            workflow.add_node(node_name, node.run)\n",
    "\n",
    "        # Define edges between nodes\n",
    "        workflow.add_edge(\"node_1\", \"node_2\")\n",
    "        workflow.add_edge(\"node_2\", \"node_3\")\n",
    "        workflow.set_entry_point(\"node_1\")\n",
    "        workflow.add_edge(\"node_3\", END)\n",
    "\n",
    "        return workflow\n",
    "\n",
    "    async def execute(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the workflow.\n",
    "        Returns:\n",
    "            The final result of the workflow.\n",
    "        \"\"\"\n",
    "        workflow_graph = self._create_workflow_graph()\n",
    "        compiled_workflow = workflow_graph.compile()\n",
    "\n",
    "        await self.log(f\"Starting workflow for task: {self.task.get('query')}\", \"WORKFLOW\")\n",
    "        result = await compiled_workflow.ainvoke({\"task\": self.task})\n",
    "        return result\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "# call the workflow\n",
    "#----------------------------------------------\n",
    "task = {\n",
    "    \"query\": \"Process some data\",  # The main query or input\n",
    "    \"verbose\": True,               # Optional: Whether to log detailed output\n",
    "}\n",
    "\n",
    "# Initialize the workflow\n",
    "workflow = Workflow(task=task)\n",
    "\n",
    "# Execute the workflow\n",
    "result = await workflow.execute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LangSmith is a platform by LangChain that helps developers trace, debug, and evaluate LLM (Large Language Model) applications. \n",
    "# It provides tools for observability (seeing how your app works), testing (ensuring your app behaves as expected), and feedback collection \n",
    "# (improving your app based on user input).\n",
    "\n",
    "# Why is LangSmith Useful?\n",
    "    # Tracing : See how your app processes inputs and generates outputs (Logs every step of your app's execution).\n",
    "    # Testing : Evaluate your app's performance with datasets (Runs your app on datasets to measure performance).\n",
    "    # Feedback : Collect user feedback to improve your app.\n",
    "    # Observability : Monitor your app in production/real-time to catch issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- Setting Up LangSmith ---------------------------------\n",
    "import os\n",
    "from langsmith import Client, traceable, wrappers\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"my-project\" \n",
    "# Load environment variables from a .env file (optional)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Tracing with LangSmith ---------------------------------\n",
    "# @traceable Decorator : Automatically logs function calls.\n",
    "# Use trace context manager for specific blocks of code.\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "# Use @traceable to log function calls\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "# Use context manager for fine-grained tracing\n",
    "from langsmith import trace\n",
    "with trace(name=\"Generate Response\", run_type=\"chain\") as ls_trace:\n",
    "    response = call_openai(messages)\n",
    "    ls_trace.end(outputs={\"output\": response})\n",
    "\n",
    "# Wrap OpenAI client for automatic tracing\n",
    "from langsmith.wrappers import wrap_openai\n",
    "openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Testing and Evaluation with LangSmith ---------------------------------\n",
    "# Use create_dataset to create and manage datasets.\n",
    "# Define custom evaluators to score your app's outputs.\n",
    "# Use evaluate to run experiments and measure performance.\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "# Create a dataset\n",
    "client = Client()\n",
    "\n",
    "#---------------------------------\n",
    "# Create a dataset\n",
    "#---------------------------------\n",
    "\n",
    "# Create dataset for testing our AI agents\n",
    "dataset_input = [\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\"},\n",
    "    {\"input\": \"Who wrote the book '1984'?\",  \"output\": \"George Orwell\"},\n",
    "    {\"input\": \"What is the square root of 16?\",  \"output\": \"4\"},\n",
    "]\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name = \"my-dataset\", \n",
    "    description=\"A dataset for testing AI agents.\")\n",
    "\n",
    "for data in dataset_input:\n",
    "    try:\n",
    "        client.create_example(\n",
    "            inputs={\"question\": data['input']},  # Wrapping the input into a dictionary\n",
    "            outputs={\"answer\": data['output']},  # Wrapping the output into a dictionary\n",
    "            dataset_id=dataset.id  # Assuming dataset.id is already created\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create example for input: {data['input']}, Error: {e}\")\n",
    "    \n",
    "# or use create_examples to add multiple examples at once\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": data['input']}],\n",
    "    outputs=[{\"output\": data['output']}],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "\n",
    "#---------------------------------\n",
    "# Create a Target or label\n",
    "#---------------------------------\n",
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs: dict) -> dict:\n",
    "  response = openai_client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },\n",
    "          { \"role\": \"user\", \"content\": inputs[\"question\"] },\n",
    "      ],\n",
    "  )\n",
    "  return { \"response\": response.choices[0].message.content.strip() }\n",
    "\n",
    "\n",
    "#---------------------------------\n",
    "# Define an Evaluator\n",
    "#---------------------------------\n",
    "\n",
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "  score: bool = Field(\n",
    "      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"\n",
    "  )\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "  response = openai_client.beta.chat.completions.parse(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "          { \"role\": \"system\", \"content\": instructions },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]}; \n",
    "              Student's Answer: {outputs[\"response\"]}\"\"\"\n",
    "          },\n",
    "      ],\n",
    "      response_format=Grade,\n",
    "  )\n",
    "  return response.choices[0].message.parsed.score\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------\n",
    "# Run and View results\n",
    "#---------------------------------\n",
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "  target,\n",
    "  data=\"my-dataset\",\n",
    "  evaluators=[\n",
    "      accuracy,\n",
    "      # can add multiple evaluators here\n",
    "  ],\n",
    "  experiment_prefix=\"first-eval-in-langsmith\",\n",
    "  max_concurrency=2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Prompt Engineering ---------------------------------\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "# Pull a prompt from LangSmith Prompt Hub\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"your-prompt-id\")\n",
    "\n",
    "# Use the prompt in your app\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is LangSmith?\"})\n",
    "messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "response = openai_client.chat.completions.create(model=\"gpt-4\", messages=messages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Collecting Human Feedback ---------------------------------\n",
    "# Add feedback to a run\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "import uuid\n",
    "\n",
    "@traceable\n",
    "def foo():\n",
    "    return \"This is a sample Run!\"\n",
    "\n",
    "\n",
    "client = Client()\n",
    "client.create_feedback(\n",
    "    run_id=\"your-run-id\",\n",
    "    key=\"user_feedback\",\n",
    "    score=1.0,\n",
    "    comment=\"The response was helpful.\"\n",
    ")\n",
    "\n",
    "# Pre-generate run IDs for feedback\n",
    "pre_defined_run_id = uuid.uuid4()\n",
    "foo(langsmith_extra={\"run_id\": pre_defined_run_id})\n",
    "client.create_feedback(pre_defined_run_id, \"user_feedback\", score=1)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Production Observability ---------------------------------\n",
    "# Filter runs in production\n",
    "from langsmith import Client\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = Client()\n",
    "runs = client.list_runs(\n",
    "    project_name=\"langsmith-academy\",\n",
    "    filter=\"eq(is_root, true)\",\n",
    "    start_time=datetime.now() - timedelta(days=1)\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    print(run)\n",
    "\n",
    "# Run your app to trigger online evaluations\n",
    "from app import langsmith_rag\n",
    "question = \"How do I set up tracing?\"\n",
    "langsmith_rag(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `interrupt` function instead.\n",
    "\n",
    "#------------------------- Basic Human-in-the-Loop with Breakpoints--------------------------------\n",
    "# Compile graph with breakpoint\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory, \n",
    "    interrupt_before=[\"step_for_human_in_the_loop\"] # Add breakpoint\n",
    ")\n",
    "\n",
    "# Run graph up to breakpoint\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Perform human action (e.g., approve, edit, input)\n",
    "# Resume graph execution\n",
    "# Human approval step\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "if user_approval.lower() == \"yes\":\n",
    "    # Resume graph execution\n",
    "    for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "        print(event)\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")\n",
    "\n",
    "#------------------------- Dynamic Breakpoints--------------------------------\n",
    "# Dynamic breakpoints allow the graph to interrupt itself based on conditions defined within a node.\n",
    "# can define some *condition* that must be met for a breakpoint to be triggered\n",
    "from langgraph.errors import NodeInterrupt\n",
    "\n",
    "# Define a node with dynamic breakpoint\n",
    "def my_node(state: State) -> State:\n",
    "    if len(state['input']) > 5:  # Condition for breakpoint\n",
    "        raise NodeInterrupt(f\"Input too long: {state['input']}\")\n",
    "    return state\n",
    "\n",
    "# Resume after dynamic breakpoint\n",
    "graph.update_state(config=thread_config, values={\"input\": \"foo\"})  # Update state to pass condition\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Skip node entirely\n",
    "graph.update_state(None, config=thread_config, as_node=\"my_node\")  # Skip node\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    \n",
    "\n",
    "#------------------------- Editing State with Human Feedback--------------------------------\n",
    "# You can modify the graph state during interruptions to incorporate human feedback.\n",
    "\n",
    "# Get current state after interruption\n",
    "state = graph.get_state(thread_config)\n",
    "print(state)\n",
    "\n",
    "# Update state with human feedback\n",
    "graph.update_state(\n",
    "    thread_config, \n",
    "    {\"user_input\": \"human feedback\"},  # Add human input\n",
    "    as_node=\"human_input\"  # Treat update as a node\n",
    ")\n",
    "\n",
    "# Resume graph execution\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "\n",
    "#------------------------- Input Pattern or Tool Call--------------------------------\n",
    "    \n",
    " # Compile graph with input breakpoint\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer, \n",
    "    interrupt_before=[\"human_input\"]  # Node for human input\n",
    ")\n",
    "\n",
    "# Run graph up to input breakpoint\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Add human input and resume\n",
    "graph.update_state(\n",
    "    thread_config, \n",
    "    {\"user_input\": \"human input\"},  # Provide human input or tool call\n",
    "    as_node=\"human_input\"  # Treat update as node\n",
    ")\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)   \n",
    "    \n",
    "\n",
    "\n",
    "#------------------------- LangGraph API Integration--------------------------------\n",
    "from langgraph_sdk import get_client\n",
    "\n",
    "# Connect to LangGraph Studio\n",
    "client = get_client(url=\"http://localhost:56091\")\n",
    "\n",
    "# Stream graph with breakpoint\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],  # Set breakpoint\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    print(chunk.data)\n",
    "\n",
    "# Resume from breakpoint\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    print(chunk.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------- Short term Memory --------------------------------\n",
    "# Short-term memory is managed using checkpointers , which save the state of a graph at each step.\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize a checkpointer for short-term memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile a graph with the checkpointer\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run the graph and save state\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Retrieve the state from the checkpointer\n",
    "state = graph.get_state(thread_config)\n",
    "print(state)\n",
    "\n",
    "\n",
    "#------------------------- Long-Term Memory with Stores (Memory Store) --------------------------------\n",
    "# Long-term memory is managed using stores , which persist data across threads or sessions.\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize a store for long-term memory\n",
    "store = InMemoryStore()\n",
    "\n",
    "#---------------------\n",
    "# Save a memory\n",
    "#----------------------\n",
    "# Save a memory\n",
    "user_id = \"1\"\n",
    "namespace = (user_id, \"memories\") # Namespace for user-specific memories\n",
    "key = \"profile\"\n",
    "value = {\"name\": \"Lance\", \"interests\": [\"biking\", \"bakeries\"]}\n",
    "store.put(namespace, key, value)    # Save a memory to the store\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Save a memory 2 (Memory Schema Collection)\n",
    "#---------------------\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a memory schema\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory.\")\n",
    "\n",
    "# Create a collection of memories\n",
    "memory_collection = [\n",
    "    Memory(content=\"User likes biking\"),\n",
    "    Memory(content=\"User enjoys bakeries\")\n",
    "]\n",
    "\n",
    "# Save memories to the store\n",
    "for memory in memory_collection:\n",
    "    key = str(uuid.uuid4())\n",
    "    store.put(namespace, key, memory.model_dump())\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "#---------------------------------\n",
    "# Retrieve a memory from the store\n",
    "#---------------------------------\n",
    "memories = store.get(namespace, key)\n",
    "print(memories.value)\n",
    "# or\n",
    "memories = store.search(namespace)\n",
    "for memory in memories:\n",
    "    print(memory.value)\n",
    "\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "#---------------------------------\n",
    "# Dynamic Memory Updates\n",
    "#---------------------------------\n",
    "# Dynamic memory updates allow the agent to decide when to save memories and what type of memory to update \n",
    "# (e.g., profile, collection, or instructions)\n",
    "\n",
    "def update_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memories = store.search(namespace)\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories_formatted = [(m.key, tool_name, m.value) for m in existing_memories]\n",
    "    result = trustcall_extractor.invoke({\"messages\": state[\"messages\"], \"existing\": existing_memories_formatted})\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace, rmeta.get(\"json_doc_id\", str(uuid.uuid4())), r.model_dump())\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------- Memory Agents ------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define nodes for the agent\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    # Retrieve memory and personalize responses\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    memory_content = \"\\n\".join([m.value[\"content\"] for m in memories])\n",
    "    system_msg = f\"Memory: {memory_content}\"\n",
    "    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    # Reflect on chat history and save memories\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    result = trustcall_extractor.invoke({\"messages\": state[\"messages\"]})\n",
    "    for r in result[\"responses\"]:\n",
    "        key = str(uuid.uuid4())\n",
    "        store.put(namespace, key, r.model_dump())\n",
    "\n",
    "# Compile the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Compile with memory store and checkpointer\n",
    "graph = builder.compile(checkpointer=MemorySaver(), store=InMemoryStore())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Long-Term Memory with Stores (Memory Store) --------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Long-Term Memory with Stores (Memory Store) --------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Project 1 (code that works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated, Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "import json # Parse JSON response\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "            model=\"deepseek-chat\",\n",
    "            base_url=\"https://api.deepseek.com\",\n",
    "            streaming=True,\n",
    "            callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "        )\n",
    "        \n",
    "    def call_tool(self):\n",
    "        tool = TavilySearchResults(max_results=2)\n",
    "        self.tools = [tool]\n",
    "        self.tool_node = ToolNode(tools=[tool])\n",
    "        self.llm_with_tool = self.llm.bind_tools(self.tools)\n",
    "        \n",
    "    def call_model(self, state: MessagesState):\n",
    "        \"\"\"\n",
    "        LLM node to process the user's query and invoke tools if needed.\n",
    "        \"\"\"\n",
    "        messages = state['messages']\n",
    "        latest_query = messages[-1].content if messages else \"No query provided.\"\n",
    "\n",
    "        # template = ''' \n",
    "        # You are a travel suggestion agent. Answer the user's questions based on their travel preferences. \n",
    "        # If you need to find information about a specific destination, use the search_tool. Understand that the information was retrieved from the web,\n",
    "        # interpret it, and generate a response accordingly.\n",
    "\n",
    "        # Answer the following questions as best as you can. You have access to the following tools:\n",
    "        # {tools}\n",
    "\n",
    "        # Use the following format:\n",
    "        # Question: the input question you must answer\n",
    "        # Thought: you should always think about what to do\n",
    "        # Action: the action you should take, should be one of [{tool_names}]\n",
    "        # Action Input: the input to the action\n",
    "        # Observation: the result of the action\n",
    "        # Thought: I now know the final answer\n",
    "        # Final Answer: [Your final answer here as a concise and complete sentence]\n",
    "\n",
    "        # Ensure the response strictly follows this format. Do not repeat the Final Answer multiple times.\n",
    "\n",
    "        # Begin!\n",
    "        # Question: {input}\n",
    "        # Thought: {agent_scratchpad}\n",
    "        # '''\n",
    "\n",
    "        # Improved prompt to enforce JSON output\n",
    "        template = ''' \n",
    "        You are a travel suggestion agent. Answer the user's questions based on their travel preferences. \n",
    "        If you need to find information about a specific destination, use the search_tool. Understand that the information was retrieved from the web,\n",
    "        interpret it, and generate a response accordingly.\n",
    "\n",
    "        Answer the following questions as best as you can. You have access to the following tools:\n",
    "        {tools}\n",
    "\n",
    "        Use the following format:\n",
    "        \n",
    "        \"Question\": the input question you must answer\n",
    "        \"Thought\": your reasoning about what to do next\n",
    "        \"Action\": the action you should take, one of [{tool_names}] (if no action is needed, write \"None\")\n",
    "        \"Action Input\": the input to the action (if no action is needed, write \"None\")\n",
    "        \"Observation\": the result of the action (if no action is needed, write \"None\")\n",
    "        \"Thought\": your reasoning after observing the action\n",
    "        \"Final Answer\": the final answer to the original input question\n",
    "        \n",
    "        Ensure every Thought is followed by an Action, Action Input, and Observation. If no tool is needed, explicitly write \"None\" for Action, Action Input, and Observation.\n",
    "\n",
    "        Begin!\n",
    "        Question: {input}\n",
    "        Thought: {agent_scratchpad}\n",
    "        '''\n",
    "        \n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        search_agent = create_react_agent(\n",
    "            llm=self.llm_with_tool,\n",
    "            prompt=prompt,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=search_agent,\n",
    "            tools=self.tools,\n",
    "            verbose=True,\n",
    "            return_intermediate_steps=False,\n",
    "            handle_parsing_errors=True,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = agent_executor.invoke({\n",
    "                \"input\": latest_query,\n",
    "                \"agent_scratchpad\": \"\"  # Initialize with an empty scratchpad\n",
    "            })\n",
    "\n",
    "            # Check if the response is already a dictionary\n",
    "            if isinstance(response, dict):\n",
    "                final_answer = response.get(\"output\", \"No final answer provided.\")\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected response type. Expected a dictionary.\")\n",
    "\n",
    "            # print(\"\")\n",
    "            # print(f'response: {response}')\n",
    "                \n",
    "            # # Validate and clean response\n",
    "            # if not response.startswith(\"Final Answer:\"):\n",
    "            #     raise ValueError(\"Invalid agent response format. Missing 'Final Answer:' prefix.\")\n",
    "            # final_answer = response.replace(\"Final Answer:\", \"\")[-1].strip()\n",
    "            \n",
    "            state['messages'].append(AIMessage(content=final_answer))  # Append clean response to messages\n",
    "            return state\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error: {e}\"\n",
    "            state['messages'].append(AIMessage(content=error_message))\n",
    "            return state\n",
    "\n",
    "    \n",
    "    def router_function(self, state: MessagesState) -> Literal[\"tools\", END]:\n",
    "        \"\"\"\n",
    "        Determine the next node based on tool invocation.\n",
    "        \"\"\"\n",
    "        messages = state['messages']\n",
    "        last_message = messages[-1]\n",
    "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "    \n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Build and return the workflow graph.\n",
    "        \"\"\"\n",
    "        self.call_tool()\n",
    "        workflow = StateGraph(MessagesState)\n",
    "        workflow.add_node(\"agent\", self.call_model)\n",
    "        workflow.add_node(\"tools\", self.tool_node)\n",
    "        workflow.add_edge(START, \"agent\")\n",
    "        workflow.add_conditional_edges(\"agent\", self.router_function, {\"tools\": \"tools\", END: END})\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        self.app = workflow.compile()\n",
    "        return self.app\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mybot = Chatbot()\n",
    "    workflow = mybot()\n",
    "\n",
    "    # Properly initialize MessagesState with HumanMessage objects\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Search tthe web and tell me about Airi Shimamura from Oklahoma?\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = workflow.invoke(initial_state)\n",
    "    print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Project 2 (code that works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "# from langchain_aws import ChatBedrock\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import os\n",
    "import boto3\n",
    "from typing import Annotated\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "@tool\n",
    "def compute_savings(monthly_cost: float) -> float:\n",
    "    \"\"\"\n",
    "    Tool to compute the potential savings when switching to solar energy based on the user's monthly electricity cost.\n",
    "    \n",
    "    Args:\n",
    "        monthly_cost (float): The user's current monthly electricity cost.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'number_of_panels': The estimated number of solar panels required.\n",
    "            - 'installation_cost': The estimated installation cost.\n",
    "            - 'net_savings_10_years': The net savings over 10 years after installation costs.\n",
    "    \"\"\"\n",
    "    def calculate_solar_savings(monthly_cost):\n",
    "        # Assumptions for the calculation\n",
    "        cost_per_kWh = 0.28  \n",
    "        cost_per_watt = 1.50  \n",
    "        sunlight_hours_per_day = 3.5  \n",
    "        panel_wattage = 350  \n",
    "        system_lifetime_years = 10  \n",
    "        # Monthly electricity consumption in kWh\n",
    "        monthly_consumption_kWh = monthly_cost / cost_per_kWh\n",
    "        \n",
    "        # Required system size in kW\n",
    "        daily_energy_production = monthly_consumption_kWh / 30\n",
    "        system_size_kW = daily_energy_production / sunlight_hours_per_day\n",
    "        \n",
    "        # Number of panels and installation cost\n",
    "        number_of_panels = system_size_kW * 1000 / panel_wattage\n",
    "        installation_cost = system_size_kW * 1000 * cost_per_watt\n",
    "        \n",
    "        # Annual and net savings\n",
    "        annual_savings = monthly_cost * 12\n",
    "        total_savings_10_years = annual_savings * system_lifetime_years\n",
    "        net_savings = total_savings_10_years - installation_cost\n",
    "        \n",
    "        return {\n",
    "            \"number_of_panels\": round(number_of_panels),\n",
    "            \"installation_cost\": round(installation_cost, 2),\n",
    "            \"net_savings_10_years\": round(net_savings, 2)\n",
    "        }\n",
    "    # Return calculated solar savings\n",
    "    return calculate_solar_savings(monthly_cost)\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    \"\"\"\n",
    "    Function to handle errors that occur during tool execution.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the AI agent, which includes messages and tool call details.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing error messages for each tool that encountered an issue.\n",
    "    \"\"\"\n",
    "    # Retrieve the error from the current state\n",
    "    error = state.get(\"error\")\n",
    "    \n",
    "    # Access the tool calls from the last message in the state's message history\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    \n",
    "    # Return a list of ToolMessages with error details, linked to each tool call ID\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",  # Format the error message for the user\n",
    "                tool_call_id=tc[\"id\"],  # Associate the error message with the corresponding tool call ID\n",
    "            )\n",
    "            for tc in tool_calls  # Iterate over each tool call to produce individual error messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create a tool node with fallback error handling.\n",
    "    \n",
    "    Args:\n",
    "        tools (list): A list of tools to be included in the node.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A tool node that uses fallback behavior in case of errors.\n",
    "    \"\"\"\n",
    "    # Create a ToolNode with the provided tools and attach a fallback mechanism\n",
    "    # If an error occurs, it will invoke the handle_tool_error function to manage the error\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)],  # Use a lambda function to wrap the error handler\n",
    "        exception_key=\"error\"  # Specify that this fallback is for handling errors\n",
    "    )\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        # Initialize with the runnable that defines the process for interacting with the tools\n",
    "        self.runnable = runnable\n",
    "    def __call__(self, state: State):\n",
    "        while True:\n",
    "            # Invoke the runnable with the current state (messages and context)\n",
    "            result = self.runnable.invoke(state)\n",
    "            \n",
    "            # If the tool fails to return valid output, re-prompt the user to clarify or retry\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                # Add a message to request a valid response\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                # Break the loop when valid output is obtained\n",
    "                break\n",
    "        # Return the final state after processing the runnable\n",
    "        return {\"messages\": result}\n",
    "\n",
    "# def get_bedrock_client(region):\n",
    "#     return boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "\n",
    "# def create_bedrock_llm(client):\n",
    "#     return ChatBedrock(model_id='anthropic.claude-3-sonnet-20240229-v1:0', client=client, model_kwargs={'temperature': 0}, region_name='us-east-1')\n",
    "\n",
    "# llm = create_bedrock_llm(get_bedrock_client(region='us-east-1'))\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful customer support assistant for Solar Panels Belgium.\n",
    "            You should get the following information from them:\n",
    "            - monthly electricity cost\n",
    "            If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "            After you are able to discern all the information, call the relevant tool.\n",
    "            ''',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the tools the assistant will use\n",
    "part_1_tools = [\n",
    "    compute_savings\n",
    "]\n",
    "\n",
    "# Bind the tools to the assistant's workflow\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools, tool_choice=\"any\")\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "\n",
    "builder.add_edge(START, \"assistant\")  # Start with the assistant\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition)  # Move to tools after input\n",
    "builder.add_edge(\"tools\", \"assistant\")  # Return to assistant after tool execution\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# import shutil\n",
    "import uuid\n",
    "\n",
    "def _print_event(event, _printed):\n",
    "    \"\"\"\n",
    "    Helper function to print events generated by the graph.\n",
    "    \n",
    "    Args:\n",
    "        event: The event to print.\n",
    "        _printed: A set to track already printed events to avoid duplicates.\n",
    "    \"\"\"\n",
    "    if event[\"messages\"]:\n",
    "        for message in event[\"messages\"]:\n",
    "            if message.id not in _printed:\n",
    "                # Check the type of the message and print accordingly\n",
    "                if hasattr(message, \"type\"):\n",
    "                    print(f\"Type: {message.type}, Content: {message.content}\")\n",
    "                elif hasattr(message, \"role\"):\n",
    "                    print(f\"Role: {message.role}, Content: {message.content}\")\n",
    "                else:\n",
    "                    print(f\"Message: {message}\")\n",
    "                _printed.add(message.id)\n",
    "                \n",
    "                \n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    'hey',\n",
    "    'can you calculate my energy saving',\n",
    "    \"my montly cost is $100, what will i save\"\n",
    "]\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "_printed = set()\n",
    "for question in tutorial_questions:\n",
    "    events = graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SMolAgents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SmolAgents Cheatsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmoLAGENTS Advanced Cheatsheet\n",
    "\n",
    "## Table of Contents\n",
    "    #introduction-to-smolagents)\n",
    "    #installation-and-setup)\n",
    "    #agent-types)\n",
    "    #creating-basic-agents)\n",
    "    #working-with-tools)\n",
    "    #creating-custom-tools)\n",
    "    #using-built-in-tools)\n",
    "    #loading-tools-from-hub)\n",
    "    #models-and-integration)\n",
    "    #multi-agent-systems)\n",
    "    #advanced-customization)\n",
    "    #debugging-and-best-practices)\n",
    "    #security-considerations)\n",
    "    #examples-and-use-cases)\n",
    "\n",
    "## Introduction to SmoLAGENTS\n",
    "    # SmoLAGENTS is a lightweight, minimalist library from Hugging Face designed for creating AI agents with a focus on simplicity and \n",
    "    # efficiency. It enables agents to perform actions using Python code snippets rather than JSON or text formats.\n",
    "\n",
    "    # **Key features:**\n",
    "    # - Code-based approach to agent actions\n",
    "    # - Support for various LLM providers\n",
    "    # - Minimal abstractions (~1000 lines of code for core functionality)\n",
    "    # - Deep integration with Hugging Face Hub\n",
    "    # - Support for multi-agent systems\n",
    "    # - Vision, audio, and other modality support\n",
    "\n",
    "\n",
    "## Installation and Setup\n",
    "    # Install the package\n",
    "    pip install smolagents\n",
    "\n",
    "    # Basic imports\n",
    "    from smolagents import CodeAgent, HfApiModel, tool, DuckDuckGoSearchTool\n",
    "\n",
    "\n",
    "## Agent Types\n",
    "    # SmoLAGENTS supports two primary agent types:\n",
    "\n",
    "### 1. CodeAgent\n",
    "    # CodeAgents generate Python code snippets to perform actions. They're more efficient than traditional tool-calling agents, \n",
    "    # using 30% fewer steps and achieving better performance on complex tasks.\n",
    "    \n",
    "    from transformers.agents import CodeAgent\n",
    "    agent = CodeAgent(\n",
    "        tools=[],  # List of tools the agent can use\n",
    "        model=HfApiModel(model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\"),\n",
    "        add_base_tools=True  # Adds default tools\n",
    "    )\n",
    "    agent.run(\"What is the result of 2 power 3.7384?\")\n",
    "\n",
    "### 2. ToolCallingAgent\n",
    "    # ToolCallingAgents generate actions as JSON/text blobs, similar to the approach used by OpenAI and Anthropic.\n",
    "    from smolagents import ToolCallingAgent\n",
    "    agent = ToolCallingAgent(\n",
    "        tools=[DuckDuckGoSearchTool()],\n",
    "        model=HfApiModel()\n",
    "    )\n",
    "    agent.run(\"What is the result of 2 power 3.7384?\")\n",
    "\n",
    "### 3. React AGent\n",
    "    # React Code Agent\n",
    "        from transformers.agents import ReactCodeAgent\n",
    "        agent = ReactCodeAgent(\n",
    "            tools=[DuckDuckGoSearchTool()],\n",
    "            model=HfApiModel()\n",
    "        )\n",
    "        agent.run(\"What is the result of 2 power 3.7384?\")\n",
    "\n",
    "    # ReacyJson Agent\n",
    "        from transformers.agents import ReactJsonAgent\n",
    "        agent = ReactJsonAgent(\n",
    "            tools=[DuckDuckGoSearchTool()],\n",
    "            model=HfApiModel()\n",
    "        )\n",
    "        agent.run(task = \"What is the result of 2 power 3.7384?\", stream=True)\n",
    "    \n",
    "\n",
    "\n",
    "## Creating Basic Agents\n",
    "\n",
    "    # 1. **Model** - Powers the agent's reasoning (required)\n",
    "    # 2. **Tools** - Functions the agent can use (can be empty)\n",
    "\n",
    "\n",
    "    # Minimal agent creation\n",
    "    from smolagents import CodeAgent, HfApiModel\n",
    "\n",
    "    # Choose a model\n",
    "    model = HfApiModel(model_id=\"meta-llama/Llama-3.3-70B-Instruct\")\n",
    "    # Or use default model with no token required\n",
    "    # model = HfApiModel()\n",
    "\n",
    "    # Create an agent with no tools\n",
    "    agent = CodeAgent(tools=[], model=model)\n",
    "\n",
    "    # Run a task\n",
    "    result = agent.run(\"Calculate the 10th Fibonacci number\")\n",
    "\n",
    "\n",
    "\n",
    "## Working with Tools\n",
    "    ### Creating Custom Tools\n",
    "        #### Method 1: Using the @tool Decorator (Simplest)\n",
    "            from smolagents import tool\n",
    "            from typing import Optional\n",
    "\n",
    "            @tool\n",
    "            def get_weather(location: str, date: Optional[str] = None) -> str:\n",
    "                \"\"\"Gets weather information for a location.\n",
    "                \n",
    "                Args:\n",
    "                    location: The city or place to get weather for\n",
    "                    date: Optional date in YYYY-MM-DD format, defaults to today\n",
    "                \"\"\"\n",
    "                # Tool implementation here\n",
    "                import requests\n",
    "                # API call code\n",
    "                return \"Weather data for location\"\n",
    "\n",
    "\n",
    "        #### Method 2: Creating a Tool Class (More Control)\n",
    "            from smolagents import Tool\n",
    "\n",
    "            class HFModelDownloadsTool(Tool):\n",
    "                name = \"model_download_counter\"\n",
    "                description = \"Returns the most downloaded model of a given task on the Hugging Face Hub.\"\n",
    "                inputs = {\n",
    "                    \"task\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The task to search for (e.g., 'text-classification', 'image-classification')\"\n",
    "                    }\n",
    "                }\n",
    "                output_type = \"string\"\n",
    "                \n",
    "                def forward(self, task: str) -> str:\n",
    "                    # Implementation here\n",
    "                    from huggingface_hub import HfApi\n",
    "                    api = HfApi()\n",
    "                    # Logic to find most downloaded model\n",
    "                    return \"Model name and download count\"\n",
    "\n",
    "            # or\n",
    "            \n",
    "            from transformers import Tool\n",
    "            from PIL import Image\n",
    "\n",
    "            class ImageResizeTool(Tool):\n",
    "                name = \"image-resizer\"\n",
    "                description = \"Resizes an image to the specified dimensions.\"\n",
    "                inputs = {\n",
    "                    \"image\": {\"type\": Image.Image, \"description\": \"The image to resize\"},\n",
    "                    \"width\": {\"type\": int, \"description\": \"Target width in pixels\"},\n",
    "                    \"height\": {\"type\": int, \"description\": \"Target height in pixels\"}\n",
    "                }\n",
    "                output_type = Image.Image\n",
    "                \n",
    "                def __call__(self, image, width, height):\n",
    "                    return image.resize((width, height))\n",
    "\n",
    "    ### Using PipelineTool for Models\n",
    "        # For tools that wrap Transformer models, you can use the PipelineTool class:\n",
    "        from transformers import PipelineTool, AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "        class TextGeneratorTool(PipelineTool):\n",
    "            name = \"text-generator\"\n",
    "            description = \"Generates text based on a prompt.\"\n",
    "            default_checkpoint = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "            model_class = AutoModelForCausalLM\n",
    "            pre_processor_class = AutoTokenizer\n",
    "            \n",
    "            def __call__(self, prompt: str, max_length: int = 100) -> str:\n",
    "                \"\"\"\n",
    "                Generates text based on the input prompt.\n",
    "                \n",
    "                Args:\n",
    "                    prompt: The text prompt to generate from.\n",
    "                    max_length: Maximum length of generated text.\n",
    "                    \n",
    "                Returns:\n",
    "                    Generated text.\n",
    "                \"\"\"\n",
    "                inputs = self.encode(prompt)\n",
    "                outputs = self.forward(inputs)\n",
    "                return self.decode(outputs)[0][\"generated_text\"]\n",
    "\n",
    "\n",
    "    ### Using Built-in Tools\n",
    "        # SmoLAGENTS provides a default toolbox that you can add with `add_base_tools=True`:\n",
    "\n",
    "            # Default tools include:\n",
    "            # - DuckDuckGo web search\n",
    "            # - Python code interpreter (for ToolCallingAgent)\n",
    "            # - Transcriber (speech-to-text)\n",
    "\n",
    "            # Add just the DuckDuckGo search tool explicitly\n",
    "            from smolagents import DuckDuckGoSearchTool\n",
    "            agent = CodeAgent(\n",
    "                tools=[DuckDuckGoSearchTool()], \n",
    "                model=HfApiModel()\n",
    "            )\n",
    "\n",
    "            # Using a tool directly\n",
    "            search_tool = DuckDuckGoSearchTool()\n",
    "            result = search_tool(\"What is the capital of France?\")\n",
    "            print(result)\n",
    "\n",
    "\n",
    "    ### Loading Tools from Hub\n",
    "        from smolagents import load_tool, CodeAgent, HfApiModel\n",
    "\n",
    "        # Load a tool from the Hub (requires trust_remote_code=True for security)\n",
    "        image_generation_tool = load_tool(\n",
    "            \"m-ric/text-to-image\", \n",
    "            trust_remote_code=True\n",
    "        )\n",
    "\n",
    "        # Use the loaded tool in an agent\n",
    "        agent = CodeAgent(\n",
    "            tools=[image_generation_tool],\n",
    "            model=HfApiModel()\n",
    "        )\n",
    "\n",
    "\n",
    "    #### Loading a Space as a Tool\n",
    "        from smolagents import Tool\n",
    "\n",
    "        # Import a Hugging Face Space as a tool\n",
    "        image_generation_tool = Tool.from_space(\n",
    "            \"black-forest-labs/FLUX.1-schnell\",\n",
    "            name=\"image_generator\",\n",
    "            description=\"Generate an image from a prompt\"\n",
    "        )\n",
    "\n",
    "        # or\n",
    "    \n",
    "        from transformers import Tool\n",
    "\n",
    "        image_generator = Tool.from_space(\n",
    "            space_id=\"stabilityai/stable-diffusion\",\n",
    "            name=\"image-generator\",\n",
    "            description=\"Generates images from text prompts using Stable Diffusion.\"\n",
    "        )\n",
    "\n",
    "\n",
    "    #### From other frameworks\n",
    "\n",
    "        # From LangChain\n",
    "        from transformers import Tool\n",
    "        from langchain.tools import BaseTool\n",
    "\n",
    "        class LangChainCalculatorTool(BaseTool):\n",
    "            name = \"calculator\"\n",
    "            description = \"Useful for performing calculations\"\n",
    "            \n",
    "            def _run(self, query):\n",
    "                return eval(query)\n",
    "            \n",
    "            def _arun(self, query):\n",
    "                return eval(query)\n",
    "\n",
    "        calculator_tool = Tool.from_langchain(LangChainCalculatorTool())\n",
    "\n",
    "        # From Gradio\n",
    "        import gradio as gr\n",
    "        from transformers import Tool\n",
    "\n",
    "        def calculator_fn(expression):\n",
    "            return eval(expression)\n",
    "\n",
    "        with gr.Blocks() as demo:\n",
    "            inp = gr.Textbox(label=\"Expression\")\n",
    "            out = gr.Number(label=\"Result\")\n",
    "            btn = gr.Button(\"Calculate\")\n",
    "            btn.click(fn=calculator_fn, inputs=inp, outputs=out)\n",
    "\n",
    "        calculator_tool = Tool.from_gradio(demo)\n",
    "\n",
    "\n",
    "\n",
    "## Models and Integration\n",
    "        # SmoLAGENTS is model-agnostic and supports various LLM providers:\n",
    "\n",
    "\n",
    "    ### HfApiModel (Hugging Face)\n",
    "        from smolagents import HfApiModel\n",
    "\n",
    "        # Default free model\n",
    "        model = HfApiModel()\n",
    "\n",
    "        # Specific Hugging Face model\n",
    "        model = HfApiModel(\n",
    "            model_id=\"Qwen/Qwen2.5-Coder-32B-Instruct\",\n",
    "            token=\"YOUR_HF_TOKEN\"  # Optional\n",
    "        )\n",
    "\n",
    "        # Using a specific provider\n",
    "        model = HfApiModel(\n",
    "            model_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "            provider=\"together\",\n",
    "        )\n",
    "\n",
    "\n",
    "    ### LiteLLMModel (Multiple Providers)\n",
    "        import os\n",
    "        from smolagents import LiteLLMModel\n",
    "\n",
    "        # Access models from various providers\n",
    "        model = LiteLLMModel(\n",
    "            model_id=\"anthropic/claude-3-5-sonnet-latest\",\n",
    "            temperature=0.2,\n",
    "            api_key=os.environ[\"ANTHROPIC_API_KEY\"]\n",
    "        )\n",
    "\n",
    "\n",
    "    ### OpenAIServerModel\n",
    "        import os\n",
    "        from smolagents import OpenAIServerModel\n",
    "\n",
    "        model = OpenAIServerModel(\n",
    "            model_id=\"deepseek-ai/DeepSeek-R1\",\n",
    "            api_base=\"https://api.together.xyz/v1/\",  # Leave empty for OpenAI\n",
    "            api_key=os.environ[\"TOGETHER_API_KEY\"]\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Multi-Agent Systems\n",
    "    # SmoLAGENTS supports hierarchical multi-agent systems for specialized tasks:\n",
    "        from smolagents import CodeAgent, HfApiModel, DuckDuckGoSearchTool\n",
    "\n",
    "        # Create specialized agents\n",
    "        model = HfApiModel()\n",
    "\n",
    "        # Web search agent\n",
    "        web_agent = CodeAgent(\n",
    "            tools=[DuckDuckGoSearchTool()],\n",
    "            model=model,\n",
    "            name=\"web_search\",\n",
    "            description=\"Runs web searches for you. Give it your query as an argument.\"\n",
    "        )\n",
    "\n",
    "        # Image generation agent\n",
    "        image_agent = CodeAgent(\n",
    "            tools=[image_generation_tool],  # From previous examples\n",
    "            model=model,\n",
    "            name=\"image_creator\",\n",
    "            description=\"Creates images based on prompts. Provide a detailed description of the image.\"\n",
    "        )\n",
    "\n",
    "        # Manager agent that coordinates other agents\n",
    "        manager_agent = CodeAgent(\n",
    "            tools=[],  # No direct tools\n",
    "            model=model,\n",
    "            managed_agents=[web_agent, image_agent]  # Manages other agents\n",
    "        )\n",
    "\n",
    "        # Run a complex task that requires both web search and image generation\n",
    "        manager_agent.run(\"Research current fashion trends and create an image of a modern business outfit\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Advanced Customization\n",
    "\n",
    "    ### Custom System Prompts\n",
    "\n",
    "        from smolagents import CodeAgent, HfApiModel\n",
    "        from smolagents.prompts import CODE_SYSTEM_PROMPT\n",
    "\n",
    "        # Customize the system prompt\n",
    "        modified_system_prompt = CODE_SYSTEM_PROMPT + \"\\nAlways explain your reasoning step by step.\"\n",
    "\n",
    "        # Use the modified prompt\n",
    "        agent = CodeAgent(\n",
    "            tools=[],\n",
    "            model=HfApiModel(),\n",
    "            system_prompt=modified_system_prompt\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "    ### Passing Additional Arguments\n",
    "\n",
    "        from smolagents import CodeAgent, HfApiModel\n",
    "\n",
    "        agent = CodeAgent(tools=[], model=HfApiModel())\n",
    "\n",
    "        # Pass additional data to the agent\n",
    "        result = agent.run(\n",
    "            \"Debug this code and fix any errors\",\n",
    "            additional_args={\n",
    "                \"code\": \"def fibonacci(n):\\n    if n <= 0:\\n        return 0\\n    elif n == 1:\\n        return 1\\n    else:\\n        return fibonacci(n-1) + fibonacci(n-2)\"\n",
    "            }\n",
    "        )\n",
    "\n",
    "\n",
    "## Debugging and Best Practices\n",
    "\n",
    "    ### Logging and Inspection\n",
    "\n",
    "    # Run an agent\n",
    "    agent.run(\"Your task here\")\n",
    "\n",
    "    # Access logs to inspect what happened\n",
    "    print(agent.logs)\n",
    "\n",
    "    # Get a higher-level view of the agent's memory as chat messages\n",
    "    messages = agent.write_memory_to_messages()\n",
    "    print(messages)\n",
    "\n",
    "\n",
    "## Security Considerations\n",
    "    # Code execution brings security concerns. SmoLAGENTS offers several security options:\n",
    "\n",
    "    # 1. **Default Local Execution**: Limited to provided tools and safe functions\n",
    "    # 2. **E2B Sandboxed Environment**: For safer code execution\n",
    "    # 3. **Docker Sandboxing**: Isolate code execution in containers\n",
    "\n",
    "\n",
    "    # Using E2B for sandboxed code execution\n",
    "    from smolagents import CodeAgent, HfApiModel\n",
    "    from smolagents.execution import E2BExecutor\n",
    "\n",
    "    # Create an E2B executor (requires E2B API key)\n",
    "    executor = E2BExecutor()\n",
    "\n",
    "    # Create agent with sandboxed execution\n",
    "    agent = CodeAgent(\n",
    "        tools=[],\n",
    "        model=HfApiModel(),\n",
    "        executor=executor\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> SmolAgents Examples 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smolagents import CodeAgent, HfApiModel, DuckDuckGoSearchTool\n",
    "\n",
    "# Check the name of the search tool\n",
    "search_tool = DuckDuckGoSearchTool()\n",
    "print(f\"Search tool name: {search_tool.name}\")  # See what name this tool has\n",
    "\n",
    "# Create a model\n",
    "model = HfApiModel()\n",
    "\n",
    "# Create a specialized agent with a different name\n",
    "web_search_agent = CodeAgent(\n",
    "    tools=[search_tool],\n",
    "    model=model,\n",
    "    name=\"web_search_agent\",  # Changed from 'web_search' to 'web_search_agent'\n",
    "    description=\"Runs web searches for you. Give it your query as an argument.\"\n",
    ")\n",
    "\n",
    "# Create a manager agent\n",
    "manager_agent = CodeAgent(\n",
    "    tools=[],  # No direct tools\n",
    "    model=model,\n",
    "    managed_agents=[web_search_agent]\n",
    ")\n",
    "\n",
    "# Run a task with the manager\n",
    "result = manager_agent.run(\"Who is the CEO of Hugging Face?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> > SmolAgents Examples 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Optional, List, Dict, Any\n",
    "import json\n",
    "import random\n",
    "import datetime\n",
    "from datetime import timedelta\n",
    "\n",
    "# Import SmoLAGENTS components\n",
    "from smolagents import CodeAgent, HfApiModel, DuckDuckGoSearchTool, tool\n",
    "\n",
    "class TravelPlanningSystem:\n",
    "    \"\"\"\n",
    "    Advanced Travel Planning System with multiple specialized agents.\n",
    "    This system demonstrates a comprehensive multi-agent setup using SmoLAGENTS.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, hf_token: Optional[str] = None):\n",
    "        \"\"\"Initialize the travel planning system with multiple specialized agents.\"\"\"\n",
    "        # Setup model with token if provided\n",
    "        self.model = HfApiModel(\n",
    "            model_id=\"Qwen/Qwen2.5-72B-Instruct\",\n",
    "            token=hf_token\n",
    "        )\n",
    "        \n",
    "        # Create specialized tools\n",
    "        self.setup_tools()\n",
    "        \n",
    "        # Create specialized agents\n",
    "        self.setup_agents()\n",
    "        \n",
    "        # Initialize conversation history\n",
    "        self.history = []\n",
    "    \n",
    "    def setup_tools(self):\n",
    "        \"\"\"Set up specialized tools for each agent.\"\"\"\n",
    "        # Web search tool for general information\n",
    "        self.search_tool = DuckDuckGoSearchTool()\n",
    "        \n",
    "        # Weather information tool (mock implementation)\n",
    "        @tool\n",
    "        def get_weather_forecast(location: str, date: Optional[str] = None) -> str:\n",
    "            \"\"\"Get weather forecast for a specific location and date.\n",
    "            \n",
    "            Args:\n",
    "                location: City or location\n",
    "                date: Date in YYYY-MM-DD format (default: today)\n",
    "            \"\"\"\n",
    "            # Mock implementation with consistent randomization\n",
    "            weather_conditions = [\"Sunny\", \"Partly Cloudy\", \"Cloudy\", \"Light Rain\", \"Heavy Rain\", \"Thunderstorms\", \"Snowy\"]\n",
    "            temperatures = {\n",
    "                \"Sunny\": (75, 95),\n",
    "                \"Partly Cloudy\": (70, 85),\n",
    "                \"Cloudy\": (65, 80),\n",
    "                \"Light Rain\": (60, 75),\n",
    "                \"Heavy Rain\": (55, 70),\n",
    "                \"Thunderstorms\": (50, 65),\n",
    "                \"Snowy\": (25, 40)\n",
    "            }\n",
    "            \n",
    "            # Use location and date as random seed for consistent results\n",
    "            if date is None:\n",
    "                date = datetime.datetime.now().strftime(\"%Y-%m-%d\")\n",
    "            \n",
    "            # Generate seed based on location and date\n",
    "            seed = hash(f\"{location}-{date}\")\n",
    "            random.seed(seed)\n",
    "            \n",
    "            # Select weather condition and temperature\n",
    "            condition = random.choice(weather_conditions)\n",
    "            temp_range = temperatures[condition]\n",
    "            temp = random.randint(temp_range[0], temp_range[1])\n",
    "            \n",
    "            return f\"Weather forecast for {location} on {date}: {condition}, {temp}°F\"\n",
    "        \n",
    "        self.weather_tool = get_weather_forecast\n",
    "        \n",
    "        # Flight information tool (mock implementation)\n",
    "        @tool\n",
    "        def search_flights(origin: str, destination: str, date: str) -> str:\n",
    "            \"\"\"Search for flight options between cities.\n",
    "            \n",
    "            Args:\n",
    "                origin: Departure city or airport\n",
    "                destination: Arrival city or airport\n",
    "                date: Travel date in YYYY-MM-DD format\n",
    "            \"\"\"\n",
    "            # Mock implementation\n",
    "            seed = hash(f\"{origin}-{destination}-{date}\")\n",
    "            random.seed(seed)\n",
    "            \n",
    "            airlines = [\"AirGlobal\", \"SkyWays\", \"TransWorld\", \"StarLines\", \"OceanicAir\"]\n",
    "            flight_count = random.randint(3, 5)\n",
    "            flights = []\n",
    "            \n",
    "            for i in range(flight_count):\n",
    "                airline = random.choice(airlines)\n",
    "                flight_num = f\"{airline[:2].upper()}{random.randint(100, 999)}\"\n",
    "                departure_hour = random.randint(6, 22)\n",
    "                flight_duration = random.randint(2, 8)\n",
    "                arrival_hour = (departure_hour + flight_duration) % 24\n",
    "                price = random.randint(250, 1200)\n",
    "                \n",
    "                flights.append({\n",
    "                    \"airline\": airline,\n",
    "                    \"flight\": flight_num,\n",
    "                    \"departure\": f\"{date} {departure_hour:02d}:00\",\n",
    "                    \"arrival\": f\"{date} {arrival_hour:02d}:{random.randint(0, 59):02d}\",\n",
    "                    \"duration\": f\"{flight_duration}h {random.randint(0, 59)}m\",\n",
    "                    \"price\": f\"${price}\"\n",
    "                })\n",
    "            \n",
    "            # Format flight information\n",
    "            result = f\"Found {len(flights)} flights from {origin} to {destination} on {date}:\\n\\n\"\n",
    "            for i, flight in enumerate(flights, 1):\n",
    "                result += f\"Option {i}:\\n\"\n",
    "                result += f\"  Airline: {flight['airline']}\\n\"\n",
    "                result += f\"  Flight: {flight['flight']}\\n\"\n",
    "                result += f\"  Departure: {flight['departure']}\\n\"\n",
    "                result += f\"  Arrival: {flight['arrival']}\\n\"\n",
    "                result += f\"  Duration: {flight['duration']}\\n\"\n",
    "                result += f\"  Price: {flight['price']}\\n\\n\"\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        self.flight_tool = search_flights\n",
    "        \n",
    "        # Hotel information tool (mock implementation)\n",
    "        @tool\n",
    "        def search_hotels(location: str, check_in: str, check_out: str, budget: Optional[str] = \"mid\") -> str:\n",
    "            \"\"\"Search for hotel options in a location.\n",
    "            \n",
    "            Args:\n",
    "                location: City or area\n",
    "                check_in: Check-in date in YYYY-MM-DD format\n",
    "                check_out: Check-out date in YYYY-MM-DD format\n",
    "                budget: \"budget\", \"mid\", or \"luxury\" (default: \"mid\")\n",
    "            \"\"\"\n",
    "            # Mock implementation\n",
    "            seed = hash(f\"{location}-{check_in}-{budget}\")\n",
    "            random.seed(seed)\n",
    "            \n",
    "            hotel_prefixes = [\"Grand\", \"Royal\", \"Luxury\", \"Comfort\", \"Premier\", \"Elite\", \"Plaza\"]\n",
    "            hotel_suffixes = [\"Hotel\", \"Inn\", \"Suites\", \"Resort\", \"Lodge\", \"Place\", \"Residences\"]\n",
    "            hotel_areas = [\"Downtown\", \"City Center\", \"Riverside\", \"Beachfront\", \"Old Town\", \"Business District\"]\n",
    "            \n",
    "            # Price ranges based on budget\n",
    "            price_ranges = {\n",
    "                \"budget\": (50, 120),\n",
    "                \"mid\": (130, 300),\n",
    "                \"luxury\": (320, 800)\n",
    "            }\n",
    "            \n",
    "            # Generate hotels\n",
    "            hotel_count = random.randint(3, 6)\n",
    "            hotels = []\n",
    "            \n",
    "            for _ in range(hotel_count):\n",
    "                name = f\"{random.choice(hotel_prefixes)} {location} {random.choice(hotel_suffixes)}\"\n",
    "                area = random.choice(hotel_areas)\n",
    "                price_range = price_ranges.get(budget.lower(), price_ranges[\"mid\"])\n",
    "                price = random.randint(price_range[0], price_range[1])\n",
    "                rating = round(random.uniform(3.0, 5.0), 1)\n",
    "                \n",
    "                hotels.append({\n",
    "                    \"name\": name,\n",
    "                    \"area\": area,\n",
    "                    \"price_per_night\": f\"${price}\",\n",
    "                    \"rating\": rating,\n",
    "                    \"amenities\": random.sample([\"WiFi\", \"Pool\", \"Gym\", \"Restaurant\", \"Spa\", \"Room Service\"], k=random.randint(2, 5))\n",
    "                })\n",
    "            \n",
    "            # Format hotel information\n",
    "            result = f\"Found {len(hotels)} hotels in {location} from {check_in} to {check_out} ({budget} budget):\\n\\n\"\n",
    "            for i, hotel in enumerate(hotels, 1):\n",
    "                result += f\"Option {i}: {hotel['name']}\\n\"\n",
    "                result += f\"  Location: {hotel['area']}\\n\"\n",
    "                result += f\"  Price: {hotel['price_per_night']} per night\\n\"\n",
    "                result += f\"  Rating: {hotel['rating']}/5.0\\n\"\n",
    "                result += f\"  Amenities: {', '.join(hotel['amenities'])}\\n\\n\"\n",
    "            \n",
    "            return result\n",
    "        \n",
    "        self.hotel_tool = search_hotels\n",
    "        \n",
    "        # Attraction recommendation tool\n",
    "        @tool\n",
    "        def recommend_attractions(location: str, interests: Optional[str] = \"general\") -> str:\n",
    "            \"\"\"Recommend tourist attractions in a location based on interests.\n",
    "            \n",
    "            Args:\n",
    "                location: City or area\n",
    "                interests: Comma-separated list of interests (e.g., \"history,food,nature\")\n",
    "            \"\"\"\n",
    "            # Use search for real recommendations, then format nicely\n",
    "            search_query = f\"top tourist attractions in {location} for {interests} travelers\"\n",
    "            results = self.search_tool(search_query)\n",
    "            \n",
    "            return f\"Recommended attractions in {location} for {interests} interests:\\n\\n{results}\"\n",
    "        \n",
    "        self.attraction_tool = recommend_attractions\n",
    "        \n",
    "        # Currency converter tool (mock implementation)\n",
    "        @tool\n",
    "        def convert_currency(amount: float, from_currency: str, to_currency: str) -> str:\n",
    "            \"\"\"Convert currency from one type to another.\n",
    "            \n",
    "            Args:\n",
    "                amount: Amount to convert\n",
    "                from_currency: Source currency code (e.g., USD, EUR, GBP)\n",
    "                to_currency: Target currency code (e.g., USD, EUR, GBP)\n",
    "            \"\"\"\n",
    "            # Mock exchange rates\n",
    "            rates = {\n",
    "                \"USD\": 1.0,\n",
    "                \"EUR\": 0.92,\n",
    "                \"GBP\": 0.78,\n",
    "                \"JPY\": 150.2,\n",
    "                \"CAD\": 1.35,\n",
    "                \"AUD\": 1.51,\n",
    "                \"CNY\": 7.21,\n",
    "                \"INR\": 83.12,\n",
    "            }\n",
    "            \n",
    "            from_currency = from_currency.upper()\n",
    "            to_currency = to_currency.upper()\n",
    "            \n",
    "            if from_currency not in rates:\n",
    "                return f\"Currency {from_currency} not supported. Supported currencies: {', '.join(rates.keys())}\"\n",
    "            \n",
    "            if to_currency not in rates:\n",
    "                return f\"Currency {to_currency} not supported. Supported currencies: {', '.join(rates.keys())}\"\n",
    "            \n",
    "            # Convert to USD first, then to target currency\n",
    "            usd_amount = amount / rates[from_currency]\n",
    "            target_amount = usd_amount * rates[to_currency]\n",
    "            \n",
    "            return f\"{amount} {from_currency} = {target_amount:.2f} {to_currency}\"\n",
    "        \n",
    "        self.currency_tool = convert_currency\n",
    "        \n",
    "        # Create itinerary tool\n",
    "        @tool\n",
    "        def create_daily_itinerary(location: str, date: str, start_time: str, end_time: str, \n",
    "                                 interests: Optional[str] = \"general\") -> str:\n",
    "            \"\"\"Create a daily itinerary for a specific location and date.\n",
    "            \n",
    "            Args:\n",
    "                location: City or area\n",
    "                date: Date in YYYY-MM-DD format\n",
    "                start_time: Start time in HH:MM format (24-hour)\n",
    "                end_time: End time in HH:MM format (24-hour)\n",
    "                interests: Comma-separated list of interests (e.g., \"history,food,nature\")\n",
    "            \"\"\"\n",
    "            # Use search to get recommendations\n",
    "            search_query = f\"one day itinerary {location} {interests} attractions\"\n",
    "            search_results = self.search_tool(search_query)\n",
    "            \n",
    "            # Format as itinerary\n",
    "            # Parse start and end times\n",
    "            start_hour, start_minute = map(int, start_time.split(':'))\n",
    "            end_hour, end_minute = map(int, end_time.split(':'))\n",
    "            \n",
    "            # Create time slots\n",
    "            current_hour, current_minute = start_hour, start_minute\n",
    "            end_time_minutes = end_hour * 60 + end_minute\n",
    "            itinerary = f\"Daily Itinerary for {location} on {date} ({interests}):\\n\\n\"\n",
    "            \n",
    "            # Mock itinerary based on search results\n",
    "            remaining_time_minutes = end_time_minutes - (current_hour * 60 + current_minute)\n",
    "            \n",
    "            if remaining_time_minutes <= 0:\n",
    "                return \"Invalid time range. End time must be after start time.\"\n",
    "            \n",
    "            # Add weather information\n",
    "            itinerary += f\"Weather: {self.weather_tool(location, date)}\\n\\n\"\n",
    "            \n",
    "            # Seed random generator for consistent results\n",
    "            random.seed(hash(f\"{location}-{date}-{interests}\"))\n",
    "            \n",
    "            # Create itinerary\n",
    "            while remaining_time_minutes > 0:\n",
    "                activity_duration = random.randint(1, 3) * 60  # 1-3 hours in minutes\n",
    "                \n",
    "                # Don't go past end time\n",
    "                if current_hour * 60 + current_minute + activity_duration > end_time_minutes:\n",
    "                    activity_duration = end_time_minutes - (current_hour * 60 + current_minute)\n",
    "                    \n",
    "                if activity_duration <= 0:\n",
    "                    break\n",
    "                \n",
    "                # Format time slot\n",
    "                time_slot = f\"{current_hour:02d}:{current_minute:02d}\"\n",
    "                \n",
    "                # Update time\n",
    "                current_minute += activity_duration\n",
    "                while current_minute >= 60:\n",
    "                    current_hour += 1\n",
    "                    current_minute -= 60\n",
    "                \n",
    "                end_slot = f\"{current_hour:02d}:{current_minute:02d}\"\n",
    "                \n",
    "                # Add activity to itinerary\n",
    "                itinerary += f\"{time_slot} - {end_slot}: Activity based on {interests} interests\\n\"\n",
    "                \n",
    "                # Add travel/buffer time\n",
    "                buffer_time = random.randint(15, 45)  # 15-45 minutes\n",
    "                if current_minute + buffer_time >= 60:\n",
    "                    current_hour += (current_minute + buffer_time) // 60\n",
    "                    current_minute = (current_minute + buffer_time) % 60\n",
    "                else:\n",
    "                    current_minute += buffer_time\n",
    "                \n",
    "                remaining_time_minutes = end_time_minutes - (current_hour * 60 + current_minute)\n",
    "                \n",
    "                # Add meals at appropriate times\n",
    "                if (8 <= current_hour < 10 and random.random() < 0.7):\n",
    "                    itinerary += f\"{current_hour:02d}:{current_minute:02d} - {current_hour+1:02d}:{current_minute:02d}: Breakfast\\n\"\n",
    "                    current_hour += 1\n",
    "                elif (12 <= current_hour < 14 and random.random() < 0.7):\n",
    "                    itinerary += f\"{current_hour:02d}:{current_minute:02d} - {current_hour+1:02d}:{current_minute:02d}: Lunch\\n\"\n",
    "                    current_hour += 1\n",
    "                elif (18 <= current_hour < 20 and random.random() < 0.7):\n",
    "                    itinerary += f\"{current_hour:02d}:{current_minute:02d} - {current_hour+1:02d}:{current_minute:02d}: Dinner\\n\"\n",
    "                    current_hour += 1\n",
    "                \n",
    "                remaining_time_minutes = end_time_minutes - (current_hour * 60 + current_minute)\n",
    "            \n",
    "            itinerary += f\"\\nRecommendations based on web search:\\n{search_results}\"\n",
    "            \n",
    "            return itinerary\n",
    "        \n",
    "        self.itinerary_tool = create_daily_itinerary\n",
    "    \n",
    "    def setup_agents(self):\n",
    "        \"\"\"Set up specialized agents for different travel planning tasks.\"\"\"\n",
    "        # Create the information search agent\n",
    "        self.info_agent = CodeAgent(\n",
    "            tools=[self.search_tool],\n",
    "            model=self.model,\n",
    "            name=\"info_agent\",\n",
    "            description=\"Searches for general information on destinations, travel requirements, etc.\"\n",
    "        )\n",
    "        \n",
    "        # Create the weather agent\n",
    "        self.weather_agent = CodeAgent(\n",
    "            tools=[self.weather_tool],\n",
    "            model=self.model,\n",
    "            name=\"weather_agent\",\n",
    "            description=\"Provides weather forecasts for specific destinations and dates.\"\n",
    "        )\n",
    "        \n",
    "        # Create the flight booking agent\n",
    "        self.flight_agent = CodeAgent(\n",
    "            tools=[self.flight_tool, self.search_tool],\n",
    "            model=self.model,\n",
    "            name=\"flight_agent\",\n",
    "            description=\"Searches for flight options between destinations.\"\n",
    "        )\n",
    "        \n",
    "        # Create the accommodation agent\n",
    "        self.hotel_agent = CodeAgent(\n",
    "            tools=[self.hotel_tool, self.search_tool],\n",
    "            model=self.model,\n",
    "            name=\"hotel_agent\",\n",
    "            description=\"Searches for hotel and accommodation options in destinations.\"\n",
    "        )\n",
    "        \n",
    "        # Create the attractions agent\n",
    "        self.attraction_agent = CodeAgent(\n",
    "            tools=[self.attraction_tool, self.search_tool],\n",
    "            model=self.model,\n",
    "            name=\"attraction_agent\",\n",
    "            description=\"Recommends tourist attractions and activities in destinations.\"\n",
    "        )\n",
    "        \n",
    "        # Create the currency agent\n",
    "        self.currency_agent = CodeAgent(\n",
    "            tools=[self.currency_tool],\n",
    "            model=self.model,\n",
    "            name=\"currency_agent\",\n",
    "            description=\"Converts currencies to help with budget planning.\"\n",
    "        )\n",
    "        \n",
    "        # Create the itinerary creation agent\n",
    "        self.itinerary_agent = CodeAgent(\n",
    "            tools=[self.itinerary_tool, self.weather_tool, self.search_tool],\n",
    "            model=self.model,\n",
    "            name=\"itinerary_agent\",\n",
    "            description=\"Creates detailed daily itineraries for destinations.\"\n",
    "        )\n",
    "        \n",
    "        # Create the travel planner agent (orchestrator)\n",
    "        self.planner_agent = CodeAgent(\n",
    "            tools=[],  # No direct tools\n",
    "            model=self.model,\n",
    "            managed_agents=[\n",
    "                self.info_agent,\n",
    "                self.weather_agent,\n",
    "                self.flight_agent,\n",
    "                self.hotel_agent,\n",
    "                self.attraction_agent,\n",
    "                self.currency_agent,\n",
    "                self.itinerary_agent\n",
    "            ],\n",
    "            name=\"travel_planner\",\n",
    "            description=\"Plans comprehensive travel itineraries by coordinating specialized agents.\"\n",
    "        )\n",
    "    \n",
    "    def add_to_history(self, role: str, content: str):\n",
    "        \"\"\"Add a message to the conversation history.\"\"\"\n",
    "        self.history.append({\n",
    "            \"role\": role,\n",
    "            \"content\": content,\n",
    "            \"timestamp\": datetime.datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def process_query(self, query: str) -> str:\n",
    "        \"\"\"Process a user query through the travel planning system.\"\"\"\n",
    "        # Add user message to history\n",
    "        self.add_to_history(\"user\", query)\n",
    "        \n",
    "        # Use the planner agent to process the query\n",
    "        result = self.planner_agent.run(\n",
    "            query,\n",
    "            additional_args={\"conversation_history\": self.history}\n",
    "        )\n",
    "        \n",
    "        # Add the response to history\n",
    "        self.add_to_history(\"assistant\", result)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "def main():\n",
    "    # Initialize the travel planning system\n",
    "    system = TravelPlanningSystem()\n",
    "    \n",
    "    # Example queries\n",
    "    queries = [\n",
    "        \"I'm planning a trip to Tokyo in April 2025. What's the weather like and what are the must-see attractions?\",\n",
    "        \"Can you find flights from New York to Paris on June 15, 2025 and hotels for a 5-night stay?\",\n",
    "        \"Create a one-day itinerary for London focusing on historical sites. Start at 9:00 and end at 18:00.\",\n",
    "        \"How much is 500 USD in Japanese Yen?\",\n",
    "        \"I'm planning a 3-day trip to Barcelona. Can you suggest an itinerary including flights, hotels, and attractions?\"\n",
    "    ]\n",
    "    \n",
    "    # Process queries\n",
    "    for query in queries:\n",
    "        print(f\"\\nUser: {query}\")\n",
    "        response = system.process_query(query)\n",
    "        print(f\"\\nTravel Planning System: {response}\")\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    # Interactive mode\n",
    "    print(\"\\nEnter your travel questions (type 'exit' to quit):\")\n",
    "    while True:\n",
    "        query = input(\"\\nYou: \")\n",
    "        if query.lower() in ['exit', 'quit', 'bye']:\n",
    "            break\n",
    "        \n",
    "        response = system.process_query(query)\n",
    "        print(f\"\\nTravel Planning System: {response}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
