{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "'''\n",
    "\n",
    "AI Agent Architectures\n",
    "├── Profiling Module or Perception Module (The Eyes and Ears of the Agent)\n",
    "│   ├── Sensory expertise\n",
    "│   ├── Perceives and interprets the environment and communicated with other agents.\n",
    "│   ├──  --> the agent may collect and analyze information from its environment like how human senses work.\n",
    "│   ├──  --> helps it comprehend visual signals, recognize speech patterns, and sense tactile inputs..\n",
    "│   └── Example: Recognizing objects via sensors in self-driving cars\n",
    "│\n",
    "├── Memory Module\n",
    "│   ├── Stores data, rules, and patterns\n",
    "│   ├── Enables knowledge recall and decision-making\n",
    "│   └── Example: Chatbots recalling customer preferences\n",
    "│   \n",
    "├── Planning Module\n",
    "│   ├── Analyzes current situations\n",
    "│   ├── Strategizes actions to meet goals\n",
    "│   └── Example: Optimizing delivery routes\n",
    "│\n",
    "├── Action Module\n",
    "│   ├── Executes planned actions\n",
    "│   ├── Interfaces with external systems\n",
    "│   └── Example: Robotic arms assembling parts\n",
    "│\n",
    "├── Learning Module\n",
    "│   ├── Adapts and improves performance\n",
    "│   ├── Methods include:\n",
    "│   │   ├── Supervised Learning\n",
    "│   │   ├── Unsupervised Learning\n",
    "│   │   └── Reinforcement Learning\n",
    "│   └── Example: Agents learning optimal decisions from feedback\n",
    "│\n",
    "├── Data Structuring and Transformation Module\n",
    "│   ├── Organizes and preprocesses data both from the environment as well as from the memory module\n",
    "│   ├── Converts data into trainable formats\n",
    "│   └── Example: Formatting images for neural network training\n",
    "│\n",
    "├── Training Module\n",
    "│   ├── Performs training operations and updates\n",
    "│   ├── Use methods like:\n",
    "│   │   ├── Supervised, Unsupervised, and Reinforcement Learning\n",
    "│   │   ├── Computer Vision, LLM, Time series Learning\n",
    "│   │   ├── ANN, CNN, Transformers, GANs, GNNs, \n",
    "│   │   └── Tools like TensorFlow, PyTorch, Keras, Scikit-learn\n",
    "│   └── Example: AI training itself in virtual environments\n",
    "│\n",
    "└── Other Modules\n",
    "\n",
    " \n",
    "'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "langchain_community\n",
    "tiktoken\n",
    "langchainhub\n",
    "langchain\n",
    "chromadb\n",
    "langgraph\n",
    "tavily-python\n",
    "python-dotenv\n",
    "google-generativeai\n",
    "langchain_google_genai\n",
    "langchain-nomic\n",
    "langchain-text-splitters\n",
    "langchain_mistralai\n",
    "wikipedia\n",
    "langchain_huggingface\n",
    "google-search-results\n",
    "faiss-cpu\n",
    "sentence-transformers\n",
    "youtube-search\n",
    "\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> n8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n8n interface\n",
    "\n",
    "# n8n is a workflow automation tool that enables you to connect your favorite apps, services, and devices.\n",
    "# It allows you to automate workflows and integrate your apps, services, and devices with each other.\n",
    "\n",
    "    # workflow: a sequence of connected steps that automate a process.\n",
    "    # node: a single step in a workflow.\n",
    "    # connection: a link between two nodes that passes data from one node to another.\n",
    "    # execution: a single run of a workflow.\n",
    "\n",
    "# Types of Nodes\n",
    "    # Trigger Node: The starting point of a workflow. It initiates the execution of a workflow.\n",
    "    # Regular Node: A node that performs a specific action or operation.\n",
    "    # Parameter Node: A node that stores and provides data to other nodes in the workflow.\n",
    "    # Sub-Workflow Node: A node that allows you to reuse a workflow within another workflow.\n",
    "    # Webhook Node: A node that receives data from an external service or application.\n",
    "    # Error Node: A node that handles errors that occur during the execution of a workflow.\n",
    "    # No-Operation Node: A node that does nothing. It is used for debugging and testing purposes.\n",
    "    \n",
    "    # OR\n",
    "    # Trigger Nodes: These nodes initiate the execution of a workflow. They are the starting points of a workflow.\n",
    "    # Data Transformation Nodes: These nodes perform operations on data. They transform, filter, or manipulate data in some way.\n",
    "    # Action Nodes: These nodes perform actions such as sending an email, making an API call, or updating a database.\n",
    "    # Logic Nodes: These nodes control the flow of a workflow. They make decisions based on conditions and determine the path a workflow should take.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangChain Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import agent_types\n",
    "from langchain.agents.react.agent import create_react_agent\n",
    "from langchain.agents import tools, tool \n",
    "\n",
    "\"\"\" \n",
    "Directory structure:\n",
    "└── agents/\n",
    "    ├── __init__.py\n",
    "    ├── agent.py\n",
    "    ├── agent_iterator.py\n",
    "    ├── agent_types.py\n",
    "    ├── initialize.py\n",
    "    ├── load_tools.py\n",
    "    ├── loading.py\n",
    "    ├── schema.py\n",
    "    ├── tools.py\n",
    "    ├── types.py\n",
    "    ├── utils.py\n",
    "    ├── agent_toolkits/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── azure_cognitive_services.py\n",
    "    │   ├── base.py\n",
    "    │   ├── ainetwork/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── amadeus/\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── clickup/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── conversational_retrieval/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── openai_functions.py\n",
    "    │   │   └── tool.py\n",
    "    │   ├── csv/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── file_management/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── github/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── gitlab/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── gmail/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── jira/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── json/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── multion/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── nasa/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── nla/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── tool.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── office365/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── openapi/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── planner.py\n",
    "    │   │   ├── planner_prompt.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   ├── spec.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── pandas/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── playwright/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── powerbi/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── chat_base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── python/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── slack/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── spark/\n",
    "    │   │   └── __init__.py\n",
    "    │   ├── spark_sql/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── sql/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── steam/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── vectorstore/\n",
    "    │   │   ├── __init__.py\n",
    "    │   │   ├── base.py\n",
    "    │   │   ├── prompt.py\n",
    "    │   │   └── toolkit.py\n",
    "    │   ├── xorbits/\n",
    "    │   │   └── __init__.py\n",
    "    │   └── zapier/\n",
    "    │       ├── __init__.py\n",
    "    │       └── toolkit.py\n",
    "    ├── chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── conversational/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── conversational_chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── format_scratchpad/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── log.py\n",
    "    │   ├── log_to_messages.py\n",
    "    │   ├── openai_functions.py\n",
    "    │   ├── openai_tools.py\n",
    "    │   ├── tools.py\n",
    "    │   └── xml.py\n",
    "    ├── json_chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── mrkl/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── openai_assistant/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── openai_functions_agent/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── agent_token_buffer_memory.py\n",
    "    │   └── base.py\n",
    "    ├── openai_functions_multi_agent/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── openai_tools/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── output_parsers/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── json.py\n",
    "    │   ├── openai_functions.py\n",
    "    │   ├── openai_tools.py\n",
    "    │   ├── react_json_single_input.py\n",
    "    │   ├── react_single_input.py\n",
    "    │   ├── self_ask.py\n",
    "    │   ├── tools.py\n",
    "    │   └── xml.py\n",
    "    ├── react/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── agent.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   ├── textworld_prompt.py\n",
    "    │   └── wiki_prompt.py\n",
    "    ├── self_ask_with_search/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── structured_chat/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── output_parser.py\n",
    "    │   └── prompt.py\n",
    "    ├── tool_calling_agent/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    └── xml/\n",
    "        ├── __init__.py\n",
    "        ├── base.py\n",
    "        └── prompt.py\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "from langchain.agents import tool\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "print(f'Length of the word '{get_word_length.invoke(\"hello\")})\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import retriever\n",
    "\n",
    "\"\"\" \n",
    "Directory structure:\n",
    "└── tools/\n",
    "    ├── __init__.py\n",
    "    ├── base.py\n",
    "    ├── convert_to_openai.py\n",
    "    ├── ifttt.py\n",
    "    ├── plugin.py\n",
    "    ├── render.py\n",
    "    ├── retriever.py\n",
    "    ├── yahoo_finance_news.py\n",
    "    ├── ainetwork/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── app.py\n",
    "    │   ├── base.py\n",
    "    │   ├── owner.py\n",
    "    │   ├── rule.py\n",
    "    │   ├── transfer.py\n",
    "    │   └── value.py\n",
    "    ├── amadeus/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── closest_airport.py\n",
    "    │   └── flight_search.py\n",
    "    ├── arxiv/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── azure_cognitive_services/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── form_recognizer.py\n",
    "    │   ├── image_analysis.py\n",
    "    │   ├── speech2text.py\n",
    "    │   ├── text2speech.py\n",
    "    │   └── text_analytics_health.py\n",
    "    ├── bearly/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── bing_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── brave_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── clickup/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── dataforseo_api_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── ddg_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── e2b_data_analysis/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── edenai/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── audio_speech_to_text.py\n",
    "    │   ├── audio_text_to_speech.py\n",
    "    │   ├── edenai_base_tool.py\n",
    "    │   ├── image_explicitcontent.py\n",
    "    │   ├── image_objectdetection.py\n",
    "    │   ├── ocr_identityparser.py\n",
    "    │   ├── ocr_invoiceparser.py\n",
    "    │   └── text_moderation.py\n",
    "    ├── eleven_labs/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── models.py\n",
    "    │   └── text2speech.py\n",
    "    ├── file_management/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── copy.py\n",
    "    │   ├── delete.py\n",
    "    │   ├── file_search.py\n",
    "    │   ├── list_dir.py\n",
    "    │   ├── move.py\n",
    "    │   ├── read.py\n",
    "    │   └── write.py\n",
    "    ├── github/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── gitlab/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── gmail/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── create_draft.py\n",
    "    │   ├── get_message.py\n",
    "    │   ├── get_thread.py\n",
    "    │   ├── search.py\n",
    "    │   └── send_message.py\n",
    "    ├── golden_query/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_cloud/\n",
    "    │   ├── __init__.py\n",
    "    │   └── texttospeech.py\n",
    "    ├── google_finance/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_jobs/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_lens/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_places/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_scholar/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_serper/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── google_trends/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── graphql/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── human/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── interaction/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── jira/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── json/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── memorize/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── merriam_webster/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── metaphor_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── multion/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── close_session.py\n",
    "    │   ├── create_session.py\n",
    "    │   └── update_session.py\n",
    "    ├── nasa/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── nuclia/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── office365/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── create_draft_message.py\n",
    "    │   ├── events_search.py\n",
    "    │   ├── messages_search.py\n",
    "    │   ├── send_event.py\n",
    "    │   └── send_message.py\n",
    "    ├── openapi/\n",
    "    │   ├── __init__.py\n",
    "    │   └── utils/\n",
    "    │       ├── __init__.py\n",
    "    │       ├── api_models.py\n",
    "    │       └── openapi_utils.py\n",
    "    ├── openweathermap/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── playwright/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── click.py\n",
    "    │   ├── current_page.py\n",
    "    │   ├── extract_hyperlinks.py\n",
    "    │   ├── extract_text.py\n",
    "    │   ├── get_elements.py\n",
    "    │   ├── navigate.py\n",
    "    │   └── navigate_back.py\n",
    "    ├── powerbi/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── pubmed/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── python/\n",
    "    │   └── __init__.py\n",
    "    ├── reddit_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── requests/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── scenexplain/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── searchapi/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── searx_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── shell/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── slack/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── get_channel.py\n",
    "    │   ├── get_message.py\n",
    "    │   ├── schedule_message.py\n",
    "    │   └── send_message.py\n",
    "    ├── sleep/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── spark_sql/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── sql_database/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── prompt.py\n",
    "    │   └── tool.py\n",
    "    ├── stackexchange/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── steam/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── steamship_image_generation/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── tavily_search/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── vectorstore/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── wikipedia/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── wolfram_alpha/\n",
    "    │   ├── __init__.py\n",
    "    │   └── tool.py\n",
    "    ├── youtube/\n",
    "    │   ├── __init__.py\n",
    "    │   └── search.py\n",
    "    └── zapier/\n",
    "        ├── __init__.py\n",
    "        └── tool.py\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Chains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import *\n",
    "# from langchain_community.chains import *\n",
    "\n",
    "\"\"\" \n",
    "Directory structure:\n",
    "└── chains/\n",
    "    ├── __init__.py\n",
    "    ├── base.py\n",
    "    ├── example_generator.py\n",
    "    ├── history_aware_retriever.py\n",
    "    ├── llm.py\n",
    "    ├── llm_requests.py\n",
    "    ├── loading.py\n",
    "    ├── mapreduce.py\n",
    "    ├── moderation.py\n",
    "    ├── prompt_selector.py\n",
    "    ├── retrieval.py\n",
    "    ├── sequential.py\n",
    "    ├── transform.py\n",
    "    ├── api/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── news_docs.py\n",
    "    │   ├── open_meteo_docs.py\n",
    "    │   ├── podcast_docs.py\n",
    "    │   ├── prompt.py\n",
    "    │   ├── tmdb_docs.py\n",
    "    │   └── openapi/\n",
    "    │       ├── __init__.py\n",
    "    │       ├── chain.py\n",
    "    │       ├── prompts.py\n",
    "    │       ├── requests_chain.py\n",
    "    │       └── response_chain.py\n",
    "    ├── chat_vector_db/\n",
    "    │   ├── __init__.py\n",
    "    │   └── prompts.py\n",
    "    ├── combine_documents/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── map_reduce.py\n",
    "    │   ├── map_rerank.py\n",
    "    │   ├── reduce.py\n",
    "    │   ├── refine.py\n",
    "    │   └── stuff.py\n",
    "    ├── constitutional_ai/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── models.py\n",
    "    │   ├── principles.py\n",
    "    │   └── prompts.py\n",
    "    ├── conversation/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── memory.py\n",
    "    │   └── prompt.py\n",
    "    ├── conversational_retrieval/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── elasticsearch_database/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── ernie_functions/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    ├── flare/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── graph_qa/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── arangodb.py\n",
    "    │   ├── base.py\n",
    "    │   ├── cypher.py\n",
    "    │   ├── cypher_utils.py\n",
    "    │   ├── falkordb.py\n",
    "    │   ├── gremlin.py\n",
    "    │   ├── hugegraph.py\n",
    "    │   ├── kuzu.py\n",
    "    │   ├── nebulagraph.py\n",
    "    │   ├── neptune_cypher.py\n",
    "    │   ├── neptune_sparql.py\n",
    "    │   ├── ontotext_graphdb.py\n",
    "    │   ├── prompts.py\n",
    "    │   └── sparql.py\n",
    "    ├── hyde/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts.py\n",
    "    ├── llm_bash/\n",
    "    │   └── __init__.py\n",
    "    ├── llm_checker/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── llm_math/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── llm_summarization_checker/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompts/\n",
    "    │       ├── are_all_true_prompt.txt\n",
    "    │       ├── check_facts.txt\n",
    "    │       ├── create_facts.txt\n",
    "    │       └── revise_summary.txt\n",
    "    ├── llm_symbolic_math/\n",
    "    │   └── __init__.py\n",
    "    ├── natbot/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── crawler.py\n",
    "    │   └── prompt.py\n",
    "    ├── openai_functions/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── citation_fuzzy_match.py\n",
    "    │   ├── extraction.py\n",
    "    │   ├── openapi.py\n",
    "    │   ├── qa_with_structure.py\n",
    "    │   ├── tagging.py\n",
    "    │   └── utils.py\n",
    "    ├── openai_tools/\n",
    "    │   ├── __init__.py\n",
    "    │   └── extraction.py\n",
    "    ├── qa_generation/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── qa_with_sources/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── loading.py\n",
    "    │   ├── map_reduce_prompt.py\n",
    "    │   ├── refine_prompts.py\n",
    "    │   ├── retrieval.py\n",
    "    │   ├── stuff_prompt.py\n",
    "    │   └── vector_db.py\n",
    "    ├── query_constructor/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── ir.py\n",
    "    │   ├── parser.py\n",
    "    │   ├── prompt.py\n",
    "    │   └── schema.py\n",
    "    ├── question_answering/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── chain.py\n",
    "    │   ├── map_reduce_prompt.py\n",
    "    │   ├── map_rerank_prompt.py\n",
    "    │   ├── refine_prompts.py\n",
    "    │   └── stuff_prompt.py\n",
    "    ├── retrieval_qa/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   └── prompt.py\n",
    "    ├── router/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── base.py\n",
    "    │   ├── embedding_router.py\n",
    "    │   ├── llm_router.py\n",
    "    │   ├── multi_prompt.py\n",
    "    │   ├── multi_prompt_prompt.py\n",
    "    │   ├── multi_retrieval_prompt.py\n",
    "    │   └── multi_retrieval_qa.py\n",
    "    ├── sql_database/\n",
    "    │   ├── __init__.py\n",
    "    │   ├── prompt.py\n",
    "    │   └── query.py\n",
    "    ├── structured_output/\n",
    "    │   ├── __init__.py\n",
    "    │   └── base.py\n",
    "    └── summarize/\n",
    "        ├── __init__.py\n",
    "        ├── chain.py\n",
    "        ├── map_reduce_prompt.py\n",
    "        ├── refine_prompts.py\n",
    "        └── stuff_prompt.py\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "from langchain.agents import tool\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "@tool\n",
    "def get_word_length(word: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(word)\n",
    "\n",
    "print(f'Length of the word '{get_word_length.invoke(\"hello\")})\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------------------------------\n",
    "### Custom Tools \n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaSummaryTool, CustomTool\n",
    "from langchain_community.tools.tavily_search_tool import TavilySearchResults, TavilyAnswer\n",
    "\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaSummaryTool()\n",
    "tool_3 = TavilySearchResults()\n",
    "\n",
    "tools = [tool_1, tool_2, tool_3]\n",
    "\n",
    "#----------------------------------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def get_word_length(text: str) -> int:\n",
    "    \"\"\"Return the length of a word.\"\"\"\n",
    "    return len(text)\n",
    "\n",
    "print(get_word_length.invoke(\"hello\"))\n",
    "\n",
    "print(get_word_length.name)\n",
    "print(get_word_length.description)\n",
    "print(get_word_length.args)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Web Answer\",\n",
    "        func = google_search.run,\n",
    "        description=\"Get an intermediate answer to a question.\",\n",
    "        verbose = True\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "#-----------------------------------------Custom Tool from a LangChain Chain ----------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.tools import tool\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "@tool(\"query_param_checker\")\n",
    "def query_param_checker(user_query: str, generated_query_params: str) -> str:\n",
    "    \"\"\"\n",
    "    This tool checks if the query parameters generated by query_param_generator are valid.\n",
    "    It uses an LLM to evaluate the parameters based on a simple prompt.\n",
    "    \"\"\"\n",
    "    # Define a simple prompt template\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"Check if the generated query parameters are valid for the user query.\"),\n",
    "            (\"human\", \"User Query: {user_query}\\nGenerated Query Parameters: {generated_query_params}\"),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Create a chain with the prompt and LLM\n",
    "    llm = ChatOpenAI(model=\"gpt-3.5-turbo\")\n",
    "    chain = prompt | llm\n",
    "\n",
    "    # Invoke the chain with the inputs\n",
    "    result = chain.invoke({\"user_query\": user_query, \"generated_query_params\": generated_query_params})\n",
    "\n",
    "    # Return the LLM's response\n",
    "    return result.content\n",
    "\n",
    "\n",
    "#-----------------------------Custom Tool from Class (RECOMMENDED 1) ------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "# Define Input Schema\n",
    "class SearchToolInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The search query to look up.\")\n",
    "    max_results: Optional[int] = Field(default=10, description=\"The maximum number of search results to return.\")\n",
    "\n",
    "# Define the Tool\n",
    "class TavilySearchTool:\n",
    "    def __init__(self, max_results: int = 10):\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def search(self, query: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Perform a web search using the Tavily search engine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize the Tavily search tool with the configured max_results\n",
    "            search_tool = TavilySearchResults(max_results=self.max_results)\n",
    "\n",
    "            # Perform the search\n",
    "            result = search_tool.invoke({\"query\": query})\n",
    "\n",
    "            # Return the search results\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Create the LangChain Tool\n",
    "search_tool = Tool(\n",
    "    name=\"Tavily Search\",\n",
    "    func=TavilySearchTool().search,\n",
    "    description=\"Performs web searches using the Tavily search engine, providing accurate and trusted results for general queries.\",\n",
    "    args_schema=SearchToolInput\n",
    ")\n",
    "\n",
    "\n",
    "#-----------------------------Structured Tool from Class (BEST AND MOST RECOMMENDED) ------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "from langchain.tools.base import StructuredTool\n",
    "\n",
    "# Define Input Schema\n",
    "class SearchToolInput(BaseModel):\n",
    "    query: str = Field(..., description=\"The search query to look up.\")\n",
    "    max_results: Optional[int] = Field(default=10, description=\"The maximum number of search results to return.\")\n",
    "\n",
    "# Define the Tool\n",
    "class TavilySearchTool:\n",
    "    def __init__(self, max_results: int = 10):\n",
    "        self.max_results = max_results\n",
    "\n",
    "    def search(self, query: str) -> Optional[List[Dict[str, Any]]]:\n",
    "        \"\"\"\n",
    "        Perform a web search using the Tavily search engine.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Initialize the Tavily search tool with the configured max_results\n",
    "            search_tool = TavilySearchResults(max_results=self.max_results)\n",
    "\n",
    "            # Perform the search\n",
    "            result = search_tool.invoke({\"query\": query})\n",
    "\n",
    "            # Return the search results\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "# Create the LangChain Tool\n",
    "search_tool = StructuredTool(\n",
    "    name=\"Tavily Search\",\n",
    "    func=TavilySearchTool().search,\n",
    "    description=\"Performs web searches using the Tavily search engine, providing accurate and trusted results for general queries.\",\n",
    "    args_schema=SearchToolInput\n",
    ")\n",
    "\n",
    "\n",
    "# ------------------------------------------------ Convert Tool to Structured Tool ------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------------------\n",
    "from langchain.tools.base import StructuredTool\n",
    "from langchain.agents import Tool, load_tools\n",
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def convert_to_structured_tool(tool):\n",
    "    return StructuredTool.from_function(tool.func, name=tool.name, description=tool.description)\n",
    "\n",
    "tools = load_tools(['serpapi'])\n",
    "tools = [convert_to_structured_tool(tool) for tool in tools]\n",
    "\n",
    "\n",
    "#---------------------------------- Custom Tool (RECOMMENDED 2) -------------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import Optional, Union\n",
    "import requests\n",
    "import os\n",
    "\n",
    "# Define Input Schema: Use Pydantic to define input parameters and descriptions for your tool.\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str = Field(..., description=\"Description of param1.\")\n",
    "    param2: int = Field(default=10, description=\"Description of param2.\")\n",
    "    \n",
    "# Create the Tool: Use the @tool decorator to define a custom tool.\n",
    "@tool(\"my_tool_function\", args_schema=MyToolInput, return_direct=True)\n",
    "def my_tool_function(param1: str, param2: int = 10) -> Union[Dict, str]:\n",
    "    \"\"\"\n",
    "    Description of what the tool does.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        url = (\n",
    "            f'https://api.financialdatasets.ai/insider-transactions'\n",
    "            f'?ticker={param1}'\n",
    "            f'&limit={param2}'\n",
    "            )\n",
    "        # Perform the task (e.g., call an API, process data, etc.)\n",
    "        response = requests.get(url, headers={'X-API-Key': api_key})\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "tools = [my_tool_function, annual_report_tool, get_word_length]\n",
    "\n",
    "\n",
    "#-----------------------------Custom Tool from a Custom Chain----------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.chains.base import Chain\n",
    "from typing import Dict, List\n",
    "\n",
    "class AnnualReportChain(Chain):\n",
    "    chain: Chain\n",
    "\n",
    "    @property\n",
    "    def input_keys(self) -> List[str]:\n",
    "        return list(self.chain.input_keys)\n",
    "\n",
    "    @property\n",
    "    def output_keys(self) -> List[str]:\n",
    "        return ['output']\n",
    "\n",
    "    def _call(self, inputs: Dict[str, str]) -> Dict[str, str]:\n",
    "        # Queries the database to get the relevant documents for a given query\n",
    "        query = inputs.get(\"input_documents\", \"\")\n",
    "        docs = vectorstore.similarity_search(query, include_metadata=True)\n",
    "        output = chain.run(input_documents=docs, question=query)\n",
    "        return {'output': output}\n",
    "    \n",
    "    \n",
    "\n",
    "from langchain.agents import Tool\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize your custom Chain\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "chain = load_qa_chain(llm)\n",
    "annual_report_chain = AnnualReportChain(chain=chain)\n",
    "\n",
    "# Initialize your custom Tool\n",
    "annual_report_tool = Tool(\n",
    "    name=\"Annual Report\",\n",
    "    func=annual_report_chain.run,\n",
    "    description=\"\"\"\n",
    "    useful for when you need to answer questions about a company's income statement,\n",
    "    cash flow statement, or balance sheet. This tool can help you extract data points like\n",
    "    net income, revenue, free cash flow, and total debt, among other financial line items.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------- Creating a Node from Tools ----------------------------------\n",
    "#------------------------------------------------------------------------------------------------\n",
    "from langgraph.prebuilt import ToolNode, tools_condition\n",
    "from langgraph.graph import StateGraph, START\n",
    "\n",
    "builder = StateGraph(State)\n",
    "tool_node = ToolNode(tools=tools)\n",
    "builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain, LangGraph, and LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain \n",
    "    # Tools\n",
    "    # Agents\n",
    "    # Chains\n",
    "    # Multi-Agent Systems\n",
    "    # Plan and Execute\n",
    "    # Reflection and Learning\n",
    "    # Communication\n",
    "    # Perception\n",
    "\n",
    "# LangChain is a platform that enables developers to build, test, and deploy blockchain applications using multiple programming languages.\n",
    "# It provides a set of tools and libraries that simplify the development process and make it easier to create blockchain applications.\n",
    "\n",
    "\n",
    "\n",
    "# Types of LangChain Agents\n",
    "    # LangChain offers several agentic patterns, each tailored to specific needs. These include:\n",
    "\n",
    "    # Tool Calling Agents: Designed for straightforward tool usage.\n",
    "    # React Agents: Use reasoning and action mechanisms to dynamically decide the best steps.\n",
    "    # Structured Chat Agents: Parse inputs and outputs into structured formats like JSON.\n",
    "    # Self-Ask with Search: Handle queries by splitting them into smaller, manageable steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Langchain Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Tool Calling Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_tool_calling_agent\n",
    "from langchain.agents.tool_calling_agent import base\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain import hub\n",
    "from langchain_openai import ChatOpenAI as LangchainChatDeepSeek\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults, TavilyAnswer\n",
    "from langchain_community.tools import YouTubeSearchTool, WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain.agents import AgentExecutor\n",
    "import os\n",
    "\n",
    "# Load API key\n",
    "api_key = os.getenv(\"DEEPSEEK_API_KEY\")\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "# or\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful assistant.\"),\n",
    "        # First put the history\n",
    "        (\"placeholder\", \"{chat_history}\"),\n",
    "        # Then the new input\n",
    "        (\"human\", \"{input}\"),\n",
    "        # Finally the scratchpad\n",
    "        (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Tools\n",
    "tool_1 = YouTubeSearchTool()\n",
    "tool_2 = WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper())\n",
    "tool_3 = TavilySearchResults(max_results=10)\n",
    "\n",
    "tools = [tool_1, tool_2, tool_3]\n",
    "\n",
    "# LLM\n",
    "llm = LangchainChatDeepSeek(\n",
    "            api_key=api_key,\n",
    "            model=\"deepseek-chat\",\n",
    "            base_url=\"https://api.deepseek.com\",\n",
    "        )\n",
    "\n",
    "# Agent\n",
    "\n",
    "# Create a tool-calling agent\n",
    "agent = create_tool_calling_agent(llm, tools, prompt)\n",
    "# agent = base.create_tool_calling_agent()\n",
    "\n",
    "# Agent Executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,  # Only final output. If True, returns all intermediate steps\n",
    "    handle_parsing_errors=True,  # Graceful parsing errors\n",
    ")\n",
    "        \n",
    "\n",
    "query = input(\"Enter your query: \")\n",
    "\n",
    "response = agent_executor.invoke(\n",
    "    {\n",
    "        \"input\": [HumanMessage(content=query)]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The ReActAgent employs the ReAct (Reason+Act) framework, enabling the agent to perform both reasoning and actions within a \n",
    "# single framework. It integrates chain-of-thought reasoning with action execution, allowing the agent to handle complex, \n",
    "# multi-step tasks effectively.\n",
    "\n",
    "from langchain.agents import create_react_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "from typing import Annotated\n",
    "\n",
    "template = '''Answer the following questions as best as you can. You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation sequence can be repeated N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Here is an example of how to use the tools:\n",
    "Question: Generate a chart of the Fibonacci sequence.\n",
    "Thought: I need to write Python code to generate the Fibonacci sequence and plot it.\n",
    "Action: python_repl_tool\n",
    "Action Input: \n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def fibonacci(n):\n",
    "    fib_sequence = [0, 1]\n",
    "    for i in range(2, n):\n",
    "        fib_sequence.append(fib_sequence[-1] + fib_sequence[-2])\n",
    "    return fib_sequence\n",
    "\n",
    "n = 10  # Number of Fibonacci numbers to generate\n",
    "fib_sequence = fibonacci(n)\n",
    "plt.plot(fib_sequence)\n",
    "plt.title(\"Fibonacci Sequence\")\n",
    "plt.show()\n",
    "```\n",
    "Observation: The chart was successfully generated.\n",
    "Thought: I now know the final answer.\n",
    "Final Answer: The chart of the Fibonacci sequence has been generated.\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "repl = PythonREPL()\n",
    "\n",
    "@tool\n",
    "def python_repl_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return result_str\n",
    "    \n",
    "prompt = PromptTemplate.from_template(template)\n",
    "search_agent = create_react_agent(llm, tools = [python_repl_tool], prompt=prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=search_agent, tools=[python_repl_tool], verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"create a visualization of some advanced dataset.\"})\n",
    "# agent_executor.invoke({\"input\": [HumanMessage(content=\"What is the capital of France?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph ReAct Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "LangGraph's prebuilt create_react_agent does not take a prompt template directly as a parameter, but instead takes a prompt parameter. \n",
    "This modifies the graph state before the llm is called, and can be one of four values:\n",
    "\n",
    "    1. A SystemMessage, which is added to the beginning of the list of messages.\n",
    "    2. A string, which is converted to a SystemMessage and added to the beginning of the list of messages.\n",
    "    3. A Callable, which should take in full graph state. The output is then passed to the language model.\n",
    "    4. Or a Runnable, which should take in full graph state. The output is then passed to the language model.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import SystemMessage, AnyMessage, HumanMessage, AgentMessage, AgentMessageWithScratchpad\n",
    "\n",
    "# ----------------------------------------System Message-------------------\n",
    "system_message = \"You are a helpful assistant. Respond only in Spanish.\"\n",
    "# This could also be a SystemMessage object\n",
    "# system_message = SystemMessage(content=\"You are a helpful assistant. Respond only in Spanish.\")\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(model, tools, prompt=system_message)\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# ----------------------------------------ChatPromptTemplate-------------------\n",
    "# prompt = ChatPromptTemplate.from_messages(\n",
    "#     [\n",
    "#         (\"system\", \"You are a helpful assistant.\"),\n",
    "#         # First put the history\n",
    "#         (\"placeholder\", \"{chat_history}\"),\n",
    "#         # Then the new input\n",
    "#         (\"human\", \"{input}\"),\n",
    "#         # Finally the scratchpad\n",
    "#         (\"placeholder\", \"{agent_scratchpad}\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "react_agent_prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessage(content=\"\"\"You are an advanced AI assistant designed to solve complex tasks using a systematic, step-by-step approach. \n",
    "\n",
    "CORE AGENT INSTRUCTIONS:\n",
    "1. ALWAYS follow the React (Reasoning and Acting) paradigm\n",
    "2. For EACH task, you must:\n",
    "   a) REASON about the problem\n",
    "   b) DETERMINE which TOOL to use\n",
    "   c) Take ACTION using the selected tool\n",
    "   d) OBSERVE the results\n",
    "   e) REFLECT and decide next steps\n",
    "\n",
    "AVAILABLE TOOLS:\n",
    "{tools}\n",
    "\n",
    "TOOL USAGE PROTOCOL:\n",
    "- You have access to the following tools: {tool_names}\n",
    "- BEFORE using any tool, EXPLICITLY state:\n",
    "  1. WHY you are using this tool\n",
    "  2. WHAT specific information you hope to retrieve\n",
    "  3. HOW this information will help solve the task\n",
    "\n",
    "TOOL INTERACTION FORMAT:\n",
    "When using a tool, you MUST follow this strict format:\n",
    "Thought: [Your reasoning for using the tool]\n",
    "Action: [Exact tool name]\n",
    "Action Input: [Precise input for the tool]\n",
    "\n",
    "After receiving the observation, you will:\n",
    "Observation: [Tool's response]\n",
    "Reflection: [Analysis of the observation and next steps]\n",
    "\n",
    "FINAL OUTPUT EXPECTATIONS:\n",
    "- Provide a comprehensive, step-by-step solution\n",
    "- Cite sources and tools used\n",
    "- Explain your reasoning at each stage\n",
    "- Offer clear conclusions or recommendations\n",
    "\n",
    "Are you ready to solve the task systematically and intelligently?\"\"\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\", optional=True),\n",
    "    (\"human\", \"{messages}\"),\n",
    "])\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "# Partial the prompt with tools and tool names\n",
    "prompt = react_agent_prompt.partial(\n",
    "    tools=\"\\n\".join([f\"- {tool.name}: {tool.description}\" for tool in tools]),\n",
    "    tool_names=\", \".join([tool.name for tool in tools])\n",
    ")\n",
    "\n",
    "# Create the React agent\n",
    "agent = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "query = \"Calculate the total cost of 15 items priced at $24.50 each, including a 7% sales tax\"\n",
    "\n",
    "messages = agent.invoke({\"messages\": [(\"human\", query)]})\n",
    "\n",
    "\n",
    "# -------------------------------------- STREAM MODE --------------------------------------\n",
    "# Create the React agent\n",
    "langgraph_agent_executor = create_react_agent(\n",
    "    model=llm, \n",
    "    tools=tools, \n",
    "    prompt=prompt\n",
    ")\n",
    "\n",
    "\n",
    "for step in langgraph_agent_executor.stream(\n",
    "    {\"messages\": [(\"human\", query)]}, stream_mode=\"updates\"\n",
    "):\n",
    "    print(step)\n",
    "    \n",
    "    \n",
    "# --------------------------- FOR mAX ITERATION, USE RECURSION LIMIT ---------------------------\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "RECURSION_LIMIT = 2 * 3 + 1\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(model, tools=tools)\n",
    "\n",
    "try:\n",
    "    for chunk in langgraph_agent_executor.stream(\n",
    "        {\"messages\": [(\"human\", query)]},\n",
    "        {\"recursion_limit\": RECURSION_LIMIT},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        print(chunk[\"messages\"][-1])\n",
    "except GraphRecursionError:\n",
    "    print({\"input\": query, \"output\": \"Agent stopped due to max iterations.\"})\n",
    "    \n",
    "    \n",
    "\n",
    "# --------------------------- FOR early_stopping_method ---------------------------\n",
    "from langgraph.errors import GraphRecursionError\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "RECURSION_LIMIT = 2 * 1 + 1\n",
    "\n",
    "langgraph_agent_executor = create_react_agent(model, tools=tools)\n",
    "\n",
    "try:\n",
    "    for chunk in langgraph_agent_executor.stream(\n",
    "        {\"messages\": [(\"human\", query)]},\n",
    "        {\"recursion_limit\": RECURSION_LIMIT},\n",
    "        stream_mode=\"values\",\n",
    "    ):\n",
    "        print(chunk[\"messages\"][-1])\n",
    "except GraphRecursionError:\n",
    "    print({\"input\": query, \"output\": \"Agent stopped due to max iterations.\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Self Ask with Search Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This agent incorporates a self-asking mechanism combined with search capabilities. It can autonomously formulate internal queries \n",
    "# to gather additional information necessary to answer user questions comprehensively\n",
    "\n",
    "from langchain.agents import create_self_ask_with_search_agent\n",
    "from langchain import hub\n",
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSearchAPIWrapper\n",
    "\n",
    "google_search = GoogleSearchAPIWrapper()\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"Web Answer\",\n",
    "        func = google_search.run,\n",
    "        description=\"Get an intermediate answer to a question.\",\n",
    "        verbose = True\n",
    "    )\n",
    "]\n",
    "\n",
    "prompt = hub.pull(\"hwchase17/self-ask-with-search\")\n",
    "search_agent = create_self_ask_with_search_agent(llm, tools, prompt)\n",
    "\n",
    "agent_executor = AgentExecutor(agent=search_agent, tools=tools, verbose=True, return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "agent_executor.invoke({\"input\": \"What is the capital of France?\"})\n",
    "# agent_executor.invoke({\"input\": [HumanMessage(content=\"What is the capital of France?\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> StructuredChatAgent (can use multiple inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param format_instructions: str = 'Use a json blob to specify a tool by providing an action key (tool name) and an action_input \n",
    "# key (tool input).\\n\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\n\\nProvide only ONE action per $JSON_BLOB, \n",
    "# as shown:\n",
    "# \\n\n",
    "# \\n```\n",
    "# \\n{{{{\n",
    "    # \\n  \"action\": $TOOL_NAME,\n",
    "    # \\n  \"action_input\": $INPUT\n",
    "    # \\n}}}}\n",
    "    # \\n```\n",
    "# \n",
    "# \\n\\nFollow this format:\n",
    "# \\n\\nQuestion: input question to answer\n",
    "# \\nThought: consider previous and subsequent steps\n",
    "# \\nAction:\\n```\n",
    "# \\n$JSON_BLOB\\n```\n",
    "# \\nObservation: action result\n",
    "# \\n... (repeat Thought/Action/Observation N times)\n",
    "# \\nThought: I know what to respond\\nAction:\n",
    "# \\n```\n",
    "# \\n{{{{\n",
    "    # \\n  \"action\": \"Final Answer\",\n",
    "    # \\n  \"action_input\": \"Final response to human\"\n",
    "    # \\n}}}}\n",
    "    # \\n```'\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "system_prompt = \"\"\"\n",
    "You are a helpful assistant that uses tools to answer the user's queries. Always respond with a JSON object that specifies the \n",
    "action to take. Use the following format:\n",
    "\n",
    "{\n",
    "    \"action\": \"ToolName\",\n",
    "    \"action_input\": \"Input for the tool\"\n",
    "}\n",
    "\n",
    "If the answer can be provided without using any tools, use the following format:\n",
    "{\n",
    "    \"action\": \"Final Answer\",\n",
    "    \"action_input\": \"Your final answer to the user.\"\n",
    "}\n",
    "\n",
    "Available tools: {tools}\n",
    "Begin!\n",
    "\"\"\"\n",
    "\n",
    "human_template = \"\"\"\n",
    "User: {input}\n",
    "Chat History: {agent_scratchpad}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", system_prompt),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"human\", human_template),\n",
    "])\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents import AgentExecutor, create_structured_chat_agent\n",
    "\n",
    "# Initialize the language model\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Define the tools the agent can use\n",
    "tools = [tool_1, tool_2]\n",
    "\n",
    "# Create the StructuredChatAgent\n",
    "agent = create_structured_chat_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    "    stop_sequence=True  # Ensures the agent stops at the defined stop token\n",
    ")\n",
    "\n",
    "# Create the AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "# Define the input\n",
    "input_data = {\n",
    "    \"input\": \"What is 15 multiplied by 7?\"\n",
    "}\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent_executor.invoke(input_data)\n",
    "\n",
    "# Print the structured output\n",
    "print(json.dumps(response, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> CrewAI Agents (Advanced Collaboration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crewai import Agent, Task, Crew, Process\n",
    "from crewai_tools import SerperDevTool\n",
    "from langchain_community.llms import OpenAI\n",
    "\n",
    "# Define agents\n",
    "researcher = Agent(\n",
    "    role='Senior Research Analyst',\n",
    "    goal='Uncover cutting-edge technologies and market trends',\n",
    "    backstory=\"You are an experienced technology analyst with a knack for identifying emerging trends.\",\n",
    "    max_iter=5,  # Limit reasoning steps\n",
    "    llm=ChatOpenAI(model=\"gpt-4-turbo\", temperature=0.3),\n",
    "    verbose=True,\n",
    "    memory=True,  # Maintains conversation history\n",
    "    tools=[SerperDevTool()],  # Search tool\n",
    "    allow_delegation=True\n",
    ")\n",
    "\n",
    "# # Usage\n",
    "# research_result = research_agent.execute(\n",
    "#     \"Find recent breakthroughs in AI-driven drug discovery\"\n",
    "# )\n",
    "\n",
    "writer = Agent(\n",
    "    role='Tech Content Strategist',\n",
    "    goal='Create compelling content about technology trends',\n",
    "    backstory=\"You transform complex technical concepts into engaging content.\",\n",
    "    llm=OpenAI(temperature=0.7),\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Define tasks\n",
    "research_task = Task(\n",
    "    description=\"Research emerging AI technologies focusing on practical applications in healthcare\",\n",
    "    expected_output=\"A comprehensive report on emerging AI in healthcare with at least 5 specific technologies\",\n",
    "    agent=researcher\n",
    ")\n",
    "\n",
    "writing_task = Task(\n",
    "    description=\"Create an engaging blog post based on the research findings\",\n",
    "    expected_output=\"A 1000-word blog post with sections covering each major technology\",\n",
    "    agent=writer,\n",
    "    context=[research_task]\n",
    ")\n",
    "\n",
    "# Create the crew\n",
    "tech_crew = Crew(\n",
    "    agents=[researcher, writer],\n",
    "    tasks=[research_task, writing_task],\n",
    "    process=Process.sequential,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Execute the crew\n",
    "result = tech_crew.kickoff()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> AutoGen Conversational Agents (Microsoft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autogen import ConversableAgent, GroupChatManager\n",
    "\n",
    "# Create specialized agents\n",
    "data_scientist = ConversableAgent(\n",
    "    name=\"Data_Scientist\",\n",
    "    system_message=\"Expert in statistical analysis and ML modeling\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\"}]}\n",
    ")\n",
    "\n",
    "domain_expert = ConversableAgent(\n",
    "    name=\"Medical_Expert\",\n",
    "    system_message=\"Healthcare domain expert with clinical trial experience\",\n",
    "    llm_config={\"config_list\": [{\"model\": \"gpt-4\"}]}\n",
    ")\n",
    "\n",
    "# Advanced group chat manager\n",
    "group_chat_manager = GroupChatManager(\n",
    "    groupchat_participants=[data_scientist, domain_expert],\n",
    "    max_round=10,\n",
    "    admin_name=\"Moderator\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Google Vertex AI Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai.preview.generative_models import GenerativeModel, Part, Tool, ToolConfig, ToolUseBlock\n",
    "import vertexai\n",
    "\n",
    "# Initialize Vertex AI\n",
    "vertexai.init(project=\"your-project-id\", location=\"us-central1\")\n",
    "\n",
    "# Define tools\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Gets the current weather for a given location.\"\"\"\n",
    "    # This would typically call a weather API\n",
    "    return f\"Sunny, 72°F in {location}\"\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        function_declarations=[\n",
    "            {\n",
    "                \"name\": \"get_weather\",\n",
    "                \"description\": \"Gets the current weather for a given location\",\n",
    "                \"parameters\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"location\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"description\": \"The location to get weather for, e.g. 'San Francisco, CA'\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"location\"]\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create the model with tool configuration\n",
    "model = GenerativeModel(\n",
    "    \"gemini-pro\",\n",
    "    tools=tools,\n",
    "    generation_config={\"temperature\": 0.2}\n",
    ")\n",
    "\n",
    "# Handle function execution\n",
    "def handle_tool_call(tool_call):\n",
    "    if tool_call.name == \"get_weather\":\n",
    "        location = tool_call.args[\"location\"]\n",
    "        response = get_weather(location)\n",
    "        return response\n",
    "    return \"Unknown tool\"\n",
    "\n",
    "# Generate content with tool use\n",
    "response = model.generate_content(\n",
    "    \"What's the weather like in Seattle right now?\",\n",
    ")\n",
    "\n",
    "# Process any tool calls in the response\n",
    "if hasattr(response, 'candidates') and len(response.candidates) > 0:\n",
    "    for part in response.candidates[0].content.parts:\n",
    "        if isinstance(part, ToolUseBlock):\n",
    "            # Process and respond to the tool call\n",
    "            tool_result = handle_tool_call(part.function_call)\n",
    "            \n",
    "            # Continue the conversation with the tool result\n",
    "            follow_up = model.generate_content(\n",
    "                [\n",
    "                    Part.from_text(\"What's the weather like in Seattle right now?\"),\n",
    "                    response.candidates[0].content,\n",
    "                    Part.from_function_response(\n",
    "                        name=part.function_call.name,\n",
    "                        response=tool_result\n",
    "                    )\n",
    "                ]\n",
    "            )\n",
    "            print(follow_up.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> PandasAI Data Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandasai import SmartDataFrame\n",
    "from pandasai.llm import OpenAI\n",
    "\n",
    "# Initialize advanced data agent\n",
    "llm = OpenAI(api_token=\"sk-...\", model=\"gpt-4\")\n",
    "df = SmartDataFrame(\n",
    "    \"medical_data.csv\",\n",
    "    config={\n",
    "        \"llm\": llm,\n",
    "        \"enable_cache\": False,\n",
    "        \"max_retries\": 5,\n",
    "        \"custom_prompts\": {\n",
    "            \"clean_data\": \"Automatically clean and preprocess this dataset\"\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Execute complex analysis\n",
    "response = df.chat(\n",
    "    \"Predict which drug candidates have >80% efficacy probability \"\n",
    "    \"using Bayesian regression analysis\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create Custom Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Tuple, Optional\n",
    "from pydantic import BaseModel\n",
    "import re\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.language_models import BaseLanguageModel\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "class CustomAgent(BaseModel):\n",
    "    llm: BaseLanguageModel  # The LLM to use for decision-making\n",
    "    tools: List[BaseTool]  # List of tools the agent can use\n",
    "    max_loops: int = 5  # Maximum number of loops to prevent infinite execution\n",
    "    stop_pattern: List[str]  # Stop patterns for the LLM to avoid hallucinations\n",
    "\n",
    "    @property\n",
    "    def tool_by_names(self) -> Dict[str, BaseTool]:\n",
    "        \"\"\"Map tool names to tool objects.\"\"\"\n",
    "        return {tool.name: tool for tool in self.tools}\n",
    "\n",
    "    def run(self, question: str) -> str:\n",
    "        \"\"\"Run the agent to answer a question.\"\"\"\n",
    "        name_to_tool_map = self.tool_by_names\n",
    "        previous_responses = []\n",
    "        num_loops = 0\n",
    "\n",
    "        while num_loops < self.max_loops:\n",
    "            num_loops += 1\n",
    "\n",
    "            # Format the prompt with the current state\n",
    "            curr_prompt = PROMPT_TEMPLATE.format(\n",
    "                tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in self.tools]),\n",
    "                tool_names=\", \".join([tool.name for tool in self.tools]),\n",
    "                question=question,\n",
    "                previous_responses=\"\\n\".join(previous_responses),\n",
    "            )\n",
    "\n",
    "            # Get the next action from the LLM\n",
    "            output, tool, tool_input = self._get_next_action(curr_prompt)\n",
    "\n",
    "            # If the final answer is found, return it\n",
    "            if tool == \"Final Answer\":\n",
    "                return tool_input\n",
    "\n",
    "            # Execute the tool and get the result\n",
    "            tool_result = name_to_tool_map[tool].run(tool_input)\n",
    "            output += f\"\\n{OBSERVATION_TOKEN} {tool_result}\\n{THOUGHT_TOKEN}\"\n",
    "            print(output)  # Print the agent's reasoning\n",
    "            previous_responses.append(output)\n",
    "\n",
    "        return \"Max loops reached without finding a final answer.\"\n",
    "\n",
    "    def _get_next_action(self, prompt: str) -> Tuple[str, str, str]:\n",
    "        \"\"\"Get the next action from the LLM.\"\"\"\n",
    "        result = self.llm.generate([prompt], stop=self.stop_pattern)\n",
    "        output = result.generations[0][0].text  # Get the first generation\n",
    "\n",
    "        # Parse the output to extract the tool and input\n",
    "        tool, tool_input = self._get_tool_and_input(output)\n",
    "        return output, tool, tool_input\n",
    "\n",
    "    def _get_tool_and_input(self, generated: str) -> Tuple[str, str]:\n",
    "        \"\"\"Parse the LLM output to extract the tool and input.\"\"\"\n",
    "        if FINAL_ANSWER_TOKEN in generated:\n",
    "            return \"Final Answer\", generated.split(FINAL_ANSWER_TOKEN)[-1].strip()\n",
    "\n",
    "        # Use regex to extract the tool and input\n",
    "        regex = r\"Action: (.*?)\\nAction Input:[\\s]*(.*)\"\n",
    "        match = re.search(regex, generated, re.DOTALL)\n",
    "        if not match:\n",
    "            raise ValueError(f\"Output of LLM is not parsable for next tool use: `{generated}`\")\n",
    "\n",
    "        tool = match.group(1).strip()\n",
    "        tool_input = match.group(2).strip(\" \").strip('\"')\n",
    "        return tool, tool_input\n",
    "    \n",
    "\n",
    "FINAL_ANSWER_TOKEN = \"Final Answer:\"\n",
    "OBSERVATION_TOKEN = \"Observation:\"\n",
    "THOUGHT_TOKEN = \"Thought:\"\n",
    "PROMPT_TEMPLATE = \"\"\"Answer the question as best as you can using the following tools: \n",
    "\n",
    "{tool_description}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: comment on what you want to do next\n",
    "Action: the action to take, exactly one element of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation repeats N times, use it until you are sure of the answer)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: your final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {question}\n",
    "Thought: {previous_responses}\n",
    "\"\"\"\n",
    "\n",
    "# The tool(s) that your Agent will use\n",
    "tools = [annual_report_tool]\n",
    "\n",
    "# The question that you will ask your Agent\n",
    "question = \"What was Meta's net income in 2022? What was net income the year before that?\"\n",
    "\n",
    "# The prompt that your Agent will use and update as it is \"reasoning\"\n",
    "prompt = PROMPT_TEMPLATE.format(\n",
    "  tool_description=\"\\n\".join([f\"{tool.name}: {tool.description}\" for tool in tools]),\n",
    "  tool_names=\", \".join([tool.name for tool in tools]),\n",
    "  question=question,\n",
    "  previous_responses='{previous_responses}',\n",
    ")\n",
    "\n",
    "# The LLM that your Agent will use\n",
    "llm = OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY, model_name=\"gpt-3.5-turbo\")\n",
    "\n",
    "# Initialize your Agent\n",
    "agent = CustomAgent(\n",
    "  llm=llm, \n",
    "  tools=tools, \n",
    "  prompt=prompt, \n",
    "  stop_pattern=[f'\\n{OBSERVATION_TOKEN}', f'\\n\\t{OBSERVATION_TOKEN}'],\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,  # Only final output. If True, returns all intermediate steps\n",
    "    handle_parsing_errors=True,  # Graceful parsing errors\n",
    ")\n",
    "# Run the Agent!\n",
    "result = agent.run(question)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Use Langchain \"initialize_agent\" class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "\n",
    "tools = [tool_1,tool_2]  # Add more tools as needed\n",
    "\n",
    "# Define the agent's prompt\n",
    "prompt_template = \"\"\"\n",
    "You are an advanced agent with access to multiple tools. Your task is to resolve customer queries by:\n",
    "1. Identifying the problem or request.\n",
    "2. Using the tools provided to gather additional information if needed.\n",
    "3. Synthesizing the information into a clear, concise response.\n",
    "\n",
    "You can chain tools if required. If you are unsure, respond with 'I need more details.'\n",
    "\n",
    "Query: {query}\n",
    "\"\"\"\n",
    "llm = ChatOpenAI(\n",
    "    api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "    model=\"deepseek-chat\",\n",
    "    base_url=\"https://api.deepseek.com\",\n",
    "    streaming=True,\n",
    "    callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    ")\n",
    "\n",
    "# Initialize the agent\n",
    "advanced_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    agent_kwargs={\"prompt_template\": prompt_template},\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=advanced_agent, tools=tools, verbose=True, \n",
    "                            return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "\n",
    "agent_executor.invoke({\"query\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Key Concepts\n",
    "    # Graph : A workflow of nodes and edges.\n",
    "    # Nodes : Functions or agents that perform tasks.\n",
    "    # Edges : Connections between nodes that define the flow.\n",
    "    # State : A shared data structure passed between nodes.\n",
    "    # StateGraph : A graph that manages state transitions.\n",
    "\n",
    "# Draw a directory tree for the src directory for a LangChain project.\n",
    "\"\"\"\n",
    "src/\n",
    "├── agents/\n",
    "│   ├── __init__.py\n",
    "│   ├── agent.py\n",
    "│   ├── graph.py\n",
    "│   ├── tools.py\n",
    "│   ├── configuration.py\n",
    "│   ├── state.py\n",
    "│   ├── prompts.py\n",
    "│   └── utils.py\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal, Sequence, List, Annotated\n",
    "from typing_extensions import TypedDict\n",
    "import functools\n",
    "import operator\n",
    "\n",
    "#------------------ Define the Memory Saver-----------------------\n",
    "memory = MemorySaver()\n",
    "\n",
    "#------------------ Define the State-----------------------     # You can write a custom state class by extending the TypedDict class.\n",
    "class AgentState(TypedDict):\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    # Selected Agents: Tracks which agents are active in the workflow\n",
    "    selected_analysts: List[str]\n",
    "    \n",
    "    # Current Agent Index: Tracks the progress through the selected agents\n",
    "    current_analyst_idx: int\n",
    "\n",
    "workflow = StateGraph(AgentState)   # Initialize the Graph\n",
    "\n",
    "#------------------ Create Nodes-----------------------\n",
    "def supervisor_router(state):\n",
    "    \"\"\"Route to appropriate analyst(s) based on the query\"\"\"\n",
    "    result = routing_chain.invoke(state)\n",
    "    selected_analysts = [a.strip() for a in result.content.strip().split(',')]\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [SystemMessage(content=f\"Routing query to: {', '.join(selected_analysts)}\", name=\"supervisor\")],\n",
    "        \"selected_analysts\": selected_analysts,\n",
    "        \"current_analyst_idx\": 0\n",
    "    }\n",
    "\n",
    "# or\n",
    "def agent_node(state: AgentState, agent, name: str) -> AgentState:\n",
    "    \"\"\"\n",
    "    Generic node function for an agent.\n",
    "    - `state`: The current state of the workflow.\n",
    "    - `agent`: The agent or function to process the state.\n",
    "    - `name`: The name of the agent (for logging or identification).\n",
    "    \"\"\"\n",
    "    # Invoke the agent with the current state\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # Update the state with the agent's output\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=result[\"messages\"][-1].content, name=name)],\n",
    "        \"selected_agents\": state[\"selected_agents\"],\n",
    "        \"current_agent_idx\": state[\"current_agent_idx\"] + 1\n",
    "    }\n",
    "\n",
    "\n",
    "#--------------------- Wrap the agent in a node--------------------------\n",
    "\n",
    "# Create the analysts with their specific tools\n",
    "quant_strategist = create_react_agent(llm, tools=quant_strategist_tools)\n",
    "quant_strategist_node = functools.partial(agent_node, agent=quant_strategist, name=\"quant_strategist\")\n",
    "\n",
    "macro_analyst = create_react_agent(llm, tools=macro_analyst_tools)\n",
    "macro_analyst_node = functools.partial(agent_node, agent=macro_analyst, name=\"macro_analyst\")\n",
    "\n",
    "\n",
    "#------------------- Add Nodes to Graph-----------------------\n",
    "workflow = StateGraph(AgentState)   # Initialize the Graph\n",
    "workflow.add_node(\"supervisor\", supervisor_router)  # Add the supervisor node\n",
    "workflow.add_node(\"quant_strategist\", quant_strategist_node)    # Add the quant_strategist node\n",
    "workflow.add_node(\"macro_analyst\", macro_analyst_node)        # Add the macro_analyst node\n",
    "\n",
    "#------------------- Define the Prompt-----------------------\n",
    "class SupervisorPrompt(ChatPromptTemplate):\n",
    "    \"\"\"Prompt for the supervisor node\"\"\"\n",
    "    messages: MessagesPlaceholder\n",
    "    selected_analysts: List[str]\n",
    "    current_analyst_idx: int\n",
    "\n",
    "#------------------- Define Conditional Edge-----------------------\n",
    "def get_next_step(state: AgentState) -> str:\n",
    "    \"\"\"\n",
    "    Determines the next step in the workflow.\n",
    "    - If no agents are selected, go to the final summary.\n",
    "    - If all agents have processed, go to the final summary.\n",
    "    - Otherwise, go to the next agent.\n",
    "    \"\"\"\n",
    "    if not state[\"selected_agents\"]:\n",
    "        return \"final_summary\"\n",
    "    current_idx = state[\"current_agent_idx\"]\n",
    "    if current_idx >= len(state[\"selected_agents\"]):\n",
    "        return \"final_summary\"\n",
    "    return state[\"selected_agents\"][current_idx]\n",
    "\n",
    "\n",
    "# Add conditional edges:\n",
    "workflow.add_conditional_edges(\n",
    "    \"supervisor\",  # Source node\n",
    "    get_next_step,  # Router node/Function to determine the next step\n",
    "    {\n",
    "        \"quant_strategist\": \"quant_strategist\",  # Route to quant_strategist node\n",
    "        \"macro_analyst\": \"macro_analyst\",        # Route to macro_analyst node\n",
    "        \"final_summary\": \"final_summary\"         # Route to final_summary node\n",
    "    }\n",
    ")\n",
    "\n",
    "#------------------ Add Final Edges ------------------------------------\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "workflow.add_edge(\"final_summary\", END)\n",
    "\n",
    "#-------------------- Compile the Graph --------------------------------\n",
    "graph = workflow.compile()\n",
    "# or\n",
    "graph = workflow.compile(checkpointer=memory)   # Compile the graph with memory\n",
    "# or\n",
    "graph = workflow.compile(checkpointer=memory, interrupt_before=[\"quant_strategist_node\"])  # Compile the graph with memory and interrupt before quant_strategist_node\n",
    "\n",
    "\n",
    "#------------------ Stream the Graph with Memory--------------------------------\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}   # add memory thread, we used thread_id = 2\n",
    "events = graph.stream({\"messages\": {\"Hi there, my name is Paul\"}}, config, stream_mode = \"values\")\n",
    "\n",
    "for event in events:    # Iterate over the events\n",
    "    event['messages'][-1].pretty_print()\n",
    "\n",
    "memory.get(config)  # Retrieve the memory for a specific configuration or thread_id\n",
    "\n",
    "\n",
    "#-------------------- Accessing the Graph State --------------------------------\n",
    "graph = workflow.compile()\n",
    "graph.get_state(config).values  # get the state of the graph\n",
    "graph.get_state(config).values.get(\"messages\", \"\")  # get the messages from the state\n",
    "graph.update_state(config, {\"input\": \"Hello, World!\"})  # update the state of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Nice way to execute the LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------ATLERNATIVE WAY TO RUN THE GRAPH IN A BEAUTIFUL WAY------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Run the Graph------------------------------------\n",
    "#------------------------- Custom Function----------------------------------\n",
    "from typing import Dict, Any\n",
    "import json\n",
    "import re\n",
    "from langchain_core.messages import HumanMessage\n",
    "from rich.console import Console\n",
    "from rich.panel import Panel\n",
    "from rich.text import Text\n",
    "from rich.rule import Rule\n",
    "\n",
    "#---------- Formatting Functions\n",
    "# Format Bold Text\n",
    "def format_bold_text(content: str) -> Text:\n",
    "    \"\"\"Convert **text** to rich Text with bold formatting.\"\"\"\n",
    "    text = Text()\n",
    "    pattern = r'\\*\\*(.*?)\\*\\*'\n",
    "    parts = re.split(pattern, content)\n",
    "    for i, part in enumerate(parts):\n",
    "        if i % 2 == 0:\n",
    "            text.append(part)\n",
    "        else:\n",
    "            text.append(part, style=\"bold\")\n",
    "    return text\n",
    "\n",
    "# Format Message Content\n",
    "def format_message_content(content: str) -> Union[str, Text]:\n",
    "    \"\"\"Format the message content, handling JSON and text with bold markers.\"\"\"\n",
    "    try:\n",
    "        data = json.loads(content)\n",
    "        return json.dumps(data, indent=2)\n",
    "    except:\n",
    "        if '**' in content:\n",
    "            return format_bold_text(content)\n",
    "        return content\n",
    "\n",
    "# Format Agent Message\n",
    "def format_agent_message(message: HumanMessage) -> Union[str, Text]:\n",
    "    \"\"\"Format a single agent message.\"\"\"\n",
    "    return format_message_content(message.content)\n",
    "\n",
    "# Get Agent Title\n",
    "def get_agent_title(agent: str, message: HumanMessage) -> str:\n",
    "    \"\"\"Get the title for the agent panel, with fallback handling.\"\"\"\n",
    "    base_title = agent.replace('_', ' ').title()\n",
    "    if hasattr(message, 'name') and message.name is not None:\n",
    "        try:\n",
    "            return message.name.replace('_', ' ').title()\n",
    "        except:\n",
    "            return base_title\n",
    "    return base_title\n",
    "\n",
    "# Print a Single Step\n",
    "def print_step(step: Dict[str, Any]) -> None:\n",
    "    \"\"\"Pretty print a single step of the agent execution.\"\"\"\n",
    "    console = Console()\n",
    "    for agent, data in step.items():\n",
    "        # Handle supervisor steps\n",
    "        if 'next' in data:\n",
    "            next_agent = data['next']\n",
    "            text = Text()\n",
    "            text.append(\"Portfolio Manager \", style=\"bold magenta\")\n",
    "            text.append(\"assigns next task to \", style=\"white\")\n",
    "            if next_agent == \"final_summary\":\n",
    "                text.append(\"FINAL SUMMARY\", style=\"bold yellow\")\n",
    "            elif next_agent == \"END\":\n",
    "                text.append(\"END\", style=\"bold red\")\n",
    "            else:\n",
    "                text.append(f\"{next_agent}\", style=\"bold green\")\n",
    "            console.print(Panel(\n",
    "                text,\n",
    "                title=\"[bold blue]Supervision Step\",\n",
    "                border_style=\"blue\"\n",
    "            ))\n",
    "        # Handle agent responses and final summary\n",
    "        if 'messages' in data:\n",
    "            message = data['messages'][0]\n",
    "            formatted_content = format_agent_message(message)\n",
    "            if agent == \"final_summary\":\n",
    "                # Final summary formatting\n",
    "                console.print(Rule(style=\"yellow\", title=\"Portfolio Analysis\"))\n",
    "                console.print(Panel(\n",
    "                    formatted_content,\n",
    "                    title=\"[bold yellow]Investment Summary and Recommendation\",\n",
    "                    border_style=\"yellow\",\n",
    "                    padding=(1, 2)\n",
    "                ))\n",
    "                console.print(Rule(style=\"yellow\"))\n",
    "            else:\n",
    "                # Regular analyst reports\n",
    "                title = get_agent_title(agent, message)\n",
    "                console.print(Panel(\n",
    "                    formatted_content,\n",
    "                    title=f\"[bold blue]{title} Report\",\n",
    "                    border_style=\"green\"\n",
    "                ))\n",
    "\n",
    "# Stream the Execution\n",
    "def stream_agent_execution(graph, input_data: Dict, config: Dict) -> None:\n",
    "    \"\"\"Stream and pretty print the agent execution.\"\"\"\n",
    "    console = Console()\n",
    "    console.print(\"\\n[bold blue]Starting Agent Execution...[/bold blue]\\n\")\n",
    "    for step in graph.stream(input_data, config):\n",
    "        if \"__end__\" not in step:\n",
    "            print_step(step)\n",
    "            console.print(\"\\n\")\n",
    "    console.print(\"[bold blue]Analysis Complete[/bold blue]\\n\")\n",
    "\n",
    "\n",
    "# Run the Graph\n",
    "# Define the input data\n",
    "input_data = {\n",
    "    \"messages\": [HumanMessage(content=\"What is AAPL's current price and latest revenue?\")]\n",
    "}\n",
    "\n",
    "# Define the configuration (e.g., recursion limit)\n",
    "config = {\"recursion_limit\": 10}\n",
    "\n",
    "# Stream the execution\n",
    "stream_agent_execution(graph, input_data, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph States"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangGraph State: --> Example\n",
    "# What is a LangGraph State?\n",
    "    # A LangGraph state is a data structure that holds the current state of the workflow. It is passed between nodes in the graph, \n",
    "    # and each node can modify the state as needed. The state typically contains all the information required for the workflow to function, \n",
    "    # such as inputs, intermediate results, and outputs.\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Any, Optional, Annotated\n",
    "import operator\n",
    "from langgraph.graph import Graph, StateGraph, MessageGraph, MessagesState\n",
    "\n",
    " \n",
    "#------------------ Define the State (State.py) -----------------------\n",
    "DEFAULT_EXTRACTION_SCHEMA = {\n",
    "    \"title\": \"CompanyInfo\",\n",
    "    \"description\": \"Basic information about a company\",\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"company_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Official name of the company\",\n",
    "        },\n",
    "        \"founding_year\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"description\": \"Year the company was founded\",\n",
    "        },\n",
    "        \"founder_names\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\"type\": \"string\"},\n",
    "            \"description\": \"Names of the founding team members\",\n",
    "        },\n",
    "        \"product_description\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Brief description of the company's main product or service\",\n",
    "        },\n",
    "        \"funding_summary\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"Summary of the company's funding history\",\n",
    "        },\n",
    "    },\n",
    "    \"required\": [\"company_name\"],\n",
    "}\n",
    "\n",
    "class SampleState(MessagesState):   # this state will have both company and messages (since messages is already defined in the MessagesState)\n",
    "    \"\"\"A sample state class that extends the MessagesState.\"\"\"\n",
    "    company: str\n",
    "    \"\"\"Company to research provided by the user.\"\"\"\n",
    "    \n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class InputState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    company: str\n",
    "    \"Company to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: Optional[dict[str, Any]] = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "\n",
    "@dataclass(kw_only=True)\n",
    "class OverallState:\n",
    "    \"\"\"Input state defines the interface between the graph and the user (external API).\"\"\"\n",
    "\n",
    "    # Messages: Stores conversation history\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    \n",
    "    company: str\n",
    "    \"Company to research provided by the user.\"\n",
    "\n",
    "    extraction_schema: dict[str, Any] = field(\n",
    "        default_factory=lambda: DEFAULT_EXTRACTION_SCHEMA\n",
    "    )\n",
    "    \"The json schema defines the information the agent is tasked with filling out.\"\n",
    "\n",
    "    user_notes: str = field(default=None)\n",
    "    \"Any notes from the user to start the research process.\"\n",
    "\n",
    "    search_queries: list[str] = field(default=None)\n",
    "    \"List of generated search queries to find relevant information\"\n",
    "\n",
    "    completed_notes: Annotated[list, operator.add] = field(default_factory=list)\n",
    "    \"Notes from completed research related to the schema\"\n",
    "\n",
    "    info: dict[str, Any] = field(default=None)\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "    is_satisfactory: bool = field(default=None)\n",
    "    \"True if all required fields are well populated, False otherwise\"\n",
    "\n",
    "    reflection_steps_taken: int = field(default=0)\n",
    "    \"Number of times the reflection node has been executed\"\n",
    "\n",
    "    \n",
    "@dataclass(kw_only=True)\n",
    "class OutputState:\n",
    "    \"\"\"The response object for the end user.\n",
    "\n",
    "    This class defines the structure of the output that will be provided\n",
    "    to the user after the graph's execution is complete.\n",
    "    \"\"\"\n",
    "\n",
    "    info: dict[str, Any]\n",
    "    \"\"\"\n",
    "    A dictionary containing the extracted and processed information\n",
    "    based on the user's query and the graph's execution.\n",
    "    This is the primary output of the enrichment process.\n",
    "    \"\"\"\n",
    "\n",
    "#------------------ Define the Configuration (Configuration.py) -----------------------\n",
    "@dataclass(kw_only=True)\n",
    "class Configuration:\n",
    "    \"\"\"The configurable fields for the chatbot.\"\"\"\n",
    "\n",
    "    max_search_queries: int = 3  # Max search queries per company\n",
    "    max_search_results: int = 3  # Max search results per query\n",
    "    max_reflection_steps: int = 0  # Max reflection steps\n",
    "\n",
    "    @classmethod\n",
    "    def from_runnable_config(\n",
    "        cls, config: Optional[RunnableConfig] = None\n",
    "    ) -> \"Configuration\":\n",
    "        \"\"\"Create a Configuration instance from a RunnableConfig.\"\"\"\n",
    "        configurable = (\n",
    "            config[\"configurable\"] if config and \"configurable\" in config else {}\n",
    "        )\n",
    "        values: dict[str, Any] = {\n",
    "            f.name: os.environ.get(f.name.upper(), configurable.get(f.name))\n",
    "            for f in fields(cls)\n",
    "            if f.init\n",
    "        }\n",
    "        return cls(**{k: v for k, v in values.items() if v})\n",
    "#-------------------------------------------------------------------------------\n",
    "from langgraph.graph import START, END, StateGraph\n",
    "from agent.configuration import Configuration\n",
    "\n",
    "builder = StateGraph(\n",
    "    OverallState,\n",
    "    input=InputState,\n",
    "    output=OutputState,\n",
    "    config_schema=Configuration,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DAG: Directed Acyclic Graph \n",
    "    # Definition : A graph where nodes are connected in a linear, directional manner without forming closed loops .\n",
    "    # Use Case : Used by LangChain to represent workflows where tasks are executed in a non-repeating, linear sequence .\n",
    "        ''' Start → Node A → Node B → Node C → End '''\n",
    "            # No loops : Once a node is processed, it doesn’t revisit previous nodes.\n",
    "            # Linear flow : Tasks are executed in a strict sequence.\n",
    "            \n",
    "            \n",
    "# DCG: Directed Cyclic Graph --> used by LangGraph to represent the workflow of nodes and edges.\n",
    "    # Definition : A graph where nodes are connected in a directional manner and can form loops or cycles .\n",
    "    # Use Case : Used by LangGraph to represent workflows with complex patterns , including loops and conditional branching .\n",
    "        '''\n",
    "        Start → Node A → Node B → Node C\n",
    "                ↑              ↓\n",
    "                └──────────────┘\n",
    "        '''\n",
    "            # Loops allowed : Nodes can revisit previous nodes (e.g., for iterative tasks).\n",
    "            # Complex flow : Supports conditional edges, loops, and dynamic routing.\n",
    "\n",
    "\n",
    "# Edges:\n",
    "    # Simple Edge:\n",
    "        # A direct connection between two nodes in the graph. Used whrn the flow is fixed and uncontitional.\n",
    "        ''' Start → Node A → Node B → Node C → End '''\n",
    "    \n",
    "    # Conditional Edge:\n",
    "        # A connection between two nodes that is determined by a condition or decision function.\n",
    "        '''\n",
    "            Start → Node A\n",
    "                    ↓\n",
    "                ┌─────┴─────┐\n",
    "            Condition 1   Condition 2\n",
    "                ↓             ↓\n",
    "            Node B         Node C\n",
    "                ↓             ↓\n",
    "            Node D         Node E\n",
    "                └─────┬─────┘\n",
    "                    ↓\n",
    "                    End\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph, Graph # Import the necessary classes\n",
    "from IPython.display import Image, display\n",
    "from pydantic import BaseModel, Field\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# Define the state as a Pydantic model\n",
    "class CustomerSupportState(BaseModel):\n",
    "    query: str = Field(..., description=\"The customer's query\")\n",
    "    response: str = Field(None, description=\"The response to the customer\")\n",
    "    issue_type: str = Field(None, description=\"The type of issue (FAQ, Escalation, Recommendation)\")\n",
    "    escalation_required: bool = Field(False, description=\"Whether the issue requires escalation\")\n",
    "    product_recommendation: str = Field(None, description=\"Product recommendation for the customer\")\n",
    "\n",
    "# Create the workflow graph\n",
    "workflow = StateGraph(CustomerSupportState)\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE WITHOUT LLM ---------------------------------------------\n",
    "#------------------------------------------------------------------------------------------------------------\n",
    "# Node A: Classify the customer's query\n",
    "def classify_query(state: CustomerSupportState) -> dict:\n",
    "    query = state.query.lower()\n",
    "    if \"faq\" in query or \"how to\" in query or \"what is\" in query:\n",
    "        return {\"issue_type\": \"FAQ\"}\n",
    "    elif \"issue\" in query or \"problem\" in query or \"error\" in query:\n",
    "        return {\"issue_type\": \"Escalation\"}\n",
    "    elif \"recommend\" in query or \"suggest\" in query:\n",
    "        return {\"issue_type\": \"Recommendation\"}\n",
    "    else:\n",
    "        return {\"issue_type\": \"Unknown\"}\n",
    "\n",
    "#--------------------------------------------- TOOL NODE ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "from langchain.tools.retriever import create_retriever_tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "vectorstore=Chroma.from_documents(\n",
    "    documents=doc_splits,\n",
    "    collection_name=\"rag-chrome\",\n",
    "    embedding=embeddings\n",
    "    \n",
    ")\n",
    "retriever=vectorstore.as_retriever()\n",
    "retriever_tool=create_retriever_tool(\n",
    "    retriever,\n",
    "    \"retrieve_blog_posts\",\n",
    "    \"Search and return information about Lilian Weng blog posts on LLM agents, prompt engineering, and adversarial attacks on LLMs.You are a specialized assistant. Use the 'retriever_tool' **only** when the query explicitly relates to LangChain blog data. For all other queries, respond directly without using any tool. For simple queries like 'hi', 'hello', or 'how are you', provide a normal response.\",\n",
    "    )\n",
    "\n",
    "tools=[retriever_tool]\n",
    "retrieve=ToolNode([retriever_tool])\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- TOOL NODE WITH FALLBACK ---------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    \"\"\"\n",
    "    Function to handle errors that occur during tool execution.\n",
    "    Args:\n",
    "        state (dict): The current state of the AI agent, which includes messages and tool call details.\n",
    "    Returns:\n",
    "        dict: A dictionary containing error messages for each tool that encountered an issue.\n",
    "    \"\"\"\n",
    "    # Retrieve the error from the current state\n",
    "    error = state.get(\"error\")\n",
    "    # Access the tool calls from the last message in the state's message history\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    # Return a list of ToolMessages with error details, linked to each tool call ID\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",  # Format the error message for the user\n",
    "                tool_call_id=tc[\"id\"],  # Associate the error message with the corresponding tool call ID\n",
    "            )\n",
    "            for tc in tool_calls  # Iterate over each tool call to produce individual error messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create a tool node with fallback error handling.\n",
    "    Args:\n",
    "        tools (list): A list of tools to be included in the node.\n",
    "    Returns:\n",
    "        dict: A tool node that uses fallback behavior in case of errors.\n",
    "    \"\"\"\n",
    "    # Create a ToolNode with the provided tools and attach a fallback mechanism\n",
    "    # If an error occurs, it will invoke the handle_tool_error function to manage the error\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)],  # Use a lambda function to wrap the error handler\n",
    "        exception_key=\"error\"  # Specify that this fallback is for handling errors\n",
    "    )\n",
    "\n",
    "builder = StateGraph(OverallState)\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback([retriever_tool, tool_1, tool_2]))\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM 1 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import add_messages\n",
    "from typing import Annotated, Sequence, TypedDict\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    \n",
    "def ai_assistant(state:AgentState):\n",
    "    print(\"---CALL AGENT---\")\n",
    "    messages = state['messages']\n",
    "    \n",
    "    if len(messages)>1:\n",
    "        last_message = messages[-1]\n",
    "        question = last_message.content\n",
    "        prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a helpful assistant whatever question has been asked to find out that in the given question and answer.\n",
    "                        Here is the question:{question}\n",
    "                        \"\"\",\n",
    "                        input_variables=[\"question\"]\n",
    "                        )\n",
    "            \n",
    "        chain = prompt | llm\n",
    "    \n",
    "        response=chain.invoke({\"question\": question})\n",
    "        return {\"messages\": [response]}\n",
    "    else:\n",
    "        llm_with_tool = llm.bind_tools(tools)\n",
    "        response = llm_with_tool.invoke(messages)\n",
    "        #response=handle_query(messages)\n",
    "        return {\"messages\": [response]}\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM + structured output ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "class grade(BaseModel):\n",
    "    binary_score:str=Field(description=\"Relevance score 'yes' or 'no'\")\n",
    "    \n",
    "def grade_documents(state:AgentState)->Literal[\"Output_Generator\", \"Query_Rewriter\"]:\n",
    "    llm_with_structure_op=llm.with_structured_output(grade)\n",
    "    \n",
    "    prompt=PromptTemplate(\n",
    "        template=\"\"\"You are a grader deciding if a document is relevant to a user’s question.\n",
    "                    Here is the document: {context}\n",
    "                    Here is the user’s question: {question}\n",
    "                    If the document talks about or contains information related to the user’s question, mark it as relevant. \n",
    "                    Give a 'yes' or 'no' answer to show if the document is relevant to the question.\"\"\",\n",
    "        input_variables=[\"context\", \"question\"]\n",
    "                    )\n",
    "    chain = prompt | llm_with_structure_op\n",
    "    \n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    question = messages[0].content\n",
    "    docs = last_message.content\n",
    "    scored_result = chain.invoke({\"question\": question, \"context\": docs})\n",
    "    score = scored_result.binary_score\n",
    "\n",
    "    if score == \"yes\":\n",
    "        print(\"---DECISION: DOCS RELEVANT---\")\n",
    "        return \"generator\" #this should be a node name\n",
    "    else:\n",
    "        print(\"---DECISION: DOCS NOT RELEVANT---\")\n",
    "        return \"rewriter\" #this should be a node name\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM + structured output 2 ----------------------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "query_writer_instructions=\"\"\"Your goal is to generate targeted web search query.\n",
    "\n",
    "The query will gather information related to a specific topic.\n",
    "\n",
    "Topic:\n",
    "{research_topic}\n",
    "\n",
    "Return your query as a JSON object:\n",
    "{{\n",
    "    \"query\": \"string\",\n",
    "    \"aspect\": \"string\",\n",
    "    \"rationale\": \"string\"\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "def generate_query(state: SummaryState, config: RunnableConfig):\n",
    "    \"\"\" Generate a query for web search \"\"\"\n",
    "    \n",
    "    # Format the prompt\n",
    "    query_writer_instructions_formatted = query_writer_instructions.format(research_topic=state.research_topic)\n",
    "\n",
    "    # Generate a query\n",
    "    configurable = Configuration.from_runnable_config(config)\n",
    "    llm_json_mode = ChatOllama(model=configurable.local_llm, temperature=0, format=\"json\")\n",
    "    result = llm_json_mode.invoke(\n",
    "        [SystemMessage(content=query_writer_instructions_formatted),\n",
    "        HumanMessage(content=f\"Generate a query for web search:\")]\n",
    "    )   \n",
    "    query = json.loads(result.content)\n",
    "    \n",
    "    return {\"search_query\": query['query']}\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------ NODE with LLM + structured output 3 ----------------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "RECOMMENDATION_PROMPT_2 = \"\"\"\n",
    "You are a specialized travel recommendation assistant. \n",
    "Generate at least 10 unique recommendations for the user. \n",
    "Output your response as a list of dictionaries, each containing the fields: \n",
    "  - \"key\": A short label, e.g. \"Crime Rate\"\n",
    "  - \"value\": A concise recommendation\n",
    "\n",
    "For example:\n",
    "[\n",
    "    {\"key\": \"Crime Rate\", \"value\": \"The city is generally safe but beware of pickpockets in tourist areas.\"},\n",
    "    {\"key\": \"Weather Advice\", \"value\": \"Spring is mild; pack light jackets and an umbrella.\"}\n",
    "]\n",
    "\n",
    "### User Query:\n",
    "{query}\n",
    "\"\"\"\n",
    "\n",
    "def recommendations_node_2(state: OverallState) -> OverallState:\n",
    "    import openai\n",
    "    \n",
    "    # If you haven't set up your API key globally, do so here:\n",
    "    # openai.api_key = \"YOUR_OPENAI_API_KEY\"\n",
    "    \n",
    "    client = openai  # or adapt to your environment if needed\n",
    "\n",
    "    # Combine all user messages into a single query\n",
    "    all_messages = \"\\n\".join([message.content for message in state.messages])\n",
    "    preferences_text = \"\\n\".join([f\"{key}: {value}\" for key, value in state.user_preferences.items()])\n",
    "    query = f\"{all_messages}\\n\\nUser Preferences:\\n{preferences_text}\"\n",
    "\n",
    "    # Define the structured response format with a JSON Schema\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o\",  # Replace with a valid model you have access to.\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": RECOMMENDATION_PROMPT_2},\n",
    "            {\"role\": \"user\", \"content\": query},\n",
    "        ],\n",
    "        # The 'response_format' parameter needs 'json_schema' -> 'name' + 'schema'\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"recommendation_schema\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"recommendations\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"key\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Short label for the recommendation\"\n",
    "                                    },\n",
    "                                    \"value\": {\n",
    "                                        \"type\": \"string\",\n",
    "                                        \"description\": \"Concise recommendation content\"\n",
    "                                    }\n",
    "                                },\n",
    "                                \"required\": [\"key\", \"value\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            },\n",
    "                            \"description\": \"A list of travel recommendations.\"\n",
    "                        }\n",
    "                    },\n",
    "                    \"required\": [\"recommendations\"],\n",
    "                    \"additionalProperties\": False\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Parse and return the generated structured output\n",
    "    try:\n",
    "        structured_output = completion.choices[0].message.content\n",
    "        parsed_output = json.loads(structured_output)\n",
    "        recommendation_list = parsed_output[\"recommendations\"]  # This is the list of dictionaries\n",
    "        transformed_list = [{item[\"key\"]: item[\"value\"]} for item in recommendation_list]\n",
    "        \n",
    "        state.recommendations = transformed_list\n",
    "        \n",
    "        return state \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {e}\")\n",
    "        return {\"error\": \"Failed to generate recommendations.\"}\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with LLM + Blind Tools (Used for User call) ---------------------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------------------------\n",
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "# from langchain_aws import ChatBedrock\n",
    "import boto3\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "@tool\n",
    "def compute_savings(monthly_cost: float) -> float:\n",
    "    \"\"\"\n",
    "    Tool to compute the potential savings when switching to solar energy based on the user's monthly electricity cost.\n",
    "    \n",
    "    Args:\n",
    "        monthly_cost (float): The user's current monthly electricity cost.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'number_of_panels': The estimated number of solar panels required.\n",
    "            - 'installation_cost': The estimated installation cost.\n",
    "            - 'net_savings_10_years': The net savings over 10 years after installation costs.\n",
    "    \"\"\"\n",
    "    def calculate_solar_savings(monthly_cost):\n",
    "        # Assumptions for the calculation\n",
    "        cost_per_kWh = 0.28  \n",
    "        cost_per_watt = 1.50  \n",
    "        sunlight_hours_per_day = 3.5  \n",
    "        panel_wattage = 350  \n",
    "        system_lifetime_years = 10  \n",
    "        # Monthly electricity consumption in kWh\n",
    "        monthly_consumption_kWh = monthly_cost / cost_per_kWh\n",
    "        \n",
    "        # Required system size in kW\n",
    "        daily_energy_production = monthly_consumption_kWh / 30\n",
    "        system_size_kW = daily_energy_production / sunlight_hours_per_day\n",
    "        \n",
    "        # Number of panels and installation cost\n",
    "        number_of_panels = system_size_kW * 1000 / panel_wattage\n",
    "        installation_cost = system_size_kW * 1000 * cost_per_watt\n",
    "        \n",
    "        # Annual and net savings\n",
    "        annual_savings = monthly_cost * 12\n",
    "        total_savings_10_years = annual_savings * system_lifetime_years\n",
    "        net_savings = total_savings_10_years - installation_cost\n",
    "        \n",
    "        return {\n",
    "            \"number_of_panels\": round(number_of_panels),\n",
    "            \"installation_cost\": round(installation_cost, 2),\n",
    "            \"net_savings_10_years\": round(net_savings, 2)\n",
    "        }\n",
    "    # Return calculated solar savings\n",
    "    return calculate_solar_savings(monthly_cost)\n",
    "\n",
    "# Define the state for the workflow\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "# Define the assistant class used for invoking the runnable\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        # Initialize with the runnable that defines the process for interacting with the tools\n",
    "        self.runnable = runnable\n",
    "    def __call__(self, state: State):\n",
    "        while True:\n",
    "            # Invoke the runnable with the current state (messages and context)\n",
    "            result = self.runnable.invoke(state)\n",
    "            \n",
    "            # If the tool fails to return valid output, re-prompt the user to clarify or retry\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                # Add a message to request a valid response\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                # Break the loop when valid output is obtained\n",
    "                break\n",
    "        # Return the final state after processing the runnable\n",
    "        return {\"messages\": result}\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        model=\"deepseek-chat\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful customer support assistant for Solar Panels Belgium.\n",
    "            You should get the following information from them:\n",
    "            - monthly electricity cost\n",
    "            If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "            After you are able to discern all the information, call the relevant tool.\n",
    "            ''',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the tools the assistant will use\n",
    "part_1_tools = [\n",
    "    compute_savings\n",
    "]\n",
    "\n",
    "# Bind the tools to the assistant's workflow\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools, tool_choice=\"any\")\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------- NODE with LLM + Tools with multiple parameters ----------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "def accommodation_finder_node(state: OverallState) -> OverallState:\n",
    "    \"\"\"\n",
    "    This node extracts accommodation details from the user's query in state.messages\n",
    "    and returns a structured output that can be passed to the booking tool.\n",
    "    \"\"\"\n",
    "\n",
    "    class AccommodationOutput(BaseModel):\n",
    "        location: str = Field(..., description=\"The exact location or neighborhood where the traveler wants to stay (e.g., 'Brooklyn').\")\n",
    "        checkin_date: str = Field(..., description=\"The check-in date in YYYY-MM-DD format.\")\n",
    "        checkout_date: str = Field(..., description=\"The check-out date in YYYY-MM-DD format.\")\n",
    "        adults: int = Field(default=2, description=\"The number of adult guests.\")\n",
    "        rooms: int = Field(default=1, description=\"The number of rooms.\")\n",
    "        currency: str = Field(default=\"USD\", description=\"The currency for the prices.\")\n",
    "        \n",
    "    # Create a new LLM with structured output\n",
    "    llm_with_structure = llm.with_structured_output(AccommodationOutput)\n",
    "\n",
    "    # Define the prompt template\n",
    "    prompt = PromptTemplate(\n",
    "        template=\"\"\"\n",
    "        You are an advanced travel planner assistant. Your task is to extract accommodation details\n",
    "        from the traveler's query. Use the following information to generate a structured output for\n",
    "        booking accommodations:\n",
    "\n",
    "        ### Traveler Query:\n",
    "        {query}\n",
    "\n",
    "        ### Instructions:\n",
    "        1. Extract the exact location or neighborhood where the traveler wants to stay (e.g., \"Brooklyn\").\n",
    "           - If the traveler does not specify a location, use the city or city code provided in the state.\n",
    "        2. Extract the check-in and check-out dates from the query.\n",
    "           - If the dates are not explicitly mentioned, use the default dates from the state.\n",
    "        3. Extract the number of adults and rooms from the query.\n",
    "           - If not specified, use the default values: 1 adult and 1 room.\n",
    "        4. Use the default currency 'USD' unless specified otherwise.\n",
    "        5. Return the structured output in the following format:\n",
    "           - location: The exact location or neighborhood.\n",
    "           - checkin_date: The check-in date in YYYY-MM-DD format.\n",
    "           - checkout_date: The check-out date in YYYY-MM-DD format.\n",
    "           - adults: The number of adult guests.\n",
    "           - rooms: The number of rooms.\n",
    "           - currency: The currency for the prices.\n",
    "\n",
    "        ### Example Output:\n",
    "        - location: \"Brooklyn\"\n",
    "        - checkin_date: \"2023-12-01\"\n",
    "        - checkout_date: \"2023-12-10\"\n",
    "        - adults: 2\n",
    "        - rooms: 1\n",
    "        - currency: \"USD\"\n",
    "        \"\"\",\n",
    "        input_variables=[\"query\"]\n",
    "    )\n",
    "\n",
    "    # Create the chain\n",
    "    chain = prompt | llm_with_structure\n",
    "\n",
    "    # Extract the user's query from state.messages\n",
    "    query = state.messages[-1].content  # Assuming the last message is the user's query\n",
    "\n",
    "    # Invoke the chain to generate the structured output\n",
    "    structured_output = chain.invoke({\"query\": query})\n",
    "\n",
    "    # Call Google Flights Search Tool        \n",
    "    booking_search_input = BookingSearchInput(\n",
    "        location=structured_output.location,\n",
    "        checkin_date=structured_output.checkin_date,\n",
    "        checkout_date=structured_output.checkout_date,\n",
    "        adults=structured_output.adults,\n",
    "        rooms=structured_output.rooms,\n",
    "        currency=structured_output.currency,\n",
    "    )\n",
    "\n",
    "    booking_results = booking_tool.func(booking_search_input)\n",
    "    \n",
    "    # Update the state with the structured output\n",
    "    state.accommodation = booking_results\n",
    "\n",
    "    # Return the updated state\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 1 ---------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "from langchain.agents import Tool, create_react_agent\n",
    "\n",
    "# Define a REACT-based agent node\n",
    "def react_agent_node(state: CustomerSupportState):\n",
    "    tools = [retriever_tool]  # Add your tool(s) here\n",
    "    prompt_template = \"\"\"You are a reasoning and acting agent.\n",
    "    Use the tools available to gather or verify information as needed.\n",
    "    Respond directly if no tools are required.\n",
    "\n",
    "    Question: {query}\n",
    "    \"\"\"\n",
    "    react_agent = create_react_agent(\n",
    "        tools=tools,\n",
    "        prompt_template=prompt_template,\n",
    "        llm=llm,\n",
    "    )\n",
    "    \n",
    "    agent_executor = AgentExecutor(agent=react_agent, tools=tools, verbose=True)\n",
    "    # Execute the REACT agent\n",
    "    query = state.query\n",
    "    response = agent_executor.invoke({\"query\": query})\n",
    "    state.response = response\n",
    "    return state\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 2 ---------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------------\n",
    "import functools\n",
    "\n",
    "def agent_node(state: OverallState, agent, name: str) -> OverallState:\n",
    "    \"\"\"\n",
    "    Generic node function for an agent.\n",
    "    - `state`: The current state of the workflow.\n",
    "    - `agent`: The agent or function to process the state.\n",
    "    - `name`: The name of the agent (for logging or identification).\n",
    "    \"\"\"\n",
    "    # Invoke the agent with the current state\n",
    "    result = agent.invoke(state)\n",
    "    \n",
    "    # Update the state with the agent's output\n",
    "    return {\n",
    "        \"messages\": state[\"messages\"] + [HumanMessage(content=result[\"messages\"][-1].content, name=name)],\n",
    "        \"selected_agents\": state[\"selected_agents\"],\n",
    "        \"current_agent_idx\": state[\"current_agent_idx\"] + 1\n",
    "    }\n",
    "\n",
    "# wrap the agent in a node\n",
    "def query_param_generator_node(agent_node):\n",
    "\n",
    "    query_param_generator_agent = create_react_agent(llm, tools=[retriever_tool], prompt=prompt)\n",
    "    query_param_node = functools.partial(agent_node, agent=query_param_generator_agent, name=\"query_param_generator\")\n",
    "    return query_param_node\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Agent 3 ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# more advanced node\n",
    "\n",
    "from langchain.agents import initialize_agent, Tool, AgentExecutor\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.agents import AgentType, initialize_agent, AgentExecutor\n",
    "\n",
    "\n",
    "def advanced_multi_tool_agent_node(state: CustomerSupportState):\n",
    "    \"\"\"\n",
    "    An advanced agent that uses multiple tools to handle queries.\n",
    "    \"\"\"\n",
    "    tools = [retriever_tool]  # Add more tools as needed\n",
    "\n",
    "    # Define the agent's prompt\n",
    "    prompt_template = \"\"\"\n",
    "    You are an advanced agent with access to multiple tools. Your task is to resolve customer queries by:\n",
    "    1. Identifying the problem or request.\n",
    "    2. Using the tools provided to gather additional information if needed.\n",
    "    3. Synthesizing the information into a clear, concise response.\n",
    "\n",
    "    You can chain tools if required. If you are unsure, respond with 'I need more details.'\n",
    "\n",
    "    Query: {query}\n",
    "    \"\"\"\n",
    "    llm = ChatOpenAI(\n",
    "        api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "        model=\"deepseek-chat\",\n",
    "        base_url=\"https://api.deepseek.com\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "    \n",
    "    # Initialize the agent\n",
    "    advanced_agent = initialize_agent(\n",
    "        tools=tools,\n",
    "        llm=llm,\n",
    "        agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    "        agent_kwargs={\"prompt_template\": prompt_template},\n",
    "        verbose=True,\n",
    "    )\n",
    "\n",
    "    agent_executor = AgentExecutor(agent=advanced_agent, tools=[tool_1, tool_2], verbose=True, \n",
    "                               return_intermediate_steps=True, handle_parsing_errors=True)\n",
    "\n",
    "    # Execute the agent\n",
    "    query = state.query\n",
    "    try:\n",
    "        response = agent_executor.invoke({\"query\": query})\n",
    "        state.response = response\n",
    "    except Exception as e:\n",
    "        state.response = f\"Error: {str(e)}\"\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Custom Agent --------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain.agents import BaseAgent\n",
    "from typing import Optional\n",
    "\n",
    "class AdvancedCustomAgent(BaseAgent):\n",
    "    \"\"\"\n",
    "    Custom advanced agent with LLM, human-in-the-loop, and iterative reasoning.\n",
    "    \"\"\"\n",
    "    def __init__(self, llm, tools=None, max_iterations: int = 3):\n",
    "        self.llm = llm\n",
    "        self.tools = tools or []\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    async def run(self, query: str, human_review: bool = False, **kwargs) -> str:\n",
    "        \"\"\"\n",
    "        Executes the custom agent's workflow.\n",
    "        \n",
    "        Args:\n",
    "            query (str): User's query.\n",
    "            human_review (bool): If True, adds human-in-the-loop for review.\n",
    "        \n",
    "        Returns:\n",
    "            str: Final response.\n",
    "        \"\"\"\n",
    "        response = \"\"\n",
    "        iteration = 0\n",
    "\n",
    "        while iteration < self.max_iterations:\n",
    "            iteration += 1\n",
    "            print(f\"--- Iteration {iteration}/{self.max_iterations} ---\")\n",
    "\n",
    "            # Generate a response using LLM\n",
    "            prompt = f\"\"\"\n",
    "            You are an advanced customer support agent. Use the tools provided to solve the query. \n",
    "            Tools: {', '.join([tool.name for tool in self.tools]) if self.tools else 'None'}\n",
    "\n",
    "            Query: {query}\n",
    "\n",
    "            If you need clarification or further details, request them from the user.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                response = await self.llm.apredict(prompt)\n",
    "                print(f\"Generated Response: {response}\")\n",
    "\n",
    "                # Check if human review is required\n",
    "                if human_review:\n",
    "                    review = input(\"Do you approve this response? (yes/no): \")\n",
    "                    if review.lower() == \"yes\":\n",
    "                        break\n",
    "                    else:\n",
    "                        query = input(\"Provide additional details or corrections: \")\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            except Exception as e:\n",
    "                response = f\"Error: {str(e)}\"\n",
    "                break\n",
    "\n",
    "        return response\n",
    "\n",
    "\n",
    "# Define the custom agent node\n",
    "async def custom_agent_node(state: CustomerSupportState):\n",
    "    \"\"\"\n",
    "    Node with a custom advanced agent that uses LLM and human-in-the-loop.\n",
    "    \"\"\"\n",
    "    custom_agent = AdvancedCustomAgent(llm=llm, tools=[retriever_tool], max_iterations=3)\n",
    "    query = state.query\n",
    "\n",
    "    # Human-in-the-loop enabled for critical queries\n",
    "    response = await custom_agent.run(query, human_review=True)\n",
    "    state.response = response\n",
    "    return state\n",
    "\n",
    "\n",
    "#--------------------------------------------- Supervisor NODE with Router + Command ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "# Agent Supervisor Node\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "\n",
    "'''\n",
    "User\n",
    " ├── Supervisor\n",
    " │     ├── [direct] Agent 1\n",
    " │     ├── [conditional] Agent 2 (if condition A is met)\n",
    " │     └── [direct] Agent 3\n",
    " │           ├── [conditional] Sub-Agent 3.1 (if condition B is met)\n",
    " │           └── [direct] Sub-Agent 3.2\n",
    " │\n",
    " └── Feedback Loop (User <--> Supervisor)\n",
    "\n",
    "'''\n",
    "members = [\"researcher\", \"coder\"]\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        f\" following workers: {members}. Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    class Router(TypedDict):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options]\n",
    "\n",
    "    def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto)\n",
    "\n",
    "    return supervisor_node\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "\n",
    "\n",
    "#--------------------------------------------- More Advanced Supervisor NODE -------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph_supervisor import create_supervisor\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "math_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[tool_1, tool_2],\n",
    "    name=\"math_expert\",\n",
    "    prompt=\"You are a math expert. Always use one tool at a time.\"\n",
    ")\n",
    "\n",
    "research_agent = create_react_agent(\n",
    "    model=model,\n",
    "    tools=[tool_1, tool_2],\n",
    "    name=\"research_expert\",\n",
    "    prompt=\"You are a world class researcher with access to web search. Do not do any math.\"\n",
    ")\n",
    "\n",
    "# Create supervisor workflow\n",
    "workflow = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    output_mode = \"last_message\",    # what we pass back from agent to supervisor. we dont need to always state this\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ")\n",
    "\n",
    "# Compile and run\n",
    "app = workflow.compile()\n",
    "result = app.invoke({\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"what's the combined headcount of the FAANG companies in 2024?\"\n",
    "        }\n",
    "    ]\n",
    "})\n",
    "\n",
    "\n",
    "#--------------------------------------------- Supervisor managing other Supervisors------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "math_supervisor = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    output_mode = \"last_message\",    # what we pass back from agent to supervisor. we dont need to always state this\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ").compile(name=\"math_supervisor\")\n",
    "\n",
    "\n",
    "research_supervisor = create_supervisor(\n",
    "    [research_agent, math_agent],\n",
    "    model=model,\n",
    "    output_mode = \"last_message\",    # what we pass back from agent to supervisor. we dont need to always state this\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ").compile(name=\"research_supervisor\")\n",
    "\n",
    "workflow = create_supervisor(\n",
    "    [math_supervisor, research_supervisor],\n",
    "    supervisor_name = \"top_level_supervisor\",\n",
    "    model=model,\n",
    "    prompt=(\n",
    "        \"You are a team supervisor managing a research expert and a math expert. \"\n",
    "        \"For current events, use research_agent. \"\n",
    "        \"For math problems, use math_agent.\"\n",
    "    )\n",
    ").compile()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- NODE with Command (used with tool node) ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------------------\n",
    "llm = llm.bind_tools([retriever_tool, tool_1, tool_2])\n",
    "\n",
    "def call_model(state:OverallState) -> Command[Literal['tools', END]]:\n",
    "    message = state.messages[-1].content\n",
    "    response = llm.invoke(message)\n",
    "    if len(response.tool_calls) > 0:\n",
    "        next_node = \"tools\"\n",
    "    else:\n",
    "        next_node = END\n",
    "    return Command(goto=next_node, update={\"messages\": response})   # update helps to update the state with the response from the model. You dont need always need to do this\n",
    "\n",
    "workflow = StateGraph(OverallState)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "workflow.add_node(\"tools\", create_tool_node_with_fallback([retriever_tool, tool_1, tool_2]))\n",
    "workflow.add_edge(START, \"call_model\")\n",
    "workflow.add_edge(\"call_model\", \"tools\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "\n",
    "# If you are using subgraphs, you might want to navigate from a node a subgraph to a different subgraph \n",
    "# (i.e. a different node in the parent graph).\n",
    "def my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n",
    "    return Command(\n",
    "        update={\"foo\": \"bar\"},\n",
    "        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n",
    "        graph=Command.PARENT\n",
    "    )\n",
    "\n",
    "#--------------------------------------------- LangGraph Workflow ---------------------------------------------   \n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# Add nodes to the workflow\n",
    "workflow.add_node(\"Classify Query\", classify_query)\n",
    "workflow.add_node(\"End Conversation\", end_conversation)\n",
    "\n",
    "# Define edges between nodes\n",
    "workflow.add_edge(START, \"Classify Query\")\n",
    "workflow.add_edge(\"Classify Query\", \"Answer FAQ\")\n",
    "workflow.add_edge(\"Recommend Products\", \"End Conversation\")\n",
    "workflow.add_edge(\"End Conversation\", END)\n",
    "\n",
    "# Set entry and finish points\n",
    "workflow.set_entry_point(\"Classify Query\")  # Start the conversation. Use this only when START is not used.\n",
    "workflow.set_finish_point(\"End Conversation\")   # End the conversation. Use this only when END is not used.\n",
    "\n",
    "# Compile the workflow\n",
    "app = workflow.compile()\n",
    "\n",
    "# Test the workflow with a sample query\n",
    "initial_state = CustomerSupportState(query=\"which do you recommend between product A and product B?\")\n",
    "result = app.invoke(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Graph Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.errors import GraphRecursionError\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage  # Import AIMessage for assistant responses\n",
    "\n",
    "#----------------------------------------------------- Graoh Invoke-----------------------------------\n",
    "\n",
    "# **Input Collection**\n",
    "user_input = \"I want to travel from New York to Paris on 2023-12-15 and return on 2023-12-22. There are 2 adults and 1 child. My budget is $5000. I need 1 room. I prefer a hotel with free breakfast and a swimming pool. I also want to visit the museums and enjoy local cuisine, and go to the club at night. I might also want a massage.\"    \n",
    "\n",
    "# **Input State**\n",
    "input_state = {\"messages\": [HumanMessage(content=user_input)]}\n",
    "\n",
    "# **Graph Invocation**\n",
    "output = graph.invoke(input_state, {\"recursion_limit\": 300})\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------------- Graph Stream-----------------------------------\n",
    "# Stream the Graph\n",
    "# Define the input data\n",
    "input_data = {\n",
    "    \"messages\": [HumanMessage(content=\"I want to travel from New York to Paris on 2023-12-15 and return on 2023-12-22. There are 2 adults and 1 child. My budget is $5000. I need 1 room. I prefer a hotel with free breakfast and a swimming pool. I also want to visit the museums and enjoy local cuisine, and go to the club at night. I might also want a massage.\")]\n",
    "}\n",
    "\n",
    "# Define the configuration (e.g., recursion limit)\n",
    "config = {\"recursion_limit\": 10}\n",
    "\n",
    "# Stream the execution\n",
    "events = graph.stream(input_data, config)\n",
    "for event in events:\n",
    "    print(event)\n",
    "    \n",
    "#--------------------------------------------------------- Graph Stream with Memory-----------------------------------\n",
    "\n",
    "\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "try:\n",
    "    for event in graph.stream(input_state, thread_config, stream_mode = \"values\", ):\n",
    "        messages = event['messages'][-1]\n",
    "        # Filter and print only the AIMessage content\n",
    "        if isinstance(messages, AIMessage):\n",
    "            print(messages.content)\n",
    "\n",
    "except GraphRecursionError:\n",
    "    print(\"Recursion Error\")\n",
    "\n",
    "\n",
    "#--------------------------------------------------------- Graph Stream with pretty print-----------------------------------\n",
    "\n",
    "from langgraph.pregel.remote import RemoteGraph\n",
    "from langchain_core.messages import convert_to_messages\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "graph_name = \"task_maistro\" \n",
    "\n",
    "# Connect to the deployment\n",
    "remote_graph = RemoteGraph(graph_name, url=local_deployment_url)\n",
    "\n",
    "user_input = \"Hi I'm Lance. I live in San Francisco with my wife and have a 1 year old.\"\n",
    "config = {\"configurable\": {\"user_id\": \"Test-Deployment-User\"}}\n",
    "for chunk in remote_graph.stream({\"messages\": [HumanMessage(content=user_input)]}, stream_mode=\"values\", config=config):\n",
    "    convert_to_messages(chunk[\"messages\"])[-1].pretty_print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Tool Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------------------------------- Tool with Input Class and Tool Class ----------------------------------\n",
    "#--------------------------------------------------------------------------------------------------------------------------\n",
    "# Define the input class\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str\n",
    "    param2: int\n",
    "\n",
    "# Define the tool class\n",
    "class MyTool:\n",
    "    def __call__(self, input: MyToolInput) -> str:\n",
    "        # Tool logic here\n",
    "        return f\"Processed: {input.param1}, {input.param2}\"\n",
    "\n",
    "my_tool = Tool(\n",
    "    name=\"my_tool\",\n",
    "    func=MyTool(),\n",
    "    description=\"Tool description.\",\n",
    "    args_schema=MyToolInput\n",
    ")\n",
    "\n",
    "# Call the tool\n",
    "result = my_tool.func(MyToolInput(param1=\"value1\", param2=42))\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------------------------- Tool Using @tool Decorator ----------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "from langchain.tools import tool\n",
    "\n",
    "@tool\n",
    "def my_tool(param1: str, param2: int) -> str:\n",
    "    \"\"\"Tool description.\"\"\"\n",
    "    return f\"Processed: {param1}, {param2}\"\n",
    "\n",
    "# Call the tool\n",
    "result = my_tool({\"param1\": \"value1\", \"param2\": 42})\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#-------------------------------- Tool with Structured Inputs Using \"BaseTool\" ----------------------------------\n",
    "#----------------------------------------------------------------------------------------------------------------\n",
    "from langchain.tools import BaseTool\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class MyToolInput(BaseModel):\n",
    "    param1: str\n",
    "    param2: int\n",
    "\n",
    "class MyTool(BaseTool):\n",
    "    name: str = \"my_tool\"\n",
    "    description: str = \"Tool description.\"\n",
    "\n",
    "    def _run(self, param1: str, param2: int) -> str:\n",
    "        \"\"\"Tool logic.\"\"\"\n",
    "        return f\"Processed: {param1}, {param2}\"\n",
    "\n",
    "# Create an instance of the tool\n",
    "my_tool = MyTool()\n",
    "\n",
    "# Call the tool\n",
    "result = my_tool.run({\"param1\": \"value1\", \"param2\": 42})\n",
    "print(result)\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------------  Tool call with Agent ----------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "def my_tool_func(param1: str, param2: int) -> str:\n",
    "    \"\"\"Tool logic.\"\"\"\n",
    "    return f\"Processed: {param1}, {param2}\"\n",
    "\n",
    "# Create the tool\n",
    "my_tool = Tool(\n",
    "    name=\"my_tool\",\n",
    "    func=my_tool_func,\n",
    "    description=\"Tool description.\"\n",
    ")\n",
    "\n",
    "# Add the tool to an agent\n",
    "tools = [my_tool]\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\")   # you can also use react agent here or any agent\n",
    "\n",
    "# Call the tool via the agent\n",
    "result = agent.invoke(\"Call my_tool with param1='value1' and param2=42\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Langchain Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "#--------------------------------------------- Style 1 ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "# Add a node for a model to generate a query based on the question and schema\n",
    "query_gen_system = \"\"\"You are a SQL expert with a strong attention to detail.\n",
    "\n",
    "Given an input question, output a syntactically correct SQLite query to run, then look at the results of the query and return the answer.\n",
    "DO NOT call any tool besides SubmitFinalAnswer to submit the final answer.\n",
    "\"\"\"\n",
    "\n",
    "query_gen_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", query_gen_system),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#--------------------------------------------- Style 2 ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful customer support assistant for Solar Panels Belgium.\n",
    "            You should get the following information from them:\n",
    "            - monthly electricity cost\n",
    "            If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "            After you are able to discern all the information, call the relevant tool.\n",
    "            ''',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "#--------------------------------------------- Style 3 ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "template = ''' \n",
    "You are a travel suggestion agent. Answer the user's questions based on their travel preferences. \n",
    "If you need to find information about a specific destination, use the search_tool. Understand that the information was retrieved from the web,\n",
    "interpret it, and generate a response accordingly.\n",
    "\n",
    "Answer the following questions as best as you can. You have access to the following tools:\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "\"Question\": the input question you must answer\n",
    "\"Thought\": your reasoning about what to do next\n",
    "\"Action\": the action you should take, one of [{tool_names}] (if no action is needed, write \"None\")\n",
    "\"Action Input\": the input to the action (if no action is needed, write \"None\")\n",
    "\"Observation\": the result of the action (if no action is needed, write \"None\")\n",
    "\"Thought\": your reasoning after observing the action\n",
    "\"Final Answer\": the final answer to the original input question\n",
    "\n",
    "Ensure every Thought is followed by an Action, Action Input, and Observation. If no tool is needed, explicitly write \"None\" for Action, Action Input, and Observation.\n",
    "\n",
    "Begin!\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\n",
    "'''\n",
    "\n",
    "prompt = PromptTemplate.from_template(template)\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=search_agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=False,\n",
    "    handle_parsing_errors=True,\n",
    ")\n",
    "\n",
    "response = agent_executor.invoke({\n",
    "    \"input\": latest_query,\n",
    "    \"agent_scratchpad\": \"\"  # Initialize with an empty scratchpad\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Display or Visualize LangGraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%%{init: {'flowchart': {'curve': 'linear'}}}%%\n",
      "graph TD;\n",
      "\t__start__([<p>__start__</p>]):::first\n",
      "\tNode_A(Node A)\n",
      "\tNode_B(Node B)\n",
      "\t__end__([<p>__end__</p>]):::last\n",
      "\tNode_A --> Node_B;\n",
      "\tNode_B --> __end__;\n",
      "\t__start__ --> Node_A;\n",
      "\tclassDef default fill:#f2f0ff,line-height:1.2\n",
      "\tclassDef first fill-opacity:0\n",
      "\tclassDef last fill:#bfb6fc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------------\n",
    "print(app.get_graph().draw_mermaid())       # Converting a Graph to a Mermaid Diagram\n",
    "\n",
    "\n",
    "#-------------------------Using Mermaid.Ink--------------------------------\n",
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Using Mermaid + Pyppeteer--------------------------------\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()  # Required for Jupyter Notebook to run async functions\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        app.get_graph().draw_mermaid_png(\n",
    "            curve_style=CurveStyle.LINEAR,\n",
    "            node_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"),\n",
    "            wrap_label_n_words=9,\n",
    "            output_file_path=None,\n",
    "            draw_method=MermaidDrawMethod.PYPPETEER,\n",
    "            background_color=\"white\",\n",
    "            padding=10,\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Using Graphviz--------------------------------\n",
    "%pip install pygraphviz\n",
    "\n",
    "display(Image(app.get_graph().draw_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the graph\n",
    "agent = workflow.compile()\n",
    "\n",
    "\n",
    "# requirements.txt\n",
    "langgraph==0.1.0\n",
    "langchain_core==0.1.0\n",
    "\n",
    "\n",
    "# Langgraph.json\n",
    "{\n",
    "  \"name\": \"todo_agent\",\n",
    "  \"description\": \"A simple ToDo list agent\",\n",
    "  \"graphs\": {\n",
    "    \"todo_agent\": {\n",
    "      \"entrypoint\": \"agent\",\n",
    "      \"file\": \"agent.py\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "\n",
    "# Dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY . .\n",
    "\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "CMD [\"langgraph\", \"serve\", \"--host\", \"0.0.0.0\", \"--port\", \"8123\"]\n",
    "\n",
    "\n",
    "\n",
    "# or\n",
    "# Use docker-compose.yml to create containers for Redis, PostgreSQL, and the LangGraph API.\n",
    "$ cd module-6/deployment\n",
    "$ docker compose up\n",
    "\n",
    "# Building Docker Image\n",
    "$ langgraph build -t todo_agent\n",
    "\n",
    "# Run the Docker Image\n",
    "docker run -p 8123:8123 todo_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------------------------Deployment Setup--------------------------------\n",
    "# Use docker-compose.yml to create containers for Redis, PostgreSQL, and the LangGraph API.\n",
    "$ cd module-6/deployment\n",
    "$ docker compose up\n",
    "\n",
    "# Building Docker Image\n",
    "$ langgraph build -t todo_agent\n",
    "\n",
    "# Run the Docker Image\n",
    "docker run -p 8123:8123 todo_agent\n",
    "\n",
    "#-------------------------Assistants-------------------------------------\n",
    "#---------------------\n",
    "# Creating Assistants (Connect to the Deployment)\n",
    "#---------------------\n",
    "from langgraph_sdk import get_client\n",
    "client = get_client(url=\"http://localhost:8123\")\n",
    "\n",
    "# Create a personal assistant\n",
    "personal_assistant = await client.assistants.create(\n",
    "    \"task_maistro\",\n",
    "    config={\"configurable\": {\"todo_category\": \"personal\"}}\n",
    ")\n",
    "\n",
    "#---------------------\n",
    "# Updating Assistants\n",
    "#---------------------\n",
    "personal_assistant = await client.assistants.update(\n",
    "    personal_assistant[\"assistant_id\"],\n",
    "    config={\"configurable\": {\"todo_category\": \"personal\", \"user_id\": \"lance\"}}\n",
    ")\n",
    "\n",
    "#---------------------\n",
    "# Searching and Deleting Assistants\n",
    "#---------------------\n",
    "assistants = await client.assistants.search()\n",
    "for assistant in assistants:\n",
    "    print(assistant['assistant_id'], assistant['config'])\n",
    "    \n",
    "\n",
    "await client.assistants.delete(\"assistant_id\")  # Delete an assistant\n",
    "\n",
    "\n",
    "#-------------------------Threads and Runs--------------------------------\n",
    "\n",
    "#---------------------\n",
    "# Creating Threads\n",
    "#---------------------\n",
    "thread = await client.threads.create()\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Running a Graph\n",
    "#---------------------\n",
    "run = await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"Add a ToDo\")]},\n",
    "    config={\"configurable\": {\"user_id\": \"Test\"}}\n",
    ")\n",
    "\n",
    "#---------------------\n",
    "# Streaming Runs\n",
    "#---------------------\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"What ToDo should I focus on?\")]},\n",
    "    stream_mode=\"messages-tuple\"\n",
    "):\n",
    "    if chunk.event == \"messages\":\n",
    "        print(chunk.data)\n",
    "\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Background Runs\n",
    "#---------------------\n",
    "run = await client.runs.create(thread[\"thread_id\"], \"task_maistro\", input={\"messages\": [...]})\n",
    "print(await client.runs.get(thread[\"thread_id\"], run[\"run_id\"]))\n",
    "\n",
    "\n",
    "#-------------------------Double Texting Strategies--------------------------------\n",
    "\n",
    "#---------------------\n",
    "# Reject\n",
    "#---------------------\n",
    "await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"New ToDo\")]},\n",
    "    multitask_strategy=\"reject\" # Reject the current task if another task is already in progress ()\n",
    ")\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Enqueue\n",
    "#---------------------\n",
    "await client.runs.create(\n",
    "    thread[\"thread_id\"],\n",
    "    \"task_maistro\",\n",
    "    input={\"messages\": [HumanMessage(content=\"New ToDo\")]},\n",
    "    multitask_strategy=\"enqueue\"    # Enqueue new runs (or Interrupt, Rollback etc.)\n",
    ")\n",
    "\n",
    "\n",
    "#-------------------------Human-in-the-Loop--------------------------------\n",
    "\n",
    "#---------------------\n",
    "# Forking Threads\n",
    "#---------------------\n",
    "copied_thread = await client.threads.copy(thread[\"thread_id\"])\n",
    "\n",
    "#---------------------\n",
    "# Editing State\n",
    "#---------------------\n",
    "forked_input = {\"messages\": HumanMessage(content=\"Updated ToDo\", id=message_id)}\n",
    "await client.threads.update_state(\n",
    "    thread[\"thread_id\"],\n",
    "    forked_input,\n",
    "    checkpoint_id=checkpoint_id\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Studio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create the langgraph.json file (see example)\n",
    "{\n",
    "    \"dockerfile_lines\": [], \n",
    "    \"graphs\": {\n",
    "        \"chat\": \"./src/react_agent/graph.py:graph\",\n",
    "        \"researcher\": \"./src/react_agent/graph.py:researcher\",\n",
    "        \"agent\": \"./src/react_agent/graph.py:agent\",\n",
    "    },\n",
    "    \"env\": [\n",
    "        \"OPENAI_API_KEY\",\n",
    "        \"WEAVIATE_API_KEY\",\n",
    "        \"WEAVIATE_URL\",\n",
    "        \"ANTHROPIC_API_KEY\",\n",
    "        \"ELASTIC_API_KEY\"\n",
    "    ],\n",
    "    \"python_version\": \"3.11\",\n",
    "    \"dependencies\": [\n",
    "        \".\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Step 2: Run the langgraph-cli command\n",
    "!pip install \"langgraph-cli[inmem]==0.1.55\" # Install the langgraph-cli package\n",
    "\n",
    "# Step 3: Move to the directory containing the langgraph.json file\n",
    "\n",
    "# Step 4: Install the dependencies\n",
    "    # If you are using requirements.txt:\n",
    "    python -m pip install -r requirements.txt\n",
    "\n",
    "    If you are using pyproject.toml or setuptools:\n",
    "    # python -m pip install -e .\n",
    "\n",
    "# Step 5: Run the LangGraph server\n",
    "langgraph dev # start a local development server\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Conditional Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------Basic Conditional Edge-------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from typing import TypedDict\n",
    "\n",
    "# Define the state structure\n",
    "class State(TypedDict):\n",
    "    value: int\n",
    "    query: str\n",
    "    response: str\n",
    "    \n",
    "# Define the conditional function\n",
    "def conditional_edge(state: State) -> str:\n",
    "    if state[\"value\"] > 10:\n",
    "        return \"node_b\"\n",
    "    else:\n",
    "        return \"__end__\"\n",
    "\n",
    "# Define the graph\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"node_a\", lambda state: {\"value\": state[\"value\"] + 1})\n",
    "builder.add_node(\"node_b\", lambda state: {\"value\": state[\"value\"] - 1})\n",
    "builder.add_edge(START, \"node_a\")\n",
    "builder.add_conditional_edges(\"node_a\", conditional_edge)\n",
    "builder.add_edge(\"node_b\", \"node_a\")\n",
    "graph = builder.compile()\n",
    "\n",
    "# Test the graph\n",
    "initial_state = {\"value\": 5}\n",
    "result = graph.invoke(initial_state)\n",
    "\n",
    "\n",
    "#------------------------------------Router with Multiple Conditions-------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "members = [\"researcher\", \"coder\"]\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        f\" following workers: {members}. Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    class Router(TypedDict):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options]\n",
    "\n",
    "    def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto)\n",
    "\n",
    "    return supervisor_node\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_node(\"search\", search_node)\n",
    "builder.add_node(\"web_scraper\", web_scraper_node)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "graph = builder.compile()\n",
    "\n",
    "\n",
    "#------------------------------------Using Tool Conditions-------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "builder = StateGraph(State)\n",
    "\n",
    "\n",
    "# Define nodes: these do the work\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", ToolNode([retriever_tool]))\n",
    "# Define edges: these determine how the control flow moves\n",
    "builder.add_edge(START, \"assistant\")\n",
    "builder.add_conditional_edges(\n",
    "    \"assistant\",\n",
    "    tools_condition,\n",
    ")\n",
    "builder.add_edge(\"tools\", \"assistant\")\n",
    "\n",
    "# The checkpointer lets the graph persist its state\n",
    "# this is a complete memory for the entire graph.\n",
    "memory = MemorySaver()\n",
    "part_1_graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------Using Tool conditions from Scratch-------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "def tools_condition(\n",
    "    state: Union[list[AnyMessage], dict[str, Any], BaseModel],\n",
    "    messages_key: str = \"messages\",\n",
    ") -> Literal[\"tools\", \"__end__\"]:\n",
    "    \"\"\"Use in the conditional_edge to route to the ToolNode if the last message\n",
    "\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "\n",
    "    Args:\n",
    "        state (Union[list[AnyMessage], dict[str, Any], BaseModel]): The state to check for\n",
    "            tool calls. Must have a list of messages (MessageGraph) or have the\n",
    "            \"messages\" key (StateGraph).\n",
    "\n",
    "    Returns:\n",
    "        The next node to route to.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):\n",
    "        ai_message = messages[-1]\n",
    "    elif messages := getattr(state, messages_key, []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return \"__end__\"    # you can change this to any other node name instead of \"__end__\"\n",
    "\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    tools_condition,  # decides if the agent is calling a tool or finishing\n",
    "    {\n",
    "        \"tools\": \"retrieve\",        # the dictionary is helpful if we named the nodes differently from the default tool condition function\n",
    "        END: END,  # if the agent does not call any tool, we end the graph\n",
    "    },\n",
    ")\n",
    "\n",
    "#------------------------------------Custom Condition functions-------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------\n",
    "\n",
    "from typing import Literal, Union, List, Dict, Any\n",
    "from langchain_core.messages import AnyMessage, HumanMessage\n",
    "\n",
    "def data_api_condition(\n",
    "    state: Union[List[AnyMessage], Dict[str, Any]],\n",
    "    messages_key: str = \"messages\",\n",
    ") -> Literal[\"data_api_node\", \"assistant_node\"]:\n",
    "    \"\"\"\n",
    "    Route to the Data API Node if the query involves fetching information.\n",
    "    Otherwise, route to the Assistant Node.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        user_message = state[-1]\n",
    "    elif isinstance(state, dict) and (messages := state.get(messages_key, [])):\n",
    "        user_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state: {state}\")\n",
    "    \n",
    "    # Check if the query involves fetching information\n",
    "    if isinstance(user_message, HumanMessage) and any(keyword in user_message.content.lower() for keyword in [\"weather\", \"stock\", \"price\"]):\n",
    "        return \"data_api_node\"\n",
    "    return \"assistant_node\"\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------Custom Conditional Edges 2---------------------------------\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "\n",
    "from langgraph.graph import END\n",
    "\n",
    "def should_continue(state: AgentState) -> Literal[\"tools\", \"agent\", END]:\n",
    "    messages = state[\"messages\"]\n",
    "    if not messages:\n",
    "        return \"agent\"  # Start the conversation\n",
    "    \n",
    "    last_message = messages[-1]\n",
    "    \n",
    "    # If the last message is from a tool\n",
    "    if isinstance(last_message, ToolMessage):\n",
    "        if last_message.content == \"File added successfully\":\n",
    "            state[\"file_added\"] = True\n",
    "            print(\"📌 File addition confirmed\")\n",
    "            return \"agent\"\n",
    "        print(\"🏁 Search complete, ending workflow\")\n",
    "        return END\n",
    "    \n",
    "    # If the last message is from the AI\n",
    "    if isinstance(last_message, AIMessage):\n",
    "        # If the file is added but not indexed, wait\n",
    "        if state.get(\"file_added\") and not state.get(\"indexed\"):\n",
    "            print(\"⏳ Waiting for indexing to complete...\")\n",
    "            return \"agent\"\n",
    "        \n",
    "        # If the AI asks to call a tool\n",
    "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "            print(\"🛠️ Executing tool calls...\")\n",
    "            return \"tools\"\n",
    "    \n",
    "    return \"agent\"\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"tools\", ToolNode([tool_1, tool_2]))\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"agent\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"agent\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"tools\": \"tools\",  # Route to tools if tool calls are detected\n",
    "        \"agent\": \"agent\",  # Continue with the agent if no tool calls\n",
    "        END: END,          # End the workflow if conditions are met\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangChain Messages (HumanMessage, AIMessage, SystemMessage, BaseMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, BaseMessage, ChatMessage, ToolMessage, RemoveMessage\n",
    "\n",
    "# BaseMessage\n",
    "    # The base class for all message types. Inherited by HumanMessage, AIMessage, and SystemMessage\n",
    "    class CustomMessage(BaseMessage):\n",
    "        content: str\n",
    "        role: str  # e.g., \"user\", \"assistant\", \"system\"\n",
    "\n",
    "    custom_msg = CustomMessage(content=\"Hello, world!\", role=\"user\")\n",
    "\n",
    "#------------------ Message Types -------------------------------------------------------------\n",
    "#----------------------------------------------------------------------------------------------\n",
    "# HumanMessage\n",
    "    # Represents a message from a human user.\n",
    "    human_msg = HumanMessage(content=\"My name is John Doe.\", name = \"Paul Okafor\")  # you can use the name of the node or agent\n",
    "    \n",
    "    # or\n",
    "    # Represents a message from a human user.\n",
    "    class HumanMessage(BaseMessage):\n",
    "        content: str\n",
    "\n",
    "    human_msg = HumanMessage(content=\"My name is John Doe.\")\n",
    "\n",
    "# AIMessage\n",
    "    # Represents a message generated by an AI agent.\n",
    "    ai_msg = AIMessage(content=\"I am a helpful assistant.\")\n",
    "\n",
    "# SystemMessage\n",
    "    # Represents a system message or prompt.\n",
    "    system_msg = SystemMessage(content=\"You are a helpful assistant.\")\n",
    "\n",
    "# ChatMessage\n",
    "    # Represents a message in a chat conversation.\n",
    "    chat_msg = ChatMessage(role=\"custom_role\", content=\"This is a custom message.\")\n",
    "\n",
    "# ToolMessage\n",
    "    # Represents a message generated by a tool.\n",
    "    tool_msg = ToolMessage(content=\"This is a tool message.\", tool_call_id=\"123\", tool_name=\"GradeMaster\", id=\"123\")\n",
    "\n",
    "# RemoveMessage\n",
    "    # Represents a message to remove a message from the conversation.\n",
    "    remove_msg = [RemoveMessage(id=m.id) for m in state['messages'][:-2]]\n",
    "    \n",
    "#---------------------------------- When to use it----------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Example 1:\n",
    "messages = [SystemMessage(content=\"Welcome! Please provide your name.\")]\n",
    "user_input = \"My name is John Doe.\"\n",
    "messages.append(HumanMessage(content=user_input))\n",
    "messages.append(AIMessage(content=responses))\n",
    "\n",
    "# Example 2:\n",
    "messages = [\n",
    "    HumanMessage(content=\"My name is John Doe.\"),\n",
    "    AIMessage(content=\"Hello, John Doe! How can I assist you today?\"),\n",
    "]\n",
    "\n",
    "# Example 3: Prompt Template\n",
    "chat_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        SystemMessage(content=\"Welcome! Please provide your name.\"),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "chat_template.invoke({\"messages\": [HumanMessage(content=\"John Doe\")]})\n",
    "chat_template.messages\n",
    "\n",
    "# Example 4: Agent Invocation\n",
    "agent = create_react_agent(tools=tools, llm=llm)\n",
    "messages = [    # Simulate a conversation\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Search for LangChain documentation.\"),\n",
    "]\n",
    "\n",
    "# Invoke the agent\n",
    "response = agent.invoke({\"messages\": messages})\n",
    "\n",
    "\n",
    "# Example 5: Node Example\n",
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.graph import StateGraph\n",
    "\n",
    "class OverallState(TypedDict):\n",
    "    messages: Sequence[BaseMessage]\n",
    "\n",
    "def user_node(state: OverallState) -> OverallState:\n",
    "    try:\n",
    "        # Initialize the conversation if no messages exist\n",
    "        if not state.messages:\n",
    "            state.messages = [SystemMessage(content=\"Welcome! Please provide your name.\")]\n",
    "        \n",
    "        # Check if the last message is from the user (HumanMessage)\n",
    "        if state.messages and isinstance(state.messages[-1], HumanMessage):\n",
    "            # Invoke the LLM with the current state\n",
    "            response = llm.invoke(state.messages)\n",
    "            \n",
    "            # Append the LLM's response as an Assistant Message (AIMessage)\n",
    "            state.messages.append(AIMessage(content=response.content))\n",
    "        \n",
    "        return state\n",
    "    except Exception as e:\n",
    "        # Handle errors gracefully\n",
    "        state.messages = state.messages + [SystemMessage(content=f\"An error occurred: {str(e)}\")]\n",
    "        return state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BaseAgent : The base class for all agent nodes. Provides a common interface for all agents..\n",
    "# LandGraphNode : A generic template for any node in the LandGraph.\n",
    "# Workflow : Compiles and executes the workflow by connecting all nodes.\n",
    "\n",
    "\n",
    "#--------------------------------------------- BaseAgent ---------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------\n",
    "\n",
    "from typing import Dict, Any, List, Optional\n",
    "from .utils.views import print_agent_output\n",
    "\n",
    "class BaseAgent:\n",
    "    \"\"\"\n",
    "    Base class for all agents. Provides common functionality and a standardized interface.\n",
    "    \"\"\"\n",
    "    def __init__(self, websocket=None, stream_output=None, headers=None, tools: Optional[List[Any]] = None, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the agent.\n",
    "        Args:\n",
    "            websocket: WebSocket connection for real-time communication.\n",
    "            stream_output: Function to stream output to the client.\n",
    "            headers: Additional headers or metadata for the agent.\n",
    "            tools: A list of tools (functions or objects) that the agent can use.\n",
    "            **kwargs: Additional configuration for the agent.\n",
    "        \"\"\"\n",
    "        self.websocket = websocket\n",
    "        self.stream_output = stream_output\n",
    "        self.headers = headers or {}\n",
    "        self.tools = tools or []\n",
    "        self.config = kwargs\n",
    "\n",
    "    async def log(self, message: str, agent_name: str = \"AGENT\"):\n",
    "        \"\"\"\n",
    "        Log messages to the console or stream them via WebSocket.\n",
    "        Args:\n",
    "            message: The message to log.\n",
    "            agent_name: The name of the agent (for logging purposes).\n",
    "        \"\"\"\n",
    "        if self.websocket and self.stream_output:\n",
    "            await self.stream_output(\"logs\", agent_name.lower(), message, self.websocket)\n",
    "        else:\n",
    "            print_agent_output(message, agent_name)\n",
    "\n",
    "    async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the agent's task. Subclasses must implement this method.\n",
    "        Args:\n",
    "            state: The current state of the workflow.\n",
    "        Returns:\n",
    "            Updated state after the agent's execution.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclasses must implement the `run` method.\")\n",
    "\n",
    "    def add_tool(self, tool: Any):\n",
    "        \"\"\"\n",
    "        Add a tool to the agent's toolkit.\n",
    "        Args:\n",
    "            tool: A function or object that the agent can use.\n",
    "        \"\"\"\n",
    "        self.tools.append(tool)\n",
    "\n",
    "    def get_tool(self, tool_name: str) -> Optional[Any]:\n",
    "        \"\"\"\n",
    "        Retrieve a tool by name or identifier.\n",
    "        Args:\n",
    "            tool_name: The name or identifier of the tool.\n",
    "        Returns:\n",
    "            The tool if found, otherwise None.\n",
    "        \"\"\"\n",
    "        for tool in self.tools:\n",
    "            if hasattr(tool, \"__name__\") and tool.__name__ == tool_name:\n",
    "                return tool\n",
    "            if hasattr(tool, \"name\") and tool.name == tool_name:\n",
    "                return tool\n",
    "        return None\n",
    "    \n",
    "\n",
    "#--------------------------------------------- LandGraphNode ---------------------------------------------\n",
    "#---------------------------------------------------------------------------------------------------------\n",
    "\n",
    "class LandGraphNode(BaseAgent):\n",
    "    \"\"\"\n",
    "    A generic node template for the LandGraph. Can be customized for any task.\n",
    "    \"\"\"\n",
    "    def __init__(self, node_name: str, **kwargs):\n",
    "        \"\"\"\n",
    "        Initialize the node.\n",
    "        Args:\n",
    "            node_name: The name of the node (for identification and logging).\n",
    "            **kwargs: Additional configuration for the node.\n",
    "        \"\"\"\n",
    "        super().__init__(node_name=node_name,**kwargs)\n",
    "        self.add_tool([tool_1, tool_2])\n",
    "\n",
    "    async def process(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Process the input state and return the updated state.\n",
    "        Args:\n",
    "            state: The current state of the workflow.\n",
    "        Returns:\n",
    "            Updated state after processing.\n",
    "        \"\"\"\n",
    "        query = state.get(\"task\", {}).get(\"query\", \"\")\n",
    "        await self.log(f\"Processing task in node: {query}\", self.node_name.upper())\n",
    "\n",
    "        # Use the tabular search tool\n",
    "        tool = self.get_tool(\"tabular_search_tool\")\n",
    "        if tool:\n",
    "            results = await tool(query, self.table)\n",
    "            return {\"node_name\": self.node_name, \"results\": results}\n",
    "        else:\n",
    "            return {\"node_name\": self.node_name, \"error\": \"Tool not found\"}\n",
    "\n",
    "    async def run(self, state: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Run the node's task.\n",
    "        Args:\n",
    "            state: The current state of the workflow.\n",
    "        Returns:\n",
    "            Updated state after the node's execution.\n",
    "        \"\"\"\n",
    "        return await self.process(state)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#--------------------------------------------- Workflow ----------------------------------------------------\n",
    "#-----------------------------------------------------------------------------------------------------------\n",
    "# This class is responsible for:\n",
    "    # Initializing all nodes.\n",
    "    # Defining the workflow graph.\n",
    "    # Compiling and executing the workflow.\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "import time\n",
    "\n",
    "class Workflow:\n",
    "    \"\"\"\n",
    "    The Workflow class compiles all nodes into a LandGraph and executes the workflow.\n",
    "    \"\"\"\n",
    "    def __init__(self, task: Dict[str, Any], websocket=None, stream_output=None, headers=None):\n",
    "        \"\"\"\n",
    "        Initialize the workflow.\n",
    "        Args:\n",
    "            task: The task to execute. Must include a \"query\" and can include additional metadata.\n",
    "            websocket: WebSocket connection for real-time communication.\n",
    "            stream_output: Function to stream output to the client.\n",
    "            headers: Additional headers or metadata.\n",
    "        \"\"\"\n",
    "        self.task = task\n",
    "        self.websocket = websocket\n",
    "        self.stream_output = stream_output\n",
    "        self.headers = headers or {}\n",
    "        self.task_id = self._generate_task_id()\n",
    "        self.nodes = self._initialize_nodes()\n",
    "\n",
    "    def _generate_task_id(self) -> int:\n",
    "        \"\"\"Generate a unique task ID.\"\"\"\n",
    "        return int(time.time())\n",
    "\n",
    "    def _initialize_nodes(self) -> Dict[str, LandGraphNode]:\n",
    "        \"\"\"\n",
    "        Initialize all nodes for the workflow.\n",
    "        Returns:\n",
    "            A dictionary of nodes, keyed by their names.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"node_1\": LandGraphNode(node_name=\"node_1\", websocket=self.websocket, stream_output=self.stream_output, headers=self.headers),\n",
    "            \"node_2\": LandGraphNode(node_name=\"node_2\", websocket=self.websocket, stream_output=self.stream_output, headers=self.headers),\n",
    "            \"node_3\": LandGraphNode(node_name=\"node_3\", websocket=self.websocket, stream_output=self.stream_output, headers=self.headers),\n",
    "        }\n",
    "\n",
    "    def _create_workflow_graph(self) -> StateGraph:\n",
    "        \"\"\"\n",
    "        Create the workflow graph using the initialized nodes.\n",
    "        Returns:\n",
    "            The compiled workflow graph.\n",
    "        \"\"\"\n",
    "        workflow = StateGraph(ResearchState)\n",
    "\n",
    "        # Add nodes to the graph\n",
    "        for node_name, node in self.nodes.items():\n",
    "            workflow.add_node(node_name, node.run)\n",
    "\n",
    "        # Define edges between nodes\n",
    "        workflow.add_edge(\"node_1\", \"node_2\")\n",
    "        workflow.add_edge(\"node_2\", \"node_3\")\n",
    "        workflow.set_entry_point(\"node_1\")\n",
    "        workflow.add_edge(\"node_3\", END)\n",
    "\n",
    "        return workflow\n",
    "\n",
    "    async def execute(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Execute the workflow.\n",
    "        Returns:\n",
    "            The final result of the workflow.\n",
    "        \"\"\"\n",
    "        workflow_graph = self._create_workflow_graph()\n",
    "        compiled_workflow = workflow_graph.compile()\n",
    "\n",
    "        await self.log(f\"Starting workflow for task: {self.task.get('query')}\", \"WORKFLOW\")\n",
    "        result = await compiled_workflow.ainvoke({\"task\": self.task})\n",
    "        return result\n",
    "\n",
    "\n",
    "#----------------------------------------------\n",
    "# call the workflow\n",
    "#----------------------------------------------\n",
    "task = {\n",
    "    \"query\": \"Process some data\",  # The main query or input\n",
    "    \"verbose\": True,               # Optional: Whether to log detailed output\n",
    "}\n",
    "\n",
    "# Initialize the workflow\n",
    "workflow = Workflow(task=task)\n",
    "\n",
    "# Execute the workflow\n",
    "result = await workflow.execute()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangSmith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# LangSmith is a platform by LangChain that helps developers trace, debug, and evaluate LLM (Large Language Model) applications. \n",
    "# It provides tools for observability (seeing how your app works), testing (ensuring your app behaves as expected), and feedback collection \n",
    "# (improving your app based on user input).\n",
    "\n",
    "# Why is LangSmith Useful?\n",
    "    # Tracing : See how your app processes inputs and generates outputs (Logs every step of your app's execution).\n",
    "    # Testing : Evaluate your app's performance with datasets (Runs your app on datasets to measure performance).\n",
    "    # Feedback : Collect user feedback to improve your app.\n",
    "    # Observability : Monitor your app in production/real-time to catch issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------- Setting Up LangSmith ---------------------------------\n",
    "import os\n",
    "from langsmith import Client, traceable, wrappers\n",
    "from langchain.smith import RunEvalConfig, run_on_dataset\n",
    "\n",
    "\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = str(os.getenv(\"LANGCHAIN_API_KEY\"))\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"my-project\" \n",
    "# Load environment variables from a .env file (optional)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\"../../.env\", override=True)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Tracing with LangSmith ---------------------------------\n",
    "# @traceable Decorator : Automatically logs function calls.\n",
    "# Use trace context manager for specific blocks of code.\n",
    "\n",
    "from langsmith import traceable\n",
    "\n",
    "# Use @traceable to log function calls\n",
    "@traceable(run_type=\"chain\")\n",
    "def retrieve_documents(question: str):\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "# Use context manager for fine-grained tracing\n",
    "from langsmith import trace\n",
    "with trace(name=\"Generate Response\", run_type=\"chain\") as ls_trace:\n",
    "    response = call_openai(messages)\n",
    "    ls_trace.end(outputs={\"output\": response})\n",
    "\n",
    "# Wrap OpenAI client for automatic tracing\n",
    "from langsmith.wrappers import wrap_openai\n",
    "openai_client = wrap_openai(openai.Client())\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Testing and Evaluation with LangSmith ---------------------------------\n",
    "# Use create_dataset to create and manage datasets.\n",
    "# Define custom evaluators to score your app's outputs.\n",
    "# Use evaluate to run experiments and measure performance.\n",
    "\n",
    "from langsmith import Client\n",
    "\n",
    "# Create a dataset\n",
    "client = Client()\n",
    "\n",
    "#---------------------------------\n",
    "# Create a dataset\n",
    "#---------------------------------\n",
    "\n",
    "# Create dataset for testing our AI agents\n",
    "dataset_input = [\n",
    "    {\"input\": \"What is the capital of France?\", \"output\": \"Paris\"},\n",
    "    {\"input\": \"Who wrote the book '1984'?\",  \"output\": \"George Orwell\"},\n",
    "    {\"input\": \"What is the square root of 16?\",  \"output\": \"4\"},\n",
    "]\n",
    "\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name = \"my-dataset\", \n",
    "    description=\"A dataset for testing AI agents.\")\n",
    "\n",
    "for data in dataset_input:\n",
    "    try:\n",
    "        client.create_example(\n",
    "            inputs={\"question\": data['input']},  # Wrapping the input into a dictionary\n",
    "            outputs={\"answer\": data['output']},  # Wrapping the output into a dictionary\n",
    "            dataset_id=dataset.id  # Assuming dataset.id is already created\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to create example for input: {data['input']}, Error: {e}\")\n",
    "    \n",
    "# or use create_examples to add multiple examples at once\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": data['input']}],\n",
    "    outputs=[{\"output\": data['output']}],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "\n",
    "#---------------------------------\n",
    "# Create a Target or label\n",
    "#---------------------------------\n",
    "# Define the application logic you want to evaluate inside a target function\n",
    "# The SDK will automatically send the inputs from the dataset to your target function\n",
    "def target(inputs: dict) -> dict:\n",
    "  response = openai_client.chat.completions.create(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "          { \"role\": \"system\", \"content\": \"Answer the following question accurately\" },\n",
    "          { \"role\": \"user\", \"content\": inputs[\"question\"] },\n",
    "      ],\n",
    "  )\n",
    "  return { \"response\": response.choices[0].message.content.strip() }\n",
    "\n",
    "\n",
    "#---------------------------------\n",
    "# Define an Evaluator\n",
    "#---------------------------------\n",
    "\n",
    "# Define instructions for the LLM judge evaluator\n",
    "instructions = \"\"\"Evaluate Student Answer against Ground Truth for conceptual similarity and classify true or false: \n",
    "- False: No conceptual match and similarity\n",
    "- True: Most or full conceptual match and similarity\n",
    "- Key criteria: Concept should match, not exact wording.\n",
    "\"\"\"\n",
    "\n",
    "# Define output schema for the LLM judge\n",
    "class Grade(BaseModel):\n",
    "  score: bool = Field(\n",
    "      description=\"Boolean that indicates whether the response is accurate relative to the reference answer\"\n",
    "  )\n",
    "\n",
    "# Define LLM judge that grades the accuracy of the response relative to reference output\n",
    "def accuracy(outputs: dict, reference_outputs: dict) -> bool:\n",
    "  response = openai_client.beta.chat.completions.parse(\n",
    "      model=\"gpt-4o-mini\",\n",
    "      messages=[\n",
    "          { \"role\": \"system\", \"content\": instructions },\n",
    "          {\n",
    "              \"role\": \"user\",\n",
    "              \"content\": f\"\"\"Ground Truth answer: {reference_outputs[\"answer\"]}; \n",
    "              Student's Answer: {outputs[\"response\"]}\"\"\"\n",
    "          },\n",
    "      ],\n",
    "      response_format=Grade,\n",
    "  )\n",
    "  return response.choices[0].message.parsed.score\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------\n",
    "# Run and View results\n",
    "#---------------------------------\n",
    "# After running the evaluation, a link will be provided to view the results in langsmith\n",
    "experiment_results = client.evaluate(\n",
    "  target,\n",
    "  data=\"my-dataset\",\n",
    "  evaluators=[\n",
    "      accuracy,\n",
    "      # can add multiple evaluators here\n",
    "  ],\n",
    "  experiment_prefix=\"first-eval-in-langsmith\",\n",
    "  max_concurrency=2,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Prompt Engineering ---------------------------------\n",
    "\n",
    "from langsmith import Client\n",
    "from langsmith.client import convert_prompt_to_openai_format\n",
    "\n",
    "# Pull a prompt from LangSmith Prompt Hub\n",
    "client = Client()\n",
    "prompt = client.pull_prompt(\"your-prompt-id\")\n",
    "\n",
    "# Use the prompt in your app\n",
    "hydrated_prompt = prompt.invoke({\"question\": \"What is LangSmith?\"})\n",
    "messages = convert_prompt_to_openai_format(hydrated_prompt)[\"messages\"]\n",
    "response = openai_client.chat.completions.create(model=\"gpt-4\", messages=messages)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Collecting Human Feedback ---------------------------------\n",
    "# Add feedback to a run\n",
    "from langsmith import Client\n",
    "from langsmith import traceable\n",
    "import uuid\n",
    "\n",
    "@traceable\n",
    "def foo():\n",
    "    return \"This is a sample Run!\"\n",
    "\n",
    "\n",
    "client = Client()\n",
    "client.create_feedback(\n",
    "    run_id=\"your-run-id\",\n",
    "    key=\"user_feedback\",\n",
    "    score=1.0,\n",
    "    comment=\"The response was helpful.\"\n",
    ")\n",
    "\n",
    "# Pre-generate run IDs for feedback\n",
    "pre_defined_run_id = uuid.uuid4()\n",
    "foo(langsmith_extra={\"run_id\": pre_defined_run_id})\n",
    "client.create_feedback(pre_defined_run_id, \"user_feedback\", score=1)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------- Production Observability ---------------------------------\n",
    "# Filter runs in production\n",
    "from langsmith import Client\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "client = Client()\n",
    "runs = client.list_runs(\n",
    "    project_name=\"langsmith-academy\",\n",
    "    filter=\"eq(is_root, true)\",\n",
    "    start_time=datetime.now() - timedelta(days=1)\n",
    ")\n",
    "\n",
    "for run in runs:\n",
    "    print(run)\n",
    "\n",
    "# Run your app to trigger online evaluations\n",
    "from app import langsmith_rag\n",
    "question = \"How do I set up tracing?\"\n",
    "langsmith_rag(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Human in the Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the `interrupt` function instead.\n",
    "\n",
    "#------------------------- Basic Human-in-the-Loop with Breakpoints--------------------------------\n",
    "# Compile graph with breakpoint\n",
    "graph = builder.compile(\n",
    "    checkpointer=memory, \n",
    "    interrupt_before=[\"step_for_human_in_the_loop\"] # Add breakpoint\n",
    ")\n",
    "\n",
    "# Run graph up to breakpoint\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Perform human action (e.g., approve, edit, input)\n",
    "# Resume graph execution\n",
    "# Human approval step\n",
    "user_approval = input(\"Do you want to call the tool? (yes/no): \")\n",
    "if user_approval.lower() == \"yes\":\n",
    "    # Resume graph execution\n",
    "    for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "        print(event)\n",
    "else:\n",
    "    print(\"Operation cancelled by user.\")\n",
    "\n",
    "#------------------------- Dynamic Breakpoints--------------------------------\n",
    "# Dynamic breakpoints allow the graph to interrupt itself based on conditions defined within a node.\n",
    "# can define some *condition* that must be met for a breakpoint to be triggered\n",
    "from langgraph.errors import NodeInterrupt\n",
    "\n",
    "# Define a node with dynamic breakpoint\n",
    "def my_node(state: State) -> State:\n",
    "    if len(state['input']) > 5:  # Condition for breakpoint\n",
    "        raise NodeInterrupt(f\"Input too long: {state['input']}\")\n",
    "    return state\n",
    "\n",
    "# Resume after dynamic breakpoint\n",
    "graph.update_state(config=thread_config, values={\"input\": \"foo\"})  # Update state to pass condition\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Skip node entirely\n",
    "graph.update_state(None, config=thread_config, as_node=\"my_node\")  # Skip node\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "    \n",
    "\n",
    "#------------------------- Editing State with Human Feedback--------------------------------\n",
    "# You can modify the graph state during interruptions to incorporate human feedback.\n",
    "\n",
    "# Get current state after interruption\n",
    "state = graph.get_state(thread_config)\n",
    "print(state)\n",
    "\n",
    "# Update state with human feedback\n",
    "graph.update_state(\n",
    "    thread_config, \n",
    "    {\"user_input\": \"human feedback\"},  # Add human input\n",
    "    as_node=\"human_input\"  # Treat update as a node\n",
    ")\n",
    "\n",
    "# Resume graph execution\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "\n",
    "#------------------------- Input Pattern or Tool Call--------------------------------\n",
    "    \n",
    " # Compile graph with input breakpoint\n",
    "graph = builder.compile(\n",
    "    checkpointer=checkpointer, \n",
    "    interrupt_before=[\"human_input\"]  # Node for human input\n",
    ")\n",
    "\n",
    "# Run graph up to input breakpoint\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Add human input and resume\n",
    "graph.update_state(\n",
    "    thread_config, \n",
    "    {\"user_input\": \"human input\"},  # Provide human input or tool call\n",
    "    as_node=\"human_input\"  # Treat update as node\n",
    ")\n",
    "for event in graph.stream(None, thread_config, stream_mode=\"values\"):\n",
    "    print(event)   \n",
    "    \n",
    "\n",
    "\n",
    "#------------------------- LangGraph API Integration--------------------------------\n",
    "from langgraph_sdk import get_client\n",
    "\n",
    "# Connect to LangGraph Studio\n",
    "client = get_client(url=\"http://localhost:56091\")\n",
    "\n",
    "# Stream graph with breakpoint\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=initial_input,\n",
    "    stream_mode=\"values\",\n",
    "    interrupt_before=[\"tools\"],  # Set breakpoint\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    print(chunk.data)\n",
    "\n",
    "# Resume from breakpoint\n",
    "async for chunk in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input=None,\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    print(f\"Receiving new event of type: {chunk.event}...\")\n",
    "    print(chunk.data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Memory Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------- Short term Memory --------------------------------\n",
    "# Short-term memory is managed using checkpointers , which save the state of a graph at each step.\n",
    "\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Initialize a checkpointer for short-term memory\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "# Compile a graph with the checkpointer\n",
    "graph = builder.compile(checkpointer=checkpointer)\n",
    "\n",
    "# Run the graph and save state\n",
    "thread_config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "for event in graph.stream(inputs, thread_config, stream_mode=\"values\"):\n",
    "    print(event)\n",
    "\n",
    "# Retrieve the state from the checkpointer\n",
    "state = graph.get_state(thread_config)\n",
    "print(state)\n",
    "\n",
    "\n",
    "#------------------------- Long-Term Memory with Stores (Memory Store) --------------------------------\n",
    "# Long-term memory is managed using stores , which persist data across threads or sessions.\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "\n",
    "# Initialize a store for long-term memory\n",
    "store = InMemoryStore()\n",
    "\n",
    "#---------------------\n",
    "# Save a memory\n",
    "#----------------------\n",
    "# Save a memory\n",
    "user_id = \"1\"\n",
    "namespace = (user_id, \"memories\") # Namespace for user-specific memories\n",
    "key = \"profile\"\n",
    "value = {\"name\": \"Lance\", \"interests\": [\"biking\", \"bakeries\"]}\n",
    "store.put(namespace, key, value)    # Save a memory to the store\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# Save a memory 2 (Memory Schema Collection)\n",
    "#---------------------\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define a memory schema\n",
    "class Memory(BaseModel):\n",
    "    content: str = Field(description=\"The main content of the memory.\")\n",
    "\n",
    "# Create a collection of memories\n",
    "memory_collection = [\n",
    "    Memory(content=\"User likes biking\"),\n",
    "    Memory(content=\"User enjoys bakeries\")\n",
    "]\n",
    "\n",
    "# Save memories to the store\n",
    "for memory in memory_collection:\n",
    "    key = str(uuid.uuid4())\n",
    "    store.put(namespace, key, memory.model_dump())\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "#---------------------------------\n",
    "# Retrieve a memory from the store\n",
    "#---------------------------------\n",
    "memories = store.get(namespace, key)\n",
    "print(memories.value)\n",
    "# or\n",
    "memories = store.search(namespace)\n",
    "for memory in memories:\n",
    "    print(memory.value)\n",
    "\n",
    "#---------------------\n",
    "#---------------------\n",
    "\n",
    "#---------------------------------\n",
    "# Dynamic Memory Updates\n",
    "#---------------------------------\n",
    "# Dynamic memory updates allow the agent to decide when to save memories and what type of memory to update \n",
    "# (e.g., profile, collection, or instructions)\n",
    "\n",
    "def update_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    existing_memories = store.search(namespace)\n",
    "    tool_name = \"Memory\"\n",
    "    existing_memories_formatted = [(m.key, tool_name, m.value) for m in existing_memories]\n",
    "    result = trustcall_extractor.invoke({\"messages\": state[\"messages\"], \"existing\": existing_memories_formatted})\n",
    "    for r, rmeta in zip(result[\"responses\"], result[\"response_metadata\"]):\n",
    "        store.put(namespace, rmeta.get(\"json_doc_id\", str(uuid.uuid4())), r.model_dump())\n",
    "\n",
    "\n",
    "\n",
    "#---------------------------------------------- Memory Agents ------------------------------------------\n",
    "#-------------------------------------------------------------------------------------------------------\n",
    "from langgraph.graph import StateGraph, MessagesState\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# Define nodes for the agent\n",
    "def call_model(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    # Retrieve memory and personalize responses\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    memories = store.search(namespace)\n",
    "    memory_content = \"\\n\".join([m.value[\"content\"] for m in memories])\n",
    "    system_msg = f\"Memory: {memory_content}\"\n",
    "    response = model.invoke([SystemMessage(content=system_msg)] + state[\"messages\"])\n",
    "    return {\"messages\": response}\n",
    "\n",
    "def write_memory(state: MessagesState, config: RunnableConfig, store: BaseStore):\n",
    "    # Reflect on chat history and save memories\n",
    "    user_id = config[\"configurable\"][\"user_id\"]\n",
    "    namespace = (\"memory\", user_id)\n",
    "    result = trustcall_extractor.invoke({\"messages\": state[\"messages\"]})\n",
    "    for r in result[\"responses\"]:\n",
    "        key = str(uuid.uuid4())\n",
    "        store.put(namespace, key, r.model_dump())\n",
    "\n",
    "# Compile the graph\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"call_model\", call_model)\n",
    "builder.add_node(\"write_memory\", write_memory)\n",
    "builder.add_edge(START, \"call_model\")\n",
    "builder.add_edge(\"call_model\", \"write_memory\")\n",
    "builder.add_edge(\"write_memory\", END)\n",
    "\n",
    "# Compile with memory store and checkpointer\n",
    "graph = builder.compile(checkpointer=MemorySaver(), store=InMemoryStore())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Long-Term Memory with Stores (Memory Store) --------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------- Long-Term Memory with Stores (Memory Store) --------------------------------\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Project 1 (code that works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, MessagesState, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from typing import Annotated, Literal\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.messages import HumanMessage, AIMessage, BaseMessage\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "import os\n",
    "import json # Parse JSON response\n",
    "\n",
    "\n",
    "\n",
    "class Chatbot:\n",
    "    def __init__(self):\n",
    "        self.llm = ChatOpenAI(\n",
    "            api_key=os.getenv(\"DEEPSEEK_API_KEY\"),\n",
    "            model=\"deepseek-chat\",\n",
    "            base_url=\"https://api.deepseek.com\",\n",
    "            streaming=True,\n",
    "            callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "        )\n",
    "        \n",
    "    def call_tool(self):\n",
    "        tool = TavilySearchResults(max_results=2)\n",
    "        self.tools = [tool]\n",
    "        self.tool_node = ToolNode(tools=[tool])\n",
    "        self.llm_with_tool = self.llm.bind_tools(self.tools)\n",
    "        \n",
    "    def call_model(self, state: MessagesState):\n",
    "        \"\"\"\n",
    "        LLM node to process the user's query and invoke tools if needed.\n",
    "        \"\"\"\n",
    "        messages = state['messages']\n",
    "        latest_query = messages[-1].content if messages else \"No query provided.\"\n",
    "\n",
    "        # template = ''' \n",
    "        # You are a travel suggestion agent. Answer the user's questions based on their travel preferences. \n",
    "        # If you need to find information about a specific destination, use the search_tool. Understand that the information was retrieved from the web,\n",
    "        # interpret it, and generate a response accordingly.\n",
    "\n",
    "        # Answer the following questions as best as you can. You have access to the following tools:\n",
    "        # {tools}\n",
    "\n",
    "        # Use the following format:\n",
    "        # Question: the input question you must answer\n",
    "        # Thought: you should always think about what to do\n",
    "        # Action: the action you should take, should be one of [{tool_names}]\n",
    "        # Action Input: the input to the action\n",
    "        # Observation: the result of the action\n",
    "        # Thought: I now know the final answer\n",
    "        # Final Answer: [Your final answer here as a concise and complete sentence]\n",
    "\n",
    "        # Ensure the response strictly follows this format. Do not repeat the Final Answer multiple times.\n",
    "\n",
    "        # Begin!\n",
    "        # Question: {input}\n",
    "        # Thought: {agent_scratchpad}\n",
    "        # '''\n",
    "\n",
    "        # Improved prompt to enforce JSON output\n",
    "        template = ''' \n",
    "        You are a travel suggestion agent. Answer the user's questions based on their travel preferences. \n",
    "        If you need to find information about a specific destination, use the search_tool. Understand that the information was retrieved from the web,\n",
    "        interpret it, and generate a response accordingly.\n",
    "\n",
    "        Answer the following questions as best as you can. You have access to the following tools:\n",
    "        {tools}\n",
    "\n",
    "        Use the following format:\n",
    "        \n",
    "        \"Question\": the input question you must answer\n",
    "        \"Thought\": your reasoning about what to do next\n",
    "        \"Action\": the action you should take, one of [{tool_names}] (if no action is needed, write \"None\")\n",
    "        \"Action Input\": the input to the action (if no action is needed, write \"None\")\n",
    "        \"Observation\": the result of the action (if no action is needed, write \"None\")\n",
    "        \"Thought\": your reasoning after observing the action\n",
    "        \"Final Answer\": the final answer to the original input question\n",
    "        \n",
    "        Ensure every Thought is followed by an Action, Action Input, and Observation. If no tool is needed, explicitly write \"None\" for Action, Action Input, and Observation.\n",
    "\n",
    "        Begin!\n",
    "        Question: {input}\n",
    "        Thought: {agent_scratchpad}\n",
    "        '''\n",
    "        \n",
    "        prompt = PromptTemplate.from_template(template)\n",
    "        search_agent = create_react_agent(\n",
    "            llm=self.llm_with_tool,\n",
    "            prompt=prompt,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "        agent_executor = AgentExecutor(\n",
    "            agent=search_agent,\n",
    "            tools=self.tools,\n",
    "            verbose=True,\n",
    "            return_intermediate_steps=False,\n",
    "            handle_parsing_errors=True,\n",
    "        )\n",
    "\n",
    "        try:\n",
    "            response = agent_executor.invoke({\n",
    "                \"input\": latest_query,\n",
    "                \"agent_scratchpad\": \"\"  # Initialize with an empty scratchpad\n",
    "            })\n",
    "\n",
    "            # Check if the response is already a dictionary\n",
    "            if isinstance(response, dict):\n",
    "                final_answer = response.get(\"output\", \"No final answer provided.\")\n",
    "            else:\n",
    "                raise ValueError(\"Unexpected response type. Expected a dictionary.\")\n",
    "\n",
    "            # print(\"\")\n",
    "            # print(f'response: {response}')\n",
    "                \n",
    "            # # Validate and clean response\n",
    "            # if not response.startswith(\"Final Answer:\"):\n",
    "            #     raise ValueError(\"Invalid agent response format. Missing 'Final Answer:' prefix.\")\n",
    "            # final_answer = response.replace(\"Final Answer:\", \"\")[-1].strip()\n",
    "            \n",
    "            state['messages'].append(AIMessage(content=final_answer))  # Append clean response to messages\n",
    "            return state\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = f\"Error: {e}\"\n",
    "            state['messages'].append(AIMessage(content=error_message))\n",
    "            return state\n",
    "\n",
    "    \n",
    "    def router_function(self, state: MessagesState) -> Literal[\"tools\", END]:\n",
    "        \"\"\"\n",
    "        Determine the next node based on tool invocation.\n",
    "        \"\"\"\n",
    "        messages = state['messages']\n",
    "        last_message = messages[-1]\n",
    "        if hasattr(last_message, \"tool_calls\") and last_message.tool_calls:\n",
    "            return \"tools\"\n",
    "        return END\n",
    "    \n",
    "    def __call__(self):\n",
    "        \"\"\"\n",
    "        Build and return the workflow graph.\n",
    "        \"\"\"\n",
    "        self.call_tool()\n",
    "        workflow = StateGraph(MessagesState)\n",
    "        workflow.add_node(\"agent\", self.call_model)\n",
    "        workflow.add_node(\"tools\", self.tool_node)\n",
    "        workflow.add_edge(START, \"agent\")\n",
    "        workflow.add_conditional_edges(\"agent\", self.router_function, {\"tools\": \"tools\", END: END})\n",
    "        workflow.add_edge(\"tools\", \"agent\")\n",
    "        self.app = workflow.compile()\n",
    "        return self.app\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    mybot = Chatbot()\n",
    "    workflow = mybot()\n",
    "\n",
    "    # Properly initialize MessagesState with HumanMessage objects\n",
    "    initial_state = {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Search tthe web and tell me about Airi Shimamura from Oklahoma?\")\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    response = workflow.invoke(initial_state)\n",
    "    print(response['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LangGraph Project 2 (code that works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import Runnable\n",
    "# from langchain_aws import ChatBedrock\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "import os\n",
    "import boto3\n",
    "from typing import Annotated\n",
    "from langchain_core.callbacks.manager import AsyncCallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph.message import AnyMessage, add_messages\n",
    "from langchain_core.messages import ToolMessage\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "@tool\n",
    "def compute_savings(monthly_cost: float) -> float:\n",
    "    \"\"\"\n",
    "    Tool to compute the potential savings when switching to solar energy based on the user's monthly electricity cost.\n",
    "    \n",
    "    Args:\n",
    "        monthly_cost (float): The user's current monthly electricity cost.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing:\n",
    "            - 'number_of_panels': The estimated number of solar panels required.\n",
    "            - 'installation_cost': The estimated installation cost.\n",
    "            - 'net_savings_10_years': The net savings over 10 years after installation costs.\n",
    "    \"\"\"\n",
    "    def calculate_solar_savings(monthly_cost):\n",
    "        # Assumptions for the calculation\n",
    "        cost_per_kWh = 0.28  \n",
    "        cost_per_watt = 1.50  \n",
    "        sunlight_hours_per_day = 3.5  \n",
    "        panel_wattage = 350  \n",
    "        system_lifetime_years = 10  \n",
    "        # Monthly electricity consumption in kWh\n",
    "        monthly_consumption_kWh = monthly_cost / cost_per_kWh\n",
    "        \n",
    "        # Required system size in kW\n",
    "        daily_energy_production = monthly_consumption_kWh / 30\n",
    "        system_size_kW = daily_energy_production / sunlight_hours_per_day\n",
    "        \n",
    "        # Number of panels and installation cost\n",
    "        number_of_panels = system_size_kW * 1000 / panel_wattage\n",
    "        installation_cost = system_size_kW * 1000 * cost_per_watt\n",
    "        \n",
    "        # Annual and net savings\n",
    "        annual_savings = monthly_cost * 12\n",
    "        total_savings_10_years = annual_savings * system_lifetime_years\n",
    "        net_savings = total_savings_10_years - installation_cost\n",
    "        \n",
    "        return {\n",
    "            \"number_of_panels\": round(number_of_panels),\n",
    "            \"installation_cost\": round(installation_cost, 2),\n",
    "            \"net_savings_10_years\": round(net_savings, 2)\n",
    "        }\n",
    "    # Return calculated solar savings\n",
    "    return calculate_solar_savings(monthly_cost)\n",
    "\n",
    "def handle_tool_error(state) -> dict:\n",
    "    \"\"\"\n",
    "    Function to handle errors that occur during tool execution.\n",
    "    \n",
    "    Args:\n",
    "        state (dict): The current state of the AI agent, which includes messages and tool call details.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A dictionary containing error messages for each tool that encountered an issue.\n",
    "    \"\"\"\n",
    "    # Retrieve the error from the current state\n",
    "    error = state.get(\"error\")\n",
    "    \n",
    "    # Access the tool calls from the last message in the state's message history\n",
    "    tool_calls = state[\"messages\"][-1].tool_calls\n",
    "    \n",
    "    # Return a list of ToolMessages with error details, linked to each tool call ID\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            ToolMessage(\n",
    "                content=f\"Error: {repr(error)}\\n please fix your mistakes.\",  # Format the error message for the user\n",
    "                tool_call_id=tc[\"id\"],  # Associate the error message with the corresponding tool call ID\n",
    "            )\n",
    "            for tc in tool_calls  # Iterate over each tool call to produce individual error messages\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def create_tool_node_with_fallback(tools: list) -> dict:\n",
    "    \"\"\"\n",
    "    Function to create a tool node with fallback error handling.\n",
    "    \n",
    "    Args:\n",
    "        tools (list): A list of tools to be included in the node.\n",
    "    \n",
    "    Returns:\n",
    "        dict: A tool node that uses fallback behavior in case of errors.\n",
    "    \"\"\"\n",
    "    # Create a ToolNode with the provided tools and attach a fallback mechanism\n",
    "    # If an error occurs, it will invoke the handle_tool_error function to manage the error\n",
    "    return ToolNode(tools).with_fallbacks(\n",
    "        [RunnableLambda(handle_tool_error)],  # Use a lambda function to wrap the error handler\n",
    "        exception_key=\"error\"  # Specify that this fallback is for handling errors\n",
    "    )\n",
    "\n",
    "class State(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], add_messages]\n",
    "\n",
    "class Assistant:\n",
    "    def __init__(self, runnable: Runnable):\n",
    "        # Initialize with the runnable that defines the process for interacting with the tools\n",
    "        self.runnable = runnable\n",
    "    def __call__(self, state: State):\n",
    "        while True:\n",
    "            # Invoke the runnable with the current state (messages and context)\n",
    "            result = self.runnable.invoke(state)\n",
    "            \n",
    "            # If the tool fails to return valid output, re-prompt the user to clarify or retry\n",
    "            if not result.tool_calls and (\n",
    "                not result.content\n",
    "                or isinstance(result.content, list)\n",
    "                and not result.content[0].get(\"text\")\n",
    "            ):\n",
    "                # Add a message to request a valid response\n",
    "                messages = state[\"messages\"] + [(\"user\", \"Respond with a real output.\")]\n",
    "                state = {**state, \"messages\": messages}\n",
    "            else:\n",
    "                # Break the loop when valid output is obtained\n",
    "                break\n",
    "        # Return the final state after processing the runnable\n",
    "        return {\"messages\": result}\n",
    "\n",
    "# def get_bedrock_client(region):\n",
    "#     return boto3.client(\"bedrock-runtime\", region_name=region)\n",
    "\n",
    "# def create_bedrock_llm(client):\n",
    "#     return ChatBedrock(model_id='anthropic.claude-3-sonnet-20240229-v1:0', client=client, model_kwargs={'temperature': 0}, region_name='us-east-1')\n",
    "\n",
    "# llm = create_bedrock_llm(get_bedrock_client(region='us-east-1'))\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "        api_key=OPENAI_API_KEY,\n",
    "        model=\"gpt-4o\",\n",
    "        streaming=True,\n",
    "        callbacks=AsyncCallbackManager([StreamingStdOutCallbackHandler()]),\n",
    "    )\n",
    "\n",
    "primary_assistant_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            '''You are a helpful customer support assistant for Solar Panels Belgium.\n",
    "            You should get the following information from them:\n",
    "            - monthly electricity cost\n",
    "            If you are not able to discern this info, ask them to clarify! Do not attempt to wildly guess.\n",
    "            After you are able to discern all the information, call the relevant tool.\n",
    "            ''',\n",
    "        ),\n",
    "        (\"placeholder\", \"{messages}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the tools the assistant will use\n",
    "part_1_tools = [\n",
    "    compute_savings\n",
    "]\n",
    "\n",
    "# Bind the tools to the assistant's workflow\n",
    "part_1_assistant_runnable = primary_assistant_prompt | llm.bind_tools(part_1_tools, tool_choice=\"any\")\n",
    "\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"assistant\", Assistant(part_1_assistant_runnable))\n",
    "builder.add_node(\"tools\", create_tool_node_with_fallback(part_1_tools))\n",
    "\n",
    "builder.add_edge(START, \"assistant\")  # Start with the assistant\n",
    "builder.add_conditional_edges(\"assistant\", tools_condition)  # Move to tools after input\n",
    "builder.add_edge(\"tools\", \"assistant\")  # Return to assistant after tool execution\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "# import shutil\n",
    "import uuid\n",
    "\n",
    "def _print_event(event, _printed):\n",
    "    \"\"\"\n",
    "    Helper function to print events generated by the graph.\n",
    "    \n",
    "    Args:\n",
    "        event: The event to print.\n",
    "        _printed: A set to track already printed events to avoid duplicates.\n",
    "    \"\"\"\n",
    "    if event[\"messages\"]:\n",
    "        for message in event[\"messages\"]:\n",
    "            if message.id not in _printed:\n",
    "                # Check the type of the message and print accordingly\n",
    "                if hasattr(message, \"type\"):\n",
    "                    print(f\"Type: {message.type}, Content: {message.content}\")\n",
    "                elif hasattr(message, \"role\"):\n",
    "                    print(f\"Role: {message.role}, Content: {message.content}\")\n",
    "                else:\n",
    "                    print(f\"Message: {message}\")\n",
    "                _printed.add(message.id)\n",
    "                \n",
    "                \n",
    "# Let's create an example conversation a user might have with the assistant\n",
    "tutorial_questions = [\n",
    "    'hey',\n",
    "    'can you calculate my energy saving',\n",
    "    \"my montly cost is $100, what will i save\"\n",
    "]\n",
    "thread_id = str(uuid.uuid4())\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        \"thread_id\": thread_id,\n",
    "    }\n",
    "}\n",
    "_printed = set()\n",
    "for question in tutorial_questions:\n",
    "    events = graph.stream(\n",
    "        {\"messages\": (\"user\", question)}, config, stream_mode=\"values\"\n",
    "    )\n",
    "    for event in events:\n",
    "        _print_event(event, _printed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Agent System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Agent Systems\n",
    "    # Definition : A system where multiple agents interact to achieve a common goal.\n",
    "    # Use Case : Used by LangChain to model complex workflows involving multiple agents with different capabilities.\n",
    "    # Example : A multi-agent system for customer support, where agents handle different types of customer queries (FAQs, escalations, recommendations).\n",
    "    \n",
    "    # Agent Supervisor Node : Routes queries to specific agents based on the query type.\n",
    "    # Hierarchical Agent Teams : Organizes agents into teams based on their expertise or function.\n",
    "    # Multi-Agent Collaboration : Enables agents to share information and coordinate tasks to achieve a common goal.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Advanced Agent Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "from typing import Callable, Literal\n",
    "\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_core.language_models import LanguageModelLike\n",
    "from langgraph.graph import StateGraph, START\n",
    "from langgraph.graph.state import CompiledStateGraph\n",
    "from langgraph.prebuilt.chat_agent_executor import (\n",
    "    AgentState,\n",
    "    StateSchemaType,\n",
    "    Prompt,\n",
    "    create_react_agent,\n",
    ")\n",
    "\n",
    "from langgraph_supervisor.handoff import (\n",
    "    create_handoff_tool,\n",
    "    create_handoff_back_messages,\n",
    ")\n",
    "\n",
    "\n",
    "OutputMode = Literal[\"full_history\", \"last_message\"]\n",
    "\"\"\"Mode for adding agent outputs to the message history in the multi-agent workflow\n",
    "\n",
    "- `full_history`: add the entire agent message history\n",
    "- `last_message`: add only the last message\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def _make_call_agent(\n",
    "    agent: CompiledStateGraph,\n",
    "    output_mode: OutputMode,\n",
    "    add_handoff_back_messages: bool,\n",
    "    supervisor_name: str,\n",
    ") -> Callable[[dict], dict]:\n",
    "    if output_mode not in OutputMode.__args__:\n",
    "        raise ValueError(\n",
    "            f\"Invalid agent output mode: {output_mode}. \"\n",
    "            f\"Needs to be one of {OutputMode.__args__}\"\n",
    "        )\n",
    "\n",
    "    def call_agent(state: dict) -> dict:\n",
    "        output = agent.invoke(state)\n",
    "        messages = output[\"messages\"]\n",
    "        if output_mode == \"full_history\":\n",
    "            pass\n",
    "        elif output_mode == \"last_message\":\n",
    "            messages = messages[-1:]\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Invalid agent output mode: {output_mode}. \"\n",
    "                f\"Needs to be one of {OutputMode.__args__}\"\n",
    "            )\n",
    "\n",
    "        if add_handoff_back_messages:\n",
    "            messages.extend(create_handoff_back_messages(agent.name, supervisor_name))\n",
    "\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "    return call_agent\n",
    "\n",
    "\n",
    "def create_supervisor(\n",
    "    agents: list[CompiledStateGraph],\n",
    "    *,\n",
    "    model: LanguageModelLike,\n",
    "    tools: list[Callable | BaseTool] | None = None,\n",
    "    prompt: Prompt | None = None,\n",
    "    state_schema: StateSchemaType = AgentState,\n",
    "    output_mode: OutputMode = \"last_message\",\n",
    "    add_handoff_back_messages: bool = True,\n",
    "    supervisor_name: str = \"supervisor\",\n",
    ") -> StateGraph:\n",
    "    \"\"\"Create a multi-agent supervisor.\n",
    "\n",
    "    Args:\n",
    "        agents: List of agents to manage\n",
    "        model: Language model to use for the supervisor\n",
    "        tools: Tools to use for the supervisor\n",
    "        prompt: Optional prompt to use for the supervisor. Can be one of:\n",
    "            - str: This is converted to a SystemMessage and added to the beginning of the list of messages in state[\"messages\"].\n",
    "            - SystemMessage: this is added to the beginning of the list of messages in state[\"messages\"].\n",
    "            - Callable: This function should take in full graph state and the output is then passed to the language model.\n",
    "            - Runnable: This runnable should take in full graph state and the output is then passed to the language model.\n",
    "        state_schema: State schema to use for the supervisor graph.\n",
    "        output_mode: Mode for adding managed agents' outputs to the message history in the multi-agent workflow.\n",
    "            Can be one of:\n",
    "            - `full_history`: add the entire agent message history\n",
    "            - `last_message`: add only the last message (default)\n",
    "        add_handoff_back_messages: Whether to add a pair of (AIMessage, ToolMessage) to the message history\n",
    "            when returning control to the supervisor to indicate that a handoff has occurred.\n",
    "        supervisor_name: Name of the supervisor node.\n",
    "    \"\"\"\n",
    "    agent_names = set()\n",
    "    for agent in agents:\n",
    "        if agent.name is None or agent.name == \"LangGraph\":\n",
    "            raise ValueError(\n",
    "                \"Please specify a name when you create your agent, either via `create_react_agent(..., name=agent_name)` \"\n",
    "                \"or via `graph.compile(name=name)`.\"\n",
    "            )\n",
    "\n",
    "        if agent.name in agent_names:\n",
    "            raise ValueError(\n",
    "                f\"Agent with name '{agent.name}' already exists. Agent names must be unique.\"\n",
    "            )\n",
    "\n",
    "        agent_names.add(agent.name)\n",
    "\n",
    "    handoff_tools = [create_handoff_tool(agent_name=agent.name) for agent in agents]\n",
    "    all_tools = (tools or []) + handoff_tools\n",
    "\n",
    "    if (\n",
    "        hasattr(model, \"bind_tools\")\n",
    "        and \"parallel_tool_calls\" in inspect.signature(model.bind_tools).parameters\n",
    "    ):\n",
    "        model = model.bind_tools(all_tools, parallel_tool_calls=False)\n",
    "\n",
    "    supervisor_agent = create_react_agent(\n",
    "        name=supervisor_name,\n",
    "        model=model,\n",
    "        tools=all_tools,\n",
    "        prompt=prompt,\n",
    "        state_schema=state_schema,\n",
    "    )\n",
    "\n",
    "    builder = StateGraph(state_schema)\n",
    "    builder.add_node(supervisor_agent, destinations=tuple(agent_names))\n",
    "    builder.add_edge(START, supervisor_agent.name)\n",
    "    for agent in agents:\n",
    "        builder.add_node(\n",
    "            agent.name,\n",
    "            _make_call_agent(\n",
    "                agent,\n",
    "                output_mode,\n",
    "                add_handoff_back_messages,\n",
    "                supervisor_name,\n",
    "            ),\n",
    "        )\n",
    "        builder.add_edge(agent.name, supervisor_agent.name)\n",
    "\n",
    "    return builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import uuid\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "from langchain_core.messages import AIMessage, ToolMessage, ToolCall\n",
    "from langchain_core.tools import tool, BaseTool, InjectedToolCallId\n",
    "from langgraph.types import Command\n",
    "\n",
    "\n",
    "WHITESPACE_RE = re.compile(r\"\\s+\")\n",
    "\n",
    "\n",
    "def _normalize_agent_name(agent_name: str) -> str:\n",
    "    \"\"\"Normalize an agent name to be used inside the tool name.\"\"\"\n",
    "    return WHITESPACE_RE.sub(\"_\", agent_name.strip()).lower()\n",
    "\n",
    "\n",
    "def create_handoff_tool(*, agent_name: str) -> BaseTool:\n",
    "    \"\"\"Create a tool that can handoff control to the requested agent.\n",
    "\n",
    "    Args:\n",
    "        agent_name: The name of the agent to handoff control to, i.e.\n",
    "            the name of the agent node in the multi-agent graph.\n",
    "            Agent names should be simple, clear and unique, preferably in snake_case,\n",
    "            although you are only limited to the names accepted by LangGraph\n",
    "            nodes as well as the tool names accepted by LLM providers\n",
    "            (the tool name will look like this: `transfer_to_<agent_name>`).\n",
    "    \"\"\"\n",
    "    tool_name = f\"transfer_to_{_normalize_agent_name(agent_name)}\"\n",
    "\n",
    "    @tool(tool_name)\n",
    "    def handoff_to_agent(\n",
    "        tool_call_id: Annotated[str, InjectedToolCallId],\n",
    "    ):\n",
    "        \"\"\"Ask another agent for help.\"\"\"\n",
    "        tool_message = ToolMessage(\n",
    "            content=f\"Successfully transferred to {agent_name}\",\n",
    "            name=tool_name,\n",
    "            tool_call_id=tool_call_id,\n",
    "        )\n",
    "        return Command(\n",
    "            goto=agent_name,\n",
    "            graph=Command.PARENT,\n",
    "            update={\"messages\": [tool_message]},\n",
    "        )\n",
    "\n",
    "    return handoff_to_agent\n",
    "\n",
    "\n",
    "def create_handoff_back_messages(\n",
    "    agent_name: str, supervisor_name: str\n",
    ") -> tuple[AIMessage, ToolMessage]:\n",
    "    \"\"\"Create a pair of (AIMessage, ToolMessage) to add to the message history when returning control to the supervisor.\"\"\"\n",
    "    tool_call_id = str(uuid.uuid4())\n",
    "    tool_name = f\"transfer_back_to_{_normalize_agent_name(supervisor_name)}\"\n",
    "    tool_calls = [ToolCall(name=tool_name, args={}, id=tool_call_id)]\n",
    "    return (\n",
    "        AIMessage(\n",
    "            content=f\"Transferring back to {supervisor_name}\",\n",
    "            tool_calls=tool_calls,\n",
    "            name=agent_name,\n",
    "        ),\n",
    "        ToolMessage(\n",
    "            content=f\"Successfully transferred back to {supervisor_name}\",\n",
    "            name=tool_name,\n",
    "            tool_call_id=tool_call_id,\n",
    "        ),\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Agent Supervisor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Supervisor Node\n",
    "from typing import Literal\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langgraph.graph import MessagesState\n",
    "from langgraph.types import Command\n",
    "\n",
    "'''\n",
    "User\n",
    " ├── Supervisor\n",
    " │     ├── [direct] Agent 1\n",
    " │     ├── [conditional] Agent 2 (if condition A is met)\n",
    " │     └── [direct] Agent 3\n",
    " │           ├── [conditional] Sub-Agent 3.1 (if condition B is met)\n",
    " │           └── [direct] Sub-Agent 3.2\n",
    " │\n",
    " └── Feedback Loop (User <--> Supervisor)\n",
    "\n",
    "'''\n",
    "members = [\"researcher\", \"coder\"]\n",
    "\n",
    "def make_supervisor_node(llm: BaseChatModel, members: list[str]) -> str:\n",
    "    options = [\"FINISH\"] + members\n",
    "    system_prompt = (\n",
    "        \"You are a supervisor tasked with managing a conversation between the\"\n",
    "        f\" following workers: {members}. Given the following user request,\"\n",
    "        \" respond with the worker to act next. Each worker will perform a\"\n",
    "        \" task and respond with their results and status. When finished,\"\n",
    "        \" respond with FINISH.\"\n",
    "    )\n",
    "\n",
    "    class Router(TypedDict):\n",
    "        \"\"\"Worker to route to next. If no workers needed, route to FINISH.\"\"\"\n",
    "        next: Literal[*options]\n",
    "\n",
    "    def supervisor_node(state: MessagesState) -> Command[Literal[*members, \"__end__\"]]:\n",
    "        \"\"\"An LLM-based router.\"\"\"\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "        ] + state[\"messages\"]\n",
    "        response = llm.with_structured_output(Router).invoke(messages)\n",
    "        goto = response[\"next\"]\n",
    "        if goto == \"FINISH\":\n",
    "            goto = END\n",
    "\n",
    "        return Command(goto=goto)\n",
    "\n",
    "    return supervisor_node\n",
    "\n",
    "llm = ChatAnthropic(model=\"claude-3-5-sonnet-latest\")\n",
    "supervisor_node = make_supervisor_node(llm, [\"search\", \"web_scraper\"])\n",
    "\n",
    "def research_node(state: MessagesState) -> Command[Literal[\"supervisor\"]]:\n",
    "    result = research_agent.invoke(state)\n",
    "    return Command(\n",
    "        update={\n",
    "            \"messages\": [\n",
    "                HumanMessage(content=result[\"messages\"][-1].content, name=\"researcher\")\n",
    "            ]\n",
    "        },\n",
    "        goto=\"supervisor\",  # Return to supervisor node\n",
    "    )\n",
    "\n",
    "builder = StateGraph(MessagesState)\n",
    "builder.add_node(\"supervisor\", supervisor_node)\n",
    "builder.add_edge(START, \"supervisor\")\n",
    "builder.add_node(\"researcher\", research_node)\n",
    "graph = builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Combining Several Graphs into a Supergraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/MindzKonnectedAI/SkillSync/tree/gaurav \n",
    "\n",
    "# Step 1: Export Compiled Graphs\n",
    "\n",
    "    # github_graph.py\n",
    "    def github_team_supervisor():\n",
    "        # Build and compile the GitHub graph\n",
    "        github_graph = StateGraph(GithubTeamState)\n",
    "        github_graph.add_node(\"query_param_generator\", query_param_node)\n",
    "        github_graph.add_node(\"fetch_users\", fetch_node)\n",
    "        github_graph.add_node(\"supervisor\", supervisor_agent)\n",
    "        github_graph.add_edge(START, \"query_param_generator\")\n",
    "        github_graph.add_edge(\"query_param_generator\", \"fetch_users\")\n",
    "        github_graph.add_conditional_edges(\"fetch_users\", condition)\n",
    "        github_graph.add_edge(\"supervisor\", END)\n",
    "        return github_graph.compile()\n",
    "\n",
    "    # sql_graph.py\n",
    "    def sql_agent_team_supervisor():\n",
    "        # Build and compile the SQL graph\n",
    "        builder = StateGraph(State)\n",
    "        builder.add_node(\"assistant\", Assistant(assistant_runnable))\n",
    "        builder.add_node(\"tools\", create_tool_node_with_fallback(tools))\n",
    "        builder.set_entry_point(\"assistant\")\n",
    "        builder.add_conditional_edges(\"assistant\", tools_condition, {\"tools\": \"tools\", END: END})\n",
    "        builder.add_edge(\"tools\", \"assistant\")\n",
    "        return builder.compile()\n",
    "\n",
    "\n",
    "# Step 2: Import Graphs into the Supergraph\n",
    "\n",
    "    # app.py\n",
    "    from github_graph import github_team_supervisor\n",
    "    from sql_graph import sql_agent_team_supervisor\n",
    "\n",
    "    # Import the compiled graphs\n",
    "    github_chain = github_team_supervisor()\n",
    "    sql_chain = sql_agent_team_supervisor()\n",
    "\n",
    "\n",
    "# Step 3: Define the Supergraph\n",
    "\n",
    "    from langgraph.graph import StateGraph, START, END\n",
    "\n",
    "    # Define the supergraph state\n",
    "    class SuperState(TypedDict):\n",
    "        messages: Annotated[List[BaseMessage], operator.add]\n",
    "        next: str\n",
    "\n",
    "    # Initialize the supergraph\n",
    "    super_graph = StateGraph(SuperState)\n",
    "\n",
    "    # Add the imported graphs as nodes\n",
    "    super_graph.add_node(\"GithubTeam\", github_chain)\n",
    "    super_graph.add_node(\"SqlTeam\", sql_chain)\n",
    "\n",
    "    # Add a supervisor node to manage the flow\n",
    "    super_graph.add_node(\"super_supervisor\", supervisor_node)\n",
    "\n",
    "    # Define edges\n",
    "    super_graph.add_edge(START, \"super_supervisor\")\n",
    "    super_graph.add_edge(\"GithubTeam\", \"super_supervisor\")\n",
    "    super_graph.add_edge(\"SqlTeam\", \"super_supervisor\")\n",
    "\n",
    "    # Add conditional edges\n",
    "    super_graph.add_conditional_edges(\n",
    "        \"super_supervisor\",\n",
    "        lambda x: x[\"next\"],\n",
    "        {\n",
    "            \"GithubTeam\": \"GithubTeam\",\n",
    "            \"SqlTeam\": \"SqlTeam\",\n",
    "            \"FINISH\": END,\n",
    "        },\n",
    "    )\n",
    "\n",
    "    # Compile the supergraph\n",
    "    super_graph = super_graph.compile()\n",
    "\n",
    "\n",
    "\n",
    "# Step 4: Run the Supergraph\n",
    "\n",
    "    # Example user input\n",
    "    user_input = \"Find GitHub users in India and query their data from the SQL database.\"\n",
    "\n",
    "    # Invoke the supergraph\n",
    "    result = super_graph.invoke(input={\"messages\": [HumanMessage(content=user_input)]})\n",
    "    print(result[\"messages\"][-1].content)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "app_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
