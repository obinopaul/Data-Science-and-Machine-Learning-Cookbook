{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpful Code for Training Models in Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install evaluate, rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "import accelerate   # for distributed training\n",
    "from diffusers import StableDiffusionPipeline\n",
    "import evaluate     # custom evaluation script\n",
    "import torchmetrics\n",
    "import arxiv  \n",
    "\n",
    "# Huggingface Transformers\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    Trainer, \n",
    "    TrainingArguments,\n",
    "    DataCollatorForSeq2Seq\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the search query\n",
    "search_text = \"Deep Learning for Ageing Research\" \n",
    "\n",
    "# Search for papers on arXiv\n",
    "search = arxiv.Search(query=search_text, max_results=50, sort_by=arxiv.SortCriterion.Relevance)\n",
    "\n",
    "# Collect the results\n",
    "result_list = []\n",
    "for result in search.results():\n",
    "    result_list.append({\n",
    "        \"title\": result.title,\n",
    "        \"published\": result.published,\n",
    "        \"abstract\": result.summary,\n",
    "        \"url\": result.pdf_url,\n",
    "        \"categories\": result.categories\n",
    "    })\n",
    "\n",
    "# Save the results to a JSON file (optional)\n",
    "with open('arxiv_papers.json', 'w') as f:\n",
    "    json.dump(result_list, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of dictionaries with 'document' and 'summary' keys\n",
    "# For demonstration, we'll use the abstract as both the document and the summary\n",
    "# In practice, you'd want a more meaningful summary\n",
    "train_data = []\n",
    "for paper in result_list:\n",
    "    train_data.append({\n",
    "        \"document\": paper[\"abstract\"],\n",
    "        \"summary\": paper[\"abstract\"]  # Replace with actual summaries if available\n",
    "    })\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "dataset = Dataset.from_pandas(pd.DataFrame(train_data))\n",
    "\n",
    "# Split the dataset into training and evaluation sets\n",
    "split_dataset = dataset.train_test_split(test_size=0.1)\n",
    "train_dataset = split_dataset['train']\n",
    "eval_dataset = split_dataset['test']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_input_length = 512\n",
    "max_target_length = 128\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"document\"], \n",
    "        max_length=max_input_length, \n",
    "        truncation=True\n",
    "    )\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"summary\"], \n",
    "            max_length=max_target_length, \n",
    "            truncation=True\n",
    "        )\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "# Apply the tokenization\n",
    "tokenized_train = train_dataset.map(tokenize_function, batched=True)\n",
    "tokenized_eval = eval_dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Data Collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a data collator to dynamically pad the inputs during training:\n",
    "# it loads the data from the dataset and pads it to the maximum length of the samples\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=AutoModelForSeq2SeqLM.from_pretrained(model_name), \n",
    "    padding=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Define Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models\",\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=3,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    save_steps=500,\n",
    "    eval_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU is available\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    "    greater_is_better=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Initialize the Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
    "\n",
    "\n",
    "rouge = evaluate.load(\"rouge\") # Load evaluation metric\n",
    "\n",
    "# rouge = torchmetrics.text.ROUGEScore()  # Initialize TorchMetrics ROUGE\n",
    "\n",
    "\n",
    "# Define a compute_metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them\n",
    "    labels = [[(label if label != -100 else tokenizer.pad_token_id) for label in doc] for doc in labels]\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    #------------------------------- ROUGE Score (using evaluate) -------------------------------#\n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(tokenizer.tokenize(pred)) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(tokenizer.tokenize(label)) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract the median scores\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    #--------------------------------------------------------------------------#\n",
    "    #------------------------------- ROUGE Score (using TorchMetrics) -------------------------------#\n",
    "    # # Update the ROUGE metric\n",
    "    # rouge.reset()  # Reset metrics to ensure no accumulation from previous evaluations\n",
    "    # rouge.update(predictions=decoded_preds, references=decoded_labels)\n",
    "    # rouge_scores = rouge.compute()\n",
    "    \n",
    "    # # Extract the scores\n",
    "    # result = {\n",
    "    #     \"rouge1\": rouge_scores[\"rouge1\"].mid.fmeasure * 100,\n",
    "    #     \"rouge2\": rouge_scores[\"rouge2\"].mid.fmeasure * 100,\n",
    "    #     \"rougeL\": rouge_scores[\"rougeL\"].mid.fmeasure * 100,\n",
    "    # }\n",
    "    #--------------------------------------------------------------------------#\n",
    "    return result\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_eval,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Train and Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training\n",
    "trainer.train()\n",
    "\n",
    "# Evaluate the model\n",
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"model-saved\")  # Save the model\n",
    "tokenizer.save_pretrained(\"tokenizer-saved\")  # Save the tokenizer\n",
    "\n",
    "\n",
    "# Load the saved model and tokenizer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"model-saved\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"tokenizer-saved\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Visualizing AI Generated Images, Audio, and Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "a bytes-like object is required, not 'NoneType'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\pault\\anaconda3\\Lib\\site-packages\\IPython\\core\\formatters.py:344\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[1;34m(self, obj)\u001b[0m\n\u001b[0;32m    342\u001b[0m     method \u001b[38;5;241m=\u001b[39m get_real_method(obj, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_method)\n\u001b[0;32m    343\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 344\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m method()\n\u001b[0;32m    345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\pault\\anaconda3\\Lib\\site-packages\\IPython\\core\\display.py:1201\u001b[0m, in \u001b[0;36mVideo._repr_html_\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1199\u001b[0m            b64_video \u001b[38;5;241m=\u001b[39m video\n\u001b[0;32m   1200\u001b[0m        \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1201\u001b[0m            b64_video \u001b[38;5;241m=\u001b[39m b2a_base64(video, newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mascii\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mrstrip()\n\u001b[0;32m   1203\u001b[0m        output \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124m<video \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m>\u001b[39m\n\u001b[0;32m   1204\u001b[0m \u001b[38;5;124m<source src=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata:\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m;base64,\u001b[39m\u001b[38;5;132;01m{4}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m type=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\n\u001b[0;32m   1205\u001b[0m \u001b[38;5;124mYour browser does not support the video tag.\u001b[39m\n\u001b[0;32m   1206\u001b[0m \u001b[38;5;124m</video>\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhtml_attributes, width, height, mimetype, b64_video)\n\u001b[0;32m   1207\u001b[0m        \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[1;31mTypeError\u001b[0m: a bytes-like object is required, not 'NoneType'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Video object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown, Audio, Image, Video\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import librosa.display\n",
    "import cv2\n",
    "\n",
    "# Display Generated Text (Assuming you have the text)\n",
    "generated_text = \"Once upon a time, in a land far away...\"\n",
    "display(Markdown(f\"### **Generated Text:**\\n{generated_text}\"))\n",
    "\n",
    "# Display Generated Image\n",
    "generated_image_path = \"generated_image.png\"\n",
    "display(Image(filename=generated_image_path, width=400, height=300))\n",
    "\n",
    "# Display Generated Audio\n",
    "generated_audio_path = \"generated_audio.mp3\"\n",
    "display(Audio(filename=generated_audio_path, autoplay=False))\n",
    "\n",
    "# Display Audio Waveform\n",
    "y, sr = librosa.load(generated_audio_path)\n",
    "plt.figure(figsize=(14, 5))\n",
    "librosa.display.waveshow(y, sr=sr)\n",
    "plt.title(\"AI-Generated Audio Waveform\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n",
    "\n",
    "# Display Generated Video\n",
    "generated_video_path = \"generated_video.mp4\"\n",
    "display(Video(filename=generated_video_path, embed=True, width=640, height=480))\n",
    "\n",
    "# Display First Frame of Video\n",
    "cap = cv2.VideoCapture(generated_video_path)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "if ret:\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(frame)\n",
    "    plt.axis('off')\n",
    "    plt.title(\"First Frame of the AI-Generated Video\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"Failed to read the video.\")\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <audio  controls=\"controls\" autoplay=\"autoplay\">\n",
       "                    <source src=\"https://www.soundjay.com/ambient/sounds/boarding-accouncement-1.mp3\" type=\"audio/mpeg\" />\n",
       "                    Your browser does not support the audio element.\n",
       "                </audio>\n",
       "              "
      ],
      "text/plain": [
       "<IPython.lib.display.Audio object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Audio, display\n",
    "\n",
    "# Play audio from a URL\n",
    "audio_url = \"https://www.soundjay.com/ambient/sounds/boarding-accouncement-1.mp3\"\n",
    "display(Audio(url=audio_url, autoplay=True))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Parameter-Efficient Fine-Tuning with LoRA and Other Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter-Efficient Fine-Tuning Methods for LLMs\n",
    "# ================================================\n",
    "\n",
    "# LoRA and Other PEFT Methods - The first example demonstrates several Parameter-Efficient Fine-Tuning techniques including:\n",
    "    # LoRA (Low-Rank Adaptation) which adds small trainable rank decomposition matrices\n",
    "        # LoRA is a parameter-efficient fine-tuning method that introduces\n",
    "        # new trainable parameters to modify a model's behavior without\n",
    "        # increasing its overall size. By doing so, LoRA maintains the original\n",
    "        # parameter count, reducing the memory overhead typically associated\n",
    "        # with training large models. It works by adding low-rank matrix\n",
    "        # adaptations to the model's existing layers, allowing for significant\n",
    "        # performance improvements while keeping resource consumption in\n",
    "        # check.\n",
    "    # Prefix Tuning which prepends trainable vectors to intermediate activations\n",
    "    # Prompt Tuning which adds trainable tokens to the input\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    get_peft_model,\n",
    "    LoraConfig,\n",
    "    TaskType,\n",
    "    PrefixTuningConfig,\n",
    "    PromptTuningConfig,\n",
    "    PromptTuningInit,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Configuration\n",
    "BASE_MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"  # Example foundation model\n",
    "DATASET_NAME = \"your_dataset\"  # Replace with your dataset\n",
    "OUTPUT_DIR = \"./peft_output\"\n",
    "PEFT_METHOD = \"lora\"  # Options: \"lora\", \"prefix_tuning\", \"prompt_tuning\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Ensure pad token is set\n",
    "\n",
    "# Set up tokenizer for proper truncation\n",
    "tokenizer.model_max_length = 1024  # Adjust based on your GPU memory\n",
    "\n",
    "print(\"Loading base model...\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_NAME,\n",
    "    torch_dtype=torch.float16,  # Use half precision to save memory\n",
    "    device_map=\"auto\"  # Automatically distribute model across available GPUs\n",
    ")\n",
    "\n",
    "# 1. LoRA Configuration \n",
    "# Low-Rank Adaptation - adds trainable rank decomposition matrices to existing weights\n",
    "if PEFT_METHOD == \"lora\":\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,                     # Rank of update matrices\n",
    "        lora_alpha=32,           # Parameter scaling factor\n",
    "        lora_dropout=0.1,        # Dropout probability for LoRA layers\n",
    "        target_modules=[\"q_proj\", \"v_proj\"]  # Which modules to apply LoRA to\n",
    "    )\n",
    "\n",
    "# 2. Prefix Tuning Configuration\n",
    "# Adds trainable continuous prefix vectors to activations\n",
    "elif PEFT_METHOD == \"prefix_tuning\":\n",
    "    peft_config = PrefixTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        num_virtual_tokens=20,   # Number of virtual tokens to add\n",
    "        prefix_projection=True   # Whether to use a two-layer MLP for reparameterization\n",
    "    )\n",
    "\n",
    "# 3. Prompt Tuning Configuration\n",
    "# Adds trainable soft prompt embeddings to the input\n",
    "elif PEFT_METHOD == \"prompt_tuning\":\n",
    "    peft_config = PromptTuningConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        num_virtual_tokens=20,   # Number of virtual prompt tokens to add\n",
    "        prompt_tuning_init=PromptTuningInit.TEXT,  # Initialize from text\n",
    "        prompt_tuning_init_text=\"Solve the following task: \"  # Text to initialize from\n",
    "    )\n",
    "\n",
    "else:\n",
    "    raise ValueError(f\"PEFT method {PEFT_METHOD} not supported\")\n",
    "\n",
    "# Apply the PEFT method to the model\n",
    "print(f\"Applying {PEFT_METHOD} configuration...\")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # Print number of trainable parameters\n",
    "\n",
    "# Load and prepare dataset\n",
    "# Replace this with your own dataset loading logic\n",
    "def load_custom_dataset():\n",
    "    # Example: loading a dataset from Hugging Face\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        return dataset\n",
    "    except:\n",
    "        # Placeholder for custom dataset loading\n",
    "        print(\"Replace this with your custom dataset loading code\")\n",
    "        # Example structure for a toy dataset\n",
    "        return {\n",
    "            \"train\": [{\"text\": \"Example instruction. Example response.\"}],\n",
    "            \"validation\": [{\"text\": \"Example validation instruction. Example validation response.\"}]\n",
    "        }\n",
    "\n",
    "# Format data for instruction fine-tuning\n",
    "def format_instruction(example):\n",
    "    # Adapt this to your specific dataset format\n",
    "    formatted_text = f\"### Instruction: {example['instruction']}\\n### Response: {example['response']}\"\n",
    "    return {\"text\": formatted_text}\n",
    "\n",
    "# Load and process dataset\n",
    "print(\"Loading and processing dataset...\")\n",
    "dataset = load_custom_dataset()\n",
    "\n",
    "# Apply formatting and tokenization\n",
    "def tokenize_function(examples):\n",
    "    # Tokenize the texts\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=tokenizer.model_max_length\n",
    "    )\n",
    "\n",
    "# Process dataset if it's in the expected Hugging Face format\n",
    "if hasattr(dataset, \"map\"):\n",
    "    # Format to instruction style if needed\n",
    "    if \"instruction\" in dataset[\"train\"].column_names:\n",
    "        dataset = dataset.map(format_instruction)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokenized_dataset = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=dataset[\"train\"].column_names\n",
    "    )\n",
    "else:\n",
    "    print(\"Using placeholder dataset - replace with your data\")\n",
    "    # Create a minimal example for demonstration\n",
    "    tokenized_dataset = {\n",
    "        \"train\": [{\"input_ids\": tokenizer(\"Example text for training.\").input_ids, \"attention_mask\": [1] * 10}],\n",
    "        \"validation\": [{\"input_ids\": tokenizer(\"Example text for validation.\").input_ids, \"attention_mask\": [1] * 10}]\n",
    "    }\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_train_batch_size=4,  # Adjust based on your GPU memory\n",
    "    per_device_eval_batch_size=4,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.01,\n",
    "    warmup_steps=0,\n",
    "    logging_steps=100,\n",
    "    num_train_epochs=3,\n",
    "    gradient_accumulation_steps=4,  # Increase to simulate larger batch sizes\n",
    "    fp16=True,  # Mixed precision training\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=True,\n",
    ")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False  # Not using masked language modeling\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "print(\"Setting up trainer...\")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"] if hasattr(dataset, \"map\") else tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"] if hasattr(dataset, \"map\") else tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting training...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# --- Using the fine-tuned model for inference ---\n",
    "def load_finetuned_model(model_path):\n",
    "    # Load the base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load the PEFT adapter\n",
    "    model = PeftModel.from_pretrained(base_model, model_path)\n",
    "    return model, tokenizer\n",
    "\n",
    "# Example inference\n",
    "def generate_text(model, tokenizer, prompt, max_length=100):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Example usage\n",
    "model, tokenizer = load_finetuned_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "prompt = \"Write a summary of recent advancements in artificial intelligence.\"\n",
    "generated_text = generate_text(model, tokenizer, prompt)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> QLoRA Fine-Tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QLoRA Fine-Tuning Implementation\n",
    "# ===============================\n",
    "# QLoRA combines quantization with LoRA for even more efficient fine-tuning\n",
    "\n",
    "# QLoRA Implementation - The second example shows QLoRA, which combines:\n",
    "    # 4-bit quantization to dramatically reduce memory requirements\n",
    "    # LoRA adapters for parameter-efficient training\n",
    "    # This approach allows fine-tuning of much larger models on consumer hardware\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM, \n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments, \n",
    "    Trainer, \n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import (\n",
    "    prepare_model_for_kbit_training,\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    PeftModel\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "DATASET_NAME = \"your-dataset\"  # Replace with your dataset\n",
    "OUTPUT_DIR = \"./qlora_output\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Setup 4-bit quantization configuration\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,               # Load model in 4-bit precision\n",
    "    bnb_4bit_use_double_quant=True,  # Use double quantization for 4-bit\n",
    "    bnb_4bit_quant_type=\"nf4\",       # Normalized float 4-bit quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16  # Compute in bfloat16\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\"  # Automatically distribute model across available GPUs\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = tokenizer.eos_token  # Set pad token\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "# This addresses some issues with quantized training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Define LoRA configuration\n",
    "peft_config = LoraConfig(\n",
    "    r=8,                     # Rank of update matrices\n",
    "    lora_alpha=32,           # Parameter scaling factor\n",
    "    lora_dropout=0.1,        # Dropout probability\n",
    "    # Target attention modules that match these patterns\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"v_proj\",\n",
    "        \"k_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\"\n",
    "    ],\n",
    "    bias=\"none\",              # Don't train bias parameters\n",
    "    task_type=\"CAUSAL_LM\"     # Task type\n",
    ")\n",
    "\n",
    "# Apply LoRA adapters to the model\n",
    "model = get_peft_model(model, peft_config)\n",
    "\n",
    "# Print trainable parameters info\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load and prepare dataset\n",
    "# This is a placeholder - replace with your actual dataset loading code\n",
    "def load_and_prepare_dataset():\n",
    "    # Example - using Hugging Face datasets\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            # Format instruction-response pairs if needed\n",
    "            # This is a simple example - adjust for your specific format\n",
    "            if \"instruction\" in examples and \"response\" in examples:\n",
    "                texts = [f\"### Instruction: {i}\\n### Response: {r}\" \n",
    "                        for i, r in zip(examples[\"instruction\"], examples[\"response\"])]\n",
    "            else:\n",
    "                texts = examples[\"text\"]\n",
    "            \n",
    "            return tokenizer(\n",
    "                texts,\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512  # Adjust based on your needs\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset[\"train\"].column_names \n",
    "                           if col not in [\"input_ids\", \"attention_mask\"]]\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Return a minimal example dataset for demonstration\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create random token IDs as an example\n",
    "        sample_size = 100\n",
    "        sample_length = 128\n",
    "        \n",
    "        # Create sample input IDs and attention mask\n",
    "        inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        for _ in range(sample_size):\n",
    "            input_ids = [tokenizer.bos_token_id] + np.random.randint(\n",
    "                100, 10000, sample_length - 2\n",
    "            ).tolist() + [tokenizer.eos_token_id]\n",
    "            attention_mask = [1] * sample_length\n",
    "            \n",
    "            inputs[\"input_ids\"].append(input_ids)\n",
    "            inputs[\"attention_mask\"].append(attention_mask)\n",
    "        \n",
    "        # Create HF datasets\n",
    "        train_dataset = Dataset.from_dict(inputs)\n",
    "        eval_dataset = Dataset.from_dict({k: v[:10] for k, v in inputs.items()})\n",
    "        \n",
    "        return {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,  # Can be smaller due to 4-bit quantization\n",
    "    gradient_accumulation_steps=8,  # Accumulate gradients to simulate larger batch\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,  # Mixed precision\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=True,\n",
    "    push_to_hub=False,  # Set to True if you want to push to HF Hub\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train model\n",
    "print(\"Starting QLoRA fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model adapters\n",
    "model.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Optionally save the merged model (base + adapters)\n",
    "# Note: This requires more memory as it creates a full copy of the model\n",
    "def save_merged_model():\n",
    "    # Load the base model\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.float16  # Use half-precision to save memory\n",
    "    )\n",
    "    \n",
    "    # Load the adapters into the base model\n",
    "    model = PeftModel.from_pretrained(base_model, os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Merge adapters with the base model\n",
    "    merged_model = model.merge_and_unload()\n",
    "    \n",
    "    # Save the merged model\n",
    "    merged_model.save_pretrained(os.path.join(OUTPUT_DIR, \"merged_model\"))\n",
    "    tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"merged_model\"))\n",
    "    \n",
    "    print(f\"Merged model saved to {os.path.join(OUTPUT_DIR, 'merged_model')}\")\n",
    "\n",
    "# Uncomment to save the merged model\n",
    "# print(\"Merging and saving the full model (requires more memory)...\")\n",
    "# save_merged_model()\n",
    "\n",
    "# Inference example\n",
    "def inference_example():\n",
    "    # Load fine-tuned model with adapters\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    model = PeftModel.from_pretrained(base_model, os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"### Instruction: Explain quantum computing in simple terms.\\n### Response:\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "\n",
    "# Uncomment to run an inference example\n",
    "# print(\"Running inference with the fine-tuned model...\")\n",
    "# inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Adapter-Based Fine-Tuning with Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapter-Based Fine-Tuning with Transformers\n",
    "# ===========================================\n",
    "# This script demonstrates using adapter-based fine-tuning with the adapters library\n",
    "\n",
    "# Adapter-Based Fine-Tuning - The third example implements:\n",
    "    # Traditional adapter approaches (Pfeiffer, Houlsby)\n",
    "    # IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)\n",
    "    # Adapter composition for combining multiple fine-tuning adaptations\n",
    "\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from transformers.adapters import (\n",
    "    AdapterConfig,\n",
    "    PfeifferConfig,\n",
    "    HoulsbyConfig,\n",
    "    IA3Config\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # Using a smaller model for demonstration\n",
    "DATASET_NAME = \"your-dataset\"  # Replace with actual dataset\n",
    "OUTPUT_DIR = \"./adapters_output\"\n",
    "ADAPTER_TYPE = \"pfeiffer\"  # Options: \"pfeiffer\", \"houlsby\", \"ia3\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Configure adapters\n",
    "if ADAPTER_TYPE == \"pfeiffer\":\n",
    "    # Pfeiffer adapters - add bottleneck adapters after attention block\n",
    "    adapter_config = PfeifferConfig(\n",
    "        reduction_factor=16,   # Size reduction for adapter bottleneck\n",
    "        non_linearity=\"relu\"   # Activation function\n",
    "    )\n",
    "elif ADAPTER_TYPE == \"houlsby\":\n",
    "    # Houlsby adapters - add adapters both after attention and feed-forward\n",
    "    adapter_config = HoulsbyConfig(\n",
    "        reduction_factor=16,\n",
    "        non_linearity=\"relu\"\n",
    "    )\n",
    "elif ADAPTER_TYPE == \"ia3\":\n",
    "    # IA³ (Infused Adapter by Inhibiting and Amplifying Inner Activations)\n",
    "    adapter_config = IA3Config()\n",
    "else:\n",
    "    raise ValueError(f\"Adapter type {ADAPTER_TYPE} not supported.\")\n",
    "\n",
    "# Add adapter to model\n",
    "adapter_name = \"custom_task_adapter\"\n",
    "model.add_adapter(adapter_name, config=adapter_config)\n",
    "\n",
    "# Activate adapter for training\n",
    "model.train_adapter(adapter_name)\n",
    "\n",
    "# Print number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} ({trainable_params/all_params:.2%})\")\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_and_prepare_dataset():\n",
    "    # Example - using Hugging Face datasets\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            # Format based on your dataset structure\n",
    "            # This example assumes 'text' field in the dataset\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512  # Adjust based on your needs\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset[\"train\"].column_names \n",
    "                           if col not in [\"input_ids\", \"attention_mask\"]]\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Return a minimal example dataset for demonstration\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        \n",
    "        # Create random token IDs as an example\n",
    "        sample_size = 100\n",
    "        sample_length = 128\n",
    "        \n",
    "        # Create sample input IDs and attention mask\n",
    "        inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        for _ in range(sample_size):\n",
    "            input_ids = [tokenizer.bos_token_id] + np.random.randint(\n",
    "                100, 10000, sample_length - 2\n",
    "            ).tolist() + [tokenizer.eos_token_id]\n",
    "            attention_mask = [1] * sample_length\n",
    "            \n",
    "            inputs[\"input_ids\"].append(input_ids)\n",
    "            inputs[\"attention_mask\"].append(attention_mask)\n",
    "        \n",
    "        # Create HF datasets\n",
    "        train_dataset = Dataset.from_dict(inputs)\n",
    "        eval_dataset = Dataset.from_dict({k: v[:10] for k, v in inputs.items()})\n",
    "        \n",
    "        return {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(f\"Starting adapter-based fine-tuning with {ADAPTER_TYPE} adapters...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the adapter\n",
    "model.save_adapter(os.path.join(OUTPUT_DIR, adapter_name), adapter_name)\n",
    "\n",
    "print(f\"Adapter saved to {os.path.join(OUTPUT_DIR, adapter_name)}\")\n",
    "\n",
    "# Demonstrate adapter inference\n",
    "def inference_with_adapter():\n",
    "    # Load the pre-trained model\n",
    "    inference_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Load the fine-tuned adapter\n",
    "    inference_model.load_adapter(os.path.join(OUTPUT_DIR, adapter_name))\n",
    "    \n",
    "    # Activate the adapter for inference\n",
    "    inference_model.set_active_adapters(adapter_name)\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"The future of artificial intelligence involves\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = inference_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "    \n",
    "    # Compare with base model (no adapter)\n",
    "    inference_model.set_active_adapters(None)  # Deactivate adapter\n",
    "    \n",
    "    outputs_base = inference_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response_base = tokenizer.decode(outputs_base[0], skip_special_tokens=True)\n",
    "    print(f\"Base model response: {response_base}\")\n",
    "\n",
    "# Uncomment to test inference\n",
    "# print(\"\\nTesting inference with the fine-tuned adapter...\")\n",
    "# inference_with_adapter()\n",
    "\n",
    "# Example of adapter composition (combining multiple adapters)\n",
    "def adapter_composition_example():\n",
    "    # This demonstrates how to combine multiple adapters for different tasks\n",
    "    \n",
    "    # Load the model\n",
    "    model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "    \n",
    "    # Add and train first adapter (e.g., for domain adaptation)\n",
    "    domain_adapter = \"domain_adapter\"\n",
    "    model.add_adapter(domain_adapter, config=PfeifferConfig(reduction_factor=16))\n",
    "    # Train domain adapter (simplified)\n",
    "    model.train_adapter(domain_adapter)\n",
    "    # ... training code here ...\n",
    "    model.save_adapter(os.path.join(OUTPUT_DIR, domain_adapter), domain_adapter)\n",
    "    \n",
    "    # Add and train second adapter (e.g., for task-specific adaptation)\n",
    "    task_adapter = \"task_adapter\"\n",
    "    model.add_adapter(task_adapter, config=PfeifferConfig(reduction_factor=16))\n",
    "    # Train task adapter with domain adapter frozen\n",
    "    model.train_adapter(task_adapter)\n",
    "    # ... training code here ...\n",
    "    model.save_adapter(os.path.join(OUTPUT_DIR, task_adapter), task_adapter)\n",
    "    \n",
    "    # Stack adapters for inference\n",
    "    model.set_active_adapters([domain_adapter, task_adapter])\n",
    "    \n",
    "    # Generate with stacked adapters\n",
    "    prompt = \"The new technology enables\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100\n",
    "    )\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"Stacked adapters response: {response}\")\n",
    "\n",
    "# Note: This function is just an example and won't run properly without multiple trained adapters\n",
    "# print(\"\\nAdapter composition example (conceptual)...\")\n",
    "# adapter_composition_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> BitFit Fine-Tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BitFit Fine-Tuning Implementation\n",
    "# ================================\n",
    "# BitFit only trains the bias terms in a pre-trained model, keeping all weights frozen\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # Using a smaller model for demonstration\n",
    "DATASET_NAME = \"your-dataset\"  # Replace with your actual dataset\n",
    "OUTPUT_DIR = \"./bitfit_output\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Freeze all parameters (weights)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze only the bias terms\n",
    "for name, param in model.named_parameters():\n",
    "    if \"bias\" in name:\n",
    "        param.requires_grad = True\n",
    "\n",
    "# Print number of trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "all_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params} ({trainable_params/all_params:.2%} of all parameters)\")\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_and_prepare_dataset():\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset[\"train\"].column_names \n",
    "                           if col not in [\"input_ids\", \"attention_mask\"]]\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Create a minimal example dataset\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        \n",
    "        # Sample data\n",
    "        sample_size = 100\n",
    "        sample_length = 128\n",
    "        \n",
    "        # Create sample input IDs and attention mask\n",
    "        inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        for _ in range(sample_size):\n",
    "            input_ids = [tokenizer.bos_token_id] + np.random.randint(\n",
    "                100, 10000, sample_length - 2\n",
    "            ).tolist() + [tokenizer.eos_token_id]\n",
    "            attention_mask = [1] * sample_length\n",
    "            \n",
    "            inputs[\"input_ids\"].append(input_ids)\n",
    "            inputs[\"attention_mask\"].append(attention_mask)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict(inputs)\n",
    "        eval_dataset = Dataset.from_dict({k: v[:10] for k, v in inputs.items()})\n",
    "        \n",
    "        return {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-4,  # Can use higher learning rates with BitFit\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting BitFit fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving BitFit fine-tuned model...\")\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Inference example\n",
    "def inference_example():\n",
    "    # Load the fine-tuned model\n",
    "    model = AutoModelForCausalLM.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"The future of technology will be shaped by\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "\n",
    "# Uncomment to run an inference example\n",
    "# print(\"Running inference with the BitFit fine-tuned model...\")\n",
    "# inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Selective Layer Fine-Tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selective Layer Fine-Tuning Implementation\n",
    "# =========================================\n",
    "# This script demonstrates fine-tuning only specific layers of a pre-trained model\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # Using a smaller model for demonstration\n",
    "DATASET_NAME = \"your-dataset\"  # Replace with your actual dataset\n",
    "OUTPUT_DIR = \"./selective_layer_output\"\n",
    "\n",
    "# Fine-tuning options\n",
    "NUM_LAYERS_TO_FREEZE = 8  # Number of layers to freeze from the bottom\n",
    "# Or for top layers only:\n",
    "# NUM_LAYERS_TO_UNFREEZE = 4  # Number of top layers to fine-tune\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Print model structure for inspection\n",
    "print(\"Model architecture:\")\n",
    "for name, _ in model.named_modules():\n",
    "    print(name)\n",
    "\n",
    "# Helper function to freeze bottom layers\n",
    "def freeze_bottom_layers(model, num_layers_to_freeze):\n",
    "    \"\"\"Freeze the bottom layers of the model, keeping top layers trainable\"\"\"\n",
    "    # This is specifically for GPT-2 architecture,\n",
    "    # adapt this for other model architectures as needed\n",
    "    \n",
    "    # Freeze embeddings\n",
    "    for param in model.transformer.wte.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for param in model.transformer.wpe.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Freeze the specified number of layers from the bottom\n",
    "    for i in range(num_layers_to_freeze):\n",
    "        for param in model.transformer.h[i].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Verify which layers are frozen/unfrozen\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        trainable = any(param.requires_grad for param in layer.parameters())\n",
    "        status = \"Trainable\" if trainable else \"Frozen\"\n",
    "        print(f\"Layer {i}: {status}\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params} ({trainable_params/all_params:.2%} of all parameters)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Alternative: Helper function to only unfreeze top layers\n",
    "def unfreeze_top_layers(model, num_layers_to_unfreeze):\n",
    "    \"\"\"Freeze all layers except the top N layers\"\"\"\n",
    "    \n",
    "    # First, freeze everything\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # Then unfreeze the top layers\n",
    "    total_layers = len(model.transformer.h)\n",
    "    for i in range(total_layers - num_layers_to_unfreeze, total_layers):\n",
    "        for param in model.transformer.h[i].parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Also unfreeze the final layer norm and output layers\n",
    "    for param in model.transformer.ln_f.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    # For causal language models, the output layer is often tied to the input embeddings\n",
    "    # Check if the model has a separate output layer\n",
    "    if hasattr(model, 'lm_head') and not model.config.tie_word_embeddings:\n",
    "        for param in model.lm_head.parameters():\n",
    "            param.requires_grad = True\n",
    "    \n",
    "    # Verify which layers are frozen/unfrozen\n",
    "    for i, layer in enumerate(model.transformer.h):\n",
    "        trainable = any(param.requires_grad for param in layer.parameters())\n",
    "        status = \"Trainable\" if trainable else \"Frozen\"\n",
    "        print(f\"Layer {i}: {status}\")\n",
    "    \n",
    "    # Count trainable parameters\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Trainable parameters: {trainable_params} ({trainable_params/all_params:.2%} of all parameters)\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Apply selective layer freezing\n",
    "print(f\"Freezing bottom {NUM_LAYERS_TO_FREEZE} layers...\")\n",
    "model = freeze_bottom_layers(model, NUM_LAYERS_TO_FREEZE)\n",
    "\n",
    "# If you want to use the alternative approach, uncomment this:\n",
    "# print(f\"Unfreezing only the top {NUM_LAYERS_TO_UNFREEZE} layers...\")\n",
    "# model = unfreeze_top_layers(model, NUM_LAYERS_TO_UNFREEZE)\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_and_prepare_dataset():\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset[\"train\"].column_names \n",
    "                           if col not in [\"input_ids\", \"attention_mask\"]]\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Create a minimal example dataset\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        \n",
    "        # Sample data\n",
    "        sample_size = 100\n",
    "        sample_length = 128\n",
    "        \n",
    "        # Create sample input IDs and attention mask\n",
    "        inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        for _ in range(sample_size):\n",
    "            input_ids = [tokenizer.bos_token_id] + np.random.randint(\n",
    "                100, 10000, sample_length - 2\n",
    "            ).tolist() + [tokenizer.eos_token_id]\n",
    "            attention_mask = [1] * sample_length\n",
    "            \n",
    "            inputs[\"input_ids\"].append(input_ids)\n",
    "            inputs[\"attention_mask\"].append(attention_mask)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict(inputs)\n",
    "        eval_dataset = Dataset.from_dict({k: v[:10] for k, v in inputs.items()})\n",
    "        \n",
    "        return {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting selective layer fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving selectively fine-tuned model...\")\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Inference example\n",
    "def inference_example():\n",
    "    # Load the fine-tuned model\n",
    "    model = AutoModelForCausalLM.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"In the coming decades, artificial intelligence will\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "\n",
    "# Uncomment to run an inference example\n",
    "# print(\"Running inference with the selectively fine-tuned model...\")\n",
    "# inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Multi-Task Fine-Tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-Task Fine-Tuning Implementation\n",
    "# ====================================\n",
    "# This script demonstrates fine-tuning a model on multiple tasks simultaneously\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # Using a smaller model for demonstration\n",
    "OUTPUT_DIR = \"./multitask_output\"\n",
    "\n",
    "# Tasks configuration - replace with your actual datasets and task formats\n",
    "TASKS = {\n",
    "    \"summarization\": {\n",
    "        \"dataset\": \"your-summarization-dataset\",  # Replace with actual dataset\n",
    "        \"instruction\": \"Summarize the following text: \",\n",
    "        \"separator\": \"\\nSummary: \"\n",
    "    },\n",
    "    \"sentiment\": {\n",
    "        \"dataset\": \"your-sentiment-dataset\",  # Replace with actual dataset\n",
    "        \"instruction\": \"Analyze the sentiment of the following text: \",\n",
    "        \"separator\": \"\\nSentiment: \"\n",
    "    },\n",
    "    \"qa\": {\n",
    "        \"dataset\": \"your-qa-dataset\",  # Replace with actual dataset\n",
    "        \"instruction\": \"Answer the following question: \",\n",
    "        \"separator\": \"\\nAnswer: \"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Optionally apply LoRA for parameter-efficient fine-tuning\n",
    "USE_LORA = True  # Set to False for full fine-tuning\n",
    "\n",
    "if USE_LORA:\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8,\n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"c_attn\", \"c_proj\"]  # Adapt these target modules to your model architecture\n",
    "    )\n",
    "    \n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "# Function to create a multi-task dataset\n",
    "def create_multitask_dataset():\n",
    "    # This function combines datasets from multiple tasks into a single training dataset\n",
    "    \n",
    "    # For demonstration purposes, we'll create synthetic data\n",
    "    # In a real scenario, you would load actual datasets using load_dataset()\n",
    "    \n",
    "    try:\n",
    "        all_datasets = {}\n",
    "        \n",
    "        for task_name, task_config in TASKS.items():\n",
    "            try:\n",
    "                # Try to load the real dataset (replace this with your actual dataset loading)\n",
    "                # dataset = load_dataset(task_config[\"dataset\"])\n",
    "                \n",
    "                # For demonstration, we'll create a synthetic dataset\n",
    "                print(f\"Creating synthetic data for {task_name} task...\")\n",
    "                \n",
    "                # Create a minimal example dataset\n",
    "                from datasets import Dataset\n",
    "                \n",
    "                # Sample data size\n",
    "                sample_size = 100\n",
    "                \n",
    "                # Generate synthetic data based on task type\n",
    "                if task_name == \"summarization\":\n",
    "                    texts = [\n",
    "                        f\"Long document {i} with lots of information that needs to be summarized. It contains multiple sentences and details about various topics.\" \n",
    "                        for i in range(sample_size)\n",
    "                    ]\n",
    "                    summaries = [f\"Concise summary of document {i}.\" for i in range(sample_size)]\n",
    "                    \n",
    "                    # Format according to instruction template\n",
    "                    formatted_texts = [\n",
    "                        f\"{task_config['instruction']}{text}{task_config['separator']}{summary}\"\n",
    "                        for text, summary in zip(texts, summaries)\n",
    "                    ]\n",
    "                \n",
    "                elif task_name == \"sentiment\":\n",
    "                    texts = [f\"Sample review text {i} expressing an opinion.\" for i in range(sample_size)]\n",
    "                    sentiments = np.random.choice([\"positive\", \"negative\", \"neutral\"], size=sample_size)\n",
    "                    \n",
    "                    # Format according to instruction template\n",
    "                    formatted_texts = [\n",
    "                        f\"{task_config['instruction']}{text}{task_config['separator']}{sentiment}\"\n",
    "                        for text, sentiment in zip(texts, sentiments)\n",
    "                    ]\n",
    "                \n",
    "                elif task_name == \"qa\":\n",
    "                    questions = [f\"Question {i} about a specific topic?\" for i in range(sample_size)]\n",
    "                    answers = [f\"Detailed answer to question {i}.\" for i in range(sample_size)]\n",
    "                    \n",
    "                    # Format according to instruction template\n",
    "                    formatted_texts = [\n",
    "                        f\"{task_config['instruction']}{question}{task_config['separator']}{answer}\"\n",
    "                        for question, answer in zip(questions, answers)\n",
    "                    ]\n",
    "                \n",
    "                # Create dataset dictionary with 'text' field\n",
    "                dataset_dict = {\"text\": formatted_texts}\n",
    "                \n",
    "                # Create Dataset object\n",
    "                dataset = Dataset.from_dict(dataset_dict)\n",
    "                \n",
    "                # Split into train and validation\n",
    "                dataset = dataset.train_test_split(test_size=0.1)\n",
    "                dataset = DatasetDict({\n",
    "                    \"train\": dataset[\"train\"],\n",
    "                    \"validation\": dataset[\"test\"]\n",
    "                })\n",
    "                \n",
    "                all_datasets[task_name] = dataset\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error creating dataset for {task_name}: {e}\")\n",
    "        \n",
    "        # Combine all datasets for training\n",
    "        if all_datasets:\n",
    "            combined_train = concatenate_datasets([ds[\"train\"] for ds in all_datasets.values()])\n",
    "            combined_val = concatenate_datasets([ds[\"validation\"] for ds in all_datasets.values()])\n",
    "            \n",
    "            return DatasetDict({\n",
    "                \"train\": combined_train,\n",
    "                \"validation\": combined_val\n",
    "            })\n",
    "        else:\n",
    "            raise ValueError(\"No datasets could be created\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error creating multitask dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Create a fallback minimal dataset if everything else fails\n",
    "        from datasets import Dataset\n",
    "        \n",
    "        # Sample data\n",
    "        sample_texts = [\n",
    "            \"This is an example for task A.\",\n",
    "            \"This is another example for task B.\",\n",
    "            \"A third example for task C.\"\n",
    "        ]\n",
    "        \n",
    "        # Create dataset\n",
    "        dummy_dataset = Dataset.from_dict({\"text\": sample_texts})\n",
    "        dummy_split = dummy_dataset.train_test_split(test_size=0.2)\n",
    "        \n",
    "        return DatasetDict({\n",
    "            \"train\": dummy_split[\"train\"],\n",
    "            \"validation\": dummy_split[\"test\"]\n",
    "        })\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Load and prepare multitask dataset\n",
    "print(\"Creating and preparing multitask dataset...\")\n",
    "dataset = create_multitask_dataset()\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"text\"]\n",
    ")\n",
    "\n",
    "print(f\"Train dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_dataset['validation'])}\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting multi-task fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving multi-task fine-tuned model...\")\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Inference examples for different tasks\n",
    "def inference_examples():\n",
    "    # Load the fine-tuned model\n",
    "    if USE_LORA:\n",
    "        from peft import PeftModel\n",
    "        \n",
    "        base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "        model = PeftModel.from_pretrained(base_model, os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Test each task\n",
    "    for task_name, task_config in TASKS.items():\n",
    "        print(f\"\\nTesting {task_name} task:\")\n",
    "        \n",
    "        # Create a task-specific prompt\n",
    "        if task_name == \"summarization\":\n",
    "            prompt = f\"{task_config['instruction']}The researchers conducted a comprehensive study on climate change impacts across different regions. They found that coastal areas are particularly vulnerable to rising sea levels, while inland agricultural zones face increased drought risks. The report highlights the need for adaptive strategies tailored to local conditions.{task_config['separator']}\"\n",
    "        \n",
    "        elif task_name == \"sentiment\":\n",
    "            prompt = f\"{task_config['instruction']}The new restaurant had amazing food but the service was extremely slow and the prices were too high for what they offered.{task_config['separator']}\"\n",
    "        \n",
    "        elif task_name == \"qa\":\n",
    "            prompt = f\"{task_config['instruction']}What are the main advantages of transformer-based language models compared to RNNs?{task_config['separator']}\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated response: {response}\")\n",
    "\n",
    "# Uncomment to run inference examples\n",
    "# print(\"\\nRunning inference with the multi-task fine-tuned model...\")\n",
    "# inference_examples()\n",
    "\n",
    "# Example of how to evaluate on specific tasks\n",
    "def evaluate_on_specific_task(task_name):\n",
    "    \"\"\"Evaluate the multi-task model on a specific task\"\"\"\n",
    "    \n",
    "    print(f\"Evaluating model performance on {task_name} task...\")\n",
    "    \n",
    "    # In a real scenario, you would load a task-specific evaluation dataset\n",
    "    # and implement appropriate evaluation metrics\n",
    "    \n",
    "    # Example code (not functional without actual task datasets):\n",
    "    \"\"\"\n",
    "    # Load task-specific test dataset\n",
    "    task_test_dataset = load_dataset(TASKS[task_name][\"dataset\"], split=\"test\")\n",
    "    \n",
    "    # Format according to the task template\n",
    "    def format_for_task(example):\n",
    "        formatted_text = f\"{TASKS[task_name]['instruction']}{example['input']}{TASKS[task_name]['separator']}\"\n",
    "        return {\"text\": formatted_text, \"label\": example[\"output\"]}\n",
    "    \n",
    "    test_dataset = task_test_dataset.map(format_for_task)\n",
    "    \n",
    "    # Load model and tokenizer\n",
    "    model = AutoModelForCausalLM.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Implement task-specific evaluation logic\n",
    "    # e.g., for summarization: ROUGE scores\n",
    "    # for sentiment: accuracy, F1 score\n",
    "    # for QA: exact match, F1 score\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Evaluation on {task_name} completed.\")\n",
    "\n",
    "# Example usage (commented out)\n",
    "# for task_name in TASKS.keys():\n",
    "#     evaluate_on_specific_task(task_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Knowledge Distillation Fine-Tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Knowledge Distillation Fine-Tuning Implementation\n",
    "# ===============================================\n",
    "# This script demonstrates knowledge distillation from a larger teacher model to a smaller student model\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "TEACHER_MODEL_NAME = \"gpt2-large\"  # Larger teacher model\n",
    "STUDENT_MODEL_NAME = \"gpt2\"        # Smaller student model\n",
    "DATASET_NAME = \"your-dataset\"      # Replace with your actual dataset\n",
    "OUTPUT_DIR = \"./distillation_output\"\n",
    "\n",
    "# Distillation parameters\n",
    "ALPHA = 0.5  # Weight for distillation loss vs task-specific loss (0 to 1)\n",
    "TEMPERATURE = 2.0  # Temperature for softening probability distributions\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Load tokenizers\n",
    "teacher_tokenizer = AutoTokenizer.from_pretrained(TEACHER_MODEL_NAME)\n",
    "if teacher_tokenizer.pad_token is None:\n",
    "    teacher_tokenizer.pad_token = teacher_tokenizer.eos_token\n",
    "\n",
    "student_tokenizer = AutoTokenizer.from_pretrained(STUDENT_MODEL_NAME)\n",
    "if student_tokenizer.pad_token is None:\n",
    "    student_tokenizer.pad_token = student_tokenizer.eos_token\n",
    "\n",
    "# Load models\n",
    "print(\"Loading teacher model...\")\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(TEACHER_MODEL_NAME)\n",
    "teacher_model.eval()  # Set teacher to evaluation mode\n",
    "\n",
    "print(\"Loading student model...\")\n",
    "student_model = AutoModelForCausalLM.from_pretrained(STUDENT_MODEL_NAME)\n",
    "\n",
    "# Print model sizes\n",
    "teacher_params = sum(p.numel() for p in teacher_model.parameters())\n",
    "student_params = sum(p.numel() for p in student_model.parameters())\n",
    "print(f\"Teacher model parameters: {teacher_params:,}\")\n",
    "print(f\"Student model parameters: {student_params:,}\")\n",
    "print(f\"Compression ratio: {teacher_params / student_params:.2f}x\")\n",
    "\n",
    "# Custom distillation trainer\n",
    "class DistillationTrainer(Trainer):\n",
    "    def __init__(self, *args, teacher_model=None, alpha=0.5, temperature=2.0, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.teacher_model = teacher_model\n",
    "        self.alpha = alpha  # Weight for distillation loss\n",
    "        self.temperature = temperature  # Temperature for softening distributions\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        # Get student outputs\n",
    "        student_outputs = model(**inputs)\n",
    "        student_logits = student_outputs.logits\n",
    "        \n",
    "        # Calculate standard language modeling loss\n",
    "        labels = inputs.get(\"labels\")\n",
    "        \n",
    "        # If labels are not provided, shift input_ids one position to the right\n",
    "        if labels is None:\n",
    "            labels = inputs[\"input_ids\"].clone()\n",
    "            labels = labels[:, 1:].contiguous()\n",
    "            student_logits = student_logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        # Standard cross-entropy loss\n",
    "        loss_ce = F.cross_entropy(\n",
    "            student_logits.view(-1, student_logits.size(-1)),\n",
    "            labels.view(-1),\n",
    "            ignore_index=-100\n",
    "        )\n",
    "        \n",
    "        # Get teacher logits\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = self.teacher_model(**inputs)\n",
    "            teacher_logits = teacher_outputs.logits\n",
    "            \n",
    "            # Match dimensions with student logits\n",
    "            if labels is None:\n",
    "                teacher_logits = teacher_logits[:, :-1, :].contiguous()\n",
    "        \n",
    "        # Calculate distillation loss\n",
    "        # Soften probabilities with temperature\n",
    "        soft_student_logits = F.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "        soft_teacher_logits = F.softmax(teacher_logits / self.temperature, dim=-1)\n",
    "        \n",
    "        # KL divergence loss\n",
    "        loss_kd = F.kl_div(\n",
    "            soft_student_logits.view(-1, soft_student_logits.size(-1)),\n",
    "            soft_teacher_logits.view(-1, soft_teacher_logits.size(-1)),\n",
    "            reduction='batchmean'\n",
    "        ) * (self.temperature ** 2)\n",
    "        \n",
    "        # Combined loss\n",
    "        loss = (1 - self.alpha) * loss_ce + self.alpha * loss_kd\n",
    "        \n",
    "        return (loss, student_outputs) if return_outputs else loss\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_and_prepare_dataset():\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            return teacher_tokenizer(  # Use teacher tokenizer for consistent tokenization\n",
    "                examples[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset[\"train\"].column_names \n",
    "                           if col not in [\"input_ids\", \"attention_mask\"]]\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Create a minimal example dataset\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        \n",
    "        # Sample size\n",
    "        sample_size = 100\n",
    "        sample_length = 128\n",
    "        \n",
    "        # Create sample input IDs and attention mask\n",
    "        inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        for _ in range(sample_size):\n",
    "            input_ids = [teacher_tokenizer.bos_token_id] + np.random.randint(\n",
    "                100, 10000, sample_length - 2\n",
    "            ).tolist() + [teacher_tokenizer.eos_token_id]\n",
    "            attention_mask = [1] * sample_length\n",
    "            \n",
    "            inputs[\"input_ids\"].append(input_ids)\n",
    "            inputs[\"attention_mask\"].append(attention_mask)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict(inputs)\n",
    "        eval_dataset = Dataset.from_dict({k: v[:10] for k, v in inputs.items()})\n",
    "        \n",
    "        return {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=student_tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=4,  # Smaller batch size due to both models in memory\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=500,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=500,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,  # Mixed precision training\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Distillation Trainer\n",
    "trainer = DistillationTrainer(\n",
    "    model=student_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    teacher_model=teacher_model,\n",
    "    alpha=ALPHA,\n",
    "    temperature=TEMPERATURE\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting knowledge distillation fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the distilled student model\n",
    "print(\"Saving distilled student model...\")\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "student_tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Evaluate the teacher and student models\n",
    "def evaluate_models():\n",
    "    # Load the saved student model\n",
    "    distilled_model = AutoModelForCausalLM.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Move models to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    teacher_model.to(device)\n",
    "    distilled_model.to(device)\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    teacher_model.eval()\n",
    "    distilled_model.eval()\n",
    "    \n",
    "    # Sample prompts for evaluation\n",
    "    prompts = [\n",
    "        \"The future of artificial intelligence\",\n",
    "        \"Climate change is affecting\",\n",
    "        \"The benefits of renewable energy include\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\nComparing teacher and distilled student model outputs:\")\n",
    "    \n",
    "    for prompt in prompts:\n",
    "        print(f\"\\nPrompt: {prompt}\")\n",
    "        \n",
    "        # Tokenize input\n",
    "        inputs = teacher_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate from teacher model\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=50,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                seed=42  # Use same seed for fair comparison\n",
    "            )\n",
    "        \n",
    "        teacher_text = teacher_tokenizer.decode(teacher_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # Generate from student model\n",
    "        with torch.no_grad():\n",
    "            student_outputs = distilled_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=50,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                seed=42  # Use same seed for fair comparison\n",
    "            )\n",
    "        \n",
    "        student_text = student_tokenizer.decode(student_outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        print(f\"Teacher: {teacher_text}\")\n",
    "        print(f\"Student: {student_text}\")\n",
    "    \n",
    "    # You could add quantitative evaluation here (perplexity, BLEU, etc.)\n",
    "\n",
    "# Uncomment to evaluate the models\n",
    "# print(\"\\nEvaluating teacher and distilled student models...\")\n",
    "# evaluate_models()\n",
    "\n",
    "# Optional: Measure inference speed comparison\n",
    "def benchmark_inference_speed():\n",
    "    import time\n",
    "    \n",
    "    # Load the saved student model\n",
    "    distilled_model = AutoModelForCausalLM.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Move models to appropriate device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    teacher_model.to(device)\n",
    "    distilled_model.to(device)\n",
    "    \n",
    "    # Set models to evaluation mode\n",
    "    teacher_model.eval()\n",
    "    distilled_model.eval()\n",
    "    \n",
    "    # Generate a longer sequence for meaningful timing\n",
    "    prompt = \"The history of artificial intelligence spans several decades, beginning with\"\n",
    "    inputs = teacher_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Warm-up runs\n",
    "    for _ in range(3):\n",
    "        with torch.no_grad():\n",
    "            teacher_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=100\n",
    "            )\n",
    "            distilled_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=100\n",
    "            )\n",
    "    \n",
    "    # Time teacher model\n",
    "    teacher_times = []\n",
    "    for _ in range(5):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            teacher_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=100\n",
    "            )\n",
    "        teacher_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Time student model\n",
    "    student_times = []\n",
    "    for _ in range(5):\n",
    "        start_time = time.time()\n",
    "        with torch.no_grad():\n",
    "            distilled_model.generate(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                attention_mask=inputs[\"attention_mask\"],\n",
    "                max_new_tokens=100\n",
    "            )\n",
    "        student_times.append(time.time() - start_time)\n",
    "    \n",
    "    # Calculate average times\n",
    "    avg_teacher_time = sum(teacher_times) / len(teacher_times)\n",
    "    avg_student_time = sum(student_times) / len(student_times)\n",
    "    \n",
    "    print(f\"\\nInference Speed Benchmark:\")\n",
    "    print(f\"Teacher model average generation time: {avg_teacher_time:.4f} seconds\")\n",
    "    print(f\"Student model average generation time: {avg_student_time:.4f} seconds\")\n",
    "    print(f\"Speedup: {avg_teacher_time / avg_student_time:.2f}x\")\n",
    "\n",
    "# Uncomment to benchmark inference speed\n",
    "# print(\"\\nBenchmarking inference speed...\")\n",
    "# benchmark_inference_speed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> DoRA (Weight-Decomposed Low-Rank Adaptation) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DoRA (Weight-Decomposed Low-Rank Adaptation) Implementation\n",
    "# ==========================================================\n",
    "# This implements DoRA (Weight-Decomposed Low-Rank Adaptation) for fine-tuning LLMs\n",
    "# DoRA decomposes weights into magnitude and direction components and adapts them separately\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # Using a smaller model for demonstration\n",
    "DATASET_NAME = \"your-dataset\"  # Replace with your actual dataset\n",
    "OUTPUT_DIR = \"./dora_output\"\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# DoRA parameters\n",
    "RANK = 8  # Rank for low-rank updates\n",
    "ALPHA = 16  # Scaling factor\n",
    "\n",
    "# DoRA implementation for transformers\n",
    "class DoRAModule(nn.Module):\n",
    "    \"\"\"\n",
    "    DoRA (Weight-Decomposed Low-Rank Adaptation) module\n",
    "    \"\"\"\n",
    "    def __init__(self, weight, rank=8, alpha=16, module_name=\"\", target_modules=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight  # Original frozen weight\n",
    "        self.module_name = module_name\n",
    "        \n",
    "        # Determine if this module should be adapted with DoRA\n",
    "        if target_modules is None or any(tm in module_name for tm in target_modules):\n",
    "            # Decompose the weight into magnitude and direction\n",
    "            with torch.no_grad():\n",
    "                # Compute L2 norm along specific dimension (for different layer types)\n",
    "                if \"q_proj\" in module_name or \"k_proj\" in module_name or \"v_proj\" in module_name or \"out_proj\" in module_name:\n",
    "                    # For attention layers - decompose along head dimension\n",
    "                    magnitude = torch.norm(weight, dim=0, keepdim=True)\n",
    "                    direction = weight / (magnitude + 1e-6)  # Normalized direction vectors\n",
    "                else:\n",
    "                    # For other layers - decompose along output dimension\n",
    "                    magnitude = torch.norm(weight, dim=1, keepdim=True)\n",
    "                    direction = weight / (magnitude + 1e-6)  # Normalized direction vectors\n",
    "            \n",
    "            # Initialize DoRA parameters\n",
    "            self.magnitude_delta = nn.Parameter(torch.zeros_like(magnitude))  # Magnitude shift\n",
    "            \n",
    "            # Low-rank direction adapters\n",
    "            weight_shape = weight.shape\n",
    "            if len(weight_shape) == 2:  # Linear layer\n",
    "                self.lora_A = nn.Parameter(torch.zeros((weight_shape[0], rank)))\n",
    "                self.lora_B = nn.Parameter(torch.zeros((rank, weight_shape[1])))\n",
    "            else:  # Handle other shapes as needed\n",
    "                # Simplified for demonstration - real implementation would handle various layer types\n",
    "                self.lora_A = nn.Parameter(torch.zeros((weight_shape[0], rank)))\n",
    "                self.lora_B = nn.Parameter(torch.zeros((rank, weight_shape[1])))\n",
    "            \n",
    "            # Initialize LoRA weights\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "            \n",
    "            self.alpha = alpha\n",
    "            self.scaling = alpha / rank\n",
    "            self.rank = rank\n",
    "            self.is_dora = True\n",
    "        else:\n",
    "            # Skip DoRA for modules not in target_modules\n",
    "            self.is_dora = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if not self.is_dora:\n",
    "            # Regular forward pass for non-DoRA modules\n",
    "            return F.linear(x, self.weight)\n",
    "        \n",
    "        # Compute original normalized direction and adjusted magnitude\n",
    "        with torch.no_grad():\n",
    "            if len(self.weight.shape) == 2:  # Linear layer\n",
    "                magnitude = torch.norm(self.weight, dim=1, keepdim=True)\n",
    "                direction = self.weight / (magnitude + 1e-6)\n",
    "            else:\n",
    "                # Simplified - handle other layer types as needed\n",
    "                magnitude = torch.norm(self.weight, dim=0, keepdim=True)\n",
    "                direction = self.weight / (magnitude + 1e-6)\n",
    "        \n",
    "        # Apply magnitude delta\n",
    "        adjusted_magnitude = magnitude + self.magnitude_delta\n",
    "        \n",
    "        # Compute low-rank direction update\n",
    "        direction_delta = (self.lora_A @ self.lora_B) * self.scaling\n",
    "        \n",
    "        # Combine for final adapted weight\n",
    "        adapted_weight = direction * adjusted_magnitude + direction_delta\n",
    "        \n",
    "        # Apply the adapted weight\n",
    "        return F.linear(x, adapted_weight)\n",
    "\n",
    "# DoRA wrapper for a model\n",
    "class DoRAModel(nn.Module):\n",
    "    def __init__(self, model, rank=8, alpha=16, target_modules=None):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.rank = rank\n",
    "        self.alpha = alpha\n",
    "        self.target_modules = target_modules or [\n",
    "            \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "        ]\n",
    "        \n",
    "        # Replace target layers with DoRA modules\n",
    "        self._replace_layers()\n",
    "        \n",
    "        # Freeze the original model weights\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def _replace_layers(self):\n",
    "        # This is a simplified implementation for demonstration\n",
    "        # In practice, recursively traverse the model and replace target layers\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear) and any(target in name for target in self.target_modules):\n",
    "                # Create a DoRA wrapper for this layer\n",
    "                dora_layer = DoRAModule(\n",
    "                    module.weight,\n",
    "                    rank=self.rank,\n",
    "                    alpha=self.alpha,\n",
    "                    module_name=name,\n",
    "                    target_modules=self.target_modules\n",
    "                )\n",
    "                \n",
    "                # Replace the original layer with DoRA\n",
    "                # In practice, this requires careful handling of the module hierarchy\n",
    "                # This simplified approach just illustrates the concept\n",
    "                parent_name = name.rsplit(\".\", 1)[0] if \".\" in name else \"\"\n",
    "                child_name = name.rsplit(\".\", 1)[1] if \".\" in name else name\n",
    "                \n",
    "                if parent_name:\n",
    "                    parent = self.model.get_submodule(parent_name)\n",
    "                    setattr(parent, child_name, dora_layer)\n",
    "                else:\n",
    "                    setattr(self.model, child_name, dora_layer)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Calculate and print the number of trainable parameters\"\"\"\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/all_params:.2%})\")\n",
    "        \n",
    "        # Breaking down by parameter types\n",
    "        magnitude_params = sum(p.numel() for n, p in self.named_parameters() \n",
    "                              if p.requires_grad and \"magnitude_delta\" in n)\n",
    "        lora_params = sum(p.numel() for n, p in self.named_parameters() \n",
    "                         if p.requires_grad and (\"lora_A\" in n or \"lora_B\" in n))\n",
    "        \n",
    "        print(f\"Magnitude parameters: {magnitude_params:,}\")\n",
    "        print(f\"Direction (LoRA) parameters: {lora_params:,}\")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Wrap model with DoRA\n",
    "print(\"Applying DoRA to model...\")\n",
    "model = DoRAModel(\n",
    "    base_model,\n",
    "    rank=RANK,\n",
    "    alpha=ALPHA,\n",
    "    target_modules=[\"c_attn\", \"c_proj\", \"c_fc\", \"attn.c_proj\"]  # Adjust for GPT-2 architecture\n",
    ")\n",
    "\n",
    "# Show trainable parameter count\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load and prepare dataset\n",
    "def load_and_prepare_dataset():\n",
    "    try:\n",
    "        dataset = load_dataset(DATASET_NAME)\n",
    "        \n",
    "        # Tokenize function\n",
    "        def tokenize_function(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=\"max_length\",\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            )\n",
    "        \n",
    "        # Apply tokenization\n",
    "        tokenized_dataset = dataset.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=[col for col in dataset[\"train\"].column_names \n",
    "                           if col not in [\"input_ids\", \"attention_mask\"]]\n",
    "        )\n",
    "        \n",
    "        return tokenized_dataset\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading dataset: {e}\")\n",
    "        print(\"Using a minimal example dataset for demonstration\")\n",
    "        \n",
    "        # Create a minimal example dataset\n",
    "        from datasets import Dataset\n",
    "        import numpy as np\n",
    "        \n",
    "        # Sample data\n",
    "        sample_size = 100\n",
    "        sample_length = 128\n",
    "        \n",
    "        # Create sample input IDs and attention mask\n",
    "        inputs = {\"input_ids\": [], \"attention_mask\": []}\n",
    "        for _ in range(sample_size):\n",
    "            input_ids = [tokenizer.bos_token_id] + np.random.randint(\n",
    "                100, 10000, sample_length - 2\n",
    "            ).tolist() + [tokenizer.eos_token_id]\n",
    "            attention_mask = [1] * sample_length\n",
    "            \n",
    "            inputs[\"input_ids\"].append(input_ids)\n",
    "            inputs[\"attention_mask\"].append(attention_mask)\n",
    "        \n",
    "        # Create datasets\n",
    "        train_dataset = Dataset.from_dict(inputs)\n",
    "        eval_dataset = Dataset.from_dict({k: v[:10] for k, v in inputs.items()})\n",
    "        \n",
    "        return {\"train\": train_dataset, \"validation\": eval_dataset}\n",
    "\n",
    "# Load dataset\n",
    "print(\"Loading and preparing dataset...\")\n",
    "dataset = load_and_prepare_dataset()\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-4,  # Higher learning rate is often appropriate for DoRA\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting DoRA fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving DoRA fine-tuned model...\")\n",
    "\n",
    "# Saving DoRA parameters\n",
    "# In practice, you would implement a custom save method for DoRA parameters only\n",
    "# For demonstration, we'll save the entire model\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Example: Inference with the fine-tuned model\n",
    "def inference_example():\n",
    "    # In practice, load and restore the DoRA parameters\n",
    "    # This simplified example loads the entire saved model\n",
    "    model = torch.load(os.path.join(OUTPUT_DIR, \"final_model/pytorch_model.bin\"))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Generate text\n",
    "    prompt = \"The future of artificial intelligence will be shaped by\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    \n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"],\n",
    "        attention_mask=inputs[\"attention_mask\"],\n",
    "        max_new_tokens=100,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True\n",
    "    )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"Generated response: {response}\")\n",
    "\n",
    "# Uncomment to run inference example\n",
    "# print(\"Running inference with the DoRA fine-tuned model...\")\n",
    "# inference_example()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Mixture of Experts (MoE) Fine-Tuning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixture of Experts (MoE) Fine-Tuning Implementation\n",
    "# =================================================\n",
    "# This implements fine-tuning using a Mixture of Experts (MoE) approach\n",
    "# where domain-specific experts are trained for different tasks\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"gpt2-medium\"  # Using a smaller model for demonstration\n",
    "OUTPUT_DIR = \"./moe_output\"\n",
    "\n",
    "# Expert domains/tasks\n",
    "DOMAINS = {\n",
    "    \"science\": {\n",
    "        \"dataset\": \"your-science-dataset\",  # Replace with actual dataset\n",
    "        \"token\": \"[SCIENCE]\"\n",
    "    },\n",
    "    \"finance\": {\n",
    "        \"dataset\": \"your-finance-dataset\",  # Replace with actual dataset\n",
    "        \"token\": \"[FINANCE]\"\n",
    "    },\n",
    "    \"creative\": {\n",
    "        \"dataset\": \"your-creative-dataset\",  # Replace with actual dataset\n",
    "        \"token\": \"[CREATIVE]\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Create output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# MoE Router - determines which expert(s) to use\n",
    "class ExpertRouter(nn.Module):\n",
    "    def __init__(self, hidden_size, num_experts, k=2):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k  # Top-k experts to use\n",
    "        \n",
    "        # Router network\n",
    "        self.router = nn.Linear(hidden_size, num_experts)\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # Get router logits\n",
    "        router_logits = self.router(hidden_states)  # [batch_size, seq_len, num_experts]\n",
    "        \n",
    "        # Get routing probabilities with softmax\n",
    "        routing_weights = F.softmax(router_logits, dim=-1)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        routing_weights, indices = torch.topk(routing_weights, self.k, dim=-1)\n",
    "        \n",
    "        # Normalize the routing weights for the selected experts\n",
    "        routing_weights = routing_weights / routing_weights.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        return routing_weights, indices\n",
    "\n",
    "# Expert FFN (Feed-Forward Network) layer\n",
    "class ExpertFFN(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, expert_domain=None):\n",
    "        super().__init__()\n",
    "        self.dense_h_to_4h = nn.Linear(hidden_size, intermediate_size)\n",
    "        self.dense_4h_to_h = nn.Linear(intermediate_size, hidden_size)\n",
    "        self.act = nn.GELU()\n",
    "        self.expert_domain = expert_domain  # For logging/tracking\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        hidden_states = self.dense_h_to_4h(hidden_states)\n",
    "        hidden_states = self.act(hidden_states)\n",
    "        hidden_states = self.dense_4h_to_h(hidden_states)\n",
    "        return hidden_states\n",
    "\n",
    "# MoE Layer - combines router and experts\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_experts, k=2, expert_domains=None):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_experts = num_experts\n",
    "        self.k = k\n",
    "        \n",
    "        # Create router\n",
    "        self.router = ExpertRouter(hidden_size, num_experts, k)\n",
    "        \n",
    "        # Create experts\n",
    "        self.experts = nn.ModuleList([\n",
    "            ExpertFFN(hidden_size, intermediate_size, domain) \n",
    "            for domain in (expert_domains or [f\"expert_{i}\" for i in range(num_experts)])\n",
    "        ])\n",
    "    \n",
    "    def forward(self, hidden_states):\n",
    "        # Get batch size and sequence length\n",
    "        batch_size, seq_len, _ = hidden_states.shape\n",
    "        \n",
    "        # Get routing weights and expert indices\n",
    "        routing_weights, expert_indices = self.router(hidden_states)\n",
    "        \n",
    "        # Reshape for expert processing\n",
    "        hidden_states = hidden_states.view(batch_size * seq_len, -1)\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        final_output = torch.zeros_like(hidden_states)\n",
    "        \n",
    "        # Process with each selected expert and combine weighted outputs\n",
    "        for i in range(self.k):\n",
    "            # Get the expert indices for this position\n",
    "            expert_idx = expert_indices[:, :, i].view(-1)\n",
    "            \n",
    "            # Get the corresponding routing weights\n",
    "            weight = routing_weights[:, :, i].view(-1, 1)\n",
    "            \n",
    "            # For each expert, process its assigned tokens\n",
    "            for expert_id in range(self.num_experts):\n",
    "                # Find indices where this expert is selected\n",
    "                expert_mask = (expert_idx == expert_id)\n",
    "                if expert_mask.sum() > 0:\n",
    "                    # Get inputs for this expert\n",
    "                    expert_inputs = hidden_states[expert_mask]\n",
    "                    \n",
    "                    # Process with expert\n",
    "                    expert_output = self.experts[expert_id](expert_inputs)\n",
    "                    \n",
    "                    # Apply routing weight\n",
    "                    expert_output = expert_output * weight[expert_mask]\n",
    "                    \n",
    "                    # Add to final output\n",
    "                    final_output[expert_mask] += expert_output\n",
    "        \n",
    "        # Reshape back to original dimensions\n",
    "        final_output = final_output.view(batch_size, seq_len, -1)\n",
    "        \n",
    "        return final_output\n",
    "\n",
    "# MoE Adapter - Adds MoE capabilities to a pre-trained model\n",
    "class MoEAdapter(nn.Module):\n",
    "    def __init__(self, base_model, num_experts=3, expert_domains=None):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_experts = num_experts\n",
    "        self.expert_domains = expert_domains or [f\"expert_{i}\" for i in range(num_experts)]\n",
    "        \n",
    "        # Get model configuration\n",
    "        config = base_model.config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size if hasattr(config, 'intermediate_size') else 4 * self.hidden_size\n",
    "        \n",
    "        # Add MoE layers to the model\n",
    "        self._add_moe_layers()\n",
    "        \n",
    "        # Freeze base model parameters\n",
    "        for param in base_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Domain token embeddings (optional)\n",
    "        self.domain_token_ids = {}\n",
    "        self._add_domain_tokens()\n",
    "    \n",
    "    def _add_moe_layers(self):\n",
    "        \"\"\"Add MoE layers to the model\"\"\"\n",
    "        # Simplified approach: Replace a subset of feed-forward layers with MoE layers\n",
    "        # In practice, this requires a more tailored approach based on the model architecture\n",
    "        \n",
    "        # For GPT-2, we'll modify some of the transformer blocks\n",
    "        # This is a simplified demonstration\n",
    "        self.moe_layers = nn.ModuleList()\n",
    "        \n",
    "        # Number of layers to replace (e.g., 25% of layers)\n",
    "        num_layers = len(self.base_model.transformer.h)\n",
    "        num_moe_layers = max(1, num_layers // 4)\n",
    "        \n",
    "        # Choose which layers to replace with MoE (evenly distributed)\n",
    "        moe_layer_indices = [i * (num_layers // num_moe_layers) for i in range(num_moe_layers)]\n",
    "        \n",
    "        # Save the indices for forward pass\n",
    "        self.moe_layer_indices = moe_layer_indices\n",
    "        \n",
    "        # Create MoE layers\n",
    "        for _ in range(num_moe_layers):\n",
    "            moe_layer = MoELayer(\n",
    "                self.hidden_size,\n",
    "                self.intermediate_size,\n",
    "                self.num_experts,\n",
    "                k=2,  # Use top-2 experts\n",
    "                expert_domains=self.expert_domains\n",
    "            )\n",
    "            self.moe_layers.append(moe_layer)\n",
    "    \n",
    "    def _add_domain_tokens(self):\n",
    "        \"\"\"Add domain-specific tokens to the tokenizer\"\"\"\n",
    "        # This would typically involve adding special tokens to the tokenizer\n",
    "        # For demonstration purposes, we'll just create a mapping\n",
    "        for i, domain in enumerate(self.expert_domains):\n",
    "            self.domain_token_ids[domain] = i\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
    "        # Get the base model's hidden states for each layer\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, \n",
    "                                  output_hidden_states=True, **kwargs)\n",
    "        \n",
    "        # Extract last hidden state and all hidden states\n",
    "        last_hidden_state = outputs.hidden_states[-1]\n",
    "        hidden_states = list(outputs.hidden_states)\n",
    "        \n",
    "        # Apply MoE layers at the predetermined positions\n",
    "        moe_idx = 0\n",
    "        for i in range(len(self.base_model.transformer.h)):\n",
    "            if i in self.moe_layer_indices:\n",
    "                # Apply MoE to the hidden state at this layer\n",
    "                hidden_state = hidden_states[i + 1]  # +1 because first element is embeddings\n",
    "                moe_output = self.moe_layers[moe_idx](hidden_state)\n",
    "                \n",
    "                # Replace the hidden state with the MoE output\n",
    "                hidden_states[i + 1] = moe_output\n",
    "                \n",
    "                # Move to next MoE layer\n",
    "                moe_idx += 1\n",
    "        \n",
    "        # Update the last hidden state if it was processed by MoE\n",
    "        if len(self.base_model.transformer.h) - 1 in self.moe_layer_indices:\n",
    "            last_hidden_state = hidden_states[-1]\n",
    "        \n",
    "        # Update the outputs with the new hidden states\n",
    "        outputs.hidden_states = tuple(hidden_states)\n",
    "        outputs.last_hidden_state = last_hidden_state\n",
    "        \n",
    "        return outputs\n",
    "    \n",
    "    def print_trainable_parameters(self):\n",
    "        \"\"\"Print the number of trainable parameters\"\"\"\n",
    "        trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "        all_params = sum(p.numel() for p in self.parameters())\n",
    "        print(f\"Trainable parameters: {trainable_params:,} ({trainable_params/all_params:.2%})\")\n",
    "\n",
    "# Load and prepare multi-domain dataset\n",
    "def load_multidomain_dataset():\n",
    "    # This function loads datasets from different domains and adds domain tokens\n",
    "    \n",
    "    # For demonstration purposes, we'll create synthetic datasets\n",
    "    from datasets import Dataset\n",
    "    import numpy as np\n",
    "    \n",
    "    all_datasets = {}\n",
    "    \n",
    "    for domain, config in DOMAINS.items():\n",
    "        try:\n",
    "            # In practice, you would load real datasets:\n",
    "            # dataset = load_dataset(config[\"dataset\"])\n",
    "            \n",
    "            # For demonstration, create a synthetic dataset\n",
    "            print(f\"Creating synthetic data for {domain} domain...\")\n",
    "            \n",
    "            # Sample size\n",
    "            sample_size = 100\n",
    "            sample_length = 128\n",
    "            \n",
    "            # Generate synthetic texts with domain token prepended\n",
    "            texts = [\n",
    "                f\"{config['token']} This is a sample text for {domain} domain.\" \n",
    "                for _ in range(sample_size)\n",
    "            ]\n",
    "            \n",
    "            # Create dataset\n",
    "            domain_dataset = Dataset.from_dict({\"text\": texts})\n",
    "            \n",
    "            # Split into train and validation\n",
    "            splits = domain_dataset.train_test_split(test_size=0.1)\n",
    "            all_datasets[domain] = splits\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating dataset for {domain}: {e}\")\n",
    "    \n",
    "    # Combine all domains for training\n",
    "    if all_datasets:\n",
    "        # Combine train splits\n",
    "        combined_train = concatenate_datasets([ds[\"train\"] for ds in all_datasets.values()])\n",
    "        \n",
    "        # Combine validation splits\n",
    "        combined_val = concatenate_datasets([ds[\"test\"] for ds in all_datasets.values()])\n",
    "        \n",
    "        return {\n",
    "            \"train\": combined_train,\n",
    "            \"validation\": combined_val\n",
    "        }\n",
    "    else:\n",
    "        print(\"No datasets could be created, using a minimal example\")\n",
    "        \n",
    "        # Create a fallback minimal dataset\n",
    "        texts = [\n",
    "            \"[SCIENCE] E=mc^2 is Einstein's famous equation.\",\n",
    "            \"[FINANCE] The stock market showed volatility today.\",\n",
    "            \"[CREATIVE] Once upon a time in a land far away.\"\n",
    "        ]\n",
    "        \n",
    "        dummy_dataset = Dataset.from_dict({\"text\": texts})\n",
    "        dummy_split = dummy_dataset.train_test_split(test_size=0.2)\n",
    "        \n",
    "        return {\n",
    "            \"train\": dummy_split[\"train\"],\n",
    "            \"validation\": dummy_split[\"test\"]\n",
    "        }\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Add domain tokens to tokenizer\n",
    "for domain, config in DOMAINS.items():\n",
    "    special_tokens = {\"additional_special_tokens\": [config[\"token\"]]}\n",
    "    tokenizer.add_special_tokens(special_tokens)\n",
    "\n",
    "# Load the base model\n",
    "print(\"Loading base model...\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)\n",
    "base_model.resize_token_embeddings(len(tokenizer))  # To account for new tokens\n",
    "\n",
    "# Create MoE adapter\n",
    "print(\"Creating Mixture of Experts adapter...\")\n",
    "model = MoEAdapter(\n",
    "    base_model,\n",
    "    num_experts=len(DOMAINS),\n",
    "    expert_domains=list(DOMAINS.keys())\n",
    ")\n",
    "\n",
    "# Print trainable parameter count\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# Load and prepare datasets\n",
    "print(\"Loading and preparing multi-domain datasets...\")\n",
    "dataset = load_multidomain_dataset()\n",
    "\n",
    "# Tokenize function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = {\n",
    "    split: dataset[split].map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        remove_columns=[\"text\"]\n",
    "    )\n",
    "    for split in dataset\n",
    "}\n",
    "\n",
    "print(f\"Train dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_dataset['validation'])}\")\n",
    "\n",
    "# Data collator\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer,\n",
    "    mlm=False\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=OUTPUT_DIR,\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=200,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=200,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting Mixture of Experts fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine-tuned model\n",
    "print(\"Saving MoE fine-tuned model...\")\n",
    "trainer.save_model(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "tokenizer.save_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "\n",
    "# Inference example - generate text for each domain\n",
    "def inference_examples():\n",
    "    # Load the fine-tuned model\n",
    "    model = torch.load(os.path.join(OUTPUT_DIR, \"final_model/pytorch_model.bin\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Test for each domain\n",
    "    for domain, config in DOMAINS.items():\n",
    "        print(f\"\\nTesting {domain} domain:\")\n",
    "        \n",
    "        # Create domain-specific prompt with domain token\n",
    "        prompt = f\"{config['token']} In this analysis, we will explore\"\n",
    "        \n",
    "        # Generate response\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        \n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs[\"input_ids\"],\n",
    "            attention_mask=inputs[\"attention_mask\"],\n",
    "            max_new_tokens=100,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True\n",
    "        )\n",
    "        \n",
    "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        print(f\"Prompt: {prompt}\")\n",
    "        print(f\"Generated response: {response}\")\n",
    "\n",
    "# Uncomment to run inference examples\n",
    "# print(\"\\nRunning inference with the MoE fine-tuned model...\")\n",
    "# inference_examples()\n",
    "\n",
    "# Analyzing expert utilization\n",
    "def analyze_expert_usage():\n",
    "    \"\"\"Analyze which experts are used for different domains/inputs\"\"\"\n",
    "    \n",
    "    # Load the fine-tuned model\n",
    "    model = torch.load(os.path.join(OUTPUT_DIR, \"final_model/pytorch_model.bin\"))\n",
    "    tokenizer = AutoTokenizer.from_pretrained(os.path.join(OUTPUT_DIR, \"final_model\"))\n",
    "    \n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Create sample inputs from each domain\n",
    "    sample_inputs = {}\n",
    "    for domain, config in DOMAINS.items():\n",
    "        prompt = f\"{config['token']} This is a test for the {domain} domain.\"\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        sample_inputs[domain] = inputs\n",
    "    \n",
    "    # Function to extract expert usage statistics from model\n",
    "    def get_expert_usage(model, inputs):\n",
    "        # This is a simplified placeholder - actual implementation would\n",
    "        # require model-specific hooks to extract router weights\n",
    "        # Example pseudo-code:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, output_attentions=True, output_hidden_states=True)\n",
    "            # Extract router weights from MoE layers\n",
    "            # Count which experts are selected most frequently\n",
    "            \n",
    "        # Return a dictionary with expert usage statistics\n",
    "        return {\n",
    "            \"expert_selection\": {\"expert_0\": 0.4, \"expert_1\": 0.3, \"expert_2\": 0.3},\n",
    "            \"avg_expert_confidence\": 0.8\n",
    "        }\n",
    "    \n",
    "    # Analyze expert usage for each domain\n",
    "    for domain, inputs in sample_inputs.items():\n",
    "        print(f\"\\nExpert usage analysis for {domain} domain:\")\n",
    "        expert_stats = get_expert_usage(model, inputs)\n",
    "        \n",
    "        # Print expert selection distribution\n",
    "        print(\"Expert selection distribution:\")\n",
    "        for expert, weight in expert_stats[\"expert_selection\"].items():\n",
    "            print(f\"  {expert}: {weight:.2f}\")\n",
    "        \n",
    "        print(f\"Average expert confidence: {expert_stats['avg_expert_confidence']:.2f}\")\n",
    "\n",
    "# Uncomment to analyze expert usage\n",
    "# print(\"\\nAnalyzing expert usage patterns...\")\n",
    "# analyze_expert_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantization for LLMs\n",
    "    # Quantization is a technique that reduces the precision of weights and activations in a neural network to lower the memory footprint \n",
    "    # and increase inference speed, often with minimal impact on performance.\n",
    "\n",
    "# How Quantization Works\n",
    "    # Quantization converts high-precision floating-point numbers (like FP32 or FP16) to lower-precision formats:\n",
    "\n",
    "        # FP16 (Half-precision): 16-bit floating point\n",
    "        # INT8: 8-bit integer quantization\n",
    "        # INT4: 4-bit integer quantization\n",
    "        # GPTQ: A specialized quantization method for transformer models\n",
    "        # AWQ: Activation-aware weight quantization\n",
    "        # SmoothQuant: Balances quantization between activations and weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Quantization: A Concise Overview\n",
    "\n",
    "## Number Representation Ranges\n",
    "| Format | Bits | Range | Precision | Possible Values | Notes |\n",
    "|--------|------|-------|-----------|-----------------|-------|\n",
    "| FP32   | 32   | ±3.4×10³⁸ | ~7 decimal digits | $(2^{32}$) (~4.3 billion) | Standard full precision |\n",
    "| FP16   | 16   | ±65,504 | ~3 decimal digits | $(2^{16}$) (65,536) | Half precision |\n",
    "| BF16   | 16   | ±3.4×10³⁸ | ~2-3 decimal digits | $(2^{16}$) (65,536) | \"Brain Float\", wider range than FP16 |\n",
    "| INT8   | 8    | -128 to 127 | Integers only | 256 | Common for inference |\n",
    "| INT4   | 4    | -8 to 7   | Integers only | 16  | Recent LLM optimization |\n",
    "| INT2   | 2    | -2 to 1   | Integers only | 4   | Extreme compression |\n",
    "\n",
    "\n",
    "## Quantization Formulas\n",
    "**Linear Quantization**: $q = round(x / s) + z$ where $q$ = quantized value, `x` = original float, `s` = scale factor, `z` = zero-point\n",
    "\n",
    "**Dequantization**: $x_approx = s * (q - z)$ to convert back to floating point\n",
    "\n",
    "**Scale Factor**: $s = (x_max - x_min) / (q_max - q_min)$ sets the conversion ratio\n",
    "\n",
    "## Quantization Types\n",
    "- **Symmetric**: $q = round(x / s)$ with zero-point fixed at 0\n",
    "- **Asymmetric**: $q = round(x / s) + z$ using zero-point offset for better range utilization\n",
    "- **Per-tensor**: One scale/zero-point for entire tensor (simple but less accurate)\n",
    "- **Per-channel**: Different scale/zero-point for each output channel (better accuracy)\n",
    "\n",
    "## Advanced Techniques\n",
    "- **Weight-Only**: Quantizes just weights, keeping activations in higher precision\n",
    "- **GPTQ**: Uses Hessian information to minimize error through iterative quantization\n",
    "- **NF4**: Custom 4-bit format with non-uniform quantization to preserve outliers\n",
    "- **SmoothQuant**: Balances quantization difficulty with formula $y = (W·α)·(x/α)$ using channel-wise scaling $α$\n",
    "- **AWQ**: Preserves important weights based on activation patterns\n",
    "\n",
    "## Benefits\n",
    "- **Memory**: 2-8× reduction in model size\n",
    "- **Speed**: Up to 4× faster inference\n",
    "- **Energy**: Lower power consumption\n",
    "- **Cost**: Enables larger models on consumer hardware\n",
    "\n",
    "## Performance Trade-offs\n",
    "Quantization reduces precision by mapping a continuous range of values to a discrete set, introducing quantization error. Modern techniques like NF4, AWQ, and GPTQ minimize this error by optimizing the quantization process based on the statistical properties of neural networks, preserving model quality even at extreme compression levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Model Quantization Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Quantization Implementation\n",
    "# =================================\n",
    "# Examples of different approaches to quantize LLMs\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    GPTQConfig\n",
    ")\n",
    "from accelerate import infer_auto_device_map, init_empty_weights\n",
    "\n",
    "####################################################################\n",
    "# SECTION 1: Loading Pre-Quantized Models from Hugging Face\n",
    "####################################################################\n",
    "\n",
    "def load_4bit_quantized_model():\n",
    "    \"\"\"\n",
    "    Load a model with 4-bit quantization using bitsandbytes\n",
    "    \"\"\"\n",
    "    print(\"Loading model quantized to 4-bit precision...\")\n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                 # Load model in 4-bit precision\n",
    "        bnb_4bit_compute_dtype=torch.float16,  # Compute in fp16\n",
    "        bnb_4bit_use_double_quant=True,    # Use nested quantization for more memory savings\n",
    "        bnb_4bit_quant_type=\"nf4\",         # Normalized float 4-bit quantization (alternatives: \"fp4\")\n",
    "    )\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    model_id = \"meta-llama/Llama-2-7b-hf\"  # Replace with your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load the model with quantization configuration\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # Automatically distribute model across available GPUs/devices\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded in 4-bit precision\")\n",
    "    print(f\"Model size: {get_model_size_in_gb(model):.2f} GB\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_8bit_quantized_model():\n",
    "    \"\"\"\n",
    "    Load a model with 8-bit quantization using bitsandbytes\n",
    "    \"\"\"\n",
    "    print(\"Loading model quantized to 8-bit precision...\")\n",
    "    \n",
    "    # Configure 8-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,                 # Load model in 8-bit precision\n",
    "        llm_int8_threshold=6.0,            # Threshold for outlier features in LLM.int8()\n",
    "        llm_int8_has_fp16_weight=False,    # Whether INT8 was combined with FP16\n",
    "    )\n",
    "    \n",
    "    # Load the tokenizer\n",
    "    model_id = \"meta-llama/Llama-2-7b-hf\"  # Replace with your model\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load the model with quantization configuration\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",  # Automatically distribute model across available GPUs/devices\n",
    "    )\n",
    "    \n",
    "    print(f\"Model loaded in 8-bit precision\")\n",
    "    print(f\"Model size: {get_model_size_in_gb(model):.2f} GB\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_gptq_quantized_model():\n",
    "    \"\"\"\n",
    "    Load a GPTQ pre-quantized model\n",
    "    \"\"\"\n",
    "    print(\"Loading GPTQ-quantized model...\")\n",
    "    \n",
    "    # GPTQ quantized models are typically shared on HF with \"-gptq\" suffix\n",
    "    model_id = \"TheBloke/Llama-2-7B-GPTQ\"  # Replace with actual GPTQ model\n",
    "    \n",
    "    # Configure GPTQ settings\n",
    "    gptq_config = GPTQConfig(\n",
    "        bits=4,                  # Typically 3 or 4 bits per parameter\n",
    "        disable_exllama=False,   # Whether to disable exllama kernel\n",
    "        use_marlin=False,        # Whether to use marlin kernel\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load GPTQ model\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=gptq_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    print(f\"GPTQ-quantized model loaded\")\n",
    "    print(f\"Model size: {get_model_size_in_gb(model):.2f} GB\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def load_awq_quantized_model():\n",
    "    \"\"\"\n",
    "    Load an AWQ pre-quantized model\n",
    "    \"\"\"\n",
    "    print(\"Loading AWQ-quantized model...\")\n",
    "    \n",
    "    # AWQ-quantized models are typically shared on HF with \"-awq\" suffix\n",
    "    model_id = \"TheBloke/Llama-2-7B-AWQ\"  # Replace with actual AWQ model\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load AWQ model - requires autoawq library\n",
    "    try:\n",
    "        from awq import AutoAWQForCausalLM\n",
    "        \n",
    "        model = AutoAWQForCausalLM.from_quantized(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "            max_memory=None,\n",
    "        )\n",
    "    except ImportError:\n",
    "        print(\"To use AWQ models, please install autoawq: pip install autoawq\")\n",
    "        # Fallback to regular loading, which may not work properly for AWQ models\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "    \n",
    "    print(f\"AWQ-quantized model loaded\")\n",
    "    print(f\"Model size: {get_model_size_in_gb(model):.2f} GB\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "# Load the model\n",
    "model, tokenizer = load_4bit_quantized_model()\n",
    "\n",
    "# Run inference\n",
    "prompt = \"Explain quantum computing in simple terms.\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# Generate output\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=512,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "\n",
    "# Decode and print response\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(response)\n",
    "\n",
    "####################################################################\n",
    "# SECTION 2: Quantizing a Model Yourself\n",
    "####################################################################\n",
    "\n",
    "def quantize_to_4bit(model_id):\n",
    "    \"\"\"\n",
    "    Quantize a model to 4-bit precision using BitsAndBytes\n",
    "    \"\"\"\n",
    "    print(f\"Quantizing {model_id} to 4-bit precision...\")\n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "    )\n",
    "    \n",
    "    # Load the model with 4-bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "    # Save as 4-bit quantized\n",
    "    output_dir = f\"{model_id.split('/')[-1]}-4bit\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # BitsAndBytes models can't be directly saved with model.save_pretrained()\n",
    "    # We need to save the quantization config separately\n",
    "    model.config.quantization_config = quantization_config\n",
    "    model.config.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Quantization config saved to {output_dir}\")\n",
    "    print(\"Note: The actual 4-bit weights aren't saved. To share this model, use a library like\")\n",
    "    print(\"bitsandbytes that can load the original model with the saved quantization config.\")\n",
    "\n",
    "    # -------------------------------------------------------------------------------------------------------------\n",
    "    # Run simple test\n",
    "    print(\"Running test inference...\")\n",
    "    prompt = \"Hello, how are you?\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=20,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "        )\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Test response: {response}\")\n",
    "        \n",
    "    return model, tokenizer\n",
    "\n",
    "def quantize_to_8bit(model_id):\n",
    "    \"\"\"\n",
    "    Quantize a model to 8-bit precision using BitsAndBytes\n",
    "    \"\"\"\n",
    "    print(f\"Quantizing {model_id} to 8-bit precision...\")\n",
    "    \n",
    "    # Configure 8-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,\n",
    "        llm_int8_threshold=6.0,\n",
    "        llm_int8_has_fp16_weight=False,\n",
    "    )\n",
    "    \n",
    "    # Load the model with 8-bit quantization\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Save as 8-bit quantized\n",
    "    output_dir = f\"{model_id.split('/')[-1]}-8bit\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # BitsAndBytes models can't be directly saved with model.save_pretrained()\n",
    "    # We need to save the quantization config separately\n",
    "    model.config.quantization_config = quantization_config\n",
    "    model.config.save_pretrained(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    print(f\"Quantization config saved to {output_dir}\")\n",
    "    print(\"Note: The actual 8-bit weights aren't saved. To share this model, use a library like\")\n",
    "    print(\"bitsandbytes that can load the original model with the saved quantization config.\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def quantize_with_gptq(model_id):\n",
    "    \"\"\"\n",
    "    Quantize a model using GPTQ\n",
    "    Note: This requires additional libraries and is more complex\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if auto-gptq is installed\n",
    "        import auto_gptq\n",
    "    except ImportError:\n",
    "        print(\"GPTQ quantization requires auto-gptq library.\")\n",
    "        print(\"Install with: pip install auto-gptq\")\n",
    "        return None, None\n",
    "    \n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\n",
    "    \n",
    "    print(f\"Quantizing {model_id} with GPTQ...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # This is a simplification - actual GPTQ requires calibration data\n",
    "    # and more complex setup\n",
    "    \n",
    "    # Define quantization configuration\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=4,                      # Quantize to 4-bits\n",
    "        group_size=128,              # Size of quantization groups\n",
    "        desc_act=False,              # Whether to use descending activations  \n",
    "    )\n",
    "    \n",
    "    # Prepare model for quantization\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantize_config=quantize_config,\n",
    "    )\n",
    "    \n",
    "    # This is just a skeleton - actual quantization requires:\n",
    "    # 1. Loading calibration data\n",
    "    # 2. Running the quantization process with examples\n",
    "    # 3. Saving the quantized model\n",
    "    \n",
    "    print(\"Note: Full GPTQ quantization requires calibration data and more setup.\")\n",
    "    print(\"See the auto-gptq documentation for complete implementation.\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def quantize_with_awq(model_id):\n",
    "    \"\"\"\n",
    "    Quantize a model using AWQ\n",
    "    Note: This requires additional libraries and is more complex\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if autoawq is installed\n",
    "        import awq\n",
    "    except ImportError:\n",
    "        print(\"AWQ quantization requires autoawq library.\")\n",
    "        print(\"Install with: pip install autoawq\")\n",
    "        return None, None\n",
    "    \n",
    "    from awq import AutoAWQForCausalLM\n",
    "    \n",
    "    print(f\"Quantizing {model_id} with AWQ...\")\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load model in FP16 for quantization\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "    \n",
    "    # This is a simplified skeleton - actual AWQ quantization requires:\n",
    "    # 1. Loading or generating calibration data\n",
    "    # 2. Running the quantization process\n",
    "    # 3. Exporting the quantized model\n",
    "    \n",
    "    # Pseudo-code for AWQ quantization:\n",
    "    \"\"\"\n",
    "    # Generate or load calibration data\n",
    "    texts = [\"sample text 1\", \"sample text 2\", ...]\n",
    "    \n",
    "    # Quantize the model\n",
    "    model.quantize(\n",
    "        tokenizer=tokenizer,\n",
    "        quant_config={\n",
    "            \"bits\": 4,                # Quantize to 4-bits\n",
    "            \"group_size\": 128,        # Group size\n",
    "            \"zero_point\": True,       # Use zero-point quantization\n",
    "            \"q_group_size\": 128,      # Quantization group size\n",
    "        },\n",
    "        calib_data=texts,             # Calibration data\n",
    "    )\n",
    "    \n",
    "    # Save the quantized model\n",
    "    model.save_quantized(\"./awq-model-4bit\")\n",
    "    tokenizer.save_pretrained(\"./awq-model-4bit\")\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"Note: Full AWQ quantization requires calibration data and more setup.\")\n",
    "    print(\"See the autoawq documentation for complete implementation.\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "####################################################################\n",
    "# SECTION 3: Utility Functions\n",
    "####################################################################\n",
    "\n",
    "def get_model_size_in_gb(model):\n",
    "    \"\"\"Calculate model size in GB\"\"\"\n",
    "    param_size = 0\n",
    "    for param in model.parameters():\n",
    "        param_size += param.nelement() * param.element_size()\n",
    "    buffer_size = 0\n",
    "    for buffer in model.buffers():\n",
    "        buffer_size += buffer.nelement() * buffer.element_size()\n",
    "    \n",
    "    size_in_bytes = param_size + buffer_size\n",
    "    size_in_gb = size_in_bytes / (1024 ** 3)\n",
    "    return size_in_gb\n",
    "\n",
    "def run_inference_example(model, tokenizer):\n",
    "    \"\"\"Run a simple inference example\"\"\"\n",
    "    print(\"\\nRunning inference example...\")\n",
    "    \n",
    "    # Prepare input\n",
    "    input_text = \"The future of artificial intelligence will\"\n",
    "    input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **input_ids,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    # Decode and print generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "def compare_model_sizes():\n",
    "    \"\"\"Compare model sizes with different quantization methods\"\"\"\n",
    "    model_id = \"meta-llama/Llama-2-7b-hf\"  # Replace with your model\n",
    "    \n",
    "    print(\"\\nComparing model sizes:\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Check FP16 size (estimated, not loaded to save memory)\n",
    "    fp16_size_gb = 13.0  # ~13GB for Llama-2-7b in FP16\n",
    "    print(f\"FP16 Model (estimated): {fp16_size_gb:.2f} GB\")\n",
    "    \n",
    "    # Load and check 8-bit size\n",
    "    try:\n",
    "        model_8bit, _ = load_8bit_quantized_model()\n",
    "        size_8bit = get_model_size_in_gb(model_8bit)\n",
    "        print(f\"8-bit Quantized Model: {size_8bit:.2f} GB\")\n",
    "        del model_8bit\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading 8-bit model: {e}\")\n",
    "    \n",
    "    # Load and check 4-bit size\n",
    "    try:\n",
    "        model_4bit, _ = load_4bit_quantized_model()\n",
    "        size_4bit = get_model_size_in_gb(model_4bit)\n",
    "        print(f\"4-bit Quantized Model: {size_4bit:.2f} GB\")\n",
    "        del model_4bit\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading 4-bit model: {e}\")\n",
    "    \n",
    "    # GPTQ and AWQ sizes (estimated, as loading depends on external libraries)\n",
    "    print(f\"GPTQ 4-bit Model (estimated): ~3.5 GB\")\n",
    "    print(f\"AWQ 4-bit Model (estimated): ~3.5 GB\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Example usage (commented out to avoid actual execution)\n",
    "if __name__ == \"__main__\":\n",
    "    # Uncomment any of these to run the respective functions\n",
    "    \n",
    "    # 1. Load pre-quantized models\n",
    "    # model, tokenizer = load_4bit_quantized_model()\n",
    "    # run_inference_example(model, tokenizer)\n",
    "    \n",
    "    # model, tokenizer = load_8bit_quantized_model()\n",
    "    # run_inference_example(model, tokenizer)\n",
    "    \n",
    "    # model, tokenizer = load_gptq_quantized_model()\n",
    "    # run_inference_example(model, tokenizer)\n",
    "    \n",
    "    # 2. Quantize models yourself\n",
    "    # model_id = \"gpt2\"  # Use a small model for testing\n",
    "    # model, tokenizer = quantize_to_4bit(model_id)\n",
    "    # run_inference_example(model, tokenizer)\n",
    "    \n",
    "    # 3. Compare sizes of different quantization methods\n",
    "    # compare_model_sizes()\n",
    "    \n",
    "    print(\"Script completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using Llama.cpp for quantized inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_and_use_llamacpp():\n",
    "    \"\"\"Setup and use llama.cpp for quantized inference\"\"\"\n",
    "    \n",
    "    # Clone llama.cpp if not already present\n",
    "    llamacpp_dir = Path(\"./llama.cpp\")\n",
    "    if not llamacpp_dir.exists():\n",
    "        print(\"Cloning llama.cpp repository...\")\n",
    "        subprocess.run([\"git\", \"clone\", \"https://github.com/ggerganov/llama.cpp.git\", llamacpp_dir])\n",
    "    \n",
    "    # Compile llama.cpp\n",
    "    print(\"Compiling llama.cpp...\")\n",
    "    os.chdir(llamacpp_dir)\n",
    "    subprocess.run([\"make\"])\n",
    "    \n",
    "    # Download model (or use an existing one)\n",
    "    model_path = Path(\"./models/llama-2-7b.gguf\")\n",
    "    if not model_path.exists():\n",
    "        print(\"You need to download a GGUF model or convert one.\")\n",
    "        print(\"Example models can be found on Hugging Face:\")\n",
    "        print(\"https://huggingface.co/TheBloke/Llama-2-7B-GGUF\")\n",
    "        \n",
    "        # Simulating downloading a model\n",
    "        os.makedirs(model_path.parent, exist_ok=True)\n",
    "        print(\"Download a GGUF model to:\", model_path.absolute())\n",
    "        \n",
    "        # For actual implementation, you might use:\n",
    "        # subprocess.run([\"wget\", \"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_K_M.gguf\", \"-O\", model_path])\n",
    "    \n",
    "    # Create a prompt file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.txt') as f:\n",
    "        prompt_file = f.name\n",
    "        f.write(\"Explain the theory of relativity in simple terms.\\n\")\n",
    "    \n",
    "    print(f\"Created prompt file: {prompt_file}\")\n",
    "    \n",
    "    # Run llama.cpp\n",
    "    print(\"Running inference with llama.cpp...\")\n",
    "    command = [\n",
    "        \"./main\",\n",
    "        \"-m\", str(model_path),\n",
    "        \"-f\", prompt_file,\n",
    "        \"--temp\", \"0.7\",\n",
    "        \"--top-p\", \"0.9\",\n",
    "        \"-n\", \"512\",  # Number of tokens to generate\n",
    "        \"--repeat-penalty\", \"1.1\",\n",
    "        \"-t\", \"8\"  # Number of threads\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.run(command, check=True)\n",
    "        print(\"Inference completed successfully\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error running llama.cpp: {e}\")\n",
    "    \n",
    "    # Clean up the prompt file\n",
    "    os.unlink(prompt_file)\n",
    "    \n",
    "    # Return to original directory\n",
    "    os.chdir(\"..\")\n",
    "\n",
    "# Alternative: Using Python bindings for llama.cpp\n",
    "def use_llamacpp_python():\n",
    "    \"\"\"Use Python bindings for llama.cpp\"\"\"\n",
    "    try:\n",
    "        from llama_cpp import Llama\n",
    "        \n",
    "        # Path to your GGUF model\n",
    "        model_path = \"./llama.cpp/models/llama-2-7b.Q4_K_M.gguf\"\n",
    "        \n",
    "        # Load the model\n",
    "        llm = Llama(\n",
    "            model_path=model_path,\n",
    "            n_ctx=2048,  # Context size\n",
    "            n_threads=8,  # CPU threads\n",
    "            n_gpu_layers=0  # Set higher for GPU offloading\n",
    "        )\n",
    "        \n",
    "        # Run inference\n",
    "        prompt = \"Explain how quantization works in neural networks.\"\n",
    "        output = llm(\n",
    "            prompt,\n",
    "            max_tokens=512,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            repeat_penalty=1.1,\n",
    "            echo=True  # Include prompt in the output\n",
    "        )\n",
    "        \n",
    "        # Print generated text\n",
    "        print(output['choices'][0]['text'])\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"Python bindings for llama.cpp not found.\")\n",
    "        print(\"Install with: pip install llama-cpp-python\")\n",
    "\n",
    "# Run the examples\n",
    "if __name__ == \"__main__\":\n",
    "    # Choose which function to run\n",
    "    # load_quantized_model_from_huggingface()\n",
    "    # quantize_large_model_from_huggingface()\n",
    "    # setup_and_use_llamacpp()\n",
    "    # use_llamacpp_python()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Full GPTQ Quantization and Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Full GPTQ Quantization and Export Implementation\n",
    "# ===============================================\n",
    "# This script shows a complete GPTQ quantization process and export\n",
    "\n",
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Check if auto-gptq is installed\n",
    "try:\n",
    "    from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig, get_gptq_peft_model\n",
    "    GPTQ_AVAILABLE = True\n",
    "except ImportError:\n",
    "    GPTQ_AVAILABLE = False\n",
    "    print(\"GPTQ quantization requires auto-gptq library.\")\n",
    "    print(\"Install with: pip install auto-gptq\")\n",
    "\n",
    "def prepare_calibration_data(tokenizer, num_samples=128):\n",
    "    \"\"\"\n",
    "    Prepare calibration data for GPTQ quantization\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # You can use any dataset with texts for calibration\n",
    "        # Here we use WikiText-2 as an example\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        \n",
    "        # Process the dataset\n",
    "        def preprocess(examples):\n",
    "            return tokenizer(\n",
    "                examples[\"text\"],\n",
    "                padding=False,\n",
    "                truncation=True,\n",
    "                max_length=512,\n",
    "            )\n",
    "        \n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = dataset.map(\n",
    "            preprocess,\n",
    "            batched=True,\n",
    "            remove_columns=[\"text\"],\n",
    "        )\n",
    "        \n",
    "        # Extract input_ids and limit to a subset for calibration\n",
    "        calibration_data = [\n",
    "            sample[\"input_ids\"] for sample in tokenized_dataset \n",
    "            if len(sample[\"input_ids\"]) > 128  # Filter out short sequences\n",
    "        ][:num_samples]\n",
    "        \n",
    "        print(f\"Prepared {len(calibration_data)} calibration samples\")\n",
    "        return calibration_data\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing calibration data: {e}\")\n",
    "        print(\"Falling back to synthetic calibration data\")\n",
    "        \n",
    "        # Create synthetic calibration data as a fallback\n",
    "        synthetic_texts = [\n",
    "            \"The quick brown fox jumps over the lazy dog.\" * 10,\n",
    "            \"Artificial intelligence is transforming industries worldwide.\" * 8,\n",
    "            \"Machine learning algorithms improve with more training data.\" * 8,\n",
    "            \"The history of computing spans several decades of technological innovation.\" * 6,\n",
    "            \"Quantum computers use quantum physics to solve complex problems.\" * 8,\n",
    "        ]\n",
    "        \n",
    "        # Tokenize the synthetic texts\n",
    "        calibration_data = [\n",
    "            tokenizer(text, return_tensors=\"pt\")[\"input_ids\"][0].tolist()\n",
    "            for text in synthetic_texts\n",
    "        ]\n",
    "        \n",
    "        # Create more variations of calibration data to reach desired number\n",
    "        while len(calibration_data) < num_samples:\n",
    "            calibration_data.append(calibration_data[len(calibration_data) % len(synthetic_texts)])\n",
    "        \n",
    "        print(f\"Prepared {len(calibration_data)} synthetic calibration samples\")\n",
    "        return calibration_data\n",
    "\n",
    "def quantize_with_gptq_full(model_id, output_dir, bits=4, group_size=128, use_exllama=True):\n",
    "    \"\"\"\n",
    "    Full GPTQ quantization process with calibration data and model export\n",
    "    \"\"\"\n",
    "    if not GPTQ_AVAILABLE:\n",
    "        print(\"Cannot perform GPTQ quantization without auto-gptq library\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Starting GPTQ quantization for {model_id}\")\n",
    "    output_dir = output_dir or f\"{model_id.split('/')[-1]}-GPTQ-{bits}bit\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Prepare calibration data\n",
    "    calibration_data = prepare_calibration_data(tokenizer)\n",
    "    \n",
    "    # Define quantization configuration\n",
    "    quantize_config = BaseQuantizeConfig(\n",
    "        bits=bits,                # Quantize to specified bits (usually 3 or 4)\n",
    "        group_size=group_size,    # Group size for quantization (typically 128)\n",
    "        desc_act=False,           # Whether to use descending activations\n",
    "    )\n",
    "    \n",
    "    # Load the model and perform quantization\n",
    "    print(\"Loading model and performing GPTQ quantization...\")\n",
    "    model = AutoGPTQForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantize_config=quantize_config,\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Run the quantization process\n",
    "    model.quantize(\n",
    "        calibration_data,\n",
    "        use_triton=False,  # Whether to use Triton for faster inference\n",
    "    )\n",
    "    \n",
    "    # Save the quantized model\n",
    "    print(f\"Saving GPTQ quantized model to {output_dir}\")\n",
    "    model.save_quantized(output_dir, use_safetensors=True)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save a README with quantization information\n",
    "    with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
    "        f.write(f\"# GPTQ Quantized Model\\n\\n\")\n",
    "        f.write(f\"Original model: {model_id}\\n\")\n",
    "        f.write(f\"Bits: {bits}\\n\")\n",
    "        f.write(f\"Group size: {group_size}\\n\")\n",
    "        f.write(f\"Calibration samples: {len(calibration_data)}\\n\\n\")\n",
    "        f.write(\"## Usage\\n\\n\")\n",
    "        f.write(\"```python\\n\")\n",
    "        f.write(\"from auto_gptq import AutoGPTQForCausalLM\\n\")\n",
    "        f.write(\"from transformers import AutoTokenizer\\n\\n\")\n",
    "        f.write(f'tokenizer = AutoTokenizer.from_pretrained(\"{output_dir}\")\\n')\n",
    "        f.write(f'model = AutoGPTQForCausalLM.from_quantized(\\n')\n",
    "        f.write(f'    \"{output_dir}\",\\n')\n",
    "        f.write(f'    device=\"cuda:0\",\\n')\n",
    "        f.write(f'    use_triton=False,\\n')\n",
    "        f.write(f')\\n\\n')\n",
    "        f.write('inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(\"cuda:0\")\\n')\n",
    "        f.write('outputs = model.generate(**inputs, max_new_tokens=50)\\n')\n",
    "        f.write('print(tokenizer.decode(outputs[0]))\\n')\n",
    "        f.write(\"```\\n\")\n",
    "    \n",
    "    # Optionally, reload the model in inference mode to test\n",
    "    print(\"Reloading quantized model for inference...\")\n",
    "    quantized_model = AutoGPTQForCausalLM.from_quantized(\n",
    "        output_dir,\n",
    "        device=\"cuda:0\" if torch.cuda.is_available() else \"cpu\",\n",
    "        use_triton=False,\n",
    "        use_exllama=use_exllama and bits == 4,  # exllama only supports 4-bit\n",
    "    )\n",
    "    \n",
    "    return quantized_model, tokenizer\n",
    "\n",
    "def quantize_with_awq_full(model_id, output_dir, bits=4, group_size=128):\n",
    "    \"\"\"\n",
    "    Full AWQ quantization process with calibration data and model export\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if AWQ is installed\n",
    "        from awq import AutoAWQForCausalLM\n",
    "        AWQ_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        AWQ_AVAILABLE = False\n",
    "        print(\"AWQ quantization requires autoawq library.\")\n",
    "        print(\"Install with: pip install autoawq\")\n",
    "        return None, None\n",
    "    \n",
    "    if not AWQ_AVAILABLE:\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"Starting AWQ quantization for {model_id}\")\n",
    "    output_dir = output_dir or f\"{model_id.split('/')[-1]}-AWQ-{bits}bit\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model for quantization\n",
    "    model = AutoAWQForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    \n",
    "    # Prepare calibration texts for AWQ\n",
    "    print(\"Preparing calibration data...\")\n",
    "    try:\n",
    "        # You can use any dataset with texts for calibration\n",
    "        # Here we use WikiText as an example\n",
    "        dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
    "        all_texts = dataset[\"text\"]\n",
    "        \n",
    "        # Get a sample of non-empty texts\n",
    "        calibration_texts = [\n",
    "            text for text in all_texts\n",
    "            if len(text.strip()) > 200  # Get reasonably long texts\n",
    "        ][:128]  # Limit to a small number\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading calibration data: {e}\")\n",
    "        print(\"Falling back to synthetic calibration data\")\n",
    "        \n",
    "        # Create synthetic calibration texts as a fallback\n",
    "        calibration_texts = [\n",
    "            \"The quick brown fox jumps over the lazy dog. \" * 20,\n",
    "            \"Artificial intelligence is transforming industries worldwide. \" * 16,\n",
    "            \"Machine learning algorithms improve with more training data. \" * 16,\n",
    "            \"The history of computing spans several decades of technological innovation. \" * 12,\n",
    "            \"Quantum computers use quantum physics to solve complex problems. \" * 16,\n",
    "        ] * 25  # Multiply to get enough examples\n",
    "        \n",
    "        calibration_texts = calibration_texts[:128]  # Limit number of examples\n",
    "    \n",
    "    print(f\"Prepared {len(calibration_texts)} calibration texts\")\n",
    "    \n",
    "    # Run AWQ Quantization\n",
    "    print(\"Quantizing model with AWQ...\")\n",
    "    model.quantize(\n",
    "        tokenizer=tokenizer,\n",
    "        quant_config={\n",
    "            \"bits\": bits,                # Quantize to 4-bits\n",
    "            \"group_size\": group_size,    # Group size (typically 128)\n",
    "            \"zero_point\": True,          # Use zero-point quantization\n",
    "            \"q_group_size\": group_size,  # Quantization group size\n",
    "        },\n",
    "        calib_data=calibration_texts,    # Calibration data\n",
    "    )\n",
    "    \n",
    "    # Save the quantized model\n",
    "    print(f\"Saving AWQ quantized model to {output_dir}\")\n",
    "    model.save_quantized(output_dir)\n",
    "    tokenizer.save_pretrained(output_dir)\n",
    "    \n",
    "    # Save a README with quantization information\n",
    "    with open(os.path.join(output_dir, \"README.md\"), \"w\") as f:\n",
    "        f.write(f\"# AWQ Quantized Model\\n\\n\")\n",
    "        f.write(f\"Original model: {model_id}\\n\")\n",
    "        f.write(f\"Bits: {bits}\\n\")\n",
    "        f.write(f\"Group size: {group_size}\\n\")\n",
    "        f.write(f\"Calibration samples: {len(calibration_texts)}\\n\\n\")\n",
    "        f.write(\"## Usage\\n\\n\")\n",
    "        f.write(\"```python\\n\")\n",
    "        f.write(\"from awq import AutoAWQForCausalLM\\n\")\n",
    "        f.write(\"from transformers import AutoTokenizer\\n\\n\")\n",
    "        f.write(f'tokenizer = AutoTokenizer.from_pretrained(\"{output_dir}\")\\n')\n",
    "        f.write(f'model = AutoAWQForCausalLM.from_quantized(\\n')\n",
    "        f.write(f'    \"{output_dir}\",\\n')\n",
    "        f.write(f'    device_map=\"auto\",\\n')\n",
    "        f.write(f')\\n\\n')\n",
    "        f.write('inputs = tokenizer(\"Hello, how are you?\", return_tensors=\"pt\").to(model.device)\\n')\n",
    "        f.write('outputs = model.generate(**inputs, max_new_tokens=50)\\n')\n",
    "        f.write('print(tokenizer.decode(outputs[0]))\\n')\n",
    "        f.write(\"```\\n\")\n",
    "    \n",
    "    # Optionally, reload for inference (can be skipped to save memory)\n",
    "    print(\"Quantization complete. AWQ model saved.\")\n",
    "    \n",
    "    return model, tokenizer\n",
    "\n",
    "def run_exllama_example():\n",
    "    \"\"\"\n",
    "    Example showing how to use ExLlama backend with GPTQ models\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from exllama.model import ExLlama, ExLlamaCache, ExLlamaConfig\n",
    "        from exllama.tokenizer import ExLlamaTokenizer\n",
    "        EXLLAMA_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        EXLLAMA_AVAILABLE = False\n",
    "        print(\"ExLlama backend requires the exllama library.\")\n",
    "        print(\"Install with: pip install exllama\")\n",
    "        return\n",
    "    \n",
    "    print(\"Running example with ExLlama backend for GPTQ models...\")\n",
    "    \n",
    "    # Path to a GPTQ model\n",
    "    model_path = \"TheBloke/Llama-2-7B-GPTQ\"  # Replace with actual path\n",
    "    \n",
    "    # Configure ExLlama\n",
    "    config = ExLlamaConfig(\n",
    "        model_path,\n",
    "        max_seq_len=2048,        # Maximum sequence length\n",
    "        gpu_split=None,          # GPU split (None for auto)\n",
    "    )\n",
    "    \n",
    "    # Create model\n",
    "    model = ExLlama(config)\n",
    "    \n",
    "    # Create tokenizer\n",
    "    tokenizer = ExLlamaTokenizer(model_path)\n",
    "    \n",
    "    # Create cache for generation\n",
    "    cache = ExLlamaCache(model)\n",
    "    \n",
    "    # Generate text with ExLlama\n",
    "    prompt = \"The future of artificial intelligence will\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    input_ids = tokenizer.encode(prompt)\n",
    "    \n",
    "    # Generate text\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        cache,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    \n",
    "    # Decode and print generated text\n",
    "    generated_text = tokenizer.decode(output_ids)\n",
    "    print(f\"Input: {prompt}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"ExLlama example completed\")\n",
    "\n",
    "def run_smoothquant_example():\n",
    "    \"\"\"\n",
    "    Example of SmoothQuant quantization approach\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import optimum\n",
    "        from optimum.intel import IPEXModelForCausalLM\n",
    "        SMOOTHQUANT_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        SMOOTHQUANT_AVAILABLE = False\n",
    "        print(\"SmoothQuant requires the optimum-intel package.\")\n",
    "        print(\"Install with: pip install optimum[intel]\")\n",
    "        return\n",
    "    \n",
    "    print(\"Running SmoothQuant quantization example...\")\n",
    "    \n",
    "    # SmoothQuant is particularly good for balancing activation and weight quantization\n",
    "    # It's implemented in the optimum-intel library\n",
    "    \n",
    "    model_id = \"gpt2\"  # Using a small model for demonstration\n",
    "    \n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    \n",
    "    # Load model with SmoothQuant quantization\n",
    "    print(\"Loading model with SmoothQuant INT8 quantization...\")\n",
    "    model = IPEXModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        export=True,\n",
    "        quantization_approach=\"smooth_quant\",\n",
    "        target_precision=\"int8\",  # INT8 quantization\n",
    "    )\n",
    "    \n",
    "    # Generate text\n",
    "    input_text = \"Artificial intelligence will\"\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    \n",
    "    # Generate with quantized model\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=50,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "    )\n",
    "    \n",
    "    # Decode and print generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Input: {input_text}\")\n",
    "    print(f\"Generated: {generated_text}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    print(\"SmoothQuant example completed\")\n",
    "\n",
    "def quantize_and_compare_performance():\n",
    "    \"\"\"\n",
    "    Quantize a model with different methods and compare performance\n",
    "    \"\"\"\n",
    "    model_id = \"gpt2\"  # Using a small model for demonstration\n",
    "    \n",
    "    # Set up performance metrics\n",
    "    results = {}\n",
    "    \n",
    "    # 1. Baseline FP16 model\n",
    "    print(\"\\nTesting FP16 baseline model...\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load model in FP16\n",
    "        model_fp16 = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id, \n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        \n",
    "        # Run inference\n",
    "        inputs = tokenizer(\"The future of artificial intelligence will\", return_tensors=\"pt\").to(model_fp16.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Run multiple times for better timing\n",
    "                outputs = model_fp16.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False  # Deterministic for fair comparison\n",
    "                )\n",
    "        \n",
    "        inference_time = (time.time() - start_time) / 10\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "        \n",
    "        results[\"fp16\"] = {\n",
    "            \"inference_time\": inference_time,\n",
    "            \"memory_usage\": memory_usage,\n",
    "            \"model_size\": get_model_size_in_gb(model_fp16)\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del model_fp16\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing FP16 model: {e}\")\n",
    "    \n",
    "    # 2. 8-bit quantized model\n",
    "    print(\"\\nTesting INT8 quantized model...\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load 8-bit quantized model\n",
    "        model_int8 = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            load_in_8bit=True,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Run inference\n",
    "        inputs = tokenizer(\"The future of artificial intelligence will\", return_tensors=\"pt\").to(model_int8.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Run multiple times for better timing\n",
    "                outputs = model_int8.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False  # Deterministic for fair comparison\n",
    "                )\n",
    "        \n",
    "        inference_time = (time.time() - start_time) / 10\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "        \n",
    "        results[\"int8\"] = {\n",
    "            \"inference_time\": inference_time,\n",
    "            \"memory_usage\": memory_usage,\n",
    "            \"model_size\": get_model_size_in_gb(model_int8)\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del model_int8\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing INT8 model: {e}\")\n",
    "    \n",
    "    # 3. 4-bit quantized model\n",
    "    print(\"\\nTesting INT4 quantized model...\")\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Load 4-bit quantized model\n",
    "        quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\"\n",
    "        )\n",
    "        \n",
    "        model_int4 = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=quantization_config,\n",
    "            device_map=\"auto\"\n",
    "        )\n",
    "        \n",
    "        # Run inference\n",
    "        inputs = tokenizer(\"The future of artificial intelligence will\", return_tensors=\"pt\").to(model_int4.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):  # Run multiple times for better timing\n",
    "                outputs = model_int4.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=50,\n",
    "                    do_sample=False  # Deterministic for fair comparison\n",
    "                )\n",
    "        \n",
    "        inference_time = (time.time() - start_time) / 10\n",
    "        memory_usage = torch.cuda.max_memory_allocated() / (1024 ** 3)  # GB\n",
    "        \n",
    "        results[\"int4\"] = {\n",
    "            \"inference_time\": inference_time,\n",
    "            \"memory_usage\": memory_usage,\n",
    "            \"model_size\": get_model_size_in_gb(model_int4)\n",
    "        }\n",
    "        \n",
    "        # Clean up\n",
    "        del model_int4\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error testing INT4 model: {e}\")\n",
    "    \n",
    "    # Print comparison results\n",
    "    print(\"\\nPerformance Comparison:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Model Type':<10} | {'Size (GB)':<10} | {'Memory (GB)':<12} | {'Time (s)':<10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_type, metrics in results.items():\n",
    "        print(f\"{model_type:<10} | {metrics.get('model_size', 'N/A'):<10.2f} | {metrics.get('memory_usage', 'N/A'):<12.2f} | {metrics.get('inference_time', 'N/A'):<10.4f}\")\n",
    "    \n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Example usage (commented out to avoid actual execution)\n",
    "if __name__ == \"__main__\":\n",
    "    import time\n",
    "    \n",
    "    # Uncomment any of these to run the respective functions\n",
    "    \n",
    "    # 1. Full GPTQ quantization with model export\n",
    "    # Note: This requires significant computation\n",
    "    # model_id = \"gpt2\"  # Use a small model for testing\n",
    "    # quantize_with_gptq_full(model_id, output_dir=\"./gpt2-GPTQ-4bit\")\n",
    "    \n",
    "    # 2. Full AWQ quantization with model export\n",
    "    # model_id = \"gpt2\"  # Use a small model for testing\n",
    "    # quantize_with_awq_full(model_id, output_dir=\"./gpt2-AWQ-4bit\")\n",
    "    \n",
    "    # 3. Run ExLlama backend example (for already-quantized GPTQ models)\n",
    "    # run_exllama_example()\n",
    "    \n",
    "    # 4. Run SmoothQuant example\n",
    "    # run_smoothquant_example()\n",
    "    \n",
    "    # 5. Compare different quantization methods\n",
    "    # quantize_and_compare_performance()\n",
    "    \n",
    "    print(\"Script completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Practical Example: Loading and Using Quantized Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Practical Example: Loading and Using Quantized Models\n",
    "# ====================================================\n",
    "# This script demonstrates how to load and use pre-quantized models from Hugging Face\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, TextStreaming\n",
    "import time\n",
    "import gc\n",
    "\n",
    "def load_and_run_4bit_model():\n",
    "    \"\"\"\n",
    "    Load a 4-bit quantized model from Hugging Face and run inference\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading 4-bit Quantized Model ===\")\n",
    "    \n",
    "    # Import required libraries\n",
    "    from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    \n",
    "    # Model ID - you can replace this with any model that supports 4-bit quantization\n",
    "    model_id = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your model choice\n",
    "    \n",
    "    # Configure 4-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,                       # Load in 4-bit precision\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,   # Compute dtype (bfloat16 or float16)\n",
    "        bnb_4bit_use_double_quant=True,          # Use nested quantization\n",
    "        bnb_4bit_quant_type=\"nf4\",               # Normalized float 4-bit (nf4) or pure int4 (fp4)\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with quantization\n",
    "    print(\"Loading model in 4-bit precision...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,  # Apply quantization\n",
    "        device_map=\"auto\",                        # Automatically distribute across available devices\n",
    "        trust_remote_code=True,                   # Trust remote code if needed\n",
    "    )\n",
    "    \n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"Model loaded in {loading_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate and display memory usage\n",
    "    memory_used = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"GPU memory used: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Run sample inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    \n",
    "    # For LLaMA-2-Chat models, use the proper prompt format\n",
    "    prompt = \"\"\"<s>[INST] Write a short paragraph about climate change. [/INST]\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with streaming for a more interactive experience\n",
    "    streamer = TextStreaming(tokenizer)\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\nGenerated text:\")\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            streamer=streamer\n",
    "        )\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n4-bit model demo completed\")\n",
    "\n",
    "def load_and_run_8bit_model():\n",
    "    \"\"\"\n",
    "    Load an 8-bit quantized model from Hugging Face and run inference\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading 8-bit Quantized Model ===\")\n",
    "    \n",
    "    # Import required libraries\n",
    "    from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "    \n",
    "    # Model ID - you can replace this with any model that supports 8-bit quantization\n",
    "    model_id = \"meta-llama/Llama-2-7b-chat-hf\"  # Replace with your model choice\n",
    "    \n",
    "    # Configure 8-bit quantization\n",
    "    quantization_config = BitsAndBytesConfig(\n",
    "        load_in_8bit=True,                      # Load in 8-bit precision\n",
    "        llm_int8_threshold=6.0,                 # LLM.int8() threshold\n",
    "        llm_int8_has_fp16_weight=False,         # Whether INT8 was combined with FP16\n",
    "    )\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load model with quantization\n",
    "    print(\"Loading model in 8-bit precision...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_id,\n",
    "        quantization_config=quantization_config,  # Apply quantization\n",
    "        device_map=\"auto\",                        # Automatically distribute across available devices\n",
    "        trust_remote_code=True,                   # Trust remote code if needed\n",
    "    )\n",
    "    \n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"Model loaded in {loading_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate and display memory usage\n",
    "    memory_used = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"GPU memory used: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Run sample inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    \n",
    "    # For LLaMA-2-Chat models, use the proper prompt format\n",
    "    prompt = \"\"\"<s>[INST] Explain the concept of quantum computing in simple terms. [/INST]\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate with streaming for a more interactive experience\n",
    "    streamer = TextStreaming(tokenizer)\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\nGenerated text:\")\n",
    "    with torch.no_grad():\n",
    "        _ = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "            streamer=streamer\n",
    "        )\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\n8-bit model demo completed\")\n",
    "\n",
    "def load_and_run_gptq_model():\n",
    "    \"\"\"\n",
    "    Load a GPTQ-quantized model from Hugging Face and run inference\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading GPTQ Quantized Model ===\")\n",
    "    \n",
    "    try:\n",
    "        from auto_gptq import AutoGPTQForCausalLM\n",
    "        GPTQ_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        GPTQ_AVAILABLE = False\n",
    "        print(\"GPTQ support requires the auto-gptq library.\")\n",
    "        print(\"Install with: pip install auto-gptq\")\n",
    "        return\n",
    "    \n",
    "    if not GPTQ_AVAILABLE:\n",
    "        return\n",
    "    \n",
    "    # Model ID for a GPTQ-quantized model\n",
    "    model_id = \"TheBloke/Llama-2-7b-Chat-GPTQ\"  # Replace with your model choice\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load GPTQ model\n",
    "    print(\"Loading GPTQ model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoGPTQForCausalLM.from_quantized(\n",
    "        model_id,\n",
    "        use_triton=False,         # Whether to use Triton for inference\n",
    "        use_exllama=True,         # Whether to use exllama for inference\n",
    "        device_map=\"auto\",        # Automatically distribute across available devices\n",
    "        trust_remote_code=True,   # Trust remote code if needed\n",
    "    )\n",
    "    \n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"Model loaded in {loading_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate and display memory usage\n",
    "    memory_used = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"GPU memory used: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Run sample inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    \n",
    "    # For LLaMA-2-Chat models, use the proper prompt format\n",
    "    prompt = \"\"\"<s>[INST] Write a brief poem about artificial intelligence. [/INST]\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\nGenerated text:\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    # Decode and print generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nGPTQ model demo completed\")\n",
    "\n",
    "def load_and_run_awq_model():\n",
    "    \"\"\"\n",
    "    Load an AWQ-quantized model from Hugging Face and run inference\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Loading AWQ Quantized Model ===\")\n",
    "    \n",
    "    try:\n",
    "        from awq import AutoAWQForCausalLM\n",
    "        AWQ_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        AWQ_AVAILABLE = False\n",
    "        print(\"AWQ support requires the autoawq library.\")\n",
    "        print(\"Install with: pip install autoawq\")\n",
    "        return\n",
    "    \n",
    "    if not AWQ_AVAILABLE:\n",
    "        return\n",
    "    \n",
    "    # Model ID for an AWQ-quantized model\n",
    "    model_id = \"TheBloke/Llama-2-7b-Chat-AWQ\"  # Replace with your model choice\n",
    "    \n",
    "    # Load tokenizer\n",
    "    print(\"Loading tokenizer...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "    # Load AWQ model\n",
    "    print(\"Loading AWQ model...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    model = AutoAWQForCausalLM.from_quantized(\n",
    "        model_id,\n",
    "        device_map=\"auto\",        # Automatically distribute across available devices\n",
    "        trust_remote_code=True,   # Trust remote code if needed\n",
    "    )\n",
    "    \n",
    "    loading_time = time.time() - start_time\n",
    "    print(f\"Model loaded in {loading_time:.2f} seconds\")\n",
    "    \n",
    "    # Calculate and display memory usage\n",
    "    memory_used = torch.cuda.max_memory_allocated() / (1024 ** 3)  # Convert to GB\n",
    "    print(f\"GPU memory used: {memory_used:.2f} GB\")\n",
    "    \n",
    "    # Run sample inference\n",
    "    print(\"\\nRunning inference...\")\n",
    "    \n",
    "    # For LLaMA-2-Chat models, use the proper prompt format\n",
    "    prompt = \"\"\"<s>[INST] What are the benefits and drawbacks of remote work? [/INST]\"\"\"\n",
    "    \n",
    "    # Tokenize input\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate text\n",
    "    print(\"\\nGenerated text:\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.95,\n",
    "            do_sample=True,\n",
    "        )\n",
    "    \n",
    "    # Decode and print generated text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(generated_text)\n",
    "    \n",
    "    # Clean up to free memory\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(\"\\nAWQ model demo completed\")\n",
    "\n",
    "def benchmark_and_compare_models():\n",
    "    \"\"\"\n",
    "    Benchmark and compare different quantized models\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Benchmarking Different Quantized Models ===\")\n",
    "    \n",
    "    # This is a simplified benchmark for demonstration purposes\n",
    "    # For a real benchmark, you would typically:\n",
    "    # 1. Use the same model architecture quantized with different methods\n",
    "    # 2. Run multiple iterations with different prompts\n",
    "    # 3. Measure throughput (tokens/sec), latency, and memory usage\n",
    "    # 4. Compare quality metrics (perplexity, accuracy on tasks)\n",
    "    \n",
    "    models_to_benchmark = [\n",
    "        {\n",
    "            \"name\": \"4-bit (NF4)\",\n",
    "            \"load_function\": load_and_run_4bit_model,\n",
    "            \"results\": {}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"8-bit\",\n",
    "            \"load_function\": load_and_run_8bit_model,\n",
    "            \"results\": {}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"GPTQ\",\n",
    "            \"load_function\": load_and_run_gptq_model,\n",
    "            \"results\": {}\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"AWQ\",\n",
    "            \"load_function\": load_and_run_awq_model,\n",
    "            \"results\": {}\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Run benchmarks\n",
    "    print(\"Starting benchmark...\")\n",
    "    \n",
    "    for model_config in models_to_benchmark:\n",
    "        try:\n",
    "            print(f\"\\nBenchmarking {model_config['name']} model...\")\n",
    "            \n",
    "            # Run the model and collect metrics\n",
    "            # In a real benchmark, you would measure these metrics directly\n",
    "            # rather than using the convenience functions\n",
    "            model_config[\"load_function\"]()\n",
    "            \n",
    "            # Simplified - in reality you would collect actual measurements\n",
    "            model_config[\"results\"][\"success\"] = True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error benchmarking {model_config['name']} model: {e}\")\n",
    "            model_config[\"results\"][\"success\"] = False\n",
    "    \n",
    "    print(\"\\nBenchmark completed.\")\n",
    "\n",
    "# Main demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"=== Quantized Model Usage Demonstration ===\")\n",
    "    print(\"This script shows how to load and use different types of quantized models\")\n",
    "    \n",
    "    # Uncomment the functions you want to run\n",
    "    # Each function demonstrates loading and running a different type of quantized model\n",
    "    \n",
    "    # load_and_run_4bit_model()\n",
    "    # load_and_run_8bit_model()\n",
    "    # load_and_run_gptq_model()\n",
    "    # load_and_run_awq_model()\n",
    "    \n",
    "    # To run a benchmark comparing different models\n",
    "    # benchmark_and_compare_models()\n",
    "    \n",
    "    print(\"\\nDemonstration completed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
