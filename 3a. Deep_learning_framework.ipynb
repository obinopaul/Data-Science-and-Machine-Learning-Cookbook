{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep alive (right click 'inspect', then 'console', clear console and paste the below code)\n",
    "function ConnectButton(){\n",
    "    console.log(\"Connect pushed\"); \n",
    "    document.querySelector(\"#top-toolbar > colab-connect-button\").shadowRoot.querySelector(\"#connect\").click() \n",
    "}\n",
    "\n",
    "var colab = setInterval(ConnectButton,600000);   #to connect for 10 mins\n",
    "\n",
    "#clearInterval(connect)     #to clear the keep alive interval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mounting Drive and Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') \n",
    "\n",
    "#install libraries \n",
    "!pip install -q -r '/content/drive/MyDrive/Colab Notebooks/requirements.txt' \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#Basic Libraries / Data Analytics Libraries \n",
    "import random \n",
    "import sys \n",
    "import os \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path \n",
    "\n",
    "#Machine Learning Libraries \n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "\n",
    "#deep learning libraries \n",
    "import cv2 \n",
    "import torch.nn.init as init \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF \n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split, Subset, WeightedRandomSampler \n",
    "from PIL import Image \n",
    "import torchvision\n",
    "from torchvision.utils import save_image  \n",
    "from torchvision import datasets \n",
    "from torchvision.datasets import DatasetFolder, ImageFolder\n",
    "import torchvision.transforms as transforms \n",
    "import torchmetrics \n",
    "from torchsummary import summary \n",
    "import torchinfo\n",
    "import timm \n",
    "import albumentations as A \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Device-agnostic code (using PyTorch on CPU, GPU or MPS)\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\" # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = \"mps:0\" # Apple GPU\n",
    "else:\n",
    "    device = \"cpu\" # Defaults to CPU if NVIDIA GPU/Apple GPU aren't available"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Visualize the images from the directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set seed\n",
    "random.seed(42)  # <- try changing this and see what happens\n",
    "\n",
    "# Set directory path\n",
    "directory_path = \"path_to_directory\"  # Replace with the path to your image directory.  NB: Use TRAIN dataset\n",
    "\n",
    "# Get all image paths from the directory\n",
    "image_path_list = []\n",
    "for root, dirs, files in os.walk(directory_path):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(('.jpg', '.jpeg', '.png', '.gif')):\n",
    "            image_path_list.append(os.path.join(root, file))\n",
    "\n",
    "# Repeat the process multiple times to visualize different random images\n",
    "num_images_to_visualize = 5  # Set the desired number of random images to visualize\n",
    "\n",
    "for _ in range(num_images_to_visualize):\n",
    "    # Get random image path\n",
    "    random_image_path = random.choice(image_path_list)\n",
    "\n",
    "    # Get image class from path name (the image class is the name of the directory where the image is stored)\n",
    "    image_class = os.path.basename(os.path.dirname(random_image_path))\n",
    "\n",
    "    # Open image\n",
    "    img = Image.open(random_image_path)\n",
    "\n",
    "    # Print metadata\n",
    "    print(f\"Random image path: {random_image_path}\")\n",
    "    print(f\"Image class: {image_class}\")\n",
    "    print(f\"Image height: {img.height}\")\n",
    "    print(f\"Image width: {img.width}\")\n",
    "\n",
    "    # Display the image\n",
    "    img.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Visualize the transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_transformed_images(image_paths, transform, n=1, seed=42):\n",
    "    \"\"\"Plots a series of random images from image_paths.\n",
    "\n",
    "    Will open n image paths from image_paths, transform them\n",
    "    with transform and plot them side by side.\n",
    "\n",
    "    Args:\n",
    "        image_paths (list): List of target image paths.\n",
    "        transform (PyTorch Transforms): Transforms to apply to images.\n",
    "        n (int, optional): Number of images to plot. Defaults to 3.\n",
    "        seed (int, optional): Random seed for the random generator. Defaults to 42.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    random_image_paths = random.sample(image_paths, k=n)\n",
    "    for image_path in random_image_paths:\n",
    "        with Image.open(image_path) as f:\n",
    "            fig, ax = plt.subplots(1, 2)\n",
    "            ax[0].imshow(f)\n",
    "            ax[0].set_title(f\"Original \\nSize: {f.size}\")\n",
    "            ax[0].axis(\"off\")\n",
    "\n",
    "            # Transform and plot image\n",
    "            transformed_image = transform(f)\n",
    "            transformed_image = transformed_image.numpy()  # Convert transformed image to numpy array\n",
    "            transformed_image = np.transpose(transformed_image, (1, 2, 0))  # Reshape to (H, W, C)\n",
    "            ax[1].imshow(transformed_image)\n",
    "            ax[1].set_title(f\"Transformed \\nSize: {transformed_image.shape}\")\n",
    "            ax[1].axis(\"off\")\n",
    "\n",
    "            fig.suptitle(f\"Class: {Path(image_path).parent.stem}\", fontsize=16)\n",
    "\n",
    "# Write transform for image\n",
    "image_transforms = transforms.Compose([\n",
    "    # Resize the images to 155 * 155\n",
    "    transforms.Resize(size=(155, 155)),\n",
    "    transforms.RandomCrop(size=145),\n",
    "    transforms.ColorJitter(brightness=0.4, contrast=0.8, saturation=0.4, hue=0.1),\n",
    "    # Turn the image into a torch.Tensor\n",
    "    transforms.ToTensor(),  # this also converts all pixel values from 0 to 255 to be between 0.0 and 1.0\n",
    "    # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    # Flip the images randomly on the horizontal\n",
    "    transforms.RandomHorizontalFlip(p=0.5),  # p = probability of flip, 0.5 = 50% chance\n",
    "    transforms.RandomRotation((-270, 270)),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.Lambda(lambda x: TF.affine(x, angle=0, translate=(10, 10), scale=1.0, shear=0)),  # Apply affine transformation\n",
    "\n",
    "])\n",
    "\n",
    "# image_transforms = A.Compose(\n",
    "#     [\n",
    "#     A.Resize(Width = 1920, height = 1080),\n",
    "#     A.RandomCrop(Width = 1280, height = 720),\n",
    "#     A.Rotate(limit = 40, p = 0.9),\n",
    "#     A.HorizontalFlip(p = 0.5),\n",
    "#     A.VerticalFlip(p = 0.1),\n",
    "#     A.RGBShift(r_shift_limit = 25, g_shift_limit = 25, b_shift_limit = 25, p = 0.5 ),\n",
    "#     A.OneOf([\n",
    "#         A.Blur(blur_limit = 3, p = 0.5),\n",
    "#         A.ColorJitter(p = 0.5)\n",
    "#             ], p = 1.0)  \n",
    "#     ]\n",
    "#     ) \n",
    "\n",
    "\n",
    "plot_transformed_images(image_path_list,\n",
    "                        transform=image_transforms,\n",
    "                        n=1)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Load the Data (generic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_mnist = np.loadtxt(open(r\"/content/sample_data/mnist_train_small.csv\" , 'rb'), delimiter = ',' )\n",
    "print(\"shape: \", data_mnist.shape)\n",
    "\n",
    "#class names\n",
    "class_labels = data_mnist[:, 0].astype(int)\n",
    "unique_labels = np.unique(class_labels)\n",
    "class_names = [str(label) for label in unique_labels]\n",
    "\n",
    "\n",
    "#NB: Use Batch or Layer normalization in place of MinMaxScaler or StandardScaler for deep learning\n",
    "scaler = MinMaxScaler() \n",
    "inputs = data_mnist[:,1:]\n",
    "inputs = scaler.fit_transform(inputs)\n",
    "labels = data_mnist[:,0] \n",
    "\n",
    "# train, test = torch.utils.data.random_split(train_dataset, [800, 200]) #train/test split \n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, random_state=23, train_size=0.9)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# inputs \n",
    "X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.long)\n",
    "\n",
    "#dataset \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Load the Data (torchvision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformation to apply to the images\n",
    "\n",
    "#1. Using a DatasetFolder\n",
    "# Create an instance of the DatasetFolder\n",
    "dataset = torchvision.datasets.DatasetFolder(\n",
    "                            root='Dataset/',\n",
    "                            loader=torchvision.datasets.folder.default_loader,  # Use the default image loader\n",
    "                            extensions=\".jpg\",  # Specify the file extensions of the images\n",
    "                            transform=image_transforms  # Apply the defined transformation pipeline\n",
    "                            )\n",
    "class_labels = dataset.classes # Get the class labels\n",
    "print(class_labels) # Print the class labels\n",
    "\n",
    "\n",
    "\n",
    "#2. Using ImageFolder (a subclass of DatasetFolder)\n",
    "# Load the dataset from the image folders \n",
    "dataset = torchvision.datasets.ImageFolder(root='Dataset/', transform=image_transforms)\n",
    "class_labels = dataset.classes # Get the class labels\n",
    "print(class_labels) # Print the class labels\n",
    "#NB: ImageFolder simplifies the process of loading such datasets by automatically assigning class labels based on \n",
    "    # folder names and also uses a default data loader (unlike DatasetFolder where you can specify and custom dataloader)\n",
    "\n",
    "\n",
    "#3. Using Custom Dataset\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(csv_file)  # Read the CSV file\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.data.iloc[idx, 0]  # Get the image file name from the CSV\n",
    "        img_path = os.path.join(self.root_dir, img_name)\n",
    "        image = Image.open(img_path).convert('RGB')  # Open and convert the image to RGB\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)  # Apply transformations if provided\n",
    "\n",
    "        label = self.data.iloc[idx, 1]  # Get the corresponding label from the CSV\n",
    "\n",
    "        return image, label\n",
    "    \n",
    "    def get_class_names(self):\n",
    "        \"\"\"\n",
    "        Retrieve the class names.\n",
    "\n",
    "        Returns:\n",
    "            list: A list of strings.\n",
    "        \"\"\"\n",
    "        class_names = self.data['label'].unique().tolist()\n",
    "        return class_names\n",
    "\n",
    "face_dataset = CustomImageDataset(csv_file='data/faces/face_landmarks.csv',\n",
    "                                    root_dir='data/faces/', transform = image_transforms)\n",
    "\n",
    "class_names = face_dataset.get_class_names() # Retrieve the class names\n",
    "print(class_names)  # Print the class names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize images from the loaded dataset \n",
    "\n",
    "fig,axs = plt.subplots(3,3,figsize=(5,5))\n",
    "\n",
    "for ax in axs.flatten():\n",
    "  # select a random picture\n",
    "  randidx = np.random.choice(len(train_dataset.targets))\n",
    "\n",
    "  # extract that image\n",
    "  pic, label = train_dataset[randidx]\n",
    "\n",
    "  # Transpose the image tensor to (H, W, C) format\n",
    "  pic = np.transpose(pic, (1, 2, 0))        # sometimes: pic = np.transpose(pic.cpu().detach().numpy(), (1,2,0))\n",
    "\n",
    "  # Normalize the pixel values to the range [0, 1]\n",
    "  pic = pic.clip(0, 1)  # Clip values outside [0, 1]\n",
    "\n",
    "  # and its label\n",
    "  label = train_dataset.classes[train_dataset.targets[randidx]]\n",
    "\n",
    "  # and show!\n",
    "  ax.imshow(pic)\n",
    "  ax.text(16,0,label,ha='center',fontweight='bold',color='k',backgroundcolor='y')\n",
    "  ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show() \n",
    "\n",
    "\n",
    "# #if you want to save the transformed images\n",
    "# img_num = 0\n",
    "# for _ in range(10):\n",
    "#     for img, label in train_dataset:\n",
    "#         save_image(img, 'img' + str(img_num) + '.png')\n",
    "#         img_num +=1 "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#When using ImageFolder, DatasetFolder, or a custom dataset to load your data, you can split it into training and \n",
    "# testing sets using the random_split function from PyTorch\n",
    "\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "train_size = int(0.8 * len(dataset))  # 80% for training, adjust as desired\n",
    "test_size = len(dataset) - train_size\n",
    "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
    "\n",
    "# #or\n",
    "# train_dataset = Subset(test_dataset, randidx[:6000]) #subset of test (or use range(2000) instead of randidx[;6000])\n",
    "# test_dataset = Subset(test_dataset, randidx[6000:]) #subset of test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Balancing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to deal with unbalanced data\n",
    "    # Get more data\n",
    "    # undersample\n",
    "    # Oversample\n",
    "        # create multiple copies of the rare data. (be careful because it increases the risk of overfitting)\n",
    "    # Data Augmentation\n",
    "        # Add new features as non-linear transformations of existing data\n",
    "    # Create synthetic samples\n",
    "        # you can use SMOTE\n",
    "    # Consider whether non-deep learning would be better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root_dir = directory_path       #root directory of the train dataset\n",
    "class_weights = []\n",
    "for root, subdir, files in os.walk(root_dir):\n",
    "    if len(files) > 0:\n",
    "        class_weights.append(1/len(files))\n",
    "\n",
    "sample_weights = [0] *len(train_dataset)\n",
    "\n",
    "for idx, (data, label) in enumerate(train_dataset):\n",
    "    class_weight = class_weights[label]\n",
    "    sample_weights[idx] = class_weight\n",
    "\n",
    "# Create a sampler with weighted sampling\n",
    "sampler = WeightedRandomSampler(sample_weights, len(sample_weights), replacement=True)\n",
    "\n",
    "\n",
    "\n",
    "# # Calculate the weights for each sample based on class frequencies\n",
    "# weights = [1.0 / class_counts[label] for _, label in train_dataset]\n",
    "\n",
    "# # Create a sampler with weighted sampling\n",
    "# sampler = WeightedRandomSampler(weights, len(train_dataset), replacement=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataloader\n",
    "batch_size = 32\n",
    "shuffle = True \n",
    "drop_last = True\n",
    "\n",
    "\n",
    "def load_dataloader (batch_size = batch_size, shuffle = True, drop_last = True, sampler = sampler):\n",
    "    \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                drop_last=drop_last, sampler=sampler)   #don't shuffle when using sampler\n",
    "    test_dataloader = DataLoader(test_dataset, shuffle = False, batch_size=len(test_dataset))\n",
    "    \n",
    "    return train_dataloader, test_dataloader\n",
    "\n",
    "\n",
    "train_dataloader, test_dataloader = load_dataloader (batch_size = 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common hyperparameters/metaparameters to consider when building the model include:\n",
    "\n",
    "# Model architecture: \n",
    "    # Determines the overall structure and complexity of the neural network.\n",
    "    \n",
    "# Number of hidden layers:\n",
    "    # Determines the depth of the neural network architecture and influences the model's capacity to learn complex\n",
    "    # patterns.\n",
    "    \n",
    "# Number of neurons per layer:\n",
    "    # Defines the width of the neural network architecture and affects the model's representational capacity and\n",
    "    # computational efficiency.\n",
    "    \n",
    "# Activation functions:\n",
    "    # Determines the non-linear transformation applied to the output of each neuron, introducing non-linearity into\n",
    "    # the model.\n",
    "    \n",
    "# Batch Normalization (nn.BatchNorm2d(nUnits))\n",
    "    # used to normalize the weights and input data\n",
    "    \n",
    "# Dropout (nn.Dropout(p = 0.2))\n",
    "    # Controls the regularization technique of randomly dropping out a fraction of neurons during training, which\n",
    "    # helps prevent overfitting.\n",
    "    \n",
    "# Weight Initialization:\n",
    "    # Defines the initial values of the weights in the neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Build the Model from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider the following when building your model architecture:\n",
    "    # Model Complexity\n",
    "    # Data Augmentation\n",
    "    # Input Image Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model \n",
    "#ANN Model\n",
    "\n",
    "class iris_model(nn.Module):\n",
    "    \"\"\"Some Information about iris_model\"\"\"\n",
    "    def __init__(self, weight_init='default'):\n",
    "        super(iris_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        \n",
    "        if weight_init == 'default':\n",
    "            pass  # Default weight initialization\n",
    "\n",
    "        elif weight_init == 'xavier_uniform':\n",
    "            self._init_weights_xavier_uniform()\n",
    "\n",
    "        elif weight_init == 'kaiming_normal':\n",
    "            self._init_weights_kaiming_normal()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        # out = self.fca(out) \n",
    "        out = F.relu(self.fc2(out))\n",
    "        # out = self.fcb(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    def _init_weights_xavier_uniform(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def _init_weights_kaiming_normal(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "model = iris_model(weight_init='default')    #initializing the model  #model = iris_model(weight_init='xavier_uniform') \n",
    "\n",
    "\n",
    "#2\n",
    "\n",
    "# def create_model(nUnits, nLayers, weight_init):\n",
    "#     class iris_model(nn.Module):\n",
    "#         def __init__(self):\n",
    "#             super().__init__()\n",
    "\n",
    "#             # Create dictionary to store the layers\n",
    "#             self.layers = nn.ModuleDict()\n",
    "#             self.nLayers = nLayers \n",
    "#             self.weight_init = weight_init\n",
    "\n",
    "#             ### Input layer\n",
    "#             self.layers['input'] = nn.Linear(4, nUnits)\n",
    "#             self.layers['input_bn'] = nn.BatchNorm1d(nUnits)\n",
    "#             self.layers['input_dropout'] = nn.Dropout(0.2)\n",
    "            \n",
    "#             ### Hidden layers\n",
    "#             for i in range(nLayers):\n",
    "#                 self.layers[f'hidden{i}'] = nn.Linear(nUnits, nUnits)\n",
    "#                 self.layers[f'hidden{i}_bn'] = nn.BatchNorm1d(nUnits)\n",
    "#                 self.layers[f'hidden{i}_dropout'] = nn.Dropout(0.2)\n",
    "\n",
    "#             ### Output layer\n",
    "#             self.layers['output'] = nn.Linear(nUnits, 3)\n",
    "        \n",
    "#             # Initialize weights\n",
    "#             self._initialize_weights()\n",
    "            \n",
    "        \n",
    "#         # Forward pass\n",
    "#         def forward(self, x):\n",
    "#             # Input layer\n",
    "#             x = self.layers['input'](x)\n",
    "#             x = self.layers['input_bn'](x)\n",
    "#             x = F.relu(x)\n",
    "#             x = self.layers['input_dropout'](x)\n",
    "\n",
    "#             # Hidden layers\n",
    "#             for i in range(self.nLayers):\n",
    "#                 x = self.layers[f'hidden{i}'](x)\n",
    "#                 x = self.layers[f'hidden{i}_bn'](x)\n",
    "#                 x = F.relu(x)\n",
    "#                 x = self.layers[f'hidden{i}_dropout'](x)\n",
    "                \n",
    "#             # Output layer\n",
    "#             x = self.layers['output'](x)    #or x = F.sigmoid(self.layers['output](x)) for Binary classification \n",
    "            \n",
    "#             return x \n",
    "        \n",
    "#         def _initialize_weights(self):\n",
    "#             for name, module in self.layers.items():\n",
    "#                 if isinstance(module, nn.Linear):\n",
    "#                     weight_init = self.weight_init.get(name, 'default') #works well with sigmoid (uniform distribution)\n",
    "#                     if weight_init == 'xavier_uniform':     #works well with sigmoid\n",
    "#                         init.xavier_uniform_(module.weight)\n",
    "#                     elif weight_init == 'kaiming_normal':   #works well with ReLU activation \n",
    "#                         init.kaiming_normal_(module.weight)\n",
    "#                     if module.bias is not None:\n",
    "#                         init.constant_(module.bias, 0)\n",
    "    \n",
    "#     return iris_model()\n",
    "\n",
    "# nUnits = 64\n",
    "# nLayers = 5\n",
    "# weight_init = {\n",
    "#     'input': 'default',\n",
    "#     'hidden0': 'kaiming_normal',\n",
    "#     'hidden1': 'kaiming_normal',\n",
    "#     'hidden2': 'kaiming_normal',\n",
    "#     # 'hidden3': 'kaiming_normal',\n",
    "#     'output': 'default'\n",
    "# }\n",
    "\n",
    "# model = create_model(nUnits, nLayers, weight_init)    #initializing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN Model\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes, input_shape:tuple):    #input_shape = (3, 224, 224) where 3 is for RGB\n",
    "        super(CNNModel, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        # Calculate linear layer input size based on the output size of the last convolutional layer\n",
    "        last_conv_out_shape = self._get_last_conv_output_shape(*input_shape)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(last_conv_out_shape, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)#reshapes the tensor to x.size(0) rows, while the number of columns is inferred automatically.\n",
    "        x = self.classifier(x)\n",
    "        # x = F.log_softmax(x, dim=1)  # LogSoftmax for classification (if using CrossEntropyLoss, then no need for this)\n",
    "        return x\n",
    "\n",
    "    def _get_last_conv_output_shape(self, in_channels, height, width):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(1, in_channels, height, width)\n",
    "            x = self.features(x)\n",
    "        return x.size(1) * x.size(2) * x.size(3)\n",
    "\n",
    "\n",
    "#x.view(x.size(0), -1) reshapes the tensor x to a 2D tensor of x.size(0) rows, and automatically inferring the colunns. \n",
    "\n",
    "# Create an instance of the model\n",
    "in_channels = 3  # Number of input channels (RGB)\n",
    "num_classes = 10  # Adjust according to your classification task\n",
    "input_shape = (3, 155, 155) \n",
    "\n",
    "model = CNNModel(in_channels, num_classes, input_shape)\n",
    "\n",
    "# Move the model to GPU if available\n",
    "model = model.to(device)\n",
    "\n",
    "# summary(model, (3, 224, 224)) #torchsummary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model summary\n",
    "\n",
    "print(torchinfo.summary(model, (3, 224, 224), batch_dim = 0, \n",
    "                col_names = ('input_size', 'output_size', 'num_params', 'kernel_size', \n",
    "                            'mult_adds', 'trainable'), verbose = 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Use a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are the situations where transfer learning may not be advisable or viable, presented in bullet points:\n",
    "\n",
    "# Insufficient similarity between the source and target tasks\n",
    "    # For example, if you are trying to translate text from English to French, and the pre-trained model was trained on \n",
    "    # a natural language processing task such as sentiment analysis, then transfer learning may not be helpful\n",
    "# Limited availability of labeled data for the source task, or the source model is not deep enough to be transferred.\n",
    "# Significant differences in data distribution between the source and target domains:\n",
    "    # For example, if the source data is collected from a different time period or from a different population, \n",
    "    # it may not capture the characteristics of the target data accurately\n",
    "# Target task requires learning task-specific features not relevant to the source task\n",
    "    # For instance, if the source task is image classification, but the target task involves object detection, \n",
    "    # the features learned by the source model may not be directly applicable, and training from scratch or using \n",
    "    # domain-specific pretraining may be more appropriate.\n",
    "# You have a large dataset of labeled data. It may be better to train your model from scratch\n",
    "# Privacy or security concerns regarding the source data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show all models in torchvision\n",
    "from torchvision import models\n",
    "\n",
    "dir(models) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True   # CuDNN (CUDA Deep Neural Network library) is used to enable CuDNN auto-tuning. \n",
    "    # When torch.backends.cudnn.benchmark is set to True, CuDNN will automatically find the best algorithm configuration \n",
    "    # for the specific input size and hardware during the first forward or backward pass of the network\n",
    "\n",
    "import torchvision.models as models \n",
    "import torch.hub as hub \n",
    "\n",
    "#1: from torchvision models\n",
    "model = models.resnet18(pretrained=True) # Load the pre-trained model from torchhub\n",
    "\n",
    "\n",
    "#2: from torchhub models\n",
    "model = hub.load('pytorch/vision', 'resnet18', pretrained=True)\n",
    "\n",
    "\n",
    "#3. fromm timm (It comes packaged with >700 pretrained models, and is designed to be flexible and easy to use)\n",
    "# (https://huggingface.co/docs/timm/feature_extraction) #learn to use timm\n",
    "# https://huggingface.co/docs/timm/models   #Timm SOTA models\n",
    "model = timm.create_model('resnet34', pretrained=True) \n",
    "model = timm.create_model('resnet34', pretrained=True, num_classes=10)\n",
    "avail_pretrained_models = timm.list_models(pretrained=True) # List Models with Pretrained Weights\n",
    "all_densenet_models = timm.list_models('*densenet*')    # search for model architectures using Wildcard as below\n",
    "\n",
    "\n",
    "#4: Loading a custom model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CustomModel, self).__init__()\n",
    "        # Load the pre-trained model\n",
    "        self.model = torch.load('paul_model.pt')\n",
    "\n",
    "        # Freeze the parameters of the pre-trained model\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Modify the last layer for your custom dataset\n",
    "        num_features = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "model = CustomModel(num_classes)\n",
    "\n",
    "model = model.to(device) # Move the model to GPU if available\n",
    "\n",
    "torchinfo.summary(model, (3, 224, 224), batch_dim = 0, \n",
    "                col_names = ('input_size', 'output_size', 'num_params', 'kernel_size', \n",
    "                            'mult_adds', 'trainable'), verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first visualize all the modules and submodules of the model to know which layer(s) will or will not be frozen\n",
    "\n",
    "for names, params in model.named_modules():\n",
    "# for names, params in model.named_children():\n",
    "    print(names)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  performing transfer learning\n",
    "\n",
    "for name, param in model.named_parameters():  #for param in model.parameters()[:5]: if you wish to freeze only a few parmas\n",
    "    # if \"classifier\" not in name:  # Unfreeze parameters of MobileNetV2\n",
    "    # if not name.startswith('fc'): # Exclude the fully connected layer (output layer)\n",
    "    param.requires_grad = False\n",
    "    # #NB: the more similar your data is to the data of the source model, the more layers you can freeze\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# Train only the classifier\n",
    "# ...\n",
    "\n",
    "# or\n",
    "\n",
    "# Freeze the first six layers - this assumes that the layers are labeled 'layer**'\n",
    "for name, param in model.named_parameters():\n",
    "    if \"layer\" in name and int(name.split(\".\")[1]) < 6:\n",
    "    # if 'layer4' not in name and 'fc' not in name : \n",
    "        param.requires_grad = False\n",
    "num_ftrs = model.fc.in_features\n",
    "num_classes = 10\n",
    "model.fc = nn.Linear(num_ftrs, num_classes)\n",
    "# Train the classifier and a few other layers\n",
    "\n",
    "# Print the number of trainable parameters\n",
    "num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Number of trainable parameters: {num_trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#various methods of Transfer learning:\n",
    "\n",
    "# 1. Finetuning the entire pre-trained model\n",
    "    # In this approach, you load a pre-trained model and replace the final fully connected layer (classifier) with a \n",
    "    # new one suitable for your specific task. You then train the entire model, including the pre-trained layers and \n",
    "    # the newly added classifier, with your custom dataset    \n",
    "    model_ft = models.resnet50(pretrained=True)\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    num_classes = 10\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    # Train the entire model\n",
    "    # ...\n",
    "    \n",
    "#2. Finetuning only the classifier: Similar to the previous approach, you load a pre-trained model, but instead of \n",
    "    # training the entire model, you freeze the parameters of the pre-trained layers and only train the newly added \n",
    "    # classifier. This is useful when you have limited training data or when the pre-trained model is very different \n",
    "    # from your target task.\n",
    "    model_conv = models.resnet50(pretrained=True)\n",
    "    for param in model_conv.named_parameters():  #for param in model.parameters()[:5]: if you wish to freeze only a few parmas\n",
    "        # if \"classifier\" not in name:  # Unfreeze parameters of MobileNetV2\n",
    "        param.requires_grad = False\n",
    "        # #NB: the more similar your data is to the data of the source model, the more layers you can freeze\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    num_classes = 10\n",
    "    model_conv.fc = nn.Linear(num_ftrs, num_classes)\n",
    "    # Train only the classifier\n",
    "    # ...\n",
    "    \n",
    "    # or\n",
    "    \n",
    "    # Freeze the first six layers - this assumes that the layers are labeled 'layer**'\n",
    "    for name, param in resnet.named_parameters():\n",
    "        if \"layer\" in name and int(name.split(\".\")[1]) < 6:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # Print the number of trainable parameters\n",
    "    num_trainable_params = sum(p.numel() for p in resnet.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
    "\n",
    "#3. Feature extraction is another way to perform transfer learning. In feature extraction, we start with a \n",
    "    # pre-trained model and then use the model's features as input to a new model. We can then train the new model on \n",
    "    # our own dataset.\n",
    "    # Load the pre-trained ResNet model\n",
    "    model = models.resnet18(pretrained=True)\n",
    "\n",
    "    # Freeze the weights of the model\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Extract the features from the model\n",
    "    features = model(images)\n",
    "    num_classes = 10\n",
    "    # Create a new model to classify the features\n",
    "    new_model = torch.nn.Linear(features.size(1), num_classes)\n",
    "    new_model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Visualize weight initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize weight initialization \n",
    "layers = [name.split('.')[1] for name, _ in model.named_modules() if '.' in name if 'dropout' not in name]\n",
    "for i in layers:\n",
    "    weight_i = model.layers[i].weight.detach()\n",
    "    # print(weight_i)\n",
    "    plt.hist(weight_i, bins=30, edgecolor='black')\n",
    "    plt.title(f'Weight Initialization - {i}')\n",
    "    plt.xlabel('Weight Values')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common hyperparameters/metaparameters to consider when training the model include:\n",
    "\n",
    "# Learning rate:\n",
    "    # Determines the step size during gradient descent optimization and affects the convergence speed \n",
    "    # and accuracy of the model.\n",
    "    \n",
    "# Dropout rate:\n",
    "    # Controls the regularization technique of randomly dropping out a fraction of neurons during training, which\n",
    "    # helps prevent overfitting. \n",
    "    \n",
    "# Batch size:\n",
    "    # Specifies the number of training samples propagated through the network before updating the model's weights.\n",
    "    \n",
    "# Number of epochs:\n",
    "    # Specifies the number of times the entire training dataset is passed through the model during training.\n",
    "    \n",
    "# Regularization techniques:\n",
    "    # Include methods like L1 and L2 regularization, which help prevent overfitting by adding penalties to the \n",
    "    # loss function. \n",
    "    \n",
    "# Optimizer:\n",
    "    # Specifies the optimization algorithm used to update the model's weights during training, such as \n",
    "    # Stochastic Gradient Descent (SGD), Adam, or RMSprop.\n",
    "    \n",
    "# Loss function:\n",
    "    # Defines the objective function used to measure the discrepancy between the predicted output and the \n",
    "    # true output during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 9\n",
    "learning_rate = 0.003\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion = criterion.to(device) \n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate) # weight_decay=0.0008018107002058151)\n",
    "# Define the learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#overfit a single batch (by adjusting the epochs and batch size)\n",
    "\n",
    "num_epochs = 20\n",
    "batch_size = 15\n",
    "batchLoss = []\n",
    "\n",
    "train_dataloader, test_dataloader = load_dataloader (batch_size = batch_size, shuffle = True, drop_last = True)\n",
    "inputs, labels = next(iter(train_dataloader))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    # Zero the parameter gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    # Backward pass and optimization\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    # Compute accuracy on the training set\n",
    "    predictions = torch.argmax(outputs, axis=1)\n",
    "    \n",
    "    batchLoss.append(loss.item())\n",
    "    accuracy = torchmetrics.functional.classification.accuracy(predictions, labels, task='multiclass',\n",
    "                                                                num_classes=num_classes) * 100\n",
    "\n",
    "    if epoch % 2 == 0:\n",
    "        msg = f\"Epoch {epoch}/{num_epochs}: Loss = {np.mean(batchLoss):.4f}, train accuracy = {accuracy:.2f}%\"\n",
    "        sys.stdout.write('\\r' + msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "## metric = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)    (.Precision(), .Recall(), .F1Score(), .ConfusionMatrix())\n",
    "                #see doc. https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#functional-interface \n",
    "\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0032734813343726263\n",
    "losses = torch.zeros(num_epochs)\n",
    "ongoing_accuracy = []\n",
    "ongoing_accuracy_test = []\n",
    "num_classes = 10\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=0.0008018107002058151)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "# device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model and data to the appropriate device (e.g., GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Variables to track the best model and accuracy\n",
    "best_accuracy = 0.0\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batchAcc = []\n",
    "    batchLoss = []\n",
    "\n",
    "    # Iterate over the training dataloader\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batchLoss.append(loss.item())\n",
    "\n",
    "        # Compute accuracy on the training set\n",
    "        predictions = torch.argmax(outputs, axis=1)\n",
    "        accuracy = torchmetrics.functional.classification.accuracy(predictions, labels, task='multiclass',\n",
    "                                                                    num_classes=num_classes) * 100\n",
    "        # accuracy = torchmetrics.functional.classification.accuracy(predictions, labels, task='multiclass', num_classes=num_classes) \n",
    "        #                                                     (or metric(predictions, labels))\n",
    "        # accuracy = torchmetrics.functional.classification.binary_accuracy (predicted, labels, threshold = 0.5)   #for binary classification\n",
    "        # r2score = torchmetrics.functional.r2_score(preds, target) \n",
    "        batchAcc.append(accuracy.item())\n",
    "\n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    ongoing_accuracy.append(np.mean(batchAcc.to(device)))\n",
    "    losses[epoch] = np.mean(batchLoss)\n",
    "\n",
    "    # Print loss and accuracy for the epoch\n",
    "    if epoch % 10 == 0:\n",
    "        msg = f\"Epoch {epoch}/{num_epochs}: Loss = {np.mean(batchLoss):.4f}, Accuracy = {np.mean(batchAcc):.2f}%\"\n",
    "        sys.stdout.write('\\r' + msg)\n",
    "\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        batchAcc_test = []\n",
    "        for data in test_dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "            accuracy = torchmetrics.functional.classification.accuracy(predicted, labels,\n",
    "                                                                    task='multiclass', \n",
    "                                                                       num_classes=num_classes) * 100\n",
    "            batchAcc_test.append(accuracy.cpu())\n",
    "            \n",
    "    test_accuracy = np.mean(batchAcc_test)\n",
    "    ongoing_accuracy_test.append(test_accuracy)\n",
    "\n",
    "    if test_accuracy > best_accuracy:\n",
    "        best_accuracy = test_accuracy\n",
    "        best_model_state = model.state_dict().copy()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        msg = f\"Epoch {epoch}/{num_epochs}: Loss = {np.mean(batchLoss):.4f}, train accuracy = {np.mean(batchAcc):.2f}%, test accuracy = {test_accuracy:.2f}%\"\n",
    "        sys.stdout.write('\\r' + msg)\n",
    "\n",
    "print(' ')\n",
    "print('Finished Training')\n",
    "print(' ')\n",
    "\n",
    "# Load the best model state\n",
    "model.load_state_dict(best_model_state)\n",
    "\n",
    "# Report accuracy\n",
    "print('Final accuracy (eval): {:.2f}%'.format(ongoing_accuracy_test[-1]))\n",
    "print('Best accuracy (eval): {:.2f}%'.format(best_accuracy)) \n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(13, 4))\n",
    "\n",
    "ax[0].plot(losses.detach())\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('Epoch')\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "ax[1].plot(ongoing_accuracy, label='Training Accuracy')\n",
    "ax[1].plot(ongoing_accuracy_test, label='Evaluation Accuracy')\n",
    "ax[1].set_ylabel('Accuracy')\n",
    "ax[1].set_xlabel('Epoch')\n",
    "ax[1].set_title('Accuracy')\n",
    "ax[1].legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# run training again to see whether this performance is consistent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Compare predicted values with the actual test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_pred, y_test): \n",
    "    \"\"\"\n",
    "    Plots the predicted and actual values on separate scatter plots.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot the actual values\n",
    "    ax1.scatter(range(len(y_test)), y_test, label='Actual Values')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Actual Values')\n",
    "    ax1.set_title('Scatter plot of Actual Values')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot the predicted values\n",
    "    ax2.scatter(range(len(y_pred)), y_pred, label='Predicted Values')\n",
    "    ax2.set_xlabel('Index')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title('Scatter plot of Predicted Values')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(predicted, labels)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Confusion Matrix, Precision, Recall, Accuracy, F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (roc_auc_score,roc_curve,precision_recall_curve, auc,\n",
    "                            classification_report, confusion_matrix, average_precision_score,\n",
    "                            accuracy_score,silhouette_score,mean_squared_error)\n",
    "from inspect import signature\n",
    "\n",
    "\n",
    "#confusion matrix\n",
    "accuracy = accuracy_score(predicted, labels) \n",
    "class_names = train_dataset.classes \n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                        normalize=False,\n",
    "                        title='Confusion matrix',\n",
    "                        cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                horizontalalignment=\"center\",\n",
    "                color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_confusion_matrix(labels, predicted, classes=class_names,\n",
    "                    title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = torch.zeros(num_classes, num_classes)\n",
    "\n",
    "# Update confusion matrix\n",
    "for t, p in zip(labels.view(-1), predicted.view(-1)):\n",
    "    confusion_matrix[t.long(), p.long()] += 1\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "# accuracy = correct_predictions / total_samples \n",
    "recall = confusion_matrix.diag() / confusion_matrix.sum(1)\n",
    "precision = confusion_matrix.diag() / confusion_matrix.sum(0)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# # Calculate TP, TN, FP, and FN for each class\n",
    "# TP = confusion_matrix.diag()\n",
    "# TN = torch.diag(confusion_matrix.sum(0)) - TP\n",
    "# FP = torch.diag(confusion_matrix.sum(1)) - TP\n",
    "# FN = confusion_matrix.sum(1) - TP\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Class Names: {class_names}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"F1 Score: {f1_score}\")\n",
    "print('Test Accuracy: {:.2f}%'.format(ongoing_accuracy_test[-1])) \n",
    "\n",
    "# Precision: Precision tells us how many of the things the robot says are dogs (or cats) are actually dogs (or cats). \n",
    "    # For example, if the robot says, \"That's a dog!\" to 10 pictures, but only 7 of those pictures are actually dogs, \n",
    "    # then its precision is 7 out of 10. A high precision indicates that the model has a low rate of false positives.\n",
    "    # Precision = True Positives (TP) / (True Positives (TP) + False Positives (FP))\n",
    "    \n",
    "# Recall: Recall tells us how many of the actual dogs (or cats) the robot correctly identifies. For example, \n",
    "    # if there are 20 dogs in the pictures, but the robot only detects 15 of them as dogs, then its recall is 15 out of 20. \n",
    "    # It tells us how well the model is able to detect positive \n",
    "    # samples of a specific class. A high recall indicates that the model has a low rate of false negatives.\n",
    "    # Recall = True Positives (TP) / (True Positives (TP) + False Negatives (FN))\n",
    "    \n",
    "# F1-score: F1-score is the harmonic mean of precision and recall and provides a single score that balances both metrics. \n",
    "    # It's useful when you want to strike a balance between precision and recall. F1-score tends to be lower when either \n",
    "    # precision or recall is low.\n",
    "    # F1 Score = 2 * (Precision * Recall) / (Precision + Recall) \n",
    "\n",
    "# A high recall indicates that the model is good at capturing positive instances, minimizing false negatives.\n",
    "# A high precision indicates that when the model predicts positive, it is likely to be correct, minimizing false positives.\n",
    "# The F1 score is a balanced metric that takes both precision and recall into account. It is useful when there is an uneven \n",
    "#     class distribution.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Analyze Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the errors are normally distributed around zero, it may indicate that the model is making unbiased predictions. \n",
    "# If there is a pattern or trend in the errors, it may suggest that the model has systematic biases or is making \n",
    "# consistent errors in certain regions of the input space\n",
    "\n",
    "\n",
    "\n",
    "def analyze_error_distribution(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to analyze the error distribution by plotting histograms and scatter plots.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Array of true labels or ground truth.\n",
    "    y_pred : array-like\n",
    "        Array of predicted values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate errors\n",
    "    errors = y_true - y_pred\n",
    "\n",
    "    # Plot histogram of errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(errors, bins=20, alpha=0.75)\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution (Histogram)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot scatter plot of true labels vs. errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, errors, alpha=0.75)\n",
    "    plt.xlabel('True Labels')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Error Distribution (Scatter Plot)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot scatter plot of predicted values vs. errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, errors, alpha=0.75)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Error Distribution (Scatter Plot)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_error_distribution(labels, predicted)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Error Analysis - Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error analysis is the process of analyzing the errors made by a machine learning model and identifying the patterns \n",
    "# or trends that may be causing the errors. The goal of error analysis is to gain insight into the behavior of the \n",
    "# model and identify areas for improvement.\n",
    "\n",
    "# The steps involved in error analysis:\n",
    "    # Collect error data (Collect and visualize Misclassified Samples)\n",
    "    # Categorize errors\n",
    "    # Identify patterns:    \n",
    "        # Look for recurring patterns among the misclassified samples. Are certain classes consistently misclassified? \n",
    "        # Are there specific types of images that the model struggles with?\n",
    "    # Analyze False Positives and False Negatives: \n",
    "        # For each misclassified sample, determine whether it is a false positive or false negative. A false positive \n",
    "        # occurs when the model predicts a positive class when the actual class is negative, and a false negative occurs \n",
    "        # when the model predicts a negative class when the actual class is positive.\n",
    "    # Analyze causes:\n",
    "        # Examine misclassified samples that are particularly challenging, and try to understand the reasons behind the \n",
    "        # errors. Are there any ambiguous or low-quality images? Are there classes that are inherently difficult to \n",
    "        # distinguish from others?\n",
    "    # Prioritize fixes:\n",
    "        # Based on the insights gained from error analysis, you can consider adjusting the training strategy. \n",
    "        # This might involve collecting more data for specific classes, using data augmentation techniques, or \n",
    "        # fine-tuning hyperparameters.\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Based on the insights gained from the error analysis, you can perform the following.\n",
    "# False negatives:\n",
    "    # False negatives occur when the model predicts that a customer will not churn when they actually do churn. \n",
    "    # To fix this issue, you may consider the following:\n",
    "        #  Increase the weight of the features that are more indicative of churn for low-usage customers, \n",
    "        #     such as frequency of usage or specific product usage. (adjust the model parameters)\n",
    "        #  Add new features that may be predictive of churn, such as customer sentiment or customer service interactions.\n",
    "        #  Use a different model architecture that is better suited for handling imbalanced data, such as a decision tree \n",
    "        #     or ensemble model.\n",
    "# False positives:\n",
    "    # False positives occur when the model predicts that a customer will churn when they actually do not churn. \n",
    "    # To fix this issue, you may consider the following:\n",
    "        # Decrease the weight of features that are causing false positives, such as age or income, if they are not as \n",
    "        #    indicative of churn for low-usage customers. (adjust the model parameters)\n",
    "        # Remove features that are causing false positives altogether, if they are not providing significant value to the \n",
    "        #    model.\n",
    "        # Increase the size of the training dataset to capture a more representative sample of customers who do not churn, \n",
    "        #    which may help the model learn more accurately which customers are likely to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_tp_tn_fp_fn(confusion_matrix):\n",
    "    num_classes = confusion_matrix.size(0)\n",
    "    tp = torch.zeros(num_classes)\n",
    "    tn = torch.zeros(num_classes)\n",
    "    fp = torch.zeros(num_classes)\n",
    "    fn = torch.zeros(num_classes)\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        # True positives (diagonal elements of the confusion matrix)\n",
    "        tp[i] = confusion_matrix[i, i]\n",
    "        \n",
    "        # True negatives (sum of all elements in the matrix except the current row and column)\n",
    "        tn[i] = confusion_matrix[:i, :i].sum() + confusion_matrix[:i, i+1:].sum() + \\\n",
    "                confusion_matrix[i+1:, :i].sum() + confusion_matrix[i+1:, i+1:].sum()\n",
    "        \n",
    "        # False positives (sum of values in the column excluding the diagonal element)\n",
    "        fp[i] = confusion_matrix[:, i].sum() - tp[i]\n",
    "        \n",
    "        # False negatives (sum of values in the row excluding the diagonal element)\n",
    "        fn[i] = confusion_matrix[i, :].sum() - tp[i]\n",
    "    \n",
    "    return tp, tn, fp, fn\n",
    "\n",
    "true_positives, true_negatives, false_positives, false_negatives = calculate_tp_tn_fp_fn(confusion_matrix)\n",
    "\n",
    "print(\"True Positives (TP):\", true_positives)\n",
    "print(\"True Negatives (TN):\", true_negatives)\n",
    "print(\"False Positives (FP):\", false_positives)\n",
    "print(\"False Negatives (FN):\", false_negatives)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto Tune using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Define your objective function\n",
    "def objective(trial):\n",
    "    # Define your hyperparameters to be tuned\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) \n",
    "    nUnits = trial.suggest_categorical('nUnits', 4, 128, step=8) \n",
    "    nLayers = trial.suggest_int('nLayers', 1, 6, step = 1) \n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)    \n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])    \n",
    "    # weight_inits = trial.suggest_categorical('weight_init', ['default', 'kaiming_normal', 'xavier_uniform_']) \n",
    "    # num_epochs = trial.suggest_int('num_epochs', 30, 300)  \n",
    "    # optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])\n",
    "    # activation = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    # patience = trial.suggest_int('patience', 5, 20)\n",
    "\n",
    "    # Define your model architecture with the hyperparameters\n",
    "    model = mnist_model(nUnits, nLayers, weight_inits, dropout_rate) \n",
    "\n",
    "    num_epochs = 50\n",
    "    learning_rate = learning_rate\n",
    "    losses = torch.zeros(num_epochs)\n",
    "    ongoing_accuracy = []\n",
    "    num_classes = 10\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay) \n",
    "\n",
    "    # Define the learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Move the model and data to the appropriate device (e.g., GPU if available)\n",
    "    model.to(device)\n",
    "\n",
    "    # Loop over the dataset for multiple epochs\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        batchAcc  = []\n",
    "        batchLoss = []\n",
    "\n",
    "        # Iterate over the training dataloader\n",
    "        for inputs, labels in train_dataloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            batchLoss.append(loss.item())\n",
    "\n",
    "            # Compute accuracy on the training set\n",
    "            predictions = torch.argmax(outputs, axis=1)\n",
    "            accuracy = torchmetrics.functional.classification.accuracy(predictions, labels, task='multiclass',\n",
    "                                                                        num_classes=num_classes) * 100\n",
    "            batchAcc.append(accuracy.item())\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "        ongoing_accuracy.append(np.mean(batchAcc))\n",
    "        losses[epoch] = np.mean(batchLoss)\n",
    "\n",
    "\n",
    "        #evaluation\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    # Disable gradient computation for evaluation\n",
    "    with torch.inference_mode():        #or torch.no_grad()\n",
    "        for data in test_dataloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            # Calculate predictions\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    # accuracy = 100 * (total_correct / total_samples)\n",
    "    accuracy = torchmetrics.functional.classification.accuracy(predicted, labels, task='multiclass', num_classes=num_classes) * 100\n",
    "\n",
    "    return accuracy \n",
    "\n",
    "# Define the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best hyperparameters\n",
    "print(\" Value: \", study.best_trial.value)\n",
    "print(\" Params: \")\n",
    "for key, value in study.best_trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#How to prevent overfitting\n",
    "# Get more data\t\n",
    "    # Having more data gives the model more opportunities to learn patterns, patterns which may be more generalizable \n",
    "    # to new examples.\n",
    "# Simplify your model\t\n",
    "    # If the current model is already overfitting the training data, it may be too complicated of a model. \n",
    "    # This means it's learning the patterns of the data too well and isn't able to generalize well to unseen data. \n",
    "    # One way to simplify a model is to reduce the number of layers it uses or to reduce the number of hidden units in \n",
    "    # each layer.\n",
    "# Use data augmentation\t\n",
    "    # Data augmentation manipulates the training data in a way so that's harder for the model to learn as it artificially \n",
    "    # adds more variety to the data. If a model is able to learn patterns in augmented data, the model may be able to \n",
    "    # generalize better to unseen data.\n",
    "# Use transfer learning\t\n",
    "    # Transfer learning involves leveraging the patterns (also called pretrained weights) one model has learned to use as \n",
    "    # the foundation for your own task. In our case, we could use one computer vision model pretrained on a large variety \n",
    "    # of images and then tweak it slightly to be more specialized for food images.\n",
    "# Use dropout layers\t\n",
    "    # Dropout layers randomly remove connections between hidden layers in neural networks, effectively simplifying a model \n",
    "    # but also making the remaining connections better. See torch.nn.Dropout() for more.\n",
    "# Use learning rate decay\t\n",
    "    # The idea here is to slowly decrease the learning rate as a model trains. This is akin to reaching for a coin at the \n",
    "    # back of a couch. The closer you get, the smaller your steps. The same with the learning rate, the closer you get to \n",
    "    # convergence, the smaller you'll want your weight updates to be.\n",
    "# Use early stopping\t\n",
    "    # Early stopping stops model training before it begins to overfit. As in, say the model's loss has stopped decreasing \n",
    "    # for the past 10 epochs (this number is arbitrary), you may want to stop the model training here and go with the model \n",
    "    # weights that had the lowest loss (10 epochs prior).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#How to prevent underfitting\n",
    "# Add more layers/units to your model\t\n",
    "    # If your model is underfitting, it may not have enough capability to learn the required patterns/weights/representations \n",
    "    # of the data to be predictive. One way to add more predictive power to your model is to increase the number of hidden \n",
    "    # layers/units within those layers.\n",
    "# Tweak the learning rate\t\n",
    "    # Perhaps your model's learning rate is too high to begin with. And it's trying to update its weights each epoch too \n",
    "    # much, in turn not learning anything. In this case, you might lower the learning rate and see what happens.\n",
    "# Use transfer learning\t\n",
    "    # Transfer learning is capable of preventing overfitting and underfitting. It involves using the patterns from a \n",
    "    # previously working model and adjusting them to your own problem.\n",
    "# Train for longer\t\n",
    "    # Sometimes a model just needs more time to learn representations of data. If you find in your smaller experiments \n",
    "    # your model isn't learning anything, perhaps leaving it train for a more epochs may result in better performance.\n",
    "# Use less regularization\t\n",
    "    # Perhaps your model is underfitting because you're trying to prevent overfitting too much. Holding back on \n",
    "    # regularization techniques can help your model fit the data better.\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.inference_mode():        #or torch.no_grad()\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = torchmetrics.functional.classification.accuracy(predicted, labels, task='multiclass', num_classes=num_classes) \n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy:.2f}%\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model checkpoint\n",
    "checkpoint = {\n",
    "    'epoch': 300,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "    'hyperparameters': {\n",
    "                'hidden_units': 64,\n",
    "                'batch_size': 32\n",
    "                        },\n",
    "    # 'other_info': 'Additional information about the checkpoint'\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paul_flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
