{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio User Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio is an easy way to create machine learning demos with friendly web interfaces.\n",
    "\n",
    "https://www.gradio.app/guides/quickstart            # Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to create a Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the input and output components.\n",
    "# 2. Load the model.\n",
    "# 3. Define the function to make predictions.\n",
    "# 4. Create the Gradio interface.\n",
    "# 5. Launch the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Gradio Interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7881\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Gradio interface requires at least one function and the inputs/outputs specifications\n",
    "\n",
    "import gradio as gr                                                     # import gradio\n",
    "\n",
    "def greet(name):                                                        # function to make predictions (it takes a string as input and returns a string as output)\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")           # create the interface\n",
    "iface.launch()                                                          # launch the interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Gradio Interface with Multiple inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your function has multiple inputs or outputs, you can specify them as lists.\n",
    "\n",
    "def math_ops(number1, number2):\n",
    "    return number1 + number2, number1 * number2\n",
    "\n",
    "iface = gr.Interface(fn=math_ops, inputs=[\"number\", \"number\"], outputs=[\"text\", \"text\"])\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Input and Output components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "    # Input Components\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter text here\", label=\"Your Text\")                                 # Text\n",
    "        gr.Number(label=\"Your Number\", default=0, step=1)                                                     # Number\n",
    "        gr.Slider(minimum=0, maximum=100, step=1, default=50, label=\"Your Slider\")                            # Slider\n",
    "        gr.Checkbox(label=\"Check if True\", default=False)                                                     # Checkbox\n",
    "        gr.Radio(choices=[\"Option 1\", \"Option 2\"], label=\"Select One\", default=\"Option 1\")                    # Radio\n",
    "        gr.Dropdown(choices=[\"Option 1\", \"Option 2\"], label=\"Select One\", default=\"Option 1\")                 # Dropdown\n",
    "        gr.Image(tool=\"select\", shape=(224, 224), label=\"Upload Image\")                                       # Image\n",
    "        gr.File(label=\"Upload File\", type=[\".pdf\", \".docx\"])                                                  # File\n",
    "        gr.Dataframe(headers=[\"Column 1\", \"Column 2\"], value = [] ,datatype=[\"str\", \"number\"], label=\"Your Dataframe\")    # Dataframe\n",
    "        gr.Audio(label=\"Upload Audio\", type=\"file\", source=\"upload\")                                          # Audio\n",
    "        gr.Video(label=\"Upload Video\")                                                                        # Video\n",
    "        gr.Series(gr.Number(), label=\"Number Series\")                                                         # Series: for inputting a series of numbers.\n",
    "        \n",
    "    # Output Components\n",
    "        gr.Label(label=\"Output Label\")   # Label\n",
    "        gr.Textbox(label=\"Output Text\")   # Text\n",
    "        gr.Image(label=\"Output Image\")   # Image\n",
    "        gr.Plot(label=\"Output Plot\")  # Plot\n",
    "        gr.Dataframe(label=\"Output Dataframe\")   # Dataframe\n",
    "        gr.Audio(label=\"Output Audio\")   # Audio\n",
    "        gr.Video(label=\"Output Video\")   # Video\n",
    "        gr.JSON(label=\"Output JSON\")  # JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib    # import joblib, a library for saving and loading scikit-learn models. You can also use Pytorch or tensorflow to load your model.\n",
    "\n",
    "# Interface Functions\n",
    "model = joblib.load('file path of model')  # Load your model here\n",
    "\n",
    "def predict(var1, var2, var3, var4, var5, var6, var7, var8):        # Define the function to make predictions. parameters should match the number of inputs\n",
    "    inputs = [var1, var2, var3, var4, var5, var6, var7, var8]       # Format the inputs into a list or a numpy array\n",
    "    prediction = model.predict([inputs])[0]                         # Make prediction (adjust according to your model's prediction method)\n",
    "    \n",
    "    return prediction                                               # Return the prediction (you might want to format or round the prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more detailed way to build the Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Interface\n",
    "interface = gr.Interface(\n",
    "    fn=predict,                                                                         # The function to run when inputs are provided\n",
    "    inputs = [input1, input2],                                                          # The inputs to the model, generated from gr.inputs\n",
    "    outputs= output,                                                                    # The outputs from the model, generated from gr.outputs\n",
    "    examples = [['example_1', data], ['example_2' data]]                                # Example inputs to show in the interface\n",
    "    title=\"Model Prediction App\",\n",
    "    description=\"Enter the values for the 8 variables to get the numeric prediction from the model.\",\n",
    "    theme=\"abidlabs/pakistan\",  # You can choose from various themes like 'default', 'huggingface', 'dark', 'grass', 'peach', etc.\n",
    "                    # 'ParityError/LimeFace', 'abidlabs/pakistan', 'HaleyCH/HaleyCH_Theme', 'freddyaboulton/dracula_revamped'\n",
    "                    # https://huggingface.co/spaces/gradio/theme-gallery (visit link for more themes)\n",
    "    css=\"\"\"\n",
    "        body { font-family: Arial, sans-serif; }\n",
    "        .gr-interface { max-width: 800px; margin: auto; }\n",
    "        .gr-title, .gr-description { text-align: center; }\n",
    "        .gr-inputs, .gr-output { border-radius: 10px; }\n",
    "        .gr-group { margin-bottom: 20px; }\n",
    "        .gr-output-label { margin-top: 20px; } \n",
    "    \"\"\",\n",
    "    live=True,                                  # Set to False to disable live updates and use a submit button for triggering predictions,\n",
    "    share = True,                               # Share the interface on social media sites like Twitter and LinkedIn\n",
    "    \n",
    "    examples = [                                # Example inputs to show in the interface\n",
    "                {\"Input_Variable1\": 3, \"Input_Variable2\": 4, \"Input_Variable3\": 5, \"Input_Variable4\": 6, \"Input_Variable5\": 7, \"Input_Variable6\": 8, \"Input_Variable7\": 9, \"Input_Variable8\": 10},\n",
    "                {\"Input_Var1\": 3, \"Input_Var2\": 4, \"Input_Var3\": 5, \"Input_Var4\": 6, \"Input_Var5\": 7, \"Input_Var6\": 8, \"Input_Var7\": 9, \"Input_Var8\": 10}\n",
    "                ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launching the Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the Server (blocking call)\n",
    "interface.launch(share=True, auth=(\"username\", \"password\"), # Share the interface and set authentication (auth is optional)\n",
    "                 debug = True,   # Set debug to True to see error messages in the console when things go wrong\n",
    "                 inline = False # Set inline to True to display the interface in the Jupyter notebook instead of opening a new browser window\n",
    "                 )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More practice examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Image Classification with pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Model above has been trained on ImageNet, a large visual database designed for use in visual object recognition software research. \n",
    "# It contains over 14 million images categorized into over 20,000 categories.\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define the inference function\n",
    "def classify_image(img):        # The function takes an image as input and returns a dictionary of class probabilities\n",
    "    img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "    img = preprocess(img)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.nn.functional.softmax(model(img)[0], dim=0)\n",
    "    return {str(i): float(prediction[i]) for i in range(1000)}\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=classify_image,                                             # Image input and Label output\n",
    "                     inputs=gr.Image(label=\"Input Image\"), \n",
    "                     outputs=gr.Label(num_top_classes=3, label=\"Output Image\"),\n",
    "                     theme = 'sudeepshouche/minimalist')  \n",
    "iface.launch()      # Launch the interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Audio Processing - Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Define the transcription function\n",
    "def transcribe_audio(filepath):\n",
    "    with sr.AudioFile(filepath[\"path\"]) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    text = recognizer.recognize_google(audio_data)\n",
    "    return text\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=transcribe_audio, inputs=gr.Audio(sources=['microphone', 'upload'], type=\"filepath\"), outputs=\"text\",\n",
    "                     theme = 'sudeepshouche/minimalist',\n",
    "                     title=\"Speech Recognition\",\n",
    "                     description=\"Upload an audio file or record\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7862\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7862/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import gradio as gr\n",
    "\n",
    "# Define sentiment analysis function\n",
    "def sentiment_analysis(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    polarity = testimonial.sentiment.polarity\n",
    "\n",
    "    # Categorize polarity into sentiment labels\n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return f\"sentiment: {sentiment}, '\\n'polarity: {polarity}\"\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=sentiment_analysis, inputs=\"textbox\", outputs=\"label\", \n",
    "                     title=\"Sentiment\",\n",
    "                     theme = 'sudeepshouche/minimalist',\n",
    "                     description='A simple app to analyze the sentiment of a statement')\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drawing Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7435.61665639\n",
      "Iteration 2, loss = 7844.14105930\n",
      "Iteration 3, loss = 7841.78883508\n",
      "Iteration 4, loss = 7839.43767851\n",
      "Iteration 5, loss = 7837.08651231\n",
      "Iteration 6, loss = 7834.73664507\n",
      "Iteration 7, loss = 7832.38717793\n",
      "Iteration 8, loss = 7830.03834949\n",
      "Iteration 9, loss = 7827.69039183\n",
      "Iteration 10, loss = 7825.34298968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7863\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7863/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1594, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 689, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pault\\AppData\\Local\\Temp\\ipykernel_30620\\3699043479.py\", line 19, in recognize_digit\n",
      "    img = Image.fromarray(img.astype('uint8'), 'L').resize((28, 28))\n",
      "                          ^^^^^^^^^^\n",
      "AttributeError: 'dict' object has no attribute 'astype'\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load data and train a simple MLP classifier\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=42)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def recognize_digit(img):\n",
    "    # Convert the input image to grayscale, resize it to 28x28 (same as MNIST images) and flatten it\n",
    "    img = Image.fromarray(img.astype('uint8'), 'L').resize((28, 28))\n",
    "    img = np.array(img).reshape(1, -1) / 255.0  # Normalize the pixel values to [0, 1]\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = mlp.predict(img)\n",
    "    return int(prediction[0])  # Return the prediction as an integer\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=recognize_digit, \n",
    "                     inputs=gr.Sketchpad(image_mode='L'), \n",
    "                     outputs=\"label\", \n",
    "                     theme = 'ysharma/steampunk'\n",
    "                     )\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  return self.fget.__get__(instance, owner)()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7864\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "def summarize(text):\n",
    "    summary_text = summarizer(text)[0]['summary_text']\n",
    "    return summary_text\n",
    "\n",
    "iface = gr.Interface(fn=summarize, inputs=gr.Textbox(lines=10, placeholder=\"Type here...\"), outputs=\"text\",\n",
    "                     theme = 'finlaymacklon/boxy_violet',\n",
    "                     title=\"Text Summarizer\",\n",
    "                     description=\"Summarize your text in one click!\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
