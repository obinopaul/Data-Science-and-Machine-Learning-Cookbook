{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio User Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradio is an easy way to create machine learning demos with friendly web interfaces.\n",
    "\n",
    "https://www.gradio.app/guides/quickstart            # Quickstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Steps to create a Gradio App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Define the input and output components.\n",
    "# 2. Load the model.\n",
    "# 3. Define the function to make predictions.\n",
    "# 4. Create the Gradio interface.\n",
    "# 5. Launch the interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Gradio Interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7881\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7881/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Gradio interface requires at least one function and the inputs/outputs specifications\n",
    "\n",
    "import gradio as gr                                                     # import gradio\n",
    "\n",
    "def greet(name):                                                        # function to make predictions (it takes a string as input and returns a string as output)\n",
    "    return \"Hello \" + name + \"!\"\n",
    "\n",
    "iface = gr.Interface(fn=greet, inputs=\"text\", outputs=\"text\")           # create the interface\n",
    "iface.launch()                                                          # launch the interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Gradio Interface with Multiple inputs and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7880\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7880/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If your function has multiple inputs or outputs, you can specify them as lists.\n",
    "\n",
    "def math_ops(number1, number2):\n",
    "    return number1 + number2, number1 * number2\n",
    "\n",
    "iface = gr.Interface(fn=math_ops, inputs=[\"number\", \"number\"], outputs=[\"text\", \"text\"])\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting Input and Output components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "    # Input Components\n",
    "        gr.Textbox(lines=2, placeholder=\"Enter text here\", label=\"Your Text\")                                 # Text\n",
    "        gr.Number(label=\"Your Number\", default=0, step=1)                                                     # Number\n",
    "        gr.Slider(minimum=0, maximum=100, step=1, default=50, label=\"Your Slider\")                            # Slider\n",
    "        gr.Checkbox(label=\"Check if True\", default=False)                                                     # Checkbox\n",
    "        gr.Radio(choices=[\"Option 1\", \"Option 2\"], label=\"Select One\", default=\"Option 1\")                    # Radio\n",
    "        gr.Dropdown(choices=[\"Option 1\", \"Option 2\"], label=\"Select One\", default=\"Option 1\")                 # Dropdown\n",
    "        gr.Image(tool=\"select\", shape=(224, 224), label=\"Upload Image\")                                       # Image\n",
    "        gr.File(label=\"Upload File\", type=[\".pdf\", \".docx\"])                                                  # File\n",
    "        gr.Dataframe(headers=[\"Column 1\", \"Column 2\"], value = [] ,datatype=[\"str\", \"number\"], label=\"Your Dataframe\")    # Dataframe\n",
    "        gr.Audio(label=\"Upload Audio\", type=\"file\", source=\"upload\")                                          # Audio\n",
    "        gr.Video(label=\"Upload Video\")                                                                        # Video\n",
    "        gr.Series(gr.Number(), label=\"Number Series\")                                                         # Series: for inputting a series of numbers.\n",
    "        \n",
    "    # Output Components\n",
    "        gr.Label(label=\"Output Label\")   # Label\n",
    "        gr.Textbox(label=\"Output Text\")   # Text\n",
    "        gr.Image(label=\"Output Image\")   # Image\n",
    "        gr.Plot(label=\"Output Plot\")  # Plot\n",
    "        gr.Dataframe(label=\"Output Dataframe\")   # Dataframe\n",
    "        gr.Audio(label=\"Output Audio\")   # Audio\n",
    "        gr.Video(label=\"Output Video\")   # Video\n",
    "        gr.JSON(label=\"Output JSON\")  # JSON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up the Predict function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib    # import joblib, a library for saving and loading scikit-learn models. You can also use Pytorch or tensorflow to load your model.\n",
    "\n",
    "# Interface Functions\n",
    "model = joblib.load('file path of model')  # Load your model here\n",
    "\n",
    "def predict(var1, var2, var3, var4, var5, var6, var7, var8):        # Define the function to make predictions. parameters should match the number of inputs\n",
    "    inputs = [var1, var2, var3, var4, var5, var6, var7, var8]       # Format the inputs into a list or a numpy array\n",
    "    prediction = model.predict([inputs])[0]                         # Make prediction (adjust according to your model's prediction method)\n",
    "    \n",
    "    return prediction                                               # Return the prediction (you might want to format or round the prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A more detailed way to build the Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Interface\n",
    "interface = gr.Interface(\n",
    "    fn=predict,                                                                         # The function to run when inputs are provided\n",
    "    inputs = [input1, input2],                                                          # The inputs to the model, generated from gr.inputs\n",
    "    outputs= output,                                                                    # The outputs from the model, generated from gr.outputs\n",
    "    examples = [['example_1', data], ['example_2' data]]                                # Example inputs to show in the interface\n",
    "    title=\"Model Prediction App\",\n",
    "    description=\"Enter the values for the 8 variables to get the numeric prediction from the model.\",\n",
    "    theme=\"abidlabs/pakistan\",  # You can choose from various themes like 'default', 'huggingface', 'dark', 'grass', 'peach', etc.\n",
    "                    # 'ParityError/LimeFace', 'abidlabs/pakistan', 'HaleyCH/HaleyCH_Theme', 'freddyaboulton/dracula_revamped'\n",
    "                    # https://huggingface.co/spaces/gradio/theme-gallery (visit link for more themes)\n",
    "    css=\"\"\"\n",
    "        body { font-family: Arial, sans-serif; }\n",
    "        .gr-interface { max-width: 800px; margin: auto; }\n",
    "        .gr-title, .gr-description { text-align: center; }\n",
    "        .gr-inputs, .gr-output { border-radius: 10px; }\n",
    "        .gr-group { margin-bottom: 20px; }\n",
    "        .gr-output-label { margin-top: 20px; } \n",
    "    \"\"\",\n",
    "    live=True,                                  # Set to False to disable live updates and use a submit button for triggering predictions,\n",
    "    share = True,                               # Share the interface on social media sites like Twitter and LinkedIn\n",
    "    \n",
    "    examples = [                                # Example inputs to show in the interface\n",
    "                {\"Input_Variable1\": 3, \"Input_Variable2\": 4, \"Input_Variable3\": 5, \"Input_Variable4\": 6, \"Input_Variable5\": 7, \"Input_Variable6\": 8, \"Input_Variable7\": 9, \"Input_Variable8\": 10},\n",
    "                {\"Input_Var1\": 3, \"Input_Var2\": 4, \"Input_Var3\": 5, \"Input_Var4\": 6, \"Input_Var5\": 7, \"Input_Var6\": 8, \"Input_Var7\": 9, \"Input_Var8\": 10}\n",
    "                ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Launching the Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the Server (blocking call)\n",
    "interface.launch(share=True, auth=(\"username\", \"password\"), # Share the interface and set authentication (auth is optional)\n",
    "                 debug = True,   # Set debug to True to see error messages in the console when things go wrong\n",
    "                 inline = False # Set inline to True to display the interface in the Jupyter notebook instead of opening a new browser window\n",
    "                 )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More practice examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Image Classification with pretrained Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "798caec9fd554ffda50c51a0a90fd30d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "themes/theme_schema@0.0.2.json:   0%|          | 0.00/13.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7861/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "# Load a pre-trained model\n",
    "model = models.resnet18(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Model above has been trained on ImageNet, a large visual database designed for use in visual object recognition software research. \n",
    "# It contains over 14 million images categorized into over 20,000 categories.\n",
    "\n",
    "# Define image preprocessing\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define the inference function\n",
    "def classify_image(img):        # The function takes an image as input and returns a dictionary of class probabilities\n",
    "    img = Image.fromarray(img.astype('uint8'), 'RGB')\n",
    "    img = preprocess(img)\n",
    "    img = torch.unsqueeze(img, 0)\n",
    "    with torch.no_grad():\n",
    "        prediction = torch.nn.functional.softmax(model(img)[0], dim=0)\n",
    "    return {str(i): float(prediction[i]) for i in range(1000)}\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=classify_image,                                             # Image input and Label output\n",
    "                     inputs=gr.Image(label=\"Input Image\"), \n",
    "                     outputs=gr.Label(num_top_classes=3, label=\"Output Image\"),\n",
    "                     theme = 'sudeepshouche/minimalist')  \n",
    "iface.launch()      # Launch the interface\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Audio Processing - Speech-to-Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7878\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7878/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Initialize the speech recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Define the transcription function\n",
    "def transcribe_audio(filepath):\n",
    "    with sr.AudioFile(filepath[\"path\"]) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "    text = recognizer.recognize_google(audio_data)\n",
    "    return text\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=transcribe_audio, inputs=gr.Audio(sources=['microphone', 'upload'], type=\"filepath\"), outputs=\"text\",\n",
    "                     theme = 'sudeepshouche/minimalist',\n",
    "                     title=\"Speech Recognition\",\n",
    "                     description=\"Upload an audio file or record\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7879\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7879/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3641 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\queueing.py\", line 495, in call_prediction\n",
      "    output = await route_utils.call_process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\route_utils.py\", line 231, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1594, in process_api\n",
      "    result = await self.call_function(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\blocks.py\", line 1176, in call_function\n",
      "    prediction = await anyio.to_thread.run_sync(\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\anyio\\to_thread.py\", line 28, in run_sync\n",
      "    return await get_asynclib().run_sync_in_worker_thread(func, *args, cancellable=cancellable,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 818, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 754, in run\n",
      "    result = context.run(func, *args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\gradio\\utils.py\", line 689, in wrapper\n",
      "    response = f(*args, **kwargs)\n",
      "               ^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\pault\\AppData\\Local\\Temp\\ipykernel_35132\\3591811833.py\", line 7, in summarize\n",
      "    summary_text = summarizer(text)[0]['summary_text']\n",
      "                   ^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 269, in __call__\n",
      "    return super().__call__(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 167, in __call__\n",
      "    result = super().__call__(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1162, in __call__\n",
      "    return self.run_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1169, in run_single\n",
      "    model_outputs = self.forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 1068, in forward\n",
      "    model_outputs = self._forward(model_inputs, **forward_params)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\text2text_generation.py\", line 191, in _forward\n",
      "    output_ids = self.model.generate(**model_inputs, **generate_kwargs)\n",
      "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py\", line 115, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 1370, in generate\n",
      "    model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\generation\\utils.py\", line 491, in _prepare_encoder_decoder_kwargs_for_generation\n",
      "    model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n",
      "                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\", line 1154, in forward\n",
      "    embed_pos = self.embed_positions(input)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\transformers\\models\\bart\\modeling_bart.py\", line 135, in forward\n",
      "    return super().forward(positions + self.offset)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "           ^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py\", line 2233, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "IndexError: index out of range in self\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "import gradio as gr\n",
    "\n",
    "# Define sentiment analysis function\n",
    "def sentiment_analysis(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    polarity = testimonial.sentiment.polarity\n",
    "\n",
    "    # Categorize polarity into sentiment labels\n",
    "    if polarity > 0:\n",
    "        sentiment = \"Positive\"\n",
    "    elif polarity < 0:\n",
    "        sentiment = \"Negative\"\n",
    "    else:\n",
    "        sentiment = \"Neutral\"\n",
    "    \n",
    "    return f\"sentiment: {sentiment}, '\\n'polarity: {polarity}\"\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=sentiment_analysis, inputs=\"textbox\", outputs=\"label\", \n",
    "                     title=\"Sentiment\",\n",
    "                     theme = 'sudeepshouche/minimalist',\n",
    "                     description='A simple app to analyze the sentiment of a statement')\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Drawing Digit Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 7435.61665639\n",
      "Iteration 2, loss = 7844.14105930\n",
      "Iteration 3, loss = 7841.78883508\n",
      "Iteration 4, loss = 7839.43767851\n",
      "Iteration 5, loss = 7837.08651231\n",
      "Iteration 6, loss = 7834.73664507\n",
      "Iteration 7, loss = 7832.38717793\n",
      "Iteration 8, loss = 7830.03834949\n",
      "Iteration 9, loss = 7827.69039183\n",
      "Iteration 10, loss = 7825.34298968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\pault\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7875\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7875/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load data and train a simple MLP classifier\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/7, random_state=42)\n",
    "mlp = MLPClassifier(hidden_layer_sizes=(50,), max_iter=10, alpha=1e-4,\n",
    "                    solver='sgd', verbose=10, random_state=1,\n",
    "                    learning_rate_init=.1)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "def recognize_digit(img):\n",
    "    # Convert the input image to grayscale, resize it to 28x28 (same as MNIST images) and flatten it\n",
    "    img = Image.fromarray(img.astype('uint8'), 'L').resize((28, 28))\n",
    "    img = np.array(img).reshape(1, -1) / 255.0  # Normalize the pixel values to [0, 1]\n",
    "    \n",
    "    # Get the prediction\n",
    "    prediction = mlp.predict(img)\n",
    "    return int(prediction[0])  # Return the prediction as an integer\n",
    "\n",
    "# Build the interface\n",
    "iface = gr.Interface(fn=recognize_digit, \n",
    "                     inputs=gr.Sketchpad(image_mode='L'), \n",
    "                     outputs=\"label\", \n",
    "                     theme = 'ysharma/steampunk'\n",
    "                     )\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Text Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8eeb3e08cc5485896eff0e1755d2e49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "themes/theme_schema@0.0.2.json:   0%|          | 0.00/12.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7877\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7877/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 142, but your input_length is only 20. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=10)\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "import gradio as gr\n",
    "\n",
    "summarizer = pipeline(\"summarization\")\n",
    "\n",
    "def summarize(text):\n",
    "    summary_text = summarizer(text)[0]['summary_text']\n",
    "    return summary_text\n",
    "\n",
    "iface = gr.Interface(fn=summarize, inputs=gr.Textbox(lines=10, placeholder=\"Type here...\"), outputs=\"text\",\n",
    "                     theme = 'finlaymacklon/boxy_violet',\n",
    "                     title=\"Text Summarizer\",\n",
    "                     description=\"Summarize your text in one click!\")\n",
    "iface.launch()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
