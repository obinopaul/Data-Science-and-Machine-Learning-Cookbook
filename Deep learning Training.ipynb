{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 4 3 5]]\n",
      " \n",
      "[[1]\n",
      " [4]\n",
      " [3]\n",
      " [5]]\n",
      "tensor([[1, 5, 2, 6]])\n",
      " \n",
      "tensor([[1],\n",
      "        [5],\n",
      "        [2],\n",
      "        [6]])\n"
     ]
    }
   ],
   "source": [
    "#transpose vector and matrix\n",
    "\n",
    "arr = np.array([ [1,4,3,5]])\n",
    "print(arr)\n",
    "\n",
    "print(' ')\n",
    "print(arr.T)\n",
    "\n",
    "tens = torch.tensor([ [1,5,2,6]])\n",
    "print(tens)\n",
    "\n",
    "print(' ')\n",
    "print(tens.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27 57 53]\n",
      " [40 91 87]\n",
      " [37 82 78]]\n"
     ]
    }
   ],
   "source": [
    "arr_2 = np.array([[1,4,5],\n",
    "                   [4,5,8],\n",
    "                   [3,5,7]])\n",
    "\n",
    "arr_3 = np.array([[1,4,5],\n",
    "                   [4,7,7],\n",
    "                   [2,5,4]])\n",
    "\n",
    "print(np.dot(arr_2, arr_3))\n",
    "# print(np.sum(arr_2, arr_3)) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# array (r1, c1) * (r2, c2)\n",
    "#condition: (c1) = (r2)\n",
    "#shape = (r1, c2)\n",
    "        #where: \n",
    "            # r1 and r2 are the number of rows for 1 and 2\n",
    "            # c1 and c2 are the number of columns for 1 and 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27 57 53]\n",
      " [40 91 87]\n",
      " [37 82 78]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr_2 = np.array([[1,4,5],\n",
    "                   [4,5,8],\n",
    "                   [3,5,7]])\n",
    "\n",
    "arr_3 = np.array([[1,4,5],\n",
    "                   [4,7,7],\n",
    "                   [2,5,4]])\n",
    "\n",
    "print(np.dot(arr_2, arr_3))\n",
    "# print(np.sum(arr_2, arr_3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 5, 4, 5, 8, 3, 5, 7])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arr_2 = np.array([[1,4,5],\n",
    "                   [4,5,8],\n",
    "                   [3,5,7]])\n",
    "\n",
    "arr_2.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "208\n"
     ]
    }
   ],
   "source": [
    "arr_2 = np.array([[1,4,5],\n",
    "                   [4,5,8],\n",
    "                   [3,5,7]])\n",
    "\n",
    "arr_3 = np.array([[1,4,5],\n",
    "                   [4,7,7],\n",
    "                   [2,5,4]])\n",
    "\n",
    "# Flatten the tensors\n",
    "arr_2_flat = arr_2.flatten()\n",
    "arr_3_flat = arr_3.flatten()\n",
    "\n",
    "print(np.dot(arr_2_flat, arr_3_flat))\n",
    "# print(np.sum(arr_2, arr_3)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(208)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "arr_2 = torch.tensor([[1, 4, 5],\n",
    "                      [4, 5, 8],\n",
    "                      [3, 5, 7]])\n",
    "\n",
    "arr_3 = torch.tensor([[1, 4, 5],\n",
    "                      [4, 7, 7],\n",
    "                      [2, 5, 4]])\n",
    "\n",
    "# Flatten the tensors\n",
    "arr_2_flat = arr_2.flatten()\n",
    "arr_3_flat = arr_3.flatten()\n",
    "\n",
    "# Calculate the dot product\n",
    "result = torch.dot(arr_2_flat, arr_3_flat)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2. -1. -0.]\n",
      " [ 3. -1. -1.]\n",
      " [ 0. -1. -1.]]\n",
      " \n",
      "[[ 2. -1. -0.]\n",
      " [ 3. -1. -1.]\n",
      " [ 0. -1. -1.]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(23)\n",
    "\n",
    "a = np.random.randn(3,3)\n",
    "b = np.random.randn(3,3)\n",
    "c = np.random.randn(3,3)\n",
    "\n",
    "print(np.round(np.matmul(a,b)))\n",
    "print(' ')\n",
    "print(np.round(np.dot(a,b)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 1, 1]\n",
      "[3, 4, 6]\n",
      "[7, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Set the number of rows and columns\n",
    "rows = 3\n",
    "columns = 3\n",
    "\n",
    "# Generate random integers for each element in the grid\n",
    "grid = [[random.randint(1, 8) for _ in range(columns)] for _ in range(rows)]\n",
    "\n",
    "# Print the generated grid\n",
    "for row in grid:\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 1]\n",
      "[4, 5, 4]\n",
      "[5, 1, 5]\n"
     ]
    }
   ],
   "source": [
    "grid = []\n",
    "# row = []\n",
    "\n",
    "for _ in range(rows):\n",
    "    row = []\n",
    "    for _ in range(columns):\n",
    "        row.append(random.randint(1,6))\n",
    "    grid.append(row) \n",
    "    \n",
    "for row in grid:\n",
    "    print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8733,  0.4376, -0.4866],\n",
       "        [-0.7840, -0.2983, -0.9361]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(23)           #seed in pytorch\n",
    "\n",
    "torch.randn(2,3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn  \n",
    "import matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax:  [0.09003057 0.24472847 0.66524096]\n",
      "sum of softmax: 1.0\n"
     ]
    }
   ],
   "source": [
    "z = [1,2,3]\n",
    "\n",
    "ez = ([math.exp(i) for i in z])\n",
    "sum_ez = np.sum(ez)\n",
    "softmax = np.exp(z) / sum_ez\n",
    "print(\"softmax: \", softmax)\n",
    "print(\"sum of softmax:\", np.sum(softmax))\n",
    "# np.round(ez, 2)\n",
    "# ez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "# softmax(x_i) = exp(x_i) / sum(exp(x_j))\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    exp_values = np.exp(x)\n",
    "    sum_exp_values = np.sum(exp_values)\n",
    "    softmax_probs = exp_values / sum_exp_values\n",
    "    return softmax_probs\n",
    "\n",
    "# Example input vector\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "# Perform softmax\n",
    "softmax_probs = softmax(x)\n",
    "\n",
    "print(softmax_probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14,  1,  3,  4,  3,  8,  7,  2, 14,  1,  8,  7, 12, -3,  6,  0, -5,\n",
       "        7, 11,  4, 10, 14, -4,  2, 10])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ab = [1,2,3,5]\n",
    "np.random.seed(23)\n",
    "z = np.random.randint(-5,high=15,size=25)\n",
    "\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0960e-01, 6.9981e-07, 5.1709e-06, 1.4056e-05, 5.1709e-06, 7.6743e-04,\n",
      "        2.8232e-04, 1.9023e-06, 3.0960e-01, 6.9981e-07, 7.6743e-04, 2.8232e-04,\n",
      "        4.1900e-02, 1.2817e-08, 1.0386e-04, 2.5745e-07, 1.7347e-09, 2.8232e-04,\n",
      "        1.5414e-02, 1.4056e-05, 5.6706e-03, 3.0960e-01, 4.7153e-09, 1.9023e-06,\n",
      "        5.6706e-03])\n"
     ]
    }
   ],
   "source": [
    "from torch.nn import Softmax\n",
    "\n",
    "softfun = Softmax(dim = 0)\n",
    "sigmaT = softfun( torch.Tensor(z) )\n",
    "\n",
    "print(softfun(torch.Tensor(z)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.0960e-01, 6.9981e-07, 5.1709e-06, 1.4056e-05, 5.1709e-06, 7.6743e-04,\n",
      "        2.8232e-04, 1.9023e-06, 3.0960e-01, 6.9981e-07, 7.6743e-04, 2.8232e-04,\n",
      "        4.1900e-02, 1.2817e-08, 1.0386e-04, 2.5745e-07, 1.7347e-09, 2.8232e-04,\n",
      "        1.5414e-02, 1.4056e-05, 5.6706e-03, 3.0960e-01, 4.7153e-09, 1.9023e-06,\n",
      "        5.6706e-03])\n"
     ]
    }
   ],
   "source": [
    "# slightly more involved using torch.nn\n",
    "\n",
    "# create an instance of the softmax activation class\n",
    "softfun = nn.Softmax(dim=0)\n",
    "\n",
    "# then apply the data to that function\n",
    "sigmaT = softfun( torch.Tensor(z) )\n",
    "\n",
    "# now we get the results\n",
    "print(sigmaT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Artificial Neural Network (ANN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In an Artificial Neural Network (ANN) used for making predictions using data tables, the input values for the \n",
    "# different hidden neurons depend on the specific architecture and design choices. However, in a typical feedforward \n",
    "# neural network, each hidden neuron receives input values from the previous layer's neurons.\n",
    "\n",
    "# Let's consider a simple scenario where we have a feedforward neural network with an input layer, one hidden layer, \n",
    "# and an output layer. The input layer represents the input features or attributes of the data table, and the output \n",
    "# layer represents the predicted values or classes. In this case, the input values for the hidden neurons are the \n",
    "# outputs from the neurons in the input layer. \n",
    "\n",
    "# In a feedforward neural network, each neuron in a hidden layer has its own set of weights, which are initialized \n",
    "# randomly at the beginning of training. The purpose of these weights is to determine the contribution of each input \n",
    "# feature to the neuron's output. The weights are learned and adjusted during the training process.\n",
    "\n",
    "# To illustrate this, let's consider a simple example with three input features (feature1, feature2, feature3) and two \n",
    "# neurons in the first hidden layer (neuron1 and neuron2).\n",
    "\n",
    "# For neuron1 in the first hidden layer, the weighted sum calculation is as follows:\n",
    "# weighted_sum1 = (input1 * weight1_1) + (input2 * weight2_1) + (input3 * weight3_1)\n",
    "\n",
    "# Here, (input1, input2, input3) represents the values of the three input features, and \n",
    "# (weight1_1, weight2_1, weight3_1) represents the corresponding weights for neuron1.\n",
    "\n",
    "# Similarly, for neuron2 in the first hidden layer, the weighted sum calculation is:\n",
    "# weighted_sum2 = (input1 * weight1_2) + (input2 * weight2_2) + (input3 * weight3_2)\n",
    "\n",
    "# Here, (weight1_2, weight2_2, weight3_2) represents the weights associated with neuron2.\n",
    "\n",
    "# Each neuron in the first hidden layer has its own set of weights, which are unique and different from the weights of \n",
    "# other neurons in the same layer. These weights control the contribution of each input feature to the neuron's \n",
    "# output, allowing each neuron to learn and capture different patterns or combinations of features.\n",
    "\n",
    "# After calculating the weighted sums, an activation function is applied to introduce non-linearity into the network. \n",
    "# The activation function transforms the weighted sum into an output value for each neuron.\n",
    "# output = activation_function(weighted_sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value of array m: 10, and location 2\n",
      "max value of array n: 13, and location 3\n",
      "max value of array o: 9, and location 1\n"
     ]
    }
   ],
   "source": [
    "m = np.array([1,5, 10, 3])\n",
    "n = np.array([0,6, 4, 13])\n",
    "o = np.array([1,9, 6, 3])\n",
    "\n",
    "print(f\"max value of array m: {m.max()}, and location {m.argmax()}\")\n",
    "print(f\"max value of array n: {n.max()}, and location {n.argmax()}\")\n",
    "print(f\"max value of array o: {o.max()}, and location {o.argmax()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max value of the entire tensor: 10\n",
      "row max value of array m: tensor([10,  6,  9]), and location tensor([2, 2, 1])\n",
      "column max value of array m: tensor([ 3,  9, 10,  3]), and location tensor([1, 2, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "m = torch.tensor([[1,5, 10, 3],\n",
    "                 [3,5,6, 3],\n",
    "                 [3,9,2, 3]\n",
    "                 ])\n",
    "\n",
    "print(f\"max value of the entire tensor: {torch.max(m)}\")\n",
    "print(f\"row max value of array m: {torch.max(m, axis = 1).values}, and location {torch.max(m, axis = 1).indices}\")\n",
    "print(f\"column max value of array m: {torch.max(m, axis = 0).values}, and location {torch.max(m, axis = 0).indices}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.66698806,  0.02581308, -0.77761941],\n",
       "       [ 0.94863382,  0.70167179, -1.05108156],\n",
       "       [-0.36754812, -1.13745969, -1.32214752]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(23)  #this will apply random seed to the entire code cell\n",
    "\n",
    "var = np.random.randn(3,3)\n",
    "var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.66698806  0.02581308 -0.77761941]\n",
      " [ 0.94863382  0.70167179 -1.05108156]\n",
      " [-0.36754812 -1.13745969 -1.32214752]]\n",
      " \n",
      "[[ 1.77225828 -0.34745899  0.67014016]\n",
      " [ 0.32227152  0.06034293 -1.04345   ]\n",
      " [-1.00994188  0.44173637  1.12887685]]\n"
     ]
    }
   ],
   "source": [
    "randseed = np.random.RandomState(23) #this will apply random seed to only the variable\n",
    "\n",
    "print(randseed.randn(3,3))\n",
    "print(' ')\n",
    "print(randseed.randn(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8733,  0.4376, -0.4866],\n",
      "        [-0.7840, -0.2983, -0.9361],\n",
      "        [-0.5051,  1.5187,  0.1880]])\n",
      " \n",
      "[[ 1.77225828 -0.34745899  0.67014016]\n",
      " [ 0.32227152  0.06034293 -1.04345   ]\n",
      " [-1.00994188  0.44173637  1.12887685]]\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(23)   #pytorch alternative for random seed\n",
    "\n",
    "print(torch.randn(3,3))\n",
    "print(\" \")\n",
    "print(np.random.randn(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.0.1'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#symbolic maths in python\n",
    "import sympy as sym\n",
    "import sympy.plotting.plot as symplot\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Make: Toyota\n",
      "Model: Camry\n",
      "Year: 2021\n",
      "Wheels: 4\n",
      "Updated Year: 2022\n",
      "Engine started.\n",
      "The Toyota Camry is driving 50 miles.\n",
      "Engine stopped.\n",
      "Honk! Honk!\n"
     ]
    }
   ],
   "source": [
    "class Car:\n",
    "    # Class attribute\n",
    "    # wheels = 4\n",
    "\n",
    "    def __init__(self, make, model, year, wheels):\n",
    "        # Instance attributes\n",
    "        self.make = make \n",
    "        self.model = model  \n",
    "        self.year = year\n",
    "        self.wheels = wheels\n",
    "        self.engine_status = False\n",
    "\n",
    "    def start_engine(self):\n",
    "        self.engine_status = True\n",
    "        print(\"Engine started.\")\n",
    "\n",
    "    def stop_engine(self):\n",
    "        self.engine_status = False\n",
    "        print(\"Engine stopped.\")\n",
    "\n",
    "    def drive(self, distance):\n",
    "        if self.engine_status:\n",
    "            print(f\"The {self.make} {self.model} is driving {distance} miles.\")\n",
    "        else:\n",
    "            print(\"Start the engine first.\")\n",
    "\n",
    "    def honk_horn(self):\n",
    "        print(\"Honk! Honk!\")\n",
    "\n",
    "# Creating an instance (object) of the Car class\n",
    "my_car = Car(\"Toyota\", \"Camry\", 2021, 4)\n",
    "\n",
    "# Accessing attributes\n",
    "print(f\"Make: {my_car.make}\")\n",
    "print(f\"Model: {my_car.model}\")\n",
    "print(f\"Year: {my_car.year}\")\n",
    "print(f\"Wheels: {my_car.wheels}\")\n",
    "\n",
    "# Modifying attributes\n",
    "my_car.year = 2022\n",
    "print(f\"Updated Year: {my_car.year}\")\n",
    "\n",
    "# Calling object methods\n",
    "my_car.start_engine()\n",
    "my_car.drive(50)\n",
    "my_car.stop_engine()\n",
    "my_car.honk_horn()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(30,1)\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = nn.Sequential(\n",
    "    nn.Linear(1,1), #input layer\n",
    "    nn.ReLU(),  #activation function\n",
    "    nn.Linear(1,1)  #output layer\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ann_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adagrad(ann_model.parameters(), lr=1e-2) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Building your deep learning model (Tutorial code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(30,1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_model = nn.Sequential(\n",
    "    nn.Linear(1,1), #input layer\n",
    "    nn.ReLU(),  #activation function\n",
    "    nn.Linear(1,1)  #output layer\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning rate \n",
    "learningRate = .05 \n",
    "\n",
    "# loss function \n",
    "lossfun = nn.MSELoss() \n",
    "\n",
    "# optimizer (the flavor of gradient descent to implement) \n",
    "optimizer = optim.SGD(ann_model.parameters(),lr=learningRate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Cornel\\anaconda3\\envs\\paul_flask\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([30, 3])) that is different to the input size (torch.Size([30, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "#Lecturers own code:\n",
    "# train the model\n",
    "numepochs = 500\n",
    "losses = torch.zeros(numepochs)\n",
    "y = torch.Tensor(30, 3)\n",
    "\n",
    "## Train the model!\n",
    "for epochi in range(numepochs):\n",
    "\n",
    "  # forward pass\n",
    "  yHat = ann_model(x)\n",
    "\n",
    "  # compute loss\n",
    "  loss = lossfun(yHat,y)\n",
    "  losses[epochi] = loss\n",
    "\n",
    "  # backprop\n",
    "  optimizer.zero_grad()\n",
    "  loss.backward()\n",
    "  optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the losses\n",
    "\n",
    "# manually compute losses\n",
    "# final forward pass\n",
    "predictions = ann_model(x)\n",
    "\n",
    "# final loss (MSE)\n",
    "testloss = (predictions-y).pow(2).mean()\n",
    "\n",
    "plt.plot(losses.detach(),'o',markerfacecolor='w',linewidth=.1)\n",
    "plt.plot(numepochs,testloss.detach(),'ro')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Final loss = %g' %testloss.item())\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Build your deep learning model (my code and practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorchtf.keras.models.save_model(\n",
    "    model, filePathString, overwrite=True,\n",
    "    include_optimizer=True, save_format=None,\n",
    "    signatures=None, options=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Example data\n",
    "x = torch.randn(100, 10)  # Input data tensor\n",
    "y = torch.randn(100, 1)   # Target tensor\n",
    "\n",
    "# Model definition\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ANN, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)  # Example fully connected layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create the model instance\n",
    "# ANNreg = ANN()\n",
    "\n",
    "# Learning rate\n",
    "learningRate = 0.05\n",
    "\n",
    "# Loss function\n",
    "lossfun = nn.MSELoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.SGD(ann_model.parameters(), lr=learningRate)\n",
    "\n",
    "# Train the model\n",
    "numepochs = 500\n",
    "losses = torch.zeros(numepochs)\n",
    "\n",
    "# Train the model!\n",
    "for epochi in range(numepochs):\n",
    "    # Forward pass\n",
    "    yHat = ann_model(x)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = lossfun(yHat, y)\n",
    "    losses[epochi] = loss\n",
    "\n",
    "    # Backpropagation\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Loss: {}'.format(losses[epochi]))\n",
    "\n",
    "print('Finished Training') \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> back to practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8733,  0.4376],\n",
      "        [-0.4866, -0.7840]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(23)\n",
    "tensor_seq = [torch.randn(1,2),torch.randn(1,2)]\n",
    "x = torch.cat(tensor_seq, dim=0)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[-0.8733,  0.4376]]), tensor([[-0.4866, -0.7840]])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_seq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paul_flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
