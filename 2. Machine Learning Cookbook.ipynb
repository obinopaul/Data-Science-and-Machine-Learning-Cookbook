{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "70928dd7",
   "metadata": {},
   "source": [
    "### Machine Learning Glossary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19a8585",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ml-cheatsheet.readthedocs.io/en/latest/index.html   #this explains various ML concepts \n",
    "\n",
    "\n",
    "https://developers.google.com/machine-learning/glossary     #click the dropdown and choose fundamentals to filter for the most important.\n",
    "### Machine Learning Glossary of Terms\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "db23770f",
   "metadata": {},
   "source": [
    "### Machine Learning Models to Master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd03ac63",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Machine Learning Algorithms to Master  \n",
    "\n",
    "1. Linear and Multiple Linear Regression\n",
    "2. Logistic Regression\n",
    "3. Decision Trees\n",
    "4. Naive Bayes\n",
    "5. K-Nearest Neighbors\n",
    "6. Support Vector Machines\n",
    "7. Random Forests\n",
    "8. Neural Networks\n",
    "    1. Convolutional Neural Network (CNN)\n",
    "    2. Recurrent Neural Network (RNN)\n",
    "    3. Long Short-Term Memory (LSTM)\n",
    "    4. Generative Adversarial Network (GAN)\n",
    "    5. Deep Belief Network (DBN)\n",
    "    6. Deep Boltzmann Machine (DBM)\n",
    "    7. Autoencoders\n",
    "    8. Restricted Boltzmann Machines (RBM)\n",
    "    9. Hopfield Networks\n",
    "    10. Self-Organizing Maps (SOM)\n",
    "9. Gradient Boosting\n",
    "    1. XGBoost\n",
    "    2. LightGBM\n",
    "    3. CatBoost\n",
    "    4. Gradient Boosting Machines (GBM)\n",
    "    5. Stochastic Gradient Boosting (SGB)\n",
    "    6. Adaboost\n",
    "    7. Gradient Boosted Decision Trees (GBDT)\n",
    "    8. DeepBoost\n",
    "    9. Neural Network Boosting (NNBoost)\n",
    "    10. Gradient Boosted Regression Trees (GBRT)\n",
    "10. Reinforcement Learning\n",
    "11. Dimensionality Reduction Algorithms\n",
    "    1. Principal Component Analysis (PCA)\n",
    "    2. Linear Discriminant Analysis (LDA)\n",
    "    3. Independent Component Analysis (ICA)\n",
    "    4. Non-Negative Matrix Factorization (NMF)\n",
    "    5. Factor Analysis\n",
    "    6. Singular Value Decomposition (SVD)\n",
    "    7. t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "    8. Uniform Manifold Approximation and Projection (UMAP)\n",
    "    9. Autoencoders\n",
    "    10. Random Projection\n",
    "    11. Feature Selection\n",
    "    12. Locally Linear Embedding (LLE)\n",
    "12. Clustering Algorithms\n",
    "    1. K-Means Clustering\n",
    "    2. Hierarchical Clustering\n",
    "    3. Expectation-Maximization (EM) Clustering\n",
    "    4. Density-Based Spatial Clustering of Applications with Noise (DBSCAN)\n",
    "    5. Mean-Shift Clustering\n",
    "    6. Gaussian Mixture Model (GMM) Clustering\n",
    "    7. Spectral Clustering\n",
    "    8. Affinity Propagation Clustering\n",
    "    9. Birch Clustering\n",
    "    10. Optics Clustering\n",
    "13. Autoencoders\n",
    "14. Transfer Learning\n",
    "15. Generative Adversarial Networks (GANs)\n",
    "\n",
    "\n",
    "Data Preprocessing:\n",
    "    importing the required libraries\n",
    "    importing the dataset\n",
    "    handling missing data\n",
    "    encoding the categoical data\n",
    "    feature engineering\n",
    "    spliting the dataset into test set and training set\n",
    "    feature scaling \n",
    "\n",
    "Developing the Model:\n",
    "    model selection\n",
    "    model evaluation\n",
    "    model persistence\n",
    "    ensemble methods\n",
    "    feature extraction\n",
    "    feature selection\n",
    "    feature engineering\n",
    "    hyperparameter tuning\n",
    "   \n",
    "    '''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d80f3a91",
   "metadata": {},
   "source": [
    "### Cost Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de243c8c",
   "metadata": {
    "run_control": {
     "marked": true
    }
   },
   "outputs": [],
   "source": [
    "# #Cost Function\n",
    "# A cost function, also known as a loss function or objective function, \n",
    "# is a mathematical function that measures the difference between predicted and actual values in machine learning. \n",
    "# The purpose of a cost function is to guide the learning algorithm towards finding the optimal model parameters that minimize \n",
    "# the difference between the predicted and actual values.\n",
    "\n",
    "# The choice of cost function depends on the type of problem and the learning algorithm used. \n",
    "# Here are some common examples of cost functions and their equations:\n",
    "\n",
    "# 1. Mean Squared Error (MSE): This cost function is used for regression problems where the goal is to predict a continuous \n",
    "#     variable. It measures the average squared difference between the predicted and actual values. The equation for MSE is:\n",
    "\n",
    "#         MSE = 1/n * ∑(y - y_pred)^2\n",
    "#         where n is the number of samples, y is the actual value, and y_pred is the predicted value.\n",
    "\n",
    "# 2. Binary Cross-Entropy: This cost function is used for binary classification problems where the output is either 0 or 1. \n",
    "#     It measures the difference between the predicted probability and the actual label. \n",
    "#     The equation for binary cross-entropy is:\n",
    "\n",
    "#         Binary cross-entropy = -1/n * ∑(y * log(y_pred) + (1-y) * log(1-y_pred))\n",
    "#         where n is the number of samples, y is the actual label (0 or 1), and y_pred is the predicted probability.\n",
    "\n",
    "# 2. Categorical Cross-Entropy: This cost function is used for multi-class classification problems where the output \n",
    "#     can be one of several classes. It measures the difference between the predicted probability distribution and the actual \n",
    "#     label. The equation for categorical cross-entropy is:\n",
    "\n",
    "#         Categorical cross-entropy = -1/n * ∑∑(y_ij * log(y_pred_ij))\n",
    "#         where n is the number of samples, y_ij is the actual probability for class j in sample i, and y_pred_ij is the predicted probability for class j in sample i."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7f9dc2d9",
   "metadata": {},
   "source": [
    "### Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample to daily frequency  \n",
    "\n",
    "df_train_day = df_train.resample('D').mean() \n",
    "df_test_day = df_test.resample('D').mean()\n",
    "df['hour'] = df.index.hour \n",
    "df['day'] = df.index.day \n",
    "df['weekday'] = df.index.day_name() \n",
    "df['month'] = df.index.month \n",
    "df['year'] = df.index.year \n",
    "\n",
    "\n",
    "#plot data split \n",
    "def plot_data_splitting(train, test):\n",
    "    \"\"\"\n",
    "    Plots the training and test sets of a time series.\n",
    "\n",
    "    Args:\n",
    "    train (pandas.DataFrame): DataFrame containing the training set with a DatetimeIndex and a 'PJME_MW' column.\n",
    "    test (pandas.DataFrame): DataFrame containing the test set with a DatetimeIndex and a 'PJME_MW' column.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "\n",
    "    plt.plot(train.index, train['PJME_MW'], label='Training Set')\n",
    "    plt.plot(test.index, test['PJME_MW'], label='Test Set')\n",
    "\n",
    "    plt.title('Data Splitting', weight='bold', fontsize=25, loc= \"center\", pad=20)\n",
    "    plt.axvline('2015-09-01', color='black', ls='--', lw=3) \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#Time Series Split\n",
    "\n",
    "#using Sklearn \n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "# Assume 'X' is your feature matrix, and 'y' is your target variable \n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Split the data into training and testing sets\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "#using pandas\n",
    "import pandas as pd\n",
    "# Assume 'df' is your time series data stored in a pandas DataFrame\n",
    "# Set the split point as a percentage of the data\n",
    "train_size = 0.8\n",
    "split_point = int(len(df) * train_size)\n",
    "# Split the data into training and testing sets\n",
    "train_df = df[:split_point]\n",
    "test_df = df[split_point:]\n",
    "\n",
    "#using evalml\n",
    "import evalml\n",
    "# Assume 'df' is your time series data stored in a pandas DataFrame\n",
    "# Set the target variable name\n",
    "target_name = \"target_variable_name\"\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(df, target=target_name, problem_type=\"time series\",\n",
    "                                                                   test_size=0.2, shuffle=False)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "13de6381",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0fb24615",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-Learn Sub-modules\n",
    "\n",
    "# Scikit-Learn library is organized into several sub-modules, each of which contains a set of related functions and classes. \n",
    "# Here are the main sub-modules in scikit-learn:\n",
    "\n",
    "#from sklearn.\"sub-module\" import \"model\"\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "\n",
    "# sklearn.datasets: This sub-module provides a set of standard datasets for machine learning, including iris, \n",
    "#     digits, and breast cancer.\n",
    "        from sklearn.datasets import load_iris\n",
    "        iris_data = load_iris()\n",
    "        iris_features = iris_data.data \n",
    "        iris_target = iris_data.target\n",
    "        \n",
    "        # Convert the data to a DataFrame\n",
    "        df = pd.DataFrame(iris_features, columns=iris_data.feature_names)\n",
    "        \n",
    "        # Add the target variable to the DataFrame\n",
    "        df['target'] = iris_target \n",
    "        \n",
    "        # print(iris_data.DESCR) - Describes the data \n",
    "        # iris_data.data: An array containing the feature values for each instance of the dataset.\n",
    "        # iris_data.target: An array containing the class labels (i.e., 0, 1, or 2) for each instance of the dataset.\n",
    "        # Iris_data.target_names: An array containing the names of the three classes \n",
    "        # iris_data.feature_names: An array containing the names of the attributes \n",
    "        #or\n",
    "        X, y = load_iris(return_X_y=True, as_frame=True) \n",
    "        X, y = fetch_openml(name = 'blood-transfusion-service-center', as_frame=True, return_X_y=True)\n",
    "        \n",
    "        from sklearn.datasets import load_digits\n",
    "        from sklearn.datasets import fetch_openml\n",
    "\n",
    "        X, y = load_digits(return_X_y=True, as_frame=True)\n",
    "        X.shape, y.shape\n",
    "        # Plot the first 10 digits\n",
    "        fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5))\n",
    "        for i, ax in enumerate(axes.flat):\n",
    "        ax.imshow(X.iloc[i].values.reshape(8, 8), cmap='gray')\n",
    "        ax.set_title(f\"Digit {y.iloc[i]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "# sklearn.model_selection: This sub-module contains functions for model selection, such as splitting data into \n",
    "#     training and test sets, cross-validation, and grid search.\n",
    "\n",
    "# sklearn.preprocessing: This sub-module provides functions for preprocessing data, such as scaling, normalization, \n",
    "#     and encoding categorical variables.\n",
    "\n",
    "# sklearn.feature_extraction: This sub-module contains functions for feature extraction from raw data, \n",
    "#     such as text data, including Bag of Words, CountVectorizer, and TfidfVectorizer.\n",
    "\n",
    "# sklearn.metrics: This sub-module provides functions for evaluating the performance of machine learning models, \n",
    "#     such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "# sklearn.pipeline: This sub-module provides tools for building machine learning pipelines, \n",
    "#     which allows you to chain together multiple steps, such as feature extraction, preprocessing, and model selection.\n",
    "\n",
    "# sklearn.decomposition: This sub-module provides classes for matrix factorization and decomposition, \n",
    "#     such as Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), \n",
    "#     and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "# sklearn.discriminant_analysis: This sub-module provides classes for linear and quadratic discriminant analysis, \n",
    "#     which are used for supervised classification tasks.\n",
    "\n",
    "# sklearn.covariance: This sub-module provides classes for covariance estimation, such as Empirical Covariance and \n",
    "#     Shrunk Covariance.\n",
    "\n",
    "# sklearn.exceptions: This sub-module contains custom exceptions raised by scikit-learn, such as NotFittedError and \n",
    "#     ConvergenceWarning.\n",
    "\n",
    "\n",
    "#Models: \n",
    "\n",
    "\n",
    "# sklearn.linear_model: This sub-module contains classes for linear models, such as linear regression, \n",
    "#     logistic regression, and ridge regression.\n",
    "\n",
    "# sklearn.tree: This sub-module provides classes for decision trees, such as DecisionTreeClassifier and \n",
    "#     DecisionTreeRegressor.\n",
    "\n",
    "# sklearn.ensemble: This sub-module contains classes for ensemble models, such as random forests, AdaBoost, \n",
    "#     and Gradient Boosting.\n",
    "\n",
    "# sklearn.cluster: This sub-module provides classes for clustering, such as KMeans and Hierarchical Clustering.\n",
    "\n",
    "# sklearn.neural_network: This sub-module contains classes for neural networks, such as Multi-Layer Perceptron (MLP) \n",
    "#     and Convolutional Neural Networks (CNNs).\n",
    "\n",
    "# sklearn.svm: This sub-module contains classes for Support Vector Machines (SVMs), such as SVM classifier and regression.\n",
    "\n",
    "# sklearn.manifold: This sub-module provides classes for manifold learning, such as t-SNE and Isomap.\n",
    "\n",
    "# sklearn.naive_bayes: This sub-module provides classes for Naive Bayes models, such as Gaussian Naive Bayes and \n",
    "#     Multinomial Naive Bayes.\n",
    "\n",
    "# sklearn.neighbors: This sub-module provides classes for k-Nearest Neighbors (k-NN) models, \n",
    "#     such as KNeighborsClassifier and KNeighborsRegressor.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b17ace5a",
   "metadata": {},
   "source": [
    "### Decision Tree, Random Forest, and Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c17a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "https://ml-cheatsheet.readthedocs.io/en/latest/index.html   #use this to explain the concepts. \n",
    "\n",
    "\n",
    "# Decision Tree                          (use one hot encoding for this)\n",
    "    # A supervised learning model composed of a set of conditions and leaves organized hierarchically. Decision tree works by successively \n",
    "    # splitting the dataset into small segments until the target variable are the same or until the dataset can no longer be split. \n",
    "    # It’s a greedy algorithm which make the best decision at the given time without concern for the global optimality\n",
    "\n",
    "#steps for Decision tree (classification)\n",
    "    #Start with all examples at the root node\n",
    "    #calculate information gain for all possible features, and pick the one with the highest infrmation gain\n",
    "    #Split the dataset according to selected feature, and create left and right branches of the tree\n",
    "    #Keep repeating splitting process until stopping criteria is met:\n",
    "        #when a node is 100% one class\n",
    "        #when splitting a node will result in the tree exceeding a maximum depth\n",
    "        #information gain from additional splits is less than threshold\n",
    "        #when number of examples in a node is below threshold\n",
    "\n",
    "\n",
    "\n",
    "# Random Forest \n",
    "    # An ensemble of decision trees in which each decision tree is trained with a specific random noise, such as bagging. \n",
    "    # Random forests are a type of decision forest.\n",
    "    #Create multiple decision trees on bootstrap sample of data.\n",
    "    #Average results across all trees to improve accuracy.\n",
    "\n",
    "\n",
    "\n",
    "#Bagging (Bootstrapping + Aggregating (or voting)) i.e., randomly creating samples (subsets) of the dataset with replacement, \n",
    "    # then builds models on the random subsets. The multiple models are combined by taking a majority vote or \n",
    "    # averaging their predictions to make the final prediction or decision\n",
    "    from sklearn.ensemble import BaggingClassifier, VotingClassifier,RandomForestClassifier\n",
    "    bagging_classifier = BaggingClassifier(estimator=RandomForestClassifier(), n_estimators=15,  max_samples=200, max_features=X_train.shape[1])\n",
    "    vot_classifier = VotingClassifier(    \n",
    "                                    estimators=[('log_reg', log_classifier),\n",
    "                                                ('svc', sv_classifier),\n",
    "                                                ('sgd', sgd_classifier)], \n",
    "                                    voting='hard')  #there are several types of voting/aggregation (majority vote,\n",
    "                                                                                                    #average, \n",
    "                                                                                                    # weighted average etc.)\n",
    "#Boosting\n",
    "    #Boosting is a machine learning ensemble technique that combines multiple base models to create a stronger overall model. \n",
    "    # Unlike bagging, which creates subsets of the training data for training base models, The basic idea behind boosting is \n",
    "    # to sequentially train a series of base models, where each subsequent base model focuses on correcting the errors \n",
    "    # made by the previous base models\n",
    "    from sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier\n",
    "\n",
    "    #AdaBoost (Adaptive Boosting): \n",
    "    # AdaBoost is a widely used boosting algorithm that assigns higher weights to misclassified samples and adjusts the \n",
    "    # weights of base models based on their accuracy. It places more emphasis on samples that are misclassified by the \n",
    "    # current ensemble and updates the weights of samples and base models accordingly.\n",
    "    \n",
    "    #Gradient Boosting: \n",
    "    # Gradient Boosting is a generalization of AdaBoost that uses gradient descent optimization to minimize the loss \n",
    "    # function of the ensemble model. It sequentially fits the base models to the residuals (i.e., the differences between \n",
    "    # the true labels and the predictions) of the previous base models, resulting in a more accurate and robust model.\n",
    "    \n",
    "    #XGBoost (Extreme Gradient Boosting): XGBoost is a popular implementation of gradient boosting that incorporates \n",
    "    # additional optimizations for improved performance, such as parallelization, regularization, and handling of missing \n",
    "    # values.\n",
    "    from xgboost import XGBClassifier\n",
    "\n",
    "#Stacking and Blending \n",
    "from sklearn.ensemble import StackingClassifier, StackingRegressor\n",
    "    #Stacking refers to training a learning algorithm to combine the predictions of several other algorithms. The \n",
    "    #predictions of the base algorithms are used as input to train the stacking algorithm. This helps to create a \n",
    "    #meta-model that can leverage the predictions of the base models\n",
    "    \n",
    "    #Blending refers to simply averaging the predictions of multiple models. The predictions of the base models are\n",
    "    #combined using a weighted average to get the final prediction. \n",
    "\n",
    "stacking_classifier = StackingClassifier(estimators=[('random forest', RandomForestClassifier()), \n",
    "                                                     ('decision trees', DecisionTreeClassifier()),\n",
    "                                                     ('logistic regression', LogisticRegression())], stack_method='predict'\n",
    "                                         final_estimator=RandomForestClassifier())    \n",
    "        #final_estimator is the metal-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d259359",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "#### Ensemble Methods\n",
    "\n",
    "- Feature extraction and selection are relatively manual processes. Bagging and boosting are automated or semi-automated approaches to determine which features to include.  \n",
    "- At a high level, Ensemble Methods is about bringing together multiple models (called weak learners) so that the result is an incredibly powerful and more accurate model (called a strong learner). There are several strategies and tricks involved in this.\n",
    "\n",
    "##### Motive\n",
    "\n",
    "- There are two competing variables in finding a well-fitting machine learning model:\n",
    "  - Bias: When a model has a high bias, this means that means it doesn't do a good job of bending to the data.\n",
    "    - Linear regression is an example of an algorithm that usually has a high bias . Even with completely different datasets, we end up with the same line fit to the data.\n",
    "  - Variance: When a model has high variance, this means that it changes drastically to meet the needs of every point in our dataset.\n",
    "    - Decision tree is an example of an algorithm that tends to have high variance and low bias (especially decision trees with no early stopping parameters). A decision tree, as a high variance algorithm, will attempt to split every point into its own branch if possible. This is a trait of high variance, low bias algorithms - they are extremely flexible to fit exactly whatever data they see.\n",
    "- By combining algorithms, we can often build models that perform better by meeting in the middle in terms of bias and variance. These ideas are based on minimizing bias and variance based on mathematical theories, like the central limit theorem.\n",
    "\n",
    "##### Methods\n",
    "\n",
    "- Introducing Randomness to high variance algorithms:\n",
    "  - The introduction of randomness combats the tendency of these algorithms to overfit.\n",
    "  - There are two main ways\n",
    "    - **Bagging (Bootstrapping Aggregation)**\n",
    "      - Generate a group of weak learners that when combined together generate higher accuracy. Each weak learner is trained on a sample of the data (because the data may be huge and training all the data on multiple algorithms is computationally expensive).\n",
    "      - Sampling the data with replacement and fitting your algorithm to the sampled data.\n",
    "      - The weak learners are combined by voting. Each learner is imposed on the data, and the predicted value with most votes wins.\n",
    "      - It reduces variance but keeps bias the same.\n",
    "      - sklearn:\n",
    "        - `sklearn.ensemble.BaggingClassifier`\n",
    "        - `sklearn.ensemble.BaggingRegressor`\n",
    "    - Subset the features\n",
    "      - In each split of a decision tree or with each algorithm used in an ensemble, only a subset of the total possible features are used.\n",
    "\n",
    "- **Boosting**\n",
    "  - Assign strengths to each weak learner\n",
    "  - Iteratively train learners using misclassified examples by previous weak learners.\n",
    "  - It is used for models that have a high bias and accepts weights on individual samples.\n",
    "  - Types:\n",
    "    - AdaBoost:\n",
    "      - Discovered by Freund and Schapire in 1996\n",
    "      - Steps:\n",
    "        - First model that maximizes accuracy, minimizes errors.\n",
    "        - Identify misclassified points from the previous step and apply wights to them. So subsequent models, classifiers can focus on the misclassified samples more. Each misclassified point will have added weight with $\\frac{\\text{correct points}}{\\text{incorrect points}}$\n",
    "        - Calculate the weight for the model as $\\text{weight} = ln(\\frac{\\text{correct points}}{\\text{incorrect points}}) = ln(\\frac{\\text{accuracy}}{\\text{1 - accuracy}})$\n",
    "        - Reiterate the same steps but the new model will try to correctly classify misclassified points in the previous step, with the help of the weights added to each misclassified point.\n",
    "        - Finally, combine all models by adding the models weights for the area for one of the classes, and subtract the weight for the area of the other class.\n",
    "        ![AdaBoost-models-weights](ML%20images/adaboost-models-weights.png)\n",
    "        ![AdaBoost-combine-models](ML%20images/adaboost-combine-models.png)\n",
    "        ![AdaBoost-combined-model](ML%20images/adaboost-combined-model.png)\n",
    "      - sklearn:\n",
    "        - `sklearn.ensemble.AdaBoostClassifier`\n",
    "          - Hyperparameters:\n",
    "            - `base_estimator`: The model utilized for the weak learners (Warning: Don't forget to import the model that you decide to use for the weak learner).\n",
    "            - `n_estimators`: The maximum number of weak learners used.\n",
    "        - `sklearn.ensemble.AdaBoostRegressor`\n",
    "        - `sklearn.ensemble.GradientBoostingClassifier`\n",
    "    - XGBoost library\n",
    "    \n",
    "    ''' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cf3fc6ce",
   "metadata": {},
   "source": [
    "### Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d3db99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Data:\n",
    "#     Some machine learning models, such as decision trees and random forests, can handle missing data directly, \n",
    "#     while others may require imputation or removal of missing data. For example, models like K-nearest neighbors (KNN) \n",
    "#     and Support Vector Machines (SVM) may be sensitive to missing data and may require imputation or removal of \n",
    "#     missing values before training the model.\n",
    "\n",
    "# Data Imbalance:\n",
    "#     Techniques such as oversampling, undersampling, or using ensemble methods like SMOTE \n",
    "#     (Synthetic Minority Over-sampling Technique) can be used to address data imbalance. Some machine learning models, \n",
    "#     such as decision trees and random forests, can handle imbalanced data well, while others may require handling of \n",
    "#     imbalanced data as a preprocessing step, such as using oversampling or undersampling techniques. \n",
    "#     For example, models like logistic regression, naive Bayes, KNN,    and support vector machines (SVM) may require handling \n",
    "#     of imbalanced data.\n",
    "\n",
    "# Feature Scaling:\n",
    "#     Some machine learning models, such as k-nearest neighbors (KNN) and support vector machines (SVM), are sensitive \n",
    "#     to the scale of features and may require feature scaling.  On the other hand, decision trees and random forests \n",
    "#     are not sensitive to feature scaling and do not require this preprocessing step.\n",
    "\n",
    "# Categorical Data:\n",
    "#     Some models, like decision trees and random forests, can directly handle categorical data without encoding, \n",
    "#     while others, like logistic regression and support vector machines (SVM), may require encoding of categorical data\n",
    "\n",
    "# Outliers:\n",
    "#     Some machine learning models, such as decision trees and random forests, are less sensitive to outliers, while \n",
    "#     others, such as linear regression, SVM, and k-nearest neighbors (KNN), can be affected by outliers and may require \n",
    "#     handling of outliers as a preprocessing step.\n",
    "    \n",
    "# Dimensionality:\n",
    "    #  refers to the number of features or variables in the dataset. High-dimensional data can lead to increased \n",
    "    #  complexity, increased computation time, and reduced model performance. \n",
    "    #  Some machine learning models, such as decision trees and random forests, are less sensitive to \n",
    "    #  high-dimensional data, while others, such as logistic regression and support vector machines (SVM), \n",
    "    #  may require handling of high-dimensional data as a preprocessing step\n",
    "\n",
    "#reduce memory usage of the dataset\n",
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "        else:\n",
    "            df[col] = df[col].astype('category')\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc9fd9a",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fca474",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "1. Data Cleaning:\n",
    "    a. Missing values:\n",
    "        Removing the training example:\n",
    "        Filling in missing value manually\n",
    "        Using a standard value to replace the missing value\n",
    "        Using central tendency (mean, median, mode) for attribute to replace the missing value:\n",
    "        Using central tendency (mean, median, mode) for attribute belonging to same class to replace the missing value:\n",
    "        Using the most probable value to fill in the missing value:\n",
    "\n",
    "    b. Noisy Data and Outliers: \n",
    "        Binning: Using binning methods smooths sorted values by using the values around it. The sorted values are then divided \n",
    "            into bins. \n",
    "        Regression:  Linear regression and multiple linear regression can be used to smooth the data, where the values \n",
    "            are conformed to a function.\n",
    "        Outlier analysis: Approaches such as clustering can be used to detect outliers and deal with them.\n",
    "\n",
    "    c. Remove Unwanted Data: Unwanted data is duplicate or irrelevant data. \n",
    "    \n",
    "2. Data Integration:\n",
    "    Data consolidation: The data is physically brought together to one data store. This usually involves Data Warehousing.\n",
    "    Data propagation: Copying data from one location to another using applications is called data propagation\n",
    "    Data virtualization: An interface is used to provide a real-time and unified view of data from multiple sources. \n",
    "\n",
    "3. Data Reduction:\n",
    "    Missing values ratio: Attributes that have more missing values than a threshold are removed.\n",
    "    Low variance filter: Normalized attributes that have variance (distribution) less than a threshold are also removed \n",
    "        because little changes in data means less information.\n",
    "    High correlation filter: Normalized attributes that have correlation coefficients more than a threshold are removed \n",
    "        because similar trends means similar information is carried. A correlation coefficient is usually calculated using \n",
    "        statistical methods such as Pearson’s chi-square value.\n",
    "    Principal component analysis: Principal component analysis, or PCA, is a statistical method that reduces the numbers \n",
    "        of attributes by lumping highly correlated attributes together.\n",
    "\n",
    "4. Data Transformation:\n",
    "    Smoothing: Eliminating noise in the data to see more data patterns.\n",
    "    Attribute/feature construction: New attributes are constructed from the given set of attributes.\n",
    "    Aggregation: Summary and aggregation operations are applied on the given set of attributes to come up with new attributes\n",
    "    Normalization: The data in each attribute is scaled between a smaller range, for example, 0 to 1 or -1 to 1.\n",
    "    Discretization: Raw values of the numeric attributes are replaced by discrete or conceptual intervals, \n",
    "        which can be further organized into higher-level intervals. \n",
    "    Concept hierarchy generation for nominal data: Values for nominal data are generalized to higher-order concepts.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180bd4b2",
   "metadata": {},
   "source": [
    ">> Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e530b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# Categorical encoding\n",
    "There are four techniques to encode or convert the categorical features into numbers. Here are them:\n",
    "\n",
    "Mapping Method\n",
    "Ordinary Encoding\n",
    "Label Encoding\n",
    "Pandas Dummies\n",
    "OneHot Encoding\n",
    "\n",
    "The choice of categorical encoding method depends on several factors such as the type and nature of the data, \n",
    "the number of unique categories in the variable, the type of machine learning algorithm being used, and the performance \n",
    "of the encoding method on the dataset. Here are some general guidelines on when to use each method:\n",
    "\n",
    "One-Hot Encoding\n",
    "One-hot encoding is a useful technique for handling categorical variables with a small number of unique categories. \n",
    "It is particularly useful when the categories are nominal (unordered) or when there is no inherent order or hierarchy \n",
    "among the categories. One-hot encoding can be applied to both linear and tree-based machine learning models. \n",
    "However, one limitation of one-hot encoding is that it can lead to a high-dimensional feature space, which can be \n",
    "computationally expensive and may lead to the curse of dimensionality.\n",
    "\n",
    "Label Encoding\n",
    "Label encoding is a useful technique for handling categorical variables with a large number of unique categories. \n",
    "It is particularly useful when the categories are ordinal (ordered) or when there is an inherent order or hierarchy \n",
    "among the categories. Label encoding can be applied to both linear and tree-based machine learning models. \n",
    "However, one limitation of label encoding is that it may introduce an arbitrary ordering or hierarchy among the \n",
    "categories, which may not be appropriate for some models.\n",
    "\n",
    "In general, it is recommended to use one-hot encoding when dealing with nominal categorical variables and label encoding \n",
    "when dealing with ordinal categorical variables. However, it is important to consider the nature of the data and the \n",
    "performance of the encoding method on the specific dataset before making a decision on which method to use. \n",
    "Additionally, it is often useful to try both encoding methods and compare their performance on the dataset to determine \n",
    "the optimal encoding method.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5154f2e4",
   "metadata": {},
   "source": [
    ">> Mixed Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29da9d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ColumnTransformer is another transformer class in scikit-learn, but it is used to apply different transformations \n",
    "# to different columns of a dataset. It allows you to specify a list of transformers, where each transformer is \n",
    "# applied to a specific subset of columns.\n",
    "\n",
    "# For example, if you have a dataset with both numerical and categorical features, you can use ColumnTransformer to \n",
    "# apply different preprocessing steps to each subset of features. You could apply one transformation (e.g., scaling) \n",
    "# to the numerical features and another transformation (e.g., one-hot encoding) to the categorical features.\n",
    "\n",
    "# ColumnTransformer is especially useful when you have a dataset with a mix of different types of features and you want \n",
    "# to apply different preprocessing steps to each subset of features. It can also be used in combination with Pipeline \n",
    "# to create more complex preprocessing pipelines for your data.\n",
    "\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Define the column transformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), [0, 1, 2, 3]),  # apply StandardScaler to numerical columns\n",
    "        ('cat', OneHotEncoder(), [4])  # apply OneHotEncoder to categorical column\n",
    "    ])\n",
    "\n",
    "# Define the pipeline\n",
    "clf = Pipeline(steps=[('preprocessor', preprocessor)])\n",
    "\n",
    "# Fit the pipeline to the data\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "clf.fit(X, y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Another code\n",
    "\n",
    "# Import necessary libraries\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "# Define the transformers to be used in the pipeline\n",
    "subject_body_transformer = FunctionTransformer(\n",
    "    lambda x: x[['subject', 'body']], validate=False)\n",
    "\n",
    "text_stats_transformer = FunctionTransformer(\n",
    "    lambda x: [{'length': len(text)} for text in x['body']], validate=False)\n",
    "\n",
    "# Define the pipeline with three steps:\n",
    "# 1. Extract the 'subject' and 'body' columns\n",
    "# 2. Combine the 'subject' and 'body' features with a ColumnTransformer\n",
    "# 3. Apply a LinearSVC classifier to the combined features\n",
    "pipeline = Pipeline( transformers = [\n",
    "    # Step 1: Extract 'subject' and 'body' columns\n",
    "    ('subject_body', subject_body_transformer),\n",
    "    # Step 2: Combine 'subject' and 'body' features with a ColumnTransformer\n",
    "    ('union', ColumnTransformer([\n",
    "        # Create a bag-of-words representation for 'subject' column\n",
    "        ('subject_bow', TfidfVectorizer(min_df=50), 'subject'),\n",
    "        # Create a bag-of-words representation for 'body' column with decomposition\n",
    "        ('body_bow', Pipeline([\n",
    "            ('tfidf', TfidfVectorizer()),\n",
    "            ('best', TruncatedSVD(n_components=50))]), 'body'),\n",
    "        # Use a Pipeline to extract text stats from 'body' column and create features\n",
    "        ('body_stats', Pipeline([\n",
    "            ('stats', text_stats_transformer),\n",
    "            ('vect', DictVectorizer())]), 'body')\n",
    "    ],\n",
    "    # Weight the different feature extraction methods\n",
    "    transformer_weights={\n",
    "        'subject_bow': 0.8,\n",
    "        'body_bow': 0.5,\n",
    "        'body_stats': 1.0\n",
    "    }\n",
    "    )),\n",
    "    # Step 3: Apply a LinearSVC classifier to the combined features\n",
    "    ('svc', LinearSVC(dual=False))\n",
    "], verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef95569",
   "metadata": {},
   "source": [
    ">> Variable Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14ed328",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logarithmic (only defined for positive numbers) - log(X)\n",
    "#Exponential (square root or power transformations) - \n",
    "#Reciprocal (naturally not defined for zero, also defined for positive values) - 1/X\n",
    "#Box-Cox (defined only for positive values X>0)\n",
    "#Yeo-Johnson (is an adaptation of box-cox that can be used in negative value variables)\n",
    "\n",
    "#NB: if data is positively skewed (right skewed), use (logarithmic, reciprocal, or square root transformation)\n",
    "    #if data is negatively skewed (left skewed), use (Box-Cox or Yeo-Johnson transformations)\n",
    "\n",
    "#check if dataset is normally distributed or not.\n",
    "def diagnostic_plots(df, variable):\n",
    "\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.title(f\"Histogram of {variable}\")\n",
    "\n",
    "    # q-q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot of {variable}\")\n",
    "\n",
    "    # check for skewness\n",
    "    skewness = df[variable].skew()\n",
    "    if skewness > 0:\n",
    "        skew_type = \"positively skewed\"\n",
    "    elif skewness < 0:\n",
    "        skew_type = \"negatively skewed\"\n",
    "    else:\n",
    "        skew_type = \"approximately symmetric\"\n",
    "        \n",
    "    # print message indicating skewness type\n",
    "    print(f\"The variable {variable} is {skew_type} (skewness = {skewness:.2f})\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "#log transform \n",
    "def log_transform(df, columns):\n",
    "     \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the natural logarithm function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_log = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_log\n",
    "\n",
    "#reciprocal transformation\n",
    "def reciprocal_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the reciprocal transformation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(lambda x: 1/x, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_recip = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_recip\n",
    "\n",
    "#square root transformation\n",
    "def sqrt_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the square root function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.sqrt, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_sqrt = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_sqrt\n",
    "\n",
    "#exponential transformation\n",
    "def exp_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the exponential function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.exp, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_exp = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_exp\n",
    "\n",
    "#box-cox transformation\n",
    "def boxcox_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the Box-Cox transformation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = PowerTransformer(method='box-cox', standardize=False)\n",
    "    X = df.copy()\n",
    "    X[columns] = transformer.fit_transform(X[columns])\n",
    "    return X\n",
    "\n",
    "\n",
    "#Yeo-Johnson\n",
    "def yeo_johnson_transform(df, columns):\n",
    "    \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the Yeo-Johnson transformation.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = PowerTransformer(method='yeo-johnson', standardize=False)\n",
    "    X = df.copy()\n",
    "    X[columns] = transformer.fit_transform(X[columns])\n",
    "    return X\n",
    "\n",
    "\"\"\"\n",
    "A normal distribution is characterized by a bell-shaped curve that is symmetric around the mean. \n",
    "The mean, median, and mode of a normal distribution are all equal, and approximately 68% of the data falls within one \n",
    "standard deviation of the mean, 95% falls within two standard deviations, and 99.7% falls within three \n",
    "standard deviations.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23f2a9",
   "metadata": {},
   "source": [
    ">> Discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8469fd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization in machine learning is the process of transforming continuous variables into discrete or \n",
    "# categorical variables. This process involves dividing the range of a continuous variable into a finite number of \n",
    "# intervals or bins, and then assigning each observation to a particular bin based on the value of the continuous \n",
    "# variable. \n",
    "\n",
    "#Discretization approaches: equal width, equal frequency, K means, Decision Trees\n",
    "\n",
    "\n",
    "#use equal width or equal frequency binning method for majority of cases\n",
    "df['age_bin_equal_width'] = pd.cut(df['age'], bins=3)   # Equal width binning\n",
    "df['income_bin_equal_freq'] = pd.qcut(df['income'], q=3)    # Equal frequency binning\n",
    "\n",
    "\n",
    "# if the data contains outliers or unevenly distributed data, then equal width or frequency binning may not be \n",
    "# appropriate. In such cases, methods like K-Means clustering, decision tree discretization, and Gaussian mixture \n",
    "# modeling may be more suitable.\n",
    "# K-Means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "df['age_income_cluster'] = kmeans.fit_predict(df[['age', 'income']])\n",
    "\n",
    "# Decision tree discretization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(max_depth=1)\n",
    "dt.fit(df[['age']], df['income'])\n",
    "df['age_bin_decision_tree'] = dt.predict(df[['age']])\n",
    "\n",
    "# Gaussian mixture model\n",
    "from sklearn.mixture import GaussianMixture\n",
    "gmm = GaussianMixture(n_components=3)\n",
    "gmm.fit(df[['age', 'income']])\n",
    "df['age_income_gmm'] = gmm.predict(df[['age', 'income']])\n",
    "\n",
    "# Entropy-based discretization\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier(criterion='entropy')\n",
    "dt.fit(df[['age']], df['income'])\n",
    "df['age_bin_entropy'] = dt.predict(df[['age']]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab32be54",
   "metadata": {},
   "source": [
    ">> Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a610f6e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('from sklearn.model_selection import cross_val_score'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean training error: 0.0356386368576308\n",
      "Mean validation error: -0.009125646107584522\n"
     ]
    }
   ],
   "source": [
    "# Pipeline and FeatureUnion are classes in scikit-learn that are used to create pipelines for machine learning tasks. \n",
    "# Pipeline is used to sequentially apply a list of transformers and an estimator, while FeatureUnion is used to \n",
    "# concatenate the output of multiple transformer objects. \n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Load the iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Define pipeline using FeatureUnion\n",
    "preprocessor = FeatureUnion(transformer_list=[\n",
    "    ('numeric_transformer', Pipeline(steps=[\n",
    "                                            ('scaler', StandardScaler()),\n",
    "                                            ('pca', PCA(n_components=2)),\n",
    "                                        ])),\n",
    "    ('categorical_transformer', Pipeline(steps=[\n",
    "                                            ('onehot', OneHotEncoder(handle_unknown='ignore')),\n",
    "                                        ])),\n",
    "])\n",
    "\n",
    "# Define the pipeline\n",
    "pipe = Pipeline([\n",
    "    ('features', feature_union),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit and predict using the pipeline\n",
    "pipe.fit(iris.data, iris.target)\n",
    "preds = pipe.predict(iris.data)\n",
    "\n",
    "-----------------------------------------------------------------------------------------------------\n",
    "#use this to visualize the pipelines as a diagram \n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "#NB: use FunctionTransformer  to apply a custom function to each feature, and ColumnTransformer to apply different \n",
    "# transformers to different columns in a dataset\n",
    "\n",
    "# Define preprocessing functions\n",
    "log_transformer = FunctionTransformer(func=np.log1p, validate=True)\n",
    "scale_transformer = StandardScaler()\n",
    "ohe_transformer = OneHotEncoder(sparse=False)\n",
    "\n",
    "# Define preprocessing steps for different feature types\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('log_transform', log_transformer, ['numerical_feature']),\n",
    "    ('scale_transform', scale_transformer, ['numerical_feature_2']),\n",
    "    ('ohe_transform', ohe_transformer, ['categorical_feature']),\n",
    "])\n",
    "\n",
    "# Define pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Fit pipeline to data\n",
    "pipeline.fit(X, y)\n",
    "----------------------------------------------------------------------------------------------------\n",
    "#An Illustration\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion, Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer, StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X,y = load_breast_cancer(return_X_y=True, as_frame=True) \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=45) \n",
    "\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None): \n",
    "    # Function to treat missing values in numeric columns \n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean()) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('Standard_scalar', StandardScaler(), ['mean smoothness']),\n",
    "    ('minmax_scalar', MinMaxScaler(), ['mean area', 'mean texture', 'mean perimeter']),\n",
    "    ('NaN', FunctionTransformer(treat_missing_numeric, kw_args={'columns': ['mean symmetry', 'mean fractal dimension'], 'how': 'mean'}), \n",
    "                                       ['mean symmetry', 'mean fractal dimension']),\n",
    "])\n",
    "\n",
    "pipe = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LogisticRegression())\n",
    "    \n",
    "    ])\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred= pipe.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f35f8b",
   "metadata": {},
   "source": [
    ">> Feature Selection and Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79ec4c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection is a technique of selecting relevant features from the original feature set, while feature \n",
    "# extraction is a technique of creating new features from the original feature set. Feature selection is typically \n",
    "# used when the original features are already informative, but there are some irrelevant or redundant features that \n",
    "# need to be removed to improve the model performance. \n",
    "\n",
    "# Feature extraction, on the other hand, is used when the original features are not informative enough, and \n",
    "# new features need to be created to capture the underlying patterns in the data.\n",
    "\n",
    "#there are two ways to resolve/prevent curse of dimensionality (dimensionality reduction)\n",
    "#Feature selection\n",
    "    # Removing features with low variance (VarianceThreshold)\n",
    "    # Univariate Feature Selection (SelectKBest, SelectPercentile, GenericUnivariateSelect)\n",
    "    # Recursive Feature Elimination (RFE, RFECV)        RFECV - RFE cross validation\n",
    "    # Feature selection using SelectFromModel (SelectFromModel) - use L1-based (Lasso, Ridge, ElasticNet) or Tree-based\n",
    "    # Sequential Feature Selection (SequentialFeatureSelector) - SFS can be either forward or backward\n",
    "\n",
    "\n",
    "#Feature extraction\n",
    "    # Principal Component Analysis (PCA)\n",
    "    # Independent Component Analysis (ICA)\n",
    "    # t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Variance threshold: - # Removing features with low variance\n",
    "# This technique removes all features whose variance is below a certain threshold. \n",
    "# This is done using the VarianceThreshold function from scikit-learn library.\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "def variance_threshold(X, threshold=0.0):\n",
    "    selector = VarianceThreshold(threshold=threshold)\n",
    "    X_new = selector.fit_transform(X)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# SelectKBest:\n",
    "# This technique selects the K best features based on univariate statistical tests. \n",
    "# This is done using the SelectKBest function from scikit-learn library\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "def select_k_best(X, y, k=10):\n",
    "    selector = SelectKBest(score_func = f_classif, k=k)\n",
    "    X_new = selector.fit_transform(X, y)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "# Principal Component Analysis (PCA):\n",
    "# This technique reduces the dimensionality of the data by projecting it onto a lower dimensional space. \n",
    "# This is done using the PCA function from scikit-learn library.\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def pca(X, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_new = pca.fit_transform(X)\n",
    "    return X_new \n",
    "                    # # Get the loadings of the original variables in each component\n",
    "                    # loadings = pca.components_\n",
    "                    # # Print the names of the columns that were extracted\n",
    "                    # print(\"Columns extracted:\")\n",
    "                    # for i in range(loadings.shape[0]):\n",
    "                    #     max_loading_index = loadings[i].argmax()\n",
    "                    #     column_name = data.columns[max_loading_index]\n",
    "                    #     print(f\"Component {i+1}: {column_name}\")\n",
    "\n",
    "# Independent Component Analysis (ICA):\n",
    "# This technique extracts independent sources from the data by maximizing their statistical independence. \n",
    "# This is done using the FastICA function from scikit-learn library.\n",
    "from sklearn.decomposition import FastICA\n",
    "\n",
    "def ica(X, n_components=2):\n",
    "    ica = FastICA(n_components=n_components)\n",
    "    X_new = ica.fit_transform(X)\n",
    "    return X_new \n",
    "\n",
    "\n",
    "# t-distributed Stochastic Neighbor Embedding (t-SNE):\n",
    "# This technique is used for visualizing high-dimensional data in a low-dimensional space. \n",
    "# This is done using the TSNE function from scikit-learn library.\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def tsne(X, n_components=2, perplexity=30):\n",
    "    tsne = TSNE(n_components=n_components, perplexity=perplexity)\n",
    "    X_new = tsne.fit_transform(X)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "#Feature Selection \n",
    "from sklearn.feature_selection import SelectKBest, chi2, f_classif\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Univariate Feature Selection\n",
    "selector = SelectKBest(score_func=f_classif, k=2)\n",
    "X_new = selector.fit_transform(X, y)\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "estimator = RandomForestClassifier()\n",
    "selector = RFE(estimator, n_features_to_select=2, step=1)\n",
    "selector = selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "# PCA is a dimensionality reduction technique that projects the data onto a lower-dimensional space while preserving\n",
    "# as much variance as possible. This is done using the PCA function from scikit-learn library.\n",
    "pca = PCA(n_components=2)\n",
    "X_new = pca.fit_transform(X)\n",
    "\n",
    "# backward elimination (you can use any model of choice) \n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_scaled, y)\n",
    "model = SelectFromModel(lasso, prefit=True) \n",
    "X_new = model.transform(X_scaled) \n",
    "    # selected_features = X.columns[(X_new.get_support())] #to view the selected variables\n",
    "    # np.sum(X_new.estimator_.coef_ == 0) #to see how many variables were shrank to zero and unselected.  \n",
    "\n",
    "# Ridge Regression\n",
    "ridge = Ridge(alpha=1.0)\n",
    "ridge.fit(X_scaled, y)\n",
    "model = SelectFromModel(ridge, prefit=True)\n",
    "X_new = model.transform(X_scaled)\n",
    "\n",
    "# Elastic Net\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic.fit(X_scaled, y)\n",
    "model = SelectFromModel(elastic, prefit=True)\n",
    "X_new = model.transform(X_scaled)\n",
    "\n",
    "# Tree-based Feature Selection\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "model = SelectFromModel(rf, prefit=True)\n",
    "X_new = model.transform(X)\n",
    "\n",
    "# Mutual Information Feature Selection\n",
    "X_new = SelectKBest(score_func=mutual_info_classif, k=2).fit_transform(X, y)\n",
    "\n",
    "# Sequential Feature Selection\n",
    "estimator = RandomForestClassifier()\n",
    "selector = SequentialFeatureSelector(estimator, n_features_to_select=2)\n",
    "selector = selector.fit(X, y)\n",
    "X_new = selector.transform(X)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b33067",
   "metadata": {},
   "source": [
    ">> ML evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803a0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix \n",
    "from sklearn.metrics import confusion_matrix \n",
    "\n",
    "classes = digits.target_names #or df['target].unique()\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))\n",
    "# A confusion matrix is a table that is often used to evaluate the performance of a machine learning algorithm. \n",
    "# It shows the number of true positives, false positives, true negatives, and false negatives for a given \n",
    "# classification task.\n",
    "\n",
    "# A confusion matrix has two axes: one for the predicted values and one for the actual values. Each axis has two \n",
    "#     categories: positive and negative. Therefore, a confusion matrix for a binary classification task will have \n",
    "#     four cells:\n",
    "# True Positive (TP): the actual value was positive, and the predicted value was also positive.\n",
    "# False Positive (FP): the actual value was negative, but the predicted value was positive.\n",
    "# True Negative (TN): the actual value was negative, and the predicted value was also negative.\n",
    "# False Negative (FN): the actual value was positive, but the predicted value was negative.\n",
    "                Predicted Positive    Predicted Negative\n",
    "Actual Positive         TP                   FN\n",
    "Actual Negative         FP                   TN\n",
    "\n",
    "#Recall\n",
    "# the proportion of true positives among the total number of actual positives. It is calculated as TP / (TP + FN).\n",
    "\n",
    "#Accuracy\n",
    "# the proportion of true results (both true positives and true negatives) among the total number of cases examined. \n",
    "# It is calculated as (TP + TN) / (TP + FP + TN + FN).\n",
    "\n",
    "#Precision\n",
    "# The proportion of true positives among the total number of positive predictions. It is calculated as TP / (TP + FP).\n",
    "\n",
    "#F1-score\n",
    "# the harmonic mean of precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3c654",
   "metadata": {},
   "source": [
    ">> Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b783a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd ## For DataFrame operation\n",
    "import numpy as np ## Numerical python for matrix operations\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder ## Preprocessing function\n",
    "import pandas_profiling ## For easy profiling of pandas DataFrame \n",
    "import missingno as msno ## Missing value co-occurance analysis\n",
    "\n",
    "####### Data Exploration ############\n",
    "\n",
    "def print_dim(df):\n",
    "    '''\n",
    "    Function to print the dimensions of a given python dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Data size\n",
    "    '''\n",
    "    print(\"Data size: Rows-{0} Columns-{1}\".format(df.shape[0],df.shape[1]))\n",
    "\n",
    "\n",
    "def print_dataunique(df):\n",
    "    '''\n",
    "    Function to print unique information for each column in a python dataframe\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Column name\n",
    "        - Data type of that column\n",
    "        - Number of unique values in that column\n",
    "        - 5 unique values from that column\n",
    "    '''\n",
    "    counter = 0\n",
    "    for i in df.columns:\n",
    "        x = df.loc[:,i].unique()\n",
    "        print(counter,i,type(df.loc[0,i]), len(x), x[0:5])\n",
    "        counter +=1\n",
    "        \n",
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = pandas_profiling.ProfileReport(df)\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "def view_datatypes_in_perspective(df):\n",
    "    '''\n",
    "    Function to group dataframe columns into three common dtypes and visualize the columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - three unique datatypes (float, object, others(for the rest))\n",
    "    '''\n",
    "    float = 0\n",
    "    float_col = []\n",
    "    object = 0\n",
    "    object_col = []\n",
    "    others = 0\n",
    "    others_col = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype ==  \"float\":\n",
    "            float += 1\n",
    "            float_col.append(col) \n",
    "        elif df[col].dtypes == \"object\":\n",
    "            object += 1\n",
    "            object_col.append(col)\n",
    "        else:\n",
    "            others +=1\n",
    "            others_col.append(col)\n",
    "            others_col.append(smart_home[col].dtype)        \n",
    "    print (f\" float = {float} \\t{float_col}, \\n \\nobject = {object} \\t{object_col}, \\n\\nothers = {others} \\t{others_col} \")\n",
    "\n",
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True:\n",
    "            print(f\"there is {df[col].isnull().sum()} NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)\n",
    "\n",
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "####### Basic helper function ############\n",
    "\n",
    "def join_df(left, right, left_on, right_on=None, method='left'):\n",
    "    '''\n",
    "    Function to outer joins of pandas dataframe\n",
    "    Required Input - \n",
    "        - left = Pandas DataFrame 1\n",
    "        - right = Pandas DataFrame 2\n",
    "        - left_on = Fields in DataFrame 1 to merge on\n",
    "        - right_on = Fields in DataFrame 2 to merge with left_on fields of Dataframe 1\n",
    "        - method = Type of join\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    if right_on is None:\n",
    "        right_on = left_on\n",
    "    return left.merge(right, \n",
    "                      how=method, \n",
    "                      left_on=left_on, \n",
    "                      right_on=right_on, \n",
    "                      suffixes=(\"\",\"_y\"))\n",
    "    \n",
    "####### Pre-processing ############    \n",
    "\n",
    "def drop_allsame(df):\n",
    "    '''\n",
    "    Function to remove any columns which have same value all across\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with dropped no variation columns\n",
    "    '''\n",
    "    to_drop = list()\n",
    "    for i in df.columns:\n",
    "        if len(df.loc[:,i].unique()) == 1:\n",
    "            to_drop.append(i)\n",
    "    return df.drop(to_drop,axis =1)\n",
    "\n",
    "------------------------------------------------------------------------------------------------------\n",
    "#Handling Missing Values\n",
    "----------------------------------------------------\n",
    "#fill Nan Values in the cloudCover column\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(5)\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1)  \n",
    "\n",
    "\n",
    "def treat_missing_categorical(df, columns, how='mode', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in categorical columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mode', any string or numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mode':\n",
    "        for col in columns:\n",
    "            print(\"Filling missing values with mode for column - {0}\".format(col))\n",
    "            df[col] = df[col].fillna(df[col].mode()[0])\n",
    "            \n",
    "    elif isinstance(how, str):\n",
    "        for col in columns:\n",
    "            print(\"Filling missing values with '{0}' for column - {1}\".format(how, col))\n",
    "            df[col] = df[col].fillna(how)\n",
    "            \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "            \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(4)\n",
    "\n",
    "\n",
    "#SimpleImputer: This function replaces missing values with a specified strategy.\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "def impute_missing_values(X, strategy='mean'): #strategy = \"median\", 'most_frequent', 'constant'. (strategy=\"constant\", fill_value=-1)\n",
    "    imputer = SimpleImputer(strategy=strategy)\n",
    "    X_imputed = imputer.fit_transform(X)\n",
    "    X_imputed = pd.DataFrame(X_imputed, \n",
    "                            columns=X.columns, index=X.index )\n",
    "    return X_imputed\n",
    "\n",
    "#EvalML Time Series Imputer\n",
    "from evalml.pipelines.components import TimeSeriesImputer\n",
    "\n",
    "ts_imputer = TimeSeriesImputer(\n",
    "    categorical_impute_strategy=\"forwards_fill\",\n",
    "    numeric_impute_strategy=\"backwards_fill\",\n",
    "    target_impute_strategy=\"interpolate\",\n",
    ")\n",
    "X_train, y_train = ts_imputer.fit_transform(X_train, y_train)\n",
    "\n",
    "#MissingIndicator: This function creates a binary indicator for each feature indicating whether the value is missing or not.\n",
    "from sklearn.impute import MissingIndicator\n",
    "\n",
    "def create_missing_indicator(X):\n",
    "    indicator = MissingIndicator()\n",
    "    X_missing_indicator = indicator.fit_transform(X)\n",
    "    return X_missing_indicator\n",
    "\n",
    "#KNNImputer: The missing values are estimated as the average value from the closest K neighbours\n",
    "    # multivariate imputation\n",
    "from sklearn.impute import KNNImputer\n",
    "def knn_impute(X, k):\n",
    "    imputer = KNNImputer(n_neighbors=k, # the number of neighbours K\n",
    "                        weights='distance', # the weighting factor\n",
    "                        metric='nan_euclidean', # the metric to find the neighbours\n",
    "                        add_indicator=False, # whether to add a missing indicator\n",
    "                        )\n",
    "    imputed_X = imputer.fit_transform(X)\n",
    "    return imputed_X\n",
    "\n",
    "#IterativeImputer: This function estimates missing values using a predictive model.\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "def impute_missing_values_iteratively(X): #or (X, Columns)\n",
    "    imputer = IterativeImputer(\n",
    "        # estimator = RandomForestRegressor() \n",
    "        estimator=BayesianRidge(), # the estimator to predict the NA\n",
    "        initial_strategy='mean', # how will NA be imputed in step 1\n",
    "        max_iter=10, # number of cycles\n",
    "        imputation_order='ascending', # the order in which to impute the variables\n",
    "        n_nearest_features=None, # whether to limit the number of predictors\n",
    "        skip_complete=True, # whether to ignore variables without NA\n",
    "        random_state=0,)\n",
    "        \n",
    "    # select only the columns with missing values to be imputed\n",
    "    # X_cols = X[columns]\n",
    "    X_imputed = imputer.fit_transform(X) #or X_cols\n",
    "    return X_imputed\n",
    "\n",
    "#other predictive models include\n",
    "imputer = IterativeImputer(estimator=BayesianRidge()) #from sklearn.linear_model import BayesianRidge\n",
    "imputer = IterativeImputer(estimator=LinearRegression()) #from sklearn.linear_model import LinearRegression\n",
    "imputer = IterativeImputer(estimator=DecisionTreeRegressor()) #from sklearn.tree import DecisionTreeRegressor\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor()) #from sklearn.ensemble import RandomForestRegressor\n",
    "imputer = IterativeImputer(estimator=KNeighborsRegressor()) #from sklearn.neighbors import KNeighborsRegressor\n",
    "imputer = IterativeImputer(estimator=MLPRegressor()) #from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# IterativeImputer in SKlearn is a class that can estimate missing values in a dataset by modeling each feature with \n",
    "# missing values as a function of the other features. It does this by taking a predictive model and using it to \n",
    "# fill in the missing values iteratively. \n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "def min_max_scaler(df,columns):\n",
    "    '''\n",
    "    Function to do Min-Max scaling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = MinMaxScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = columns\n",
    "    return data, scaler\n",
    "\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df\n",
    "\n",
    "def z_scaler(df,columns):\n",
    "    '''\n",
    "    Function to standardize features by removing the mean and scaling to unit variance\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be min-max scaled\n",
    "    Expected Output -\n",
    "        - df = Python DataFrame with Min-Max scaled attributes\n",
    "        - scaler = Function which contains the scaling rules\n",
    "    '''\n",
    "    scaler = StandardScaler()\n",
    "    data = pd.DataFrame(scaler.fit_transform(df.loc[:,columns]))\n",
    "    data.index = df.index\n",
    "    data.columns = columns\n",
    "    return data, scaler\n",
    "\n",
    "mapping_dict = {        #an example\n",
    "    'First':0,\n",
    "    'Second': 1,\n",
    "    'Third': 2 \n",
    "}\n",
    "def map_encoding(data, feature_name, mapping_dict):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using mapping method.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "        mapping_dict (dict): A dictionary containing the mapping of category values to integers.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Replace the category values with their corresponding integers\n",
    "    encoded_data[feature_name] = encoded_data[feature_name].map(mapping_dict)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder\n",
    "\n",
    "def ordinal_encoding_sklearn(data, feature_name, categories):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using ordinal encoding method with scikit-learn's OrdinalEncoder class.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "        categories (list): The list of categories in the order of their numerical encoding.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Perform ordinal encoding using the OrdinalEncoder class\n",
    "    ordinal_encoder = OrdinalEncoder(categories=[categories])\n",
    "    encoded_data[feature_name] = ordinal_encoder.fit_transform(encoded_data[[feature_name]])\n",
    "    encoded_data[feature_name] = pd.DataFrame(encoded_data, columns=encoded_data.columns, index=encoded_data.index)\n",
    "    \n",
    "    return encoded_data.head\n",
    "\n",
    "\n",
    "def one_hot_encoding_sklearn(data, feature_name):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using one-hot encoding method with scikit-learn's OneHotEncoder class.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Perform one-hot encoding using the OneHotEncoder class\n",
    "    one_hot_encoder = OneHotEncoder()\n",
    "    encoded_data = pd.DataFrame(one_hot_encoder.fit_transform(encoded_data[[feature_name]]).toarray())\n",
    "    feature_names_out = [f\"{feature_name}_{category}\" for category in one_hot_encoder.categories_[0]]\n",
    "    encoded_data.columns = feature_names_out\n",
    "    encoded_data.index = data.index\n",
    "    encoded_data = pd.concat([data.drop(feature_name, axis=1), encoded_data], axis=1)\n",
    "    encoded_data[feature_name] = pd.DataFrame(encoded_data, columns=encoded_data.columns, index=encoded_data.index)\n",
    "    \n",
    "    return encoded_data.head(3) \n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def label_encoding_sklearn(data, feature_name):\n",
    "    \"\"\"\n",
    "    Encodes a categorical feature using label encoding method with scikit-learn's LabelEncoder class.\n",
    "\n",
    "    Args:\n",
    "        data (pandas.DataFrame): The DataFrame containing the categorical feature to encode.\n",
    "        feature_name (str): The name of the categorical feature to encode.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The DataFrame with the encoded categorical feature.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of the original DataFrame\n",
    "    encoded_data = data.copy()\n",
    "\n",
    "    # Perform label encoding using the LabelEncoder class\n",
    "    label_encoder = LabelEncoder()\n",
    "    encoded_data[feature_name] = label_encoder.fit_transform(encoded_data[feature_name])\n",
    "    encoded_data[feature_name] = pd.DataFrame(encoded_data, columns=encoded_data.columns, index=encoded_data.index)\n",
    "    \n",
    "    return encoded_data\n",
    "\n",
    "    \n",
    "def label_encoder(df,columns):\n",
    "    '''\n",
    "    Function to label encode\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be label encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with lable encoded columns\n",
    "        - le_dict = Dictionary of all the column and their label encoders\n",
    "    '''\n",
    "    le_dict = {}\n",
    "    for c in columns:\n",
    "        print(\"Label encoding column - {0}\".format(c))\n",
    "        lbl = LabelEncoder()\n",
    "        lbl.fit(list(df[c].values.astype('str')))\n",
    "        df[c] = lbl.transform(list(df[c].values.astype('str')))\n",
    "        le_dict[c] = lbl\n",
    "    return df, le_dict\n",
    "\n",
    "def one_hot_encoder(df, columns):\n",
    "    '''\n",
    "    Function to do one-hot encoded\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns which needs to be one-hot encoded\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with one-hot encoded columns\n",
    "    '''\n",
    "    for each in columns:\n",
    "        print(\"One-Hot encoding column - {0}\".format(each))\n",
    "        dummies = pd.get_dummies(df[each], prefix=each, drop_first=False)\n",
    "        df = pd.concat([df, dummies], axis=1)\n",
    "    return df.drop(columns,axis = 1)\n",
    "\n",
    "####### Feature Engineering ############ \n",
    "def create_date_features(df,column, date_format = None, more_features = False, time_features = False): \n",
    "    '''\n",
    "    Function to extract date features\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - date_format = Date parsing format\n",
    "        - columns = Columns name containing date field\n",
    "        - more_features = To get more feature extracted\n",
    "        - time_features = To extract hour from datetime field\n",
    "    Expected Output -\n",
    "        - df = Pandas DataFrame with additional extracted date features\n",
    "    '''\n",
    "    if date_format is None:\n",
    "        df.loc[:,column] = pd.to_datetime(df.loc[:,column])\n",
    "    else:\n",
    "        df.loc[:,column] = pd.to_datetime(df.loc[:,column],format = date_format)\n",
    "    df.loc[:,column+'_Year'] = df.loc[:,column].dt.year\n",
    "    df.loc[:,column+'_Month'] = df.loc[:,column].dt.month.astype('uint8')\n",
    "    df.loc[:,column+'_Week'] = df.loc[:,column].dt.week.astype('uint8')\n",
    "    df.loc[:,column+'_Day'] = df.loc[:,column].dt.day.astype('uint8')\n",
    "    \n",
    "    if more_features:\n",
    "        df.loc[:,column+'_Quarter'] = df.loc[:,column].dt.quarter.astype('uint8')\n",
    "        df.loc[:,column+'_DayOfWeek'] = df.loc[:,column].dt.dayofweek.astype('uint8')\n",
    "        df.loc[:,column+'_DayOfYear'] = df.loc[:,column].dt.dayofyear\n",
    "        \n",
    "    if time_features:\n",
    "        df.loc[:,column+'_Hour'] = df.loc[:,column].dt.hour.astype('uint8')\n",
    "    return df\n",
    "\n",
    "def target_encoder(train_df, col_name, target_name, test_df = None, how='mean'):\n",
    "    '''\n",
    "    Function to do target encoding\n",
    "    Required Input - \n",
    "        - train_df = Training Pandas Dataframe\n",
    "        - test_df = Testing Pandas Dataframe\n",
    "        - col_name = Name of the columns of the source variable\n",
    "        - target_name = Name of the columns of target variable\n",
    "        - how = 'mean' default but can also be 'count'\n",
    "\tExpected Output - \n",
    "\t\t- train_df = Training dataframe with added encoded features\n",
    "\t\t- test_df = Testing dataframe with added encoded features\n",
    "    '''\n",
    "    aggregate_data = train_df.groupby(col_name)[target_name] \\\n",
    "                    .agg([how]) \\\n",
    "                    .reset_index() \\\n",
    "                    .rename(columns={how: col_name+'_'+target_name+'_'+how})\n",
    "    if test_df is None:\n",
    "        return join_df(train_df,aggregate_data,left_on = col_name)\n",
    "    else:\n",
    "        return join_df(train_df,aggregate_data,left_on = col_name), join_df(test_df,aggregate_data,left_on = col_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af338aa",
   "metadata": {},
   "source": [
    "### Choosing the ML Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5707d98",
   "metadata": {},
   "source": [
    "| Model | Description and How It Works | Key Points | Example Code (Python) |\n",
    "|-------|------------------------------|------------|-----------------------|\n",
    "| **Linear Regression** | Predicts a continuous value by finding the best-fitting straight line through the data points, assuming a linear relationship between input and output. | Simple and interpretable, prone to underfitting, assumes linear relationship, sensitive to outliers. | `from sklearn.linear_model import LinearRegression` |\n",
    "| **Logistic Regression** | Estimates probabilities using a logistic function to model a binary outcome. Despite the name, it's used for classification. | Interpretable, outputs probabilities, best for binary classification, assumes linear decision boundary. | `from sklearn.linear_model import LogisticRegression` |\n",
    "| **Decision Trees** | Models decisions using a tree-like structure, splitting data on feature values to create pure subsets. | Easy to interpret, handles numerical and categorical data, prone to overfitting, can model non-linear relationships. | `from sklearn.tree import DecisionTreeClassifier` |\n",
    "| **Random Forest** | An ensemble of Decision Trees trained with the \"bagging\" method to improve accuracy and control overfitting. | Tackles overfitting of decision trees, works well with large datasets and high dimensions, maintains accuracy for missing data, handles both regression and classification tasks. | `from sklearn.ensemble import RandomForestClassifier` |\n",
    "| **SVM (Support Vector Machines)** | Finds the hyperplane that best separates different classes by constructing a hyperplane in a high-dimensional space. | Effective in high-dimensional spaces, best suited for datasets with a clear margin of separation, memory efficient, more effective when number of dimensions exceeds the number of samples. | `from sklearn.svm import SVC` |\n",
    "| **KNN (K-Nearest Neighbors)** | Classifies points based on the 'k' most similar instances using distance metrics to find the closest training examples. | Instance-based learning, minimal training, suitable for classification and regression, sensitive to localized data, depends on the number and proximity of neighbors. | `from sklearn.neighbors import KNeighborsClassifier` |\n",
    "| **Naive Bayes** | Applies Bayes' Theorem with the \"naive\" assumption of conditional independence between every pair of features to classify data. | Fast, suitable for large datasets, predominantly used in text classification, handles both continuous and discrete data, effective in multi-class prediction. | `from sklearn.naive_bayes import GaussianNB` |\n",
    "| **Ridge Regression** | Extends Linear Regression with a penalty term on the size of coefficients to address multicollinearity and prevent overfitting. | Reduces model complexity, particularly useful when dealing with multicollinearity, includes a bias-variance trade-off through regularization. | `from sklearn.linear_model import Ridge` |\n",
    "| **Lasso Regression** | Similar to Ridge, but uses L1 regularization to penalize the absolute size of coefficients, encouraging sparse solutions. | Facilitates feature selection by shrinking some coefficients to zero, useful for models with high dimensionality, offers sparse solutions. | `from sklearn.linear_model import Lasso` |\n",
    "| **XGBoost** | A decision-tree-based ensemble that uses a gradient boosting framework for supervised learning tasks, optimized for speed and performance. | Optimized for performance and speed, handles large datasets effectively, used for both classification and regression, can deal with missing data, widely used due to its efficiency and accuracy. | `import xgboost as xgb` |\n",
    "| **LightGBM** | A gradient boosting framework that uses tree-based learning algorithms, designed for distributed and efficient training. | Fast training speed, lower memory usage, supports large datasets, performs well with categorical data, capable of handling high-dimensional data. | `import lightgbm as lgb` |\n",
    "| **K-Means Clustering** | Partitions 'n' observations into 'k' clusters, where each observation belongs to the cluster with the nearest mean. | Widely used for unsupervised clustering, efficient with large datasets, sensitive to the selection of 'k', may converge to local minima. | `from sklearn.cluster import KMeans` |\n",
    "| **PCA (Principal Component Analysis)** | Reduces dimensionality by transforming to a new set of variables (principal components) that summarize the most variance. | Effective for dimensionality reduction, identifies most significant features, can improve model performance and visualization. | `from sklearn.decomposition import PCA` |\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "78800008",
   "metadata": {},
   "source": [
    "### Machine Learning Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b16720",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd ## For DataFrame operation\n",
    "import numpy as np ## Numerical python for matrix operations\n",
    "from sklearn.model_selection import KFold, train_test_split ## Creating cross validation sets\n",
    "from sklearn import metrics ## For loss functions\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Libraries for Regressiion algorithms\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb \n",
    "from sklearn.ensemble import ExtraTreesRegressor,RandomForestRegressor\n",
    "import lime \n",
    "import lime.lime_tabular\n",
    "\n",
    "\n",
    "model.get_params()  #to get the parameters of the models in order to improve it\n",
    "\n",
    "########### Cross Validation ###########\n",
    "### 1) Train test split\n",
    "def holdout_cv(X,y,size = 0.3, seed = 1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = size, random_state = seed)\n",
    "    X_train = X_train.reset_index(drop='index')\n",
    "    X_test = X_test.reset_index(drop='index')\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "### 2) Cross-Validation (K-Fold)\n",
    "def kfold_cv(X,n_folds = 5, seed = 1):\n",
    "    cv = KFold(n_splits = n_folds, random_state = seed, shuffle = True)\n",
    "    return cv.split(X)\n",
    "\n",
    "########### Model Explanation ###########\n",
    "## Variable Importance plot\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "def standardize_data(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Standardizes the training and testing data using the mean and standard deviation\n",
    "    learned from the training set.\n",
    "    \n",
    "    Args:\n",
    "    - X_train: numpy array or pandas dataframe, training data\n",
    "    - X_test: numpy array or pandas dataframe, testing data\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled: numpy array or pandas dataframe, standardized training data\n",
    "    - X_test_scaled: numpy array or pandas dataframe, standardized testing data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    # Set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the training set\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    # Transform the training and testing sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "def mean_normalize(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on both the training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy.ndarray\n",
    "        The training set features as a 2D array.\n",
    "\n",
    "    X_test: numpy.ndarray\n",
    "        The testing set features as a 2D array.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_norm: numpy.ndarray\n",
    "        The mean-normalized training set features as a 2D array.\n",
    "\n",
    "    X_test_norm: numpy.ndarray\n",
    "        The mean-normalized testing set features as a 2D array.\n",
    "    \"\"\"\n",
    "    scaler_mean = StandardScaler(with_mean=True, with_std=False) # set up the scaler\n",
    "    scaler_minmax = RobustScaler(with_centering = False, with_scaling = True,   #use this when working with outliers\n",
    "                                 quantile_range = (0,100))\n",
    "    \n",
    "    scaler_mean.fit(X_train) # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler_minmax.fit(X_train) #fit the scaler to the train set, it will learn the parameters\n",
    "    \n",
    "    X_train_norm = scaler_minmax.transform(scaler_mean.transform(X_train)) # transform train set\n",
    "    X_test_norm = scaler_minmax.transform(scaler_mean.transform(X_test)) # transform test set\n",
    "    return X_train_norm, X_test_norm\n",
    "\n",
    "\n",
    "def scale_min_max(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales the features in X_train and X_test to the range [0, 1] using MinMaxScaler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy array\n",
    "        Training data features\n",
    "        \n",
    "    X_test: numpy array\n",
    "        Test data features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_scaled: numpy array\n",
    "        Scaled training data features\n",
    "        \n",
    "    X_test_scaled: numpy array\n",
    "        Scaled test data features\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import MinMaxScaler \n",
    "    # set up the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "\n",
    "########### Functions for explaination using Lime ###########\n",
    "\n",
    "## Make a prediction function\n",
    "def make_prediction_function(model, type = None):\n",
    "    if type == 'xgb':\n",
    "        predict_fn = lambda x: model.predict(xgb.DMatrix(x)).astype(float)\n",
    "    else:\n",
    "        predict_fn = lambda x: model.predict(x).astype(float)\n",
    "    return predict_fn\n",
    "\n",
    "## Make a lime explainer\n",
    "def make_lime_explainer(df, c_names = [], verbose_val = True):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(df.values,\n",
    "                                                       class_names=c_names,\n",
    "                                                       feature_names = list(df.columns),\n",
    "                                                       kernel_width=3, \n",
    "                                                       verbose=verbose_val,\n",
    "                                                       mode='regression'\n",
    "                                                    )\n",
    "    return explainer\n",
    "\n",
    "## Lime explain function\n",
    "def lime_explain(explainer,predict_fn, df, index = 0, num_features = None,\n",
    "                 show_in_notebook = True, filename = None):\n",
    "    if num_features is not None:\n",
    "        exp = explainer.explain_instance(df.values[index], predict_fn, num_features=num_features)\n",
    "    else:\n",
    "        exp = explainer.explain_instance(df.values[index], predict_fn, num_features=df.shape[1])\n",
    "    \n",
    "    if show_in_notebook:\n",
    "        exp.show_in_notebook(show_all=False)\n",
    "    \n",
    "    if filename is not None:\n",
    "        exp.save_to_file(filename)\n",
    "        \n",
    "########### Algorithms For Regression ###########\n",
    "\n",
    "### Running Xgboost\n",
    "def runXGB(train_X, train_y, test_X, test_y=None, test_X2=None, seed_val=0, \n",
    "           rounds=500, dep=8, eta=0.05,sub_sample=0.7,col_sample=0.7,\n",
    "           min_child_weight_val=1, silent_val = 1):\n",
    "    params = {\n",
    "        \"objective\": \"reg:squarederror\",\n",
    "        \"eval_metric\": \"rmse\",\n",
    "        \"eta\": eta,\n",
    "        \"subsample\": sub_sample,\n",
    "        \"min_child_weight\": min_child_weight_val,\n",
    "        \"colsample_bytree\": col_sample,\n",
    "        \"max_depth\": dep,\n",
    "        \"seed\": seed_val,\n",
    "        \"verbosity\": 0  \n",
    "    }\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "        model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "    \n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "    \n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(xgb.DMatrix(test_X2), ntree_limit=model.best_iteration)\n",
    "    \n",
    "    loss = 0\n",
    "    r2 = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.mean_squared_error(test_y, pred_test_y)\n",
    "        r2 = r2_score(test_y, pred_test_y) \n",
    "        print(f'r2_score is: {r2}')\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "        \n",
    "### Running LightGBM\n",
    "def runLGB(train_X, train_y, test_X, test_y=None, test_X2=None, feature_names=None, \n",
    "           seed_val=0, rounds=500, dep=8, eta=0.05,sub_sample=0.7,\n",
    "           col_sample=0.7,silent_val = 1,min_data_in_leaf_val = 20, bagging_freq = 5):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"regression\"\n",
    "    params['metric'] = 'rmse'\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = min_data_in_leaf_val\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = sub_sample\n",
    "    params[\"feature_fraction\"] = col_sample\n",
    "    params[\"bagging_freq\"] = bagging_freq\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = silent_val\n",
    "    num_rounds = rounds\n",
    "    \n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "    \n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(params, lgtrain, num_rounds, valid_sets=[lgtest], early_stopping_rounds=100, verbose_eval=20)\n",
    "    else:\n",
    "        lgtest = lgb.Dataset(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "        \n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "    \n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "    \n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.mean_squared_error(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "        \n",
    "### Running Extra Trees  \n",
    "def runET(train_X, train_y, test_X, test_y=None, test_X2=None, rounds=100, depth=20,\n",
    "          leaf=10, feat=0.2, min_data_split_val=2,seed_val=0,job = -1):\n",
    "\tmodel = ExtraTreesRegressor(\n",
    "                                n_estimators = rounds,\n",
    "                                max_depth = depth,\n",
    "                                min_samples_split = min_data_split_val,\n",
    "                                min_samples_leaf = leaf,\n",
    "                                max_features =  feat,\n",
    "                                n_jobs = job,\n",
    "                                random_state = seed_val)\n",
    "\tmodel.fit(train_X, train_y)\n",
    "\ttrain_preds = model.predict(train_X)\n",
    "\ttest_preds = model.predict(test_X)\n",
    "\t\n",
    "\ttest_preds2 = 0\n",
    "\tif test_X2 is not None:\n",
    "\t\ttest_preds2 = model.predict(test_X2)\n",
    "\t\n",
    "\ttest_loss = 0\n",
    "\tif test_y is not None:\n",
    "\t\ttrain_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "\t\ttest_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "\t\tprint(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "\t\tprint(\"Train and Test loss : \", train_loss, test_loss)\n",
    "\treturn test_preds, test_loss, test_preds2, model\n",
    " \n",
    "### Running Random Forest\n",
    "def runRF(train_X, train_y, test_X, test_y=None, test_X2=None, rounds=100, depth=20, leaf=10,\n",
    "          feat=0.2,min_data_split_val=2,seed_val=0,job = -1):\n",
    "    model = RandomForestRegressor(\n",
    "                                n_estimators = rounds,\n",
    "                                max_depth = depth,\n",
    "                                min_samples_split = min_data_split_val,\n",
    "                                min_samples_leaf = leaf,\n",
    "                                max_features =  feat,\n",
    "                                n_jobs = job,\n",
    "                                random_state = seed_val)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "    \n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running Linear regression\n",
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None):\n",
    "    model = LinearRegression()\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running Decision Tree\n",
    "def runDT(train_X, train_y, test_X, test_y=None, test_X2=None, criterion='mse', \n",
    "          depth=None, min_split=2, min_leaf=1):\n",
    "    model = DecisionTreeRegressor(\n",
    "                                criterion = criterion, \n",
    "                                max_depth = depth, \n",
    "                                min_samples_split = min_split, \n",
    "                                min_samples_leaf=min_leaf)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "    \n",
    "### Running K-Nearest Neighbour\n",
    "def runKNN(train_X, train_y, test_X, test_y=None, test_X2=None, \n",
    "           neighbors=5, job = -1):\n",
    "    model = KNeighborsRegressor(\n",
    "                                n_neighbors=neighbors, \n",
    "                                n_jobs=job)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "### Running SVM\n",
    "def runSVC(train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, \n",
    "           eps=0.1, kernel_choice = 'rbf'):\n",
    "    model = SVR(\n",
    "                C=C, \n",
    "                kernel=kernel_choice,  \n",
    "                epsilon=eps)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict(train_X)\n",
    "    test_preds = model.predict(test_X)\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict(test_X2)\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    train_loss = metrics.mean_squared_error(train_y, train_preds)\n",
    "    test_loss = metrics.mean_squared_error(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0947809",
   "metadata": {},
   "source": [
    "### Machine Learning Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e42c008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing required libraries\n",
    "import pandas as pd  ## For DataFrame operation\n",
    "import numpy as np  ## Numerical python for matrix operations\n",
    "from sklearn.model_selection import (\n",
    "    KFold,\n",
    "    train_test_split,\n",
    ")  ## Creating cross validation sets\n",
    "from sklearn import metrics  ## For loss functions\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "## For evaluation\n",
    "from sklearn.metrics import (\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    roc_auc_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    average_precision_score,\n",
    ")\n",
    "from inspect import signature\n",
    "\n",
    "## Libraries for Classification algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import lime\n",
    "import lime.lime_tabular\n",
    "\n",
    "model.get_params()  #to get the parameters of the models in order to improve it\n",
    "\n",
    "\n",
    "\n",
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42): #split data into train, test, and validation\n",
    "    \"\"\"\n",
    "    This function splits the data into train and test sets, and further splits the train set into training and validation sets.\n",
    "    \n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing the input data.\n",
    "    target_col : str\n",
    "        The name of the target column in the dataframe.\n",
    "    test_size : float, optional (default=0.2)\n",
    "        The proportion of the data to be used for testing.\n",
    "    val_size : float, optional (default=0.2)\n",
    "        The proportion of the training data to be used for validation.\n",
    "    random_state : int, optional (default=42)\n",
    "        The seed used by the random number generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xtrain : pandas DataFrame\n",
    "        The training input data.\n",
    "    ytrain : pandas Series\n",
    "        The training target data.\n",
    "    xvalid : pandas DataFrame\n",
    "        The validation input data.\n",
    "    yvalid : pandas Series\n",
    "        The validation target data.\n",
    "    xtest : pandas DataFrame\n",
    "        The test input data.\n",
    "    ytest : pandas Series\n",
    "        The test target data.\n",
    "    \"\"\" \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "########### Cross Validation ###########\n",
    "### 1) Train test split\n",
    "def holdout_cv(X, y, size=0.3, seed=1):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=size, random_state=seed\n",
    "    )\n",
    "    X_train = X_train.reset_index(drop=\"index\")\n",
    "    X_test = X_test.reset_index(drop=\"index\")\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "\n",
    "### 2) Cross-Validation (K-Fold)\n",
    "def kfold_cv(X, n_folds=5, seed=1):\n",
    "    cv = KFold(n_splits=n_folds, random_state=seed, shuffle=True)\n",
    "    return cv.split(X)\n",
    "\n",
    "\n",
    "########### Model Explanation ###########\n",
    "## Plotting AUC ROC curve\n",
    "def plot_roc(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Model (AUC = %0.2f)\" % (roc_auc_score(y_actual, y_pred)),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        linestyle=\"--\",\n",
    "        lw=2,\n",
    "        color=\"r\",\n",
    "        label=\"Luck (AUC = 0.5)\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_precisionrecall(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    average_precision = average_precision_score(y_actual, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_actual, y_pred)\n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = (\n",
    "        {\"step\": \"post\"} if \"step\" in signature(plt.fill_between).parameters else {}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.step(recall, precision, color=\"b\", alpha=0.2, where=\"post\")\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color=\"b\", **step_kwargs)\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\"Precision-Recall curve: AP={0:0.2f}\".format(average_precision))\n",
    "\n",
    "\n",
    "## Plotting confusion matrix\n",
    "def plot_confusion_matrix(\n",
    "    y_true,\n",
    "    y_pred,\n",
    "    classes,\n",
    "    normalize=False,\n",
    "    title=\"Confusion matrix\",\n",
    "    cmap=plt.cm.Blues,\n",
    "):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    cm = metrics.confusion_matrix(y_true, y_pred)\n",
    "    if normalize:\n",
    "        cm = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print(\"Confusion matrix, without normalization\")\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation=\"nearest\", cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = \".2f\" if normalize else \"d\"\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(\n",
    "            j,\n",
    "            i,\n",
    "            format(cm[i, j], fmt),\n",
    "            horizontalalignment=\"center\",\n",
    "            color=\"white\" if cm[i, j] > thresh else \"black\",\n",
    "        )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "\n",
    "\n",
    "## Variable Importance plot\n",
    "def feature_importance(model, X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + 0.5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align=\"center\")\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel(\"Relative Importance\")\n",
    "    plt.title(\"Variable Importance\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "## Functions for explaination using Lime\n",
    "def make_prediction_function(model):\n",
    "    predict_fn = lambda x: model.predict_proba(x).astype(float)\n",
    "    return predict_fn\n",
    "\n",
    "\n",
    "def make_lime_explainer(df, c_names=[], k_width=3, verbose_val=True):\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        df.values,\n",
    "        class_names=c_names,\n",
    "        feature_names=list(df.columns),\n",
    "        kernel_width=3,\n",
    "        verbose=verbose_val,\n",
    "    )\n",
    "    return explainer\n",
    "\n",
    "\n",
    "def lime_explain(\n",
    "    explainer,\n",
    "    predict_fn,\n",
    "    df,\n",
    "    index=0,\n",
    "    num_features=None,\n",
    "    show_in_notebook=True,\n",
    "    filename=None,\n",
    "):\n",
    "    if num_features is not None:\n",
    "        exp = explainer.explain_instance(\n",
    "            df.values[index], predict_fn, num_features=num_features\n",
    "        )\n",
    "    else:\n",
    "        exp = explainer.explain_instance(\n",
    "            df.values[index], predict_fn, num_features=df.shape[1]\n",
    "        )\n",
    "\n",
    "    if show_in_notebook:\n",
    "        exp.show_in_notebook(show_all=False)\n",
    "\n",
    "    if filename is not None:\n",
    "        exp.save_to_file(filename)\n",
    "\n",
    "\n",
    "########### Algorithms For Binary classification ###########\n",
    "\n",
    "### Running Xgboost\n",
    "def runXGB(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    min_child_weight_val=1,\n",
    "    silent_val=1,\n",
    "):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary:logistic\"\n",
    "    params[\"eval_metric\"] = \"auc\"\n",
    "    params[\"eta\"] = eta\n",
    "    params[\"subsample\"] = sub_sample\n",
    "    params[\"min_child_weight\"] = min_child_weight_val\n",
    "    params[\"colsample_bytree\"] = col_sample\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"silent\"] = silent_val\n",
    "    params[\"seed\"] = seed_val\n",
    "    # params[\"max_delta_step\"] = 2\n",
    "    # params[\"gamma\"] = 0.5\n",
    "    num_rounds = rounds\n",
    "\n",
    "    plst = list(params.items())\n",
    "    xgtrain = xgb.DMatrix(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        xgtest = xgb.DMatrix(test_X, label=test_y)\n",
    "        watchlist = [(xgtrain, \"train\"), (xgtest, \"test\")]\n",
    "        model = xgb.train(\n",
    "            plst,\n",
    "            xgtrain,\n",
    "            num_rounds,\n",
    "            watchlist,\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "    else:\n",
    "        xgtest = xgb.DMatrix(test_X)\n",
    "        model = xgb.train(plst, xgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(xgtest, ntree_limit=model.best_iteration)\n",
    "\n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(\n",
    "            xgb.DMatrix(test_X2), ntree_limit=model.best_iteration\n",
    "        )\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = metrics.roc_auc_score(test_y, pred_test_y)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "### Running Xgboost classifier for model explaination\n",
    "def runXGBC(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    min_child_weight_val=1,\n",
    "    silent_val=1,\n",
    "):\n",
    "    model = xgb.XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        learning_rate=eta,\n",
    "        subsample=sub_sample,\n",
    "        min_child_weight=min_child_weight_val,\n",
    "        colsample_bytree=col_sample,\n",
    "        max_depth=dep,\n",
    "        silent=silent_val,\n",
    "        seed=seed_val,\n",
    "        n_estimators=rounds,\n",
    "    )\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running LightGBM\n",
    "def runLGB(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    feature_names=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    silent_val=1,\n",
    "    min_data_in_leaf_val=20,\n",
    "    bagging_freq=5,\n",
    "    n_thread=20,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    params = {}\n",
    "    params[\"objective\"] = \"binary\"\n",
    "    params[\"metric\"] = metric\n",
    "    params[\"max_depth\"] = dep\n",
    "    params[\"min_data_in_leaf\"] = min_data_in_leaf_val\n",
    "    params[\"learning_rate\"] = eta\n",
    "    params[\"bagging_fraction\"] = sub_sample\n",
    "    params[\"feature_fraction\"] = col_sample\n",
    "    params[\"bagging_freq\"] = bagging_freq\n",
    "    params[\"bagging_seed\"] = seed_val\n",
    "    params[\"verbosity\"] = silent_val\n",
    "    params[\"num_threads\"] = n_thread\n",
    "    num_rounds = rounds\n",
    "\n",
    "    lgtrain = lgb.Dataset(train_X, label=train_y)\n",
    "\n",
    "    if test_y is not None:\n",
    "        lgtest = lgb.Dataset(test_X, label=test_y)\n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            lgtrain,\n",
    "            num_rounds,\n",
    "            valid_sets=[lgtrain, lgtest],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose_eval=20,\n",
    "        )\n",
    "    else:\n",
    "        lgtest = lgb.Dataset(test_X)\n",
    "        model = lgb.train(params, lgtrain, num_rounds)\n",
    "\n",
    "    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n",
    "\n",
    "    pred_test_y2 = 0\n",
    "    if test_X2 is not None:\n",
    "        pred_test_y2 = model.predict(test_X2, num_iteration=model.best_iteration)\n",
    "\n",
    "    loss = 0\n",
    "    if test_y is not None:\n",
    "        loss = roc_auc_score(test_y, pred_test_y)\n",
    "        print(loss)\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "    else:\n",
    "        return pred_test_y, loss, pred_test_y2, model\n",
    "\n",
    "\n",
    "### Running LightGBM classifier for model explaination\n",
    "def runLGBC(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    seed_val=0,\n",
    "    rounds=500,\n",
    "    dep=8,\n",
    "    eta=0.05,\n",
    "    sub_sample=0.7,\n",
    "    col_sample=0.7,\n",
    "    silent_val=1,\n",
    "    min_data_in_leaf_val=20,\n",
    "    bagging_freq=5,\n",
    "    n_thread=20,\n",
    "    metric=\"auc\",\n",
    "):\n",
    "    model = lgb.LGBMClassifier(\n",
    "        max_depth=dep,\n",
    "        learning_rate=eta,\n",
    "        min_data_in_leaf=min_data_in_leaf_val,\n",
    "        bagging_fraction=sub_sample,\n",
    "        feature_fraction=col_sample,\n",
    "        bagging_freq=bagging_freq,\n",
    "        bagging_seed=seed_val,\n",
    "        verbosity=silent_val,\n",
    "        num_threads=n_thread,\n",
    "        n_estimators=rounds,\n",
    "        metric=metric,\n",
    "    )\n",
    "\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = roc_auc_score(train_y, train_preds)\n",
    "        test_loss = roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test AUC : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Extra Trees\n",
    "def runET(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = ExtraTreesClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Depth, leaf, feat : \", depth, leaf, feat)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Logistic Regression\n",
    "def runLR(train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, penalty=\"l1\"):\n",
    "    model = LogisticRegression(C=C, penalty=penalty, n_jobs=-1)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running Decision Tree\n",
    "def runDT(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    criterion=\"gini\",\n",
    "    depth=None,\n",
    "    min_split=2,\n",
    "    min_leaf=1,\n",
    "):\n",
    "    model = DecisionTreeClassifier(\n",
    "        criterion=criterion,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_split,\n",
    "        min_samples_leaf=min_leaf,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running K-Nearest Neighbour\n",
    "def runKNN(train_X, train_y, test_X, test_y=None, test_X2=None, neighbors=5, job=-1):\n",
    "    model = KNeighborsClassifier(n_neighbors=neighbors, n_jobs=job)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "\n",
    "### Running SVM\n",
    "def runSVC(\n",
    "    train_X, train_y, test_X, test_y=None, test_X2=None, C=1.0, kernel_choice=\"rbf\"\n",
    "):\n",
    "    model = SVC(C=C, kernel=kernel_choice, probability=True)\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "\n",
    "    train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "    test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "    print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ce368",
   "metadata": {},
   "source": [
    "### Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2be65",
   "metadata": {},
   "source": [
    "> Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed5f23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMeans Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "\n",
    "# Build the Model \n",
    "cluster = KMeans(n_clusters=9, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "df['cluster'] = cluster.fit(df)\n",
    "\n",
    "\n",
    "# Elbow Method to Optimize the Number of Clusters\n",
    "def optimize_kmeans(X, n_clusters_range=(2, 15)):\n",
    "    \"\"\"\n",
    "    Optimize K-Means clustering using the Elbow Method and Silhouette Score.\n",
    "\n",
    "    Args:\n",
    "    X: array-like, feature dataset.\n",
    "    n_clusters_range: tuple, range of n_clusters to try (min, max).\n",
    "\n",
    "    Returns:\n",
    "    dict: A dictionary containing the KMeans models, their silhouette scores, and WCSS.\n",
    "    \"\"\"\n",
    "\n",
    "    kmeans_results = {}\n",
    "    wcss = []\n",
    "\n",
    "    for n_clusters in range(n_clusters_range[0], n_clusters_range[1]+1):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, init='k-means++', max_iter=300, n_init=10, random_state=0).fit(X)\n",
    "        \n",
    "        # Silhouette Score\n",
    "        score = silhouette_score(X, kmeans.labels_)\n",
    "        kmeans_results[n_clusters] = {'model': kmeans, 'silhouette_score': score, 'wcss': kmeans.inertia_}\n",
    "        \n",
    "        # WCSS\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "    # Plot WCSS - Elbow Method\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(range(n_clusters_range[0], n_clusters_range[1]+1), wcss)\n",
    "    plt.title('Elbow Method')\n",
    "    plt.xlabel('Number of clusters')\n",
    "    plt.ylabel('WCSS')\n",
    "    plt.show()\n",
    "\n",
    "    return kmeans_results\n",
    "\n",
    "# Example usage\n",
    "# Replace 'df' with your actual DataFrame\n",
    "kmeans_optimization_results = optimize_kmeans(df.iloc[:,:-1], n_clusters_range=(2, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b44807",
   "metadata": {},
   "source": [
    "> Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a254da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Instantiate the Isolation Forest model\n",
    "iso_forest = IsolationForest(n_estimators=100, contamination=0.03, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "iso_forest.fit(X)\n",
    "\n",
    "# Predictions: -1 for anomalies and 1 for normal points\n",
    "y_pred = iso_forest.predict(X)\n",
    "anomaly_indices = np.where(y_pred == -1)[0]\n",
    "\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 7))\n",
    "\n",
    "# Plot the line, the samples, and the nearest vectors to the plane\n",
    "plt.title(\"Isolation Forest Anomaly Detection\")\n",
    "plt.scatter(X[:, 0], X[:, 1], color='k', s=30, label='Data Points')\n",
    "plt.scatter(X[anomaly_indices, 0], X[anomaly_indices, 1], color='r', s=50, \n",
    "            label='Anomalies')\n",
    "plt.legend()\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed8967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection using DBSCAN\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "# Apply DBSCAN\n",
    "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
    "dbscan.fit(X)\n",
    "\n",
    "# Labels for each point\n",
    "labels = dbscan.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80265c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection - Gaussian Anomaly Detection\n",
    "\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "class SklearnGaussianAnomalyDetection:\n",
    "    \"\"\"Anomaly detection using scikit-learn's GaussianMixture\"\"\"\n",
    "\n",
    "    def __init__(self, n_components=1):\n",
    "        \"\"\"Initialize the GaussianMixture model.\"\"\"\n",
    "        self.model = GaussianMixture(n_components=n_components)\n",
    "\n",
    "    def fit(self, X):\n",
    "        \"\"\"Fit the model to data.\"\"\"\n",
    "        self.model.fit(X)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict anomalies in the data.\"\"\"\n",
    "        # Assuming an anomaly if the probability is below a certain threshold\n",
    "        probabilities = self.model.score_samples(X)\n",
    "        threshold = np.percentile(probabilities, 5)  # setting threshold at 5th percentile\n",
    "        return probabilities < threshold\n",
    "\n",
    "    def evaluate(self, X, y_true):\n",
    "        \"\"\"Evaluate the model.\"\"\"\n",
    "        y_pred = self.predict(X)\n",
    "        precision, recall, f_score, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "        return precision, recall, f_score\n",
    "\n",
    "# Example usage\n",
    "# gad = SklearnGaussianAnomalyDetection()\n",
    "# gad.fit(X_train)  # Assuming X_train is your training data\n",
    "# precision, recall, f_score = gad.evaluate(X_test, y_test)  # Assuming X_test is your test data and y_test is the true labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9a10d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly Detection using Guassian (Normal) Distribution Method\n",
    "\n",
    "# Select Features: Choose features you think indicate anomalies.\n",
    "# Calculate Parameters: Find the mean and variance for each feature.\n",
    "  '''\n",
    "    Mean \\( \\mu_j \\):\n",
    "    \\[ \\mu_j = \\frac{1}{m} \\sum_{i=1}^{m} x_j^{(i)} \\]\n",
    "\n",
    "    Variance \\( \\sigma_j^2 \\):\n",
    "    \\[ \\sigma_j^2 = \\frac{1}{m} \\sum_{i=1}^{m} (x_j^{(i)} - \\mu_j)^2 \\] \n",
    "   '''\n",
    "# Probability Model: For a new example, calculate the probability that it fits within the normal range of each feature.\n",
    "    '''\n",
    "        \\[ p(x) = \\prod_{j=1}^{n} p(x_j; \\mu_j, \\sigma_j^2) \\]  \n",
    "\n",
    "        Where the probability density for feature \\( j \\) is:\n",
    "        \\[ p(x_j; \\mu_j, \\sigma_j^2) = \\frac{1}{\\sqrt{2\\pi\\sigma_j}} \\exp\\left(-\\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2}\\right) \\]\n",
    "\n",
    "        The combined probability of a new data point \\( x \\) is the product of its individual feature probabilities, \n",
    "            assuming feature independence:\n",
    "        \\[ p(x) = \\prod_{j=1}^{n} p(x_j; \\mu_j, \\sigma_j^2) \\]\n",
    "\n",
    "        The product is computed as follows:\n",
    "        \\[ p(x) = \\prod_{j=1}^{n} \\frac{1}{\\sqrt{2\\pi\\sigma_j^2}} \\exp\\left(-\\frac{(x_j - \\mu_j)^2}{2\\sigma_j^2}\\right) \\]\n",
    "    '''\n",
    "# Identify Anomalies: If the probability is below a certain threshold, the example is marked as an anomaly.\n",
    "    # the threshold (epsilon) is a hyperparameter you choose based on the desired sensitivity of the anomaly detection algorithm\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6eabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommmendation System \n",
    "\n",
    "import re\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances, manhattan_distances\n",
    "\n",
    "class AdvancedRecommender:\n",
    "    def __init__(self, df, feature_matrix, similarity_metric='cosine'):\n",
    "        self.df = df\n",
    "        self.feature_matrix = feature_matrix\n",
    "        self.similarity_metric = similarity_metric\n",
    "\n",
    "    def calculate_similarity(self):\n",
    "        if self.similarity_metric == 'cosine':\n",
    "            return cosine_similarity(self.feature_matrix)\n",
    "        elif self.similarity_metric == 'euclidean':\n",
    "            return euclidean_distances(self.feature_matrix)\n",
    "        elif self.similarity_metric == 'manhattan':\n",
    "            return manhattan_distances(self.feature_matrix)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported similarity metric\")\n",
    "\n",
    "    def recommend(self, title, total_result=5, threshold=0.5):\n",
    "        idx = self.find_id(title)\n",
    "        if idx == -1:\n",
    "            return \"Title not found in dataset\", []\n",
    "\n",
    "        similarity_scores = self.calculate_similarity()\n",
    "        similarity_scores = similarity_scores[idx]\n",
    "        \n",
    "        # Filter based on threshold\n",
    "        filtered_scores = [(index, score) for index, score in enumerate(similarity_scores) if score >= threshold]\n",
    "\n",
    "        # Sort by score and get top results\n",
    "        sorted_scores = sorted(filtered_scores, key=lambda x: x[1], reverse=True)[1:total_result+1]\n",
    "\n",
    "        recommendations = [self.df.iloc[i]['title'] for i, _ in sorted_scores]\n",
    "\n",
    "        return recommendations\n",
    "\n",
    "    def find_id(self, name):\n",
    "        for index, title in enumerate(self.df['title']):\n",
    "            if re.search(name, title, re.IGNORECASE):\n",
    "                return index\n",
    "        return -1\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edddfff7",
   "metadata": {},
   "source": [
    "### Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a11145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "eng_stop = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "def word_grams(text, min=1, max=4):\n",
    "    '''\n",
    "    Function to create N-grams from text\n",
    "    Required Input -\n",
    "        - text = text string for which N-gram needs to be created\n",
    "        - min = minimum number of N\n",
    "        - max = maximum number of N\n",
    "    Expected Output -\n",
    "        - s = list of N-grams \n",
    "    '''\n",
    "    s = []\n",
    "    for n in range(min, max+1):\n",
    "        for ngram in ngrams(text, n):\n",
    "            s.append(' '.join(str(i) for i in ngram))\n",
    "    return s\n",
    "\n",
    "\n",
    "def generate_bigrams_df(df, column_names):\n",
    "    \"\"\"\n",
    "    Generate bigrams from specified columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame to generate bigrams from.\n",
    "    column_names (list of str): List of column names to generate bigrams from.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with bigrams appended as new columns.\n",
    "    \"\"\"\n",
    "    bigram_columns = []\n",
    "    for col in column_names:\n",
    "        bigram_col = f\"{col}_bigrams\"\n",
    "        bigram_columns.append(bigram_col)\n",
    "        df[bigram_col] = df[col].apply(lambda x: generate_bigrams([x]))\n",
    "    return df[bigram_columns]\n",
    "\n",
    "def make_wordcloud(df,column, bg_color='white', w=1200, h=1000, font_size_max=50, n_words=40,g_min=1,g_max=1):\n",
    "    '''\n",
    "    Function to make wordcloud from a text corpus\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - bg_color = Background color\n",
    "        - w = width\n",
    "        - h = height\n",
    "        - font_size_max = maximum font size allowed\n",
    "        - n_word = maximum words allowed\n",
    "        - g_min = minimum n-grams\n",
    "        - g_max = maximum n-grams\n",
    "    Expected Output -\n",
    "        - World cloud image\n",
    "    '''\n",
    "    text = \"\"\n",
    "    for ind, row in df.iterrows(): \n",
    "        text += row[column] + \" \"\n",
    "    text = text.strip().split(' ') \n",
    "    text = word_grams(text,g_min,g_max)\n",
    "    \n",
    "    text = list(pd.Series(word_grams(text,1,2)).apply(lambda x: x.replace(' ','_')))\n",
    "    \n",
    "    s = \"\"\n",
    "    for i in range(len(text)):\n",
    "        s += text[i] + \" \"\n",
    "\n",
    "    wordcloud = WordCloud(background_color=bg_color, \\\n",
    "                          width=w, \\\n",
    "                          height=h, \\\n",
    "                          max_font_size=font_size_max, \\\n",
    "                          max_words=n_words).generate(s)\n",
    "    wordcloud.recolor(random_state=1)\n",
    "    plt.rcParams['figure.figsize'] = (20.0, 10.0)\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "def generate_wordcloud(df, column_names):\n",
    "    \"\"\"\n",
    "    Generates a wordcloud from a pandas DataFrame\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): DataFrame containing the data\n",
    "    column_names (list): List of column names in the DataFrame to generate the wordcloud from\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    all_words = ' '.join([' '.join(text) for col in column_names for text in df[col]])\n",
    "    wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
    "\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def get_tokens(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - tokenized list output\n",
    "    '''\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def tokenize_columns(dataframe, columns):\n",
    "    \"\"\"\n",
    "    Tokenize the values in specified columns of a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        dataframe (pandas.DataFrame): The DataFrame to tokenize.\n",
    "        columns (list): A list of column names to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: A new DataFrame with tokenized values in the specified columns.\n",
    "    \"\"\"\n",
    "    # Download necessary NLTK resources if they haven't been downloaded yet\n",
    "    nltk.download('punkt')\n",
    "\n",
    "    # Create a new DataFrame to hold the tokenized values\n",
    "    tokenized_df = pd.DataFrame()\n",
    "\n",
    "    # Tokenize the values in each specified column\n",
    "    for col in columns:\n",
    "        # Tokenize the values in the current column using NLTK's word_tokenize function\n",
    "        tokenized_values = dataframe[col].apply(nltk.word_tokenize)\n",
    "\n",
    "        # Add the tokenized values to the new DataFrame\n",
    "        tokenized_df[col] = tokenized_values\n",
    "\n",
    "    # Return the new DataFrame with tokenized values\n",
    "    return tokenized_df\n",
    "\n",
    "#another way\n",
    "--------------------------------------------------------------------------\n",
    "def tokenize(text, sep=' ', preserve_case=False):\n",
    "    \"\"\"\n",
    "    Tokenize a string into a list of tokens.\n",
    "\n",
    "    Parameters:\n",
    "    text (str): String to be tokenized\n",
    "    sep (str, optional): Separator to use for tokenization. Defaults to ' '.\n",
    "    preserve_case (bool, optional): Whether to preserve the case of the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    list: List of tokens\n",
    "    \"\"\"\n",
    "    if not preserve_case:\n",
    "        text = text.lower()\n",
    "    tokens = text.split(sep)\n",
    "    return tokens\n",
    "\n",
    "def tokenize_df(df, column_names, sep=' ', preserve_case=False):\n",
    "    \"\"\"\n",
    "    Tokenize a pandas dataframe with multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to be tokenized\n",
    "    columns (list of str): List of column names to be tokenized\n",
    "    sep (str, optional): Separator to use for tokenization. Defaults to ' '.\n",
    "    preserve_case (bool, optional): Whether to preserve the case of the text. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Tokenized dataframe\n",
    "    \"\"\"\n",
    "    for col in column_names:\n",
    "        df[col] = df[col].apply(lambda x: tokenize(x, sep, preserve_case))\n",
    "    return df\n",
    "\n",
    "carbon_google1 = tokenize_df (carbon_google1, column_names =  [\"title\"], sep=' ', preserve_case=False)\n",
    "--------------------------------------------------------------------------------------------------------------\n",
    "def bag_of_words_features(df, text_columns, target_columns):\n",
    "    \"\"\"\n",
    "    This function takes in a DataFrame and one or two columns and returns a bag of words representation of the data as a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas DataFrame): The DataFrame to extract features from.\n",
    "    column1 (str): The name of the first column to use as input data.\n",
    "    column2 (str, optional): The name of the second column to use as input data. If not provided, only the first column will be used.\n",
    "\n",
    "    Returns:\n",
    "    pandas DataFrame: The bag of words representation of the input data as a DataFrame.\n",
    "    \"\"\"\n",
    "        \n",
    "    text_data = df[text_columns].apply(lambda x: \" \".join([str(i) for i in x]), axis=1)\n",
    "\n",
    "    text_data = text_data.str.lower()\n",
    "    vectorizer = CountVectorizer(max_df=0.90, min_df=4, max_features=1000, stop_words=None)\n",
    "    X_bow = vectorizer.fit_transform(text_data)\n",
    "    # Use the new function to get the feature names\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    df.dropna(subset=[target_column], inplace=True) if target_columns else None\n",
    "\n",
    "    X_bow = pd.DataFrame(X_bow.toarray(), columns=feature_names)\n",
    "    \n",
    "    if target_columns:        \n",
    "        y = df[target_columns]\n",
    "        return X_bow, y\n",
    "    \n",
    "    return X_bow\n",
    "\n",
    "def convert_lowercase(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be lowercased\n",
    "    Expected Output -\n",
    "        - text - lower cased text string output\n",
    "    '''\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unwanted_characters(df, columns):        #clean text A\n",
    "    \"\"\"\n",
    "    Remove unwanted characters (including smileys and emojies) from specified columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame.\n",
    "    columns (list): A list of column names to clean.\n",
    "    unwanted_chars (str): The characters to remove.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The cleaned DataFrame.\n",
    "    \"\"\"\n",
    "    import re \n",
    "    unwanted_chars = '[$#&*@%]'\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "        u\"\\u2764\\ufe0f\" # heart emoji\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].apply(lambda x: emoji_pattern.sub(r'', x))\n",
    "            df[col] = df[col].str.replace(unwanted_chars, '')\n",
    "        else:\n",
    "            print(f\"Column '{col}' does not exist in the DataFrame.\")\n",
    "    return df\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string \n",
    "    Expected Output -\n",
    "        - text - text string with punctuation removed\n",
    "    '''\n",
    "    return text.translate(None,string.punctuation)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - text - text string which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - list output with stopwords removed\n",
    "    '''\n",
    "    return [word for word in text.split() if word not in eng_stop]\n",
    "\n",
    "def remove_short_words(df, column_names, min_length=3):\n",
    "    \"\"\"Remove short words from columns in a pandas DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): The DataFrame to modify.\n",
    "    column_names (List[str]): A list of column names to modify.\n",
    "    min_length (int, optional): The minimum length of words to keep. Default is 3.\n",
    "\n",
    "    Returns:\n",
    "    pandas.DataFrame: The modified DataFrame with short words removed from specified columns.\n",
    "    \"\"\"\n",
    "    for column_name in column_names:\n",
    "        df[column_name] = df[column_name].apply(\n",
    "            lambda x: ' '.join([word for word in x.split() if len(word) >= min_length])\n",
    "        )\n",
    "    return df\n",
    "\n",
    "def convert_stemmer(word):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - word - word which needs to be tokenized\n",
    "    Expected Output -\n",
    "        - text - word output after stemming\n",
    "    '''\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    return porter_stemmer.stem(word)\n",
    "\n",
    "def stem_df(df, column_names):\n",
    "    \"\"\"\n",
    "    Perform stemming on a pandas dataframe with multiple columns.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe to be stemmed\n",
    "    columns (list of str): List of column names to be stemmed\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Stemmed dataframe\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    for col in column_names:\n",
    "        df[col] = df[col].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    return df\n",
    "\n",
    "def convert_lemmatizer(word):\n",
    "    '''\n",
    "    Function to tokenize the text\n",
    "    Required Input - \n",
    "        - word - word which needs to be lemmatized\n",
    "    Expected Output -\n",
    "        - word - word output after lemmatizing\n",
    "    '''\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    return wordnet_lemmatizer.lemmatize(word)\n",
    "    \n",
    "def create_tf_idf(df, column, train_df = None, test_df = None,n_features = None):\n",
    "    '''\n",
    "    Function to do tf-idf on a pandas dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - train_df(optional) = Train DataFrame\n",
    "        - test_df(optional) = Test DataFrame\n",
    "        - n_features(optional) = Maximum number of features needed\n",
    "    Expected Output -\n",
    "        - train_tfidf = train tf-idf sparse matrix output\n",
    "        - test_tfidf = test tf-idf sparse matrix output\n",
    "        - tfidf_obj = tf-idf model\n",
    "    '''\n",
    "    tfidf_obj = TfidfVectorizer(ngram_range=(1,1), stop_words='english', \n",
    "                                analyzer='word', max_features = n_features)\n",
    "    tfidf_text = tfidf_obj.fit_transform(df.ix[:,column].values)\n",
    "    \n",
    "    if train_df is not None:        \n",
    "        train_tfidf = tfidf_obj.transform(train_df.ix[:,column].values)\n",
    "    else:\n",
    "        train_tfidf = tfidf_text\n",
    "\n",
    "    test_tfidf = None\n",
    "    if test_df is not None:\n",
    "        test_tfidf = tfidf_obj.transform(test_df.ix[:,column].values)\n",
    "\n",
    "    return train_tfidf, test_tfidf, tfidf_obj\n",
    "    \n",
    "def create_countvector(df, column, train_df = None, test_df = None,n_features = None):\n",
    "    '''\n",
    "    Function to do count vectorizer on a pandas dataframe\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame\n",
    "        - column = name of column containing text\n",
    "        - train_df(optional) = Train DataFrame\n",
    "        - test_df(optional) = Test DataFrame\n",
    "        - n_features(optional) = Maximum number of features needed\n",
    "    Expected Output -\n",
    "        - train_cvect = train count vectorized sparse matrix output\n",
    "        - test_cvect = test count vectorized sparse matrix output\n",
    "        - cvect_obj = count vectorized model\n",
    "    '''\n",
    "    cvect_obj = CountVectorizer(ngram_range=(1,1), stop_words='english', \n",
    "                                analyzer='word', max_features = n_features)\n",
    "    cvect_text = cvect_obj.fit_transform(df.ix[:,column].values)\n",
    "    \n",
    "    if train_df is not None:\n",
    "        train_cvect = cvect_obj.transform(train_df.ix[:,column].values)\n",
    "    else:\n",
    "        train_cvect = cvect_text\n",
    "        \n",
    "    test_cvect = None\n",
    "    if test_df is not None:\n",
    "        test_cvect = cvect_obj.transform(test_df.ix[:,column].values)\n",
    "\n",
    "    return train_cvect, test_cvect, cvect_obj\n",
    "\n",
    "#Remove Punctuation\n",
    "import string \n",
    "\n",
    "table = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\"string. With. Punctuation?\".translate(table) \n",
    "\n",
    "#\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8168156b",
   "metadata": {},
   "source": [
    ">> NLP Text Preprocessing Steps for Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebe169",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Removing HTML tags, punctuation, and special characters\n",
    "# 2. Converting to Lowercase\n",
    "# 3. Handling contractions and acronyms \n",
    "# 4. Tokenization\n",
    "# 5. Spell Checking and Correction\n",
    "# 6. Stopword Removal\n",
    "# 7. Part-of-speech (POS) tagging\n",
    "# 8. Named Entity Recognition (NER)\n",
    "# 9. Stemming or Lemmatization\n",
    "# 10. Text Vectorization\n",
    "\n",
    "\n",
    "\n",
    "# 1. Tokenization\n",
    "# The process of converting a raw text into a sequence of tokens (words, phrases, symbols, etc.) is called tokenization.\n",
    "\n",
    "    from nltk.tokenize import word_tokenize\n",
    "    text = \"This is a sample text for tokenization.\"\n",
    "    tokens = word_tokenize(text)\n",
    "    print(tokens)\n",
    "\n",
    "# 2. Stopword Removal\n",
    "# Stopwords are commonly used words in a language, such as “the,” “and,” “a,” etc., that do not add much meaning to the \n",
    "# text. Removing these words helps to reduce the noise in the text data.\n",
    "\n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    tokens = [word for word in tokens if not word in stop_words]\n",
    "    print(tokens)\n",
    "\n",
    "# 3. Stemming\n",
    "# Stemming is the process of reducing a word to its base or root form. For example, the words “jumping”, “jumps”, and \n",
    "# “jumped” would all be reduced to “jump” by a stemming algorithm. The main goal of stemming is to reduce different \n",
    "# forms of a word to a common base form, which can help in tasks like text classification, sentiment analysis, and \n",
    "# information retrieval.\n",
    "\n",
    "    from nltk.stem import PorterStemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_words = [stemmer.stem(word) for word in tokens]\n",
    "    print(stemmed_words)\n",
    "\n",
    "# 4. Lemmatization\n",
    "# Lemmatization is the process of reducing words to their base or dictionary form (known as a lemma) so that they can be \n",
    "# analyzed as a single item, rather than multiple different forms. For example, the word “running” can be reduced to \n",
    "# its base form “run” through lemmatization.\n",
    "\n",
    "    from nltk.stem import WordNetLemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    print(lemmatized_words)\n",
    "\n",
    "# 5. Part-of-speech (POS) tagging\n",
    "# Part-of-speech (POS) tagging is the process of identifying and labeling the part of speech of each word in a sentence, \n",
    "# such as a noun, verb, adjective, adverb, etc. POS tagging is useful in various natural languages processing tasks like \n",
    "# sentiment analysis, text classification, information extraction, and machine translation.\n",
    "\n",
    "    import nltk\n",
    "    # Sample sentence\n",
    "    sentence = \"The quick brown fox jumps over the lazy dog\"\n",
    "    # Tokenize the sentence into words\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    # Perform POS tagging\n",
    "    pos_tags = nltk.pos_tag(words)\n",
    "    # Print the POS tags\n",
    "    print(pos_tags)\n",
    "\n",
    "# 6. Named Entity Recognition (NER)\n",
    "# Named Entity Recognition (NER) is a natural language processing technique that is used to identify and extract the \n",
    "# named entities from a given text. Named entities can be anything like a person, organization, location, product, etc.\n",
    "\n",
    "    import spacy\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    # Sample text for NER\n",
    "    text = \"Apple is looking at buying U.K. startup for $1 billion\"\n",
    "    # Process the text with the language model\n",
    "    doc = nlp(text)\n",
    "    # Extract named entities from the text\n",
    "    for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n",
    "\n",
    "# 7. Spell Checking and Correction\n",
    "# Spell checking and correction is the process of identifying and correcting spelling errors in the text. It is an \n",
    "# important step in text preprocessing as it can improve the accuracy of natural language processing algorithms that \n",
    "# are applied to text data.\n",
    "\n",
    "    !pip install pyspellchecker\n",
    "\n",
    "    from spellchecker import SpellChecker\n",
    "    # initialize spell checker\n",
    "    spell = SpellChecker()\n",
    "    # example sentence with spelling errors\n",
    "    sentence = \"Ths sentnce hs spellng erors that nd to b corcted.\"\n",
    "    # tokenize sentence\n",
    "    tokens = sentence.split()\n",
    "    # iterate over tokens and correct spelling errors\n",
    "    for i in range(len(tokens)):\n",
    "    # check if token is misspelled\n",
    "    if not spell.correction(tokens[i]) == tokens[i]:\n",
    "    # replace misspelled token with corrected spelling\n",
    "    tokens[i] = spell.correction(tokens[i])\n",
    "    # join corrected tokens back into sentence\n",
    "    corrected_sentence = ' '.join(tokens)\n",
    "    print(corrected_sentence)\n",
    "\n",
    "# 8. Removing HTML tags, punctuation, and special characters\n",
    "# Removing HTML tags, punctuation, and special characters is necessary for text preprocessing to clean the text data \n",
    "# and make it ready for further processing. HTML tags, punctuation, and special characters do not contribute to the \n",
    "# meaning of the text and can cause issues during text analysis.\n",
    "\n",
    "    import re\n",
    "    import string\n",
    "\n",
    "    def remove_html_tags(text):\n",
    "    clean_text = re.sub('<.*?>', '', text)\n",
    "    return clean_text\n",
    "\n",
    "    def remove_punctuation(text):\n",
    "    clean_text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    return clean_text\n",
    "\n",
    "    def remove_special_characters(text):\n",
    "    clean_text = re.sub('[^a-zA-Z0–9\\s]', '', text)\n",
    "    return clean_text\n",
    "\n",
    "    text = \"<p>Hello, world!</p>\"\n",
    "    clean_text = remove_html_tags(text)\n",
    "    clean_text = remove_punctuation(clean_text)\n",
    "    clean_text = remove_special_characters(clean_text)\n",
    "    print(clean_text)\n",
    "\n",
    "# 9. Converting to Lowercase\n",
    "# Lowercasing the text is a common preprocessing step in natural language processing (NLP) to make text data \n",
    "# consistent and easier to analyze. This step involves converting all the letters in the text to lowercase so \n",
    "# that words that differ only by the case are treated as the same word.\n",
    "\n",
    "    text = \"This is a sample TEXT for preprocessing\"\n",
    "    text = text.lower()\n",
    "    print(text)\n",
    "\n",
    "# 10. Text Vectorization\n",
    "# Text vectorization is the process of transforming raw text into a numerical representation that can be used by \n",
    "# machine learning algorithms. This is a crucial step in text preprocessing as most machine learning algorithms work \n",
    "# with numerical data. There are several ways to vectorize text, including Bag of Words (BoW), Term Frequency-Inverse \n",
    "# Document Frequency (TF-IDF), and Word Embeddings.\n",
    "\n",
    "    from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "    # Example text corpus\n",
    "    corpus = [\"This is the first document.\", \n",
    "    \"This document is the second document.\", \n",
    "    \"And this is the third one.\", \n",
    "    \"Is this the first document?\"]\n",
    "\n",
    "    # Vectorize text using BoW representation\n",
    "    vectorizer = CountVectorizer()\n",
    "    X_bow = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    print(\"BoW representation:\")\n",
    "    print(X_bow.toarray())\n",
    "    print(\"Vocabulary:\")\n",
    "    print(vectorizer.get_feature_names())\n",
    "\n",
    "    # Vectorize text using TF-IDF representation\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_tfidf = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    print(\"TF-IDF representation:\")\n",
    "    print(X_tfidf.toarray())\n",
    "\n",
    "\n",
    "\n",
    "#Clean text B\n",
    "\n",
    "import re\n",
    "import copy\n",
    "import string\n",
    "\n",
    "def clean_text(text, full_clean=False, punctuation=False, numbers=False, lower=False, extra_spaces=False,\n",
    "               control_characters=False, tokenize_whitespace=False, remove_characters=''):\n",
    "    r\"\"\"\n",
    "    Clean text using various techniques.\n",
    "\n",
    "    I took inspiration from the cleantext library `https://github.com/prasanthg3/cleantext`. I did not like the whole\n",
    "    implementation so I made my own changes.\n",
    "\n",
    "    Note:\n",
    "        As in the original cleantext library I will add: stop words removal, stemming and\n",
    "        negative-positive words removal.\n",
    "\n",
    "    Arguments:\n",
    "\n",
    "        text (:obj:`str`):\n",
    "            String that needs cleaning.\n",
    "\n",
    "        full_clean (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Remove: punctuation, numbers, extra space, control characters and lower case. This argument is optional and\n",
    "            it has a default value attributed inside the function.\n",
    "\n",
    "        punctuation (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Remove punctuation from text. This argument is optional and it has a default value attributed inside\n",
    "            the function.\n",
    "\n",
    "        numbers (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Remove digits from text. This argument is optional and it has a default value attributed inside\n",
    "            the function.\n",
    "\n",
    "        lower (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Lower case all text. This argument is optional and it has a default value attributed inside the function.\n",
    "\n",
    "        extra_spaces (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Remove extra spaces - everything beyond one space. This argument is optional and it has a default value\n",
    "            attributed inside the function.\n",
    "\n",
    "        control_characters (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Remove characters like `\\n`, `\\t` etc.This argument is optional and it has a default value attributed\n",
    "            inside the function.\n",
    "\n",
    "        tokenize_whitespace (:obj:`bool`, `optional`, defaults to :obj:`False`):\n",
    "            Return a list of tokens split on whitespace. This argument is optional and it has a default value\n",
    "            attributed inside the function.\n",
    "\n",
    "        remove_characters (:obj:`str`, `optional`, defaults to :obj:`''`):\n",
    "            Remove defined characters form text. This argument is optional and it has a default value attributed\n",
    "            inside the function.\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        :obj:`str`: Clean string.\n",
    "\n",
    "    Raises:\n",
    "\n",
    "        ValueError: If `text` is not of type string.\n",
    "\n",
    "        ValueError: If `remove_characters` needs to be a string.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        # `text` is not type of string\n",
    "        raise ValueError(\"`text` is not of type str!\")\n",
    "\n",
    "    if not isinstance(remove_characters, str):\n",
    "        # remove characters need to be a string\n",
    "        raise ValueError(\"`remove_characters` needs to be a string!\")\n",
    "\n",
    "    # all control characters like `\\t` `\\n` `\\r` etc.\n",
    "    # Stack Overflow: https://stackoverflow.com/a/8115378/11281368\n",
    "    control_characters_list = ''.join([chr(char) for char in range(1, 32)])\n",
    "\n",
    "    # define control characters table\n",
    "    table_control_characters = str.maketrans(dict.fromkeys(control_characters_list))\n",
    "\n",
    "    # remove punctuation table\n",
    "    table_punctuation = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "\n",
    "    # remove numbers table\n",
    "    table_digits = str.maketrans(dict.fromkeys('0123456789'))\n",
    "\n",
    "    # remove certain characters table\n",
    "    table_remove_characters = str.maketrans(dict.fromkeys(remove_characters))\n",
    "\n",
    "    # make a copy of text to make sure it doesn't affect original text\n",
    "    cleaned = copy.deepcopy(text)\n",
    "\n",
    "    if full_clean or punctuation:\n",
    "        # remove punctuation\n",
    "        cleaned = cleaned.translate(table_punctuation)\n",
    "\n",
    "    if full_clean or numbers:\n",
    "        # remove numbers\n",
    "        cleaned = cleaned.translate(table_digits)\n",
    "\n",
    "    if full_clean or extra_spaces:\n",
    "        # remove extra spaces - also removes control characters\n",
    "        # Stack Overflow https://stackoverflow.com/a/2077906/11281368\n",
    "        cleaned = re.sub('\\s+', ' ', cleaned).strip()\n",
    "\n",
    "    if full_clean or lower:\n",
    "        # lowercase\n",
    "        cleaned = cleaned.lower()\n",
    "\n",
    "    if control_characters:\n",
    "        # remove control characters\n",
    "        cleaned = cleaned.translate(table_control_characters)\n",
    "\n",
    "    if tokenize_whitespace:\n",
    "        # tokenizes text n whitespace\n",
    "        cleaned = re.split('\\s+', cleaned)\n",
    "\n",
    "    if remove_characters:\n",
    "        # remove these characters from text\n",
    "        cleaned = cleaned.translate(table_remove_characters)\n",
    "\n",
    "    return cleaned\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "309b99a5",
   "metadata": {},
   "source": [
    ">> Sentiment Analysis of Tweets (Unsupervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbcd84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "nltk.download('vader_lexicon')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "from sklearn.cluster import KMeans\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer \n",
    "\n",
    "# Load data from file or API\n",
    "tweets = pd.read_csv(r'C:\\Users\\Cornel\\Downloads\\tweets.csv')\n",
    "\n",
    "# tweets = tweets['text'] \n",
    "# Preprocess text data\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r\"http\\S+\", \"\", text) # remove URLs\n",
    "    text = re.sub('@[^\\s]+', '', text) # remove usernames\n",
    "    text = re.sub('#', '', text) # remove hashtags\n",
    "    text = re.sub(r'\\d+', '', text) # remove numbers\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    text = text.lower() # convert to lowercase\n",
    "    return text\n",
    "\n",
    "tweets['text_clean'] = tweets['text'].apply(preprocess_text)\n",
    "\n",
    "# Extract features\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(tweets['text_clean'])\n",
    "\n",
    "# Cluster tweets using K-means\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "kmeans.fit(X)\n",
    "\n",
    "# Analyze sentiment of each cluster using a lexicon-based approach\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "cluster_sentiment = []\n",
    "for i in range(5):\n",
    "    cluster_text = tweets[kmeans.labels_ == i]['text_clean']\n",
    "    sentiment_scores = [sia.polarity_scores(text)['compound'] for text in cluster_text]\n",
    "    cluster_sentiment.append(np.mean(sentiment_scores))\n",
    "\n",
    "# Visualize results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.add_axes([0,0,1,1])\n",
    "clusters = ['Cluster 1', 'Cluster 2', 'Cluster 3', 'Cluster 4', 'Cluster 5']\n",
    "ax.bar(clusters, cluster_sentiment)\n",
    "plt.title('Sentiment Analysis of Tweets')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.show() \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f06a90e",
   "metadata": {},
   "source": [
    ">> NLP Text Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c63f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "import os \n",
    "import pickle \n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from nltk import sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk import map_tag\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Load NLTK's English stop-words list\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "\n",
    "#\n",
    "# embeddings vector representations\n",
    "#\n",
    "\n",
    "def tag_pos(x):\n",
    "    sentences = sent_tokenize(x.decode(\"utf8\"))\n",
    "    sents = []\n",
    "    for s in sentences:\n",
    "        text = word_tokenize(s)\n",
    "        pos_tagged = pos_tag(text)\n",
    "        simplified_tags = [\n",
    "            (word, map_tag('en-ptb', 'universal', tag)) for word, tag in pos_tagged]\n",
    "        sents.append(simplified_tags)\n",
    "    return sents\n",
    "\n",
    "\n",
    "def post_tag_documents(data_df):\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    total = len(data_df['plot'].as_matrix().tolist())\n",
    "    plots = data_df['plot'].as_matrix().tolist()\n",
    "    genres = data_df.drop(['plot', 'title', 'plot_lang'], axis=1).as_matrix()\n",
    "    for i in range(len(plots)):\n",
    "        sents = tag_pos(plots[i])\n",
    "        x_data.append(sents)\n",
    "        y_data.append(genres[i])\n",
    "        i += 1\n",
    "        if i % 5000 == 0:\n",
    "            print i, \"/\", total\n",
    "\n",
    "    return x_data, y_data\n",
    "\n",
    "\n",
    "def word2vec(x_data, pos_filter):\n",
    "\n",
    "    print \"Loading GoogleNews-vectors-negative300.bin\"\n",
    "    google_vecs = KeyedVectors.load_word2vec_format(\n",
    "        'GoogleNews-vectors-negative300.bin', binary=True, limit=200000)\n",
    "\n",
    "    print \"Considering only\", pos_filter\n",
    "    print \"Averaging Word Embeddings...\"\n",
    "    x_data_embeddings = []\n",
    "    total = len(x_data)\n",
    "    processed = 0\n",
    "    for tagged_plot in x_data:\n",
    "        count = 0\n",
    "        doc_vector = np.zeros(300)\n",
    "        for sentence in tagged_plot:\n",
    "            for tagged_word in sentence:\n",
    "                if tagged_word[1] in pos_filter:\n",
    "                    try:\n",
    "                        doc_vector += google_vecs[tagged_word[0]]\n",
    "                        count += 1\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "\n",
    "        doc_vector /= count\n",
    "        if np.isnan(np.min(doc_vector)):\n",
    "            continue\n",
    "\n",
    "        x_data_embeddings.append(doc_vector)\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 10000 == 0:\n",
    "            print processed, \"/\", total\n",
    "\n",
    "    return np.array(x_data_embeddings)\n",
    "\n",
    "\n",
    "def doc2vec(data_df):\n",
    "    data = []\n",
    "    print \"Building TaggedDocuments\"\n",
    "    total = len(data_df[['title', 'plot']].as_matrix().tolist())\n",
    "    processed = 0\n",
    "    for x in data_df[['title', 'plot']].as_matrix().tolist():\n",
    "        label = [\"_\".join(x[0].split())]\n",
    "        words = []\n",
    "        sentences = sent_tokenize(x[1].decode(\"utf8\"))\n",
    "        for s in sentences:\n",
    "            words.extend([x.lower() for x in word_tokenize(s)])\n",
    "        doc = TaggedDocument(words, label)\n",
    "        data.append(doc)\n",
    "\n",
    "        processed += 1\n",
    "        if processed % 10000 == 0:\n",
    "            print processed, \"/\", total\n",
    "\n",
    "    model = Doc2Vec(min_count=1, window=10, size=300, sample=1e-4, negative=5, workers=2)\n",
    "    print \"Building Vocabulary\"\n",
    "    model.build_vocab(data)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        print \"Training epoch %s\" % epoch\n",
    "        model.train(data)\n",
    "        model.alpha -= 0.002  # decrease the learning rate\n",
    "        model.min_alpha = model.alpha  # fix the learning rate, no decay\n",
    "        model.train(data)\n",
    "\n",
    "    # Build doc2vec vectors\n",
    "    x_data = []\n",
    "    y_data = []\n",
    "    genres = data_df.drop(['title', 'plot', 'plot_lang'], axis=1).as_matrix()\n",
    "    names = data_df[['title']].as_matrix().tolist()\n",
    "    for i in range(len(names)):\n",
    "        name = names[i][0]\n",
    "        label = \"_\".join(name.split())\n",
    "        x_data.append(model.docvecs[label])\n",
    "        y_data.append(genres[i])\n",
    "\n",
    "    return np.array(x_data), np.array(y_data)\n",
    "\n",
    "\n",
    "#\n",
    "# train classifiers and argument handling\n",
    "#\n",
    "\n",
    "def train_test_svm(x_data, y_data, genres):\n",
    "\n",
    "    stratified_split = StratifiedShuffleSplit(n_splits=2, test_size=0.33)\n",
    "    for train_index, test_index in stratified_split.split(x_data, y_data):\n",
    "        x_train, x_test = x_data[train_index], x_data[test_index]\n",
    "        y_train, y_test = y_data[train_index], y_data[test_index]\n",
    "\n",
    "    \"\"\"\n",
    "    print \"LinearSVC\"\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    grid_search(x_train, y_train, x_test, y_test, genres, parameters, pipeline)\n",
    "\n",
    "    print \"LogisticRegression\"\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "    ])\n",
    "    parameters = {\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    grid_search(x_train, y_train, x_test, y_test, genres, parameters, pipeline)\n",
    "    \"\"\"\n",
    "\n",
    "    print \"LinearSVC\"\n",
    "    pipeline = Pipeline([\n",
    "        ('clf', OneVsRestClassifier(SVC(), n_jobs=1)),\n",
    "    ])\n",
    "    \"\"\"\n",
    "    parameters = {\n",
    "        \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "        \"clf__estimator__class_weight\": ['balanced', None],\n",
    "    }\n",
    "    \"\"\"\n",
    "    parameters = [\n",
    "\n",
    "        {'clf__estimator__kernel': ['rbf'],\n",
    "         'clf__estimator__gamma': [1e-3, 1e-4],\n",
    "         'clf__estimator__C': [1, 10]\n",
    "        },\n",
    "\n",
    "        {'clf__estimator__kernel': ['poly'],\n",
    "         'clf__estimator__C': [1, 10]\n",
    "        }\n",
    "         ]\n",
    "\n",
    "    grid_search(x_train, y_train, x_test, y_test, genres, parameters, pipeline)\n",
    "\n",
    "\n",
    "def grid_search(train_x, train_y, test_x, test_y, genres, parameters, pipeline):\n",
    "    grid_search_tune = GridSearchCV(pipeline, parameters, cv=2, n_jobs=3, verbose=10)\n",
    "    grid_search_tune.fit(train_x, train_y)\n",
    "\n",
    "    print\n",
    "    print(\"Best parameters set:\")\n",
    "    print grid_search_tune.best_estimator_.steps\n",
    "    print\n",
    "\n",
    "    # measuring performance on test set\n",
    "    print \"Applying best classifier on test data:\"\n",
    "    best_clf = grid_search_tune.best_estimator_\n",
    "    predictions = best_clf.predict(test_x)\n",
    "\n",
    "    print classification_report(test_y, predictions, target_names=genres)\n",
    "\n",
    "\n",
    "def parse_arguments():\n",
    "    arg_parser = ArgumentParser()\n",
    "\n",
    "    arg_parser.add_argument(\n",
    "        '--clf', dest='classifier', choices=['nb', 'linearSVC', 'logit'])\n",
    "\n",
    "    arg_parser.add_argument(\n",
    "        '--vectors', dest='vectors', type=str, choices=['tfidf', 'word2vec', 'doc2vec'])\n",
    "\n",
    "    return arg_parser, arg_parser.parse_args()\n",
    "\n",
    "\n",
    "def main():\n",
    "    args_parser, args = parse_arguments()\n",
    "\n",
    "    if len(sys.argv) == 1:\n",
    "        args_parser.print_help()\n",
    "        sys.exit(1)\n",
    "\n",
    "    # load pre-processed data\n",
    "    print \"Loading already processed training data\"\n",
    "    data_df = pd.read_csv(\"movies_genres_en.csv\", delimiter='\\t')\n",
    "    # all the list of genres to be used by the classification report\n",
    "    genres = list(data_df.drop(['title', 'plot', 'plot_lang'], axis=1).columns.values)\n",
    "\n",
    "    if args.vectors == 'tfidf':\n",
    "\n",
    "        # split the data, leave 1/3 out for testing\n",
    "        data_x = data_df[['plot']].as_matrix()\n",
    "        data_y = data_df.drop(['title', 'plot', 'plot_lang'], axis=1).as_matrix()\n",
    "        stratified_split = StratifiedShuffleSplit(n_splits=2, test_size=0.33)\n",
    "        for train_index, test_index in stratified_split.split(data_x, data_y):\n",
    "            x_train, x_test = data_x[train_index], data_x[test_index]\n",
    "            y_train, y_test = data_y[train_index], data_y[test_index]\n",
    "\n",
    "        # transform matrix of plots into lists to pass to a TfidfVectorizer\n",
    "        train_x = [x[0].strip() for x in x_train.tolist()]\n",
    "        test_x = [x[0].strip() for x in x_test.tolist()]\n",
    "\n",
    "        if args.classifier == 'nb':\n",
    "            # MultinomialNB: Multi-Class OneVsRestClassifier\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "            parameters = {\n",
    "                'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                'clf__estimator__alpha': (1e-2, 1e-3)\n",
    "            }\n",
    "            grid_search(train_x, y_train, test_x, y_test, genres, parameters, pipeline)\n",
    "            exit(-1)\n",
    "\n",
    "        if args.classifier == 'linearSVC':\n",
    "            # LinearSVC\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "            parameters = {\n",
    "                'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None],\n",
    "            }\n",
    "            grid_search(train_x, y_train, test_x, y_test, genres, parameters, pipeline)\n",
    "            exit(-1)\n",
    "\n",
    "        if args.classifier == 'logit':\n",
    "            # LogisticRegression\n",
    "            pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stop_words)),\n",
    "                ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "            ])\n",
    "            parameters = {\n",
    "                'tfidf__max_df': (0.25, 0.5, 0.75),\n",
    "                'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "                \"clf__estimator__C\": [0.01, 0.1, 1],\n",
    "                \"clf__estimator__class_weight\": ['balanced', None],\n",
    "            }\n",
    "            grid_search(train_x, y_train, test_x, y_test, genres, parameters, pipeline)\n",
    "            exit(-1)\n",
    "\n",
    "    if args.vectors == 'word2vec':\n",
    "        if os.path.exists(\"pos_tagged_data.dat\"):\n",
    "            print \"Loading Part-of-Speech tagged data...\"\n",
    "            with open('pos_tagged_data.dat', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                x_data, y_data = data[0], data[1]\n",
    "        else:\n",
    "            print \"Part-of-Speech tagging...\"\n",
    "            x_data, y_data = post_tag_documents(data_df)\n",
    "            with open('pos_tagged_data.dat', 'w') as f:\n",
    "                pickle.dump((x_data, y_data), f)\n",
    "\n",
    "        pos_filter = ['NOUN', 'ADJ']\n",
    "\n",
    "        # get embeddings for train and test data\n",
    "        x_embeddings = word2vec(x_data, pos_filter)\n",
    "\n",
    "        # need to transform back into numpy array to apply StratifiedShuffleSplit\n",
    "        y_data = np.array(y_data)\n",
    "\n",
    "        train_test_svm(x_embeddings, y_data, genres)\n",
    "        exit(-1)\n",
    "\n",
    "    if args.vectors == 'doc2vec':\n",
    "        if os.path.exists(\"doc2vec_data.dat\"):\n",
    "            print \"Loading Doc2Vec vectors\"\n",
    "            with open('doc2vec_data.dat', 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                x_data, y_data = data[0], data[1]\n",
    "        else:\n",
    "            print \"Generating Doc2Vec vectors\"\n",
    "            x_data, y_data = doc2vec(data_df)\n",
    "            with open('doc2vec_data.dat', 'w') as f:\n",
    "                pickle.dump((x_data, y_data), f)\n",
    "\n",
    "        train_test_svm(x_data, y_data, genres)\n",
    "        exit(-1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12652f36",
   "metadata": {},
   "source": [
    "### Recommendation Systems (Recsys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20910ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from lightfm import LightFM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def create_interaction_matrix(df,user_col, item_col, rating_col, norm= False, threshold = None):\n",
    "    '''\n",
    "    Function to create an interaction matrix dataframe from transactional type interactions\n",
    "    Required Input -\n",
    "        - df = Pandas DataFrame containing user-item interactions\n",
    "        - user_col = column name containing user's identifier\n",
    "        - item_col = column name containing item's identifier\n",
    "        - rating col = column name containing user feedback on interaction with a given item\n",
    "        - norm (optional) = True if a normalization of ratings is needed\n",
    "        - threshold (required if norm = True) = value above which the rating is favorable\n",
    "    Expected output - \n",
    "        - Pandas dataframe with user-item interactions ready to be fed in a recommendation algorithm\n",
    "    '''\n",
    "    interactions = df.groupby([user_col, item_col])[rating_col] \\\n",
    "            .sum().unstack().reset_index(). \\\n",
    "            fillna(0).set_index(user_col)\n",
    "    if norm:\n",
    "        interactions = interactions.applymap(lambda x: 1 if x > threshold else 0)\n",
    "    return interactions\n",
    "\n",
    "def create_user_dict(interactions):\n",
    "    '''\n",
    "    Function to create a user dictionary based on their index and number in interaction dataset\n",
    "    Required Input - \n",
    "        interactions - dataset create by create_interaction_matrix\n",
    "    Expected Output -\n",
    "        user_dict - Dictionary type output containing interaction_index as key and user_id as value\n",
    "    '''\n",
    "    user_id = list(interactions.index)\n",
    "    user_dict = {}\n",
    "    counter = 0 \n",
    "    for i in user_id:\n",
    "        user_dict[i] = counter\n",
    "        counter += 1\n",
    "    return user_dict\n",
    "    \n",
    "def create_item_dict(df,id_col,name_col):\n",
    "    '''\n",
    "    Function to create an item dictionary based on their item_id and item name\n",
    "    Required Input - \n",
    "        - df = Pandas dataframe with Item information\n",
    "        - id_col = Column name containing unique identifier for an item\n",
    "        - name_col = Column name containing name of the item\n",
    "    Expected Output -\n",
    "        item_dict = Dictionary type output containing item_id as key and item_name as value\n",
    "    '''\n",
    "    item_dict ={}\n",
    "    for i in range(df.shape[0]):\n",
    "        item_dict[(df.loc[i,id_col])] = df.loc[i,name_col]\n",
    "    return item_dict\n",
    "\n",
    "def runMF(interactions, n_components=30, loss='warp', k=15, epoch=30,n_jobs = 4):\n",
    "    '''\n",
    "    Function to run matrix-factorization algorithm\n",
    "    Required Input -\n",
    "        - interactions = dataset create by create_interaction_matrix\n",
    "        - n_components = number of embeddings you want to create to define Item and user\n",
    "        - loss = loss function other options are logistic, brp\n",
    "        - epoch = number of epochs to run \n",
    "        - n_jobs = number of cores used for execution \n",
    "    Expected Output  -\n",
    "        Model - Trained model\n",
    "    '''\n",
    "    x = sparse.csr_matrix(interactions.values)\n",
    "    model = LightFM(no_components= n_components, loss=loss,k=k)\n",
    "    model.fit(x,epochs=epoch,num_threads = n_jobs)\n",
    "    return model\n",
    "\n",
    "def sample_recommendation_user(model, interactions, user_id, user_dict, \n",
    "                               item_dict,threshold = 0,nrec_items = 10, show = True):\n",
    "    '''\n",
    "    Function to produce user recommendations\n",
    "    Required Input - \n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "        - user_id = user ID for which we need to generate recommendation\n",
    "        - user_dict = Dictionary type input containing interaction_index as key and user_id as value\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - threshold = value above which the rating is favorable in new interaction matrix\n",
    "        - nrec_items = Number of output recommendation needed\n",
    "    Expected Output - \n",
    "        - Prints list of items the given user has already bought\n",
    "        - Prints list of N recommended items  which user hopefully will be interested in\n",
    "    '''\n",
    "    n_users, n_items = interactions.shape\n",
    "    user_x = user_dict[user_id]\n",
    "    scores = pd.Series(model.predict(user_x,np.arange(n_items)))\n",
    "    scores.index = interactions.columns\n",
    "    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n",
    "    \n",
    "    known_items = list(pd.Series(interactions.loc[user_id,:] \\\n",
    "                                 [interactions.loc[user_id,:] > threshold].index) \\\n",
    "\t\t\t\t\t\t\t\t .sort_values(ascending=False))\n",
    "    \n",
    "    scores = [x for x in scores if x not in known_items]\n",
    "    return_score_list = scores[0:nrec_items]\n",
    "    known_items = list(pd.Series(known_items).apply(lambda x: item_dict[x]))\n",
    "    scores = list(pd.Series(return_score_list).apply(lambda x: item_dict[x]))\n",
    "    if show == True:\n",
    "        print(\"Known Likes:\")\n",
    "        counter = 1\n",
    "        for i in known_items:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "\n",
    "        print(\"\\n Recommended Items:\")\n",
    "        counter = 1\n",
    "        for i in scores:\n",
    "            print(str(counter) + '- ' + i)\n",
    "            counter+=1\n",
    "    return return_score_list\n",
    "    \n",
    "\n",
    "def sample_recommendation_item(model,interactions,item_id,user_dict,item_dict,number_of_user):\n",
    "    '''\n",
    "    Funnction to produce a list of top N interested users for a given item\n",
    "    Required Input -\n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "        - item_id = item ID for which we need to generate recommended users\n",
    "        - user_dict =  Dictionary type input containing interaction_index as key and user_id as value\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - number_of_user = Number of users needed as an output\n",
    "    Expected Output -\n",
    "        - user_list = List of recommended users \n",
    "    '''\n",
    "    n_users, n_items = interactions.shape\n",
    "    x = np.array(interactions.columns)\n",
    "    scores = pd.Series(model.predict(np.arange(n_users), np.repeat(x.searchsorted(item_id),n_users)))\n",
    "    user_list = list(interactions.index[scores.sort_values(ascending=False).head(number_of_user).index])\n",
    "    return user_list \n",
    "\n",
    "\n",
    "def create_item_emdedding_distance_matrix(model,interactions):\n",
    "    '''\n",
    "    Function to create item-item distance embedding matrix\n",
    "    Required Input -\n",
    "        - model = Trained matrix factorization model\n",
    "        - interactions = dataset used for training the model\n",
    "    Expected Output -\n",
    "        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w items\n",
    "    '''\n",
    "    df_item_norm_sparse = sparse.csr_matrix(model.item_embeddings)\n",
    "    similarities = cosine_similarity(df_item_norm_sparse)\n",
    "    item_emdedding_distance_matrix = pd.DataFrame(similarities)\n",
    "    item_emdedding_distance_matrix.columns = interactions.columns\n",
    "    item_emdedding_distance_matrix.index = interactions.columns\n",
    "    return item_emdedding_distance_matrix\n",
    "\n",
    "def item_item_recommendation(item_emdedding_distance_matrix, item_id, \n",
    "                             item_dict, n_items = 10, show = True):\n",
    "    '''\n",
    "    Function to create item-item recommendation\n",
    "    Required Input - \n",
    "        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b/w items\n",
    "        - item_id  = item ID for which we need to generate recommended items\n",
    "        - item_dict = Dictionary type input containing item_id as key and item_name as value\n",
    "        - n_items = Number of items needed as an output\n",
    "    Expected Output -\n",
    "        - recommended_items = List of recommended items\n",
    "    '''\n",
    "    recommended_items = list(pd.Series(item_emdedding_distance_matrix.loc[item_id,:]. \\\n",
    "                                  sort_values(ascending = False).head(n_items+1). \\\n",
    "                                  index[1:n_items+1]))\n",
    "    if show == True:\n",
    "        print(\"Item of interest :{0}\".format(item_dict[item_id]))\n",
    "        print(\"Item similar to the above item:\")\n",
    "        counter = 1\n",
    "        for i in recommended_items:\n",
    "            print(str(counter) + '- ' +  item_dict[i])\n",
    "            counter+=1\n",
    "    return recommended_items"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edbe51c5",
   "metadata": {},
   "source": [
    "### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103e6861",
   "metadata": {},
   "source": [
    "> Pandas AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d94ab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q pandasai\n",
    "# pip install --user pandasai\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from pandasai import SmartDataframe \n",
    "from pandasai.llm import OpenAI, GooglePalm, GoogleVertexAI, Falcon, AzureOpenAI, HuggingFaceTextGen, Starcoder   #different LLMs \n",
    "\n",
    "\n",
    "# Pandas AI is an extension to the pandas library using OpenAI’s generative AI models. It allows you to generate insights from your \n",
    "# dataframe using just a text prompt. It works on the text-to-query generative AI developed by OpenAI\n",
    "\n",
    "\n",
    "df = pd.read_csv('data.csv')\n",
    "llm = OpenAI(\"add key here\")\n",
    "smart_df = SmartDataframe(df, config={\"llm\": llm})\n",
    "smart_df.chat('Which are the countries with GDP greater than 3000000000000?')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7ac40",
   "metadata": {},
   "source": [
    "OpenAI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba8a09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install -q openai\n",
    "\n",
    "\n",
    "# importing openai module into your openai environment \n",
    "import openai \n",
    "  \n",
    "# assigning API KEY to initialize openai environment \n",
    "openai.api_key = '<API_KEY>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a9a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# function that takes in string argument as parameter \n",
    "def comp(PROMPT, MaxToken=50, outputs=3): \n",
    "    # using OpenAI's Completion module that helps execute  \n",
    "    # any tasks involving text  \n",
    "    response = openai.Completion.create( \n",
    "        # model name used here is text-davinci-003 \n",
    "        # there are many other models available under the  \n",
    "        # umbrella of GPT-3 \n",
    "        model=\"text-davinci-003\", \n",
    "        # passing the user input  \n",
    "        prompt=PROMPT, \n",
    "        # generated output can have \"max_tokens\" number of tokens  \n",
    "        max_tokens=MaxToken, \n",
    "        # number of outputs generated in one call \n",
    "        n=outputs \n",
    "    ) \n",
    "    # creating a list to store all the outputs \n",
    "    output = list() \n",
    "    for k in response['choices']: \n",
    "        output.append(k['text'].strip()) \n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "PROMPT = \"\"\"Write a story to inspire greatness, take the antagonist as a Rabbit and protagnist as turtle.  \n",
    "Let antagonist and protagnist compete against each other for a common goal.  \n",
    "Story should atmost have 3 lines.\"\"\"\n",
    "comp(PROMPT, MaxToken=3000, outputs=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701265b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat \n",
    "\n",
    "\n",
    "# function that takes in string argument as parameter \n",
    "def chat(MSGS, MaxToken=50, outputs=3): \n",
    "    # We use the Chat Completion endpoint for chat like inputs \n",
    "    response = openai.ChatCompletion.create( \n",
    "    # model used here is ChatGPT \n",
    "    # You can use all these models for this endpoint:  \n",
    "    # gpt-4, gpt-4-0314, gpt-4-32k, gpt-4-32k-0314,  \n",
    "    # gpt-3.5-turbo, gpt-3.5-turbo-0301 \n",
    "    model=\"gpt-3.5-turbo\", \n",
    "    messages=MSGS, \n",
    "    # max_tokens generated by the AI model \n",
    "    # maximu value can be 4096 tokens for \"gpt-3.5-turbo\"  \n",
    "    max_tokens = MaxToken, \n",
    "    # number of output variations to be generated by AI model \n",
    "    n = outputs, \n",
    "    ) \n",
    "    return response.choices[0].message \n",
    "  \n",
    "# Messages must consist of a collection of message objects,  \n",
    "# each of which includes a role (either \"system,\" \"user,\" or \"assistant\")  \n",
    "# and a content field for the message's actual text.  \n",
    "# Conversations might last only 1 message or span several pages. \n",
    "MSGS = [ \n",
    "        {\"role\": \"system\", \"content\": \"<message generated by system>\"}, \n",
    "        {\"role\": \"user\", \"content\": \"<message generated by user>\"}, \n",
    "        {\"role\": \"assistant\", \"content\": \"<message generated by assistant>\"} \n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd8887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image\n",
    "\n",
    "\n",
    "\n",
    "# importing other libraries \n",
    "import requests \n",
    "from PIL import Image \n",
    "from io import BytesIO\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# function for text-to-image generation  \n",
    "# using create endpoint of DALL-E API \n",
    "# function takes in a string argument \n",
    "def generate(text): \n",
    "  res = openai.Image.create( \n",
    "    # text describing the generated image \n",
    "    prompt=text, \n",
    "    # number of images to generate  \n",
    "    n=1, \n",
    "    # size of each generated image \n",
    "    size=\"256x256\", \n",
    "  ) \n",
    "  # returning the URL of one image as  \n",
    "  # we are generating only one image \n",
    "  return res[\"data\"][0][\"url\"]\n",
    "\n",
    "\n",
    "\n",
    "# prompt describing the desired image \n",
    "text = \"batman art in red and blue color\"\n",
    "# calling the custom function \"generate\" \n",
    "# saving the output in \"url1\" \n",
    "url1 = generate(text) \n",
    "# using requests library to get the image in bytes \n",
    "response = requests.get(url1, stream=True) \n",
    "# using the Image module from PIL library to view the image \n",
    "Image.open(response.raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad448b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04b6ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bca135",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
