{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Introduction\n",
    "    # Tensors are the primary data structure in PyTorch and are similar to multi-dimensional arrays.\n",
    "    # They are the fundamental building blocks for deep learning models and computations.\n",
    "    # Tensors can be scalars (0-dimensional), vectors (1-dimensional), matrices (2-dimensional), or higher-dimensional arrays.\n",
    "\n",
    "%whos #check all the variables in the workspace (%who; %who_ls)\n",
    "\n",
    "#Creating Tensors:\n",
    "    # You can create tensors using various methods, such as:\n",
    "    torch.Tensor() #Creates an uninitialized tensor.    \n",
    "    torch.tensor() #Creates a tensor from existing data. i.e torch.tensor(array)\n",
    "    torch.zeros(), torch.ones() #Creates tensors of zeros or ones.\n",
    "    torch.rand(3,4), torch.randn(2,2) #Creates tensors with random values.\n",
    "    # You can specify the data type and device (CPU or GPU) while creating tensors.\n",
    "\n",
    "#Creating random tensors\n",
    "tensor = torch.randn(3, 4, 3)\n",
    "    # The first dimension represents the number of \"blocks\" or \"chunks.\" In this case, we have 3 blocks.\n",
    "    # The second dimension represents the number of rows within each block. Here, we have 4 rows.\n",
    "    # The third dimension represents the number of columns within each row. We have 3 columns.\n",
    "    \n",
    "tensor = torch.randn(3, 4, 3, 2)\n",
    "    # The first dimension represents the number of blocks or chunks. We have 3 blocks.\n",
    "    # The second dimension represents the number of rows within each block. Here, we have 4 rows.\n",
    "    # The third dimension represents the number of columns within each row. We have 3 columns.\n",
    "    # The fourth dimension represents the depth or the number of elements in each cell. Each cell has 2 elements.\n",
    "\n",
    "tensor = torch.randn([3, 4, 3, 2, 5])\n",
    "    # The first dimension represents the number of \"blocks\" or \"chunks.\" In this case, we have 3 blocks.\n",
    "    # The second dimension represents the number of rows within each block. Here, we have 4 rows.\n",
    "    # The third dimension represents the number of columns within each row. We have 3 columns.\n",
    "    # The fourth dimension represents the depth or the number of elements in each cell. Each cell has 2 elements.\n",
    "    # The fifth dimension represents the number of values within each element. Each element has 5 values.\n",
    "\n",
    "# Create a 2D tensor with random values from a uniform distribution between 0 and 1\n",
    "random_tensor = torch.rand(3, 4)\n",
    "# Create a 3D tensor with random values from a normal distribution\n",
    "random_tensor = torch.randn(2, 3, 4)\n",
    "# Create a random tensor with dtype=torch.float32 and allocate it on the GPU\n",
    "random_tensor = torch.randn(3, 4, dtype=torch.float32, device='cuda')\n",
    "# Create a random tensor with integer values between 0 and 9\n",
    "random_tensor = torch.randint(10, size=(3, 4))\n",
    "\n",
    "# Set the random seed\n",
    "torch.manual_seed(42)\n",
    "\n",
    "#convert numpy to tensor\n",
    "arr = np.array([10,23,23])\n",
    "arr = torch.tensor(arr)\n",
    "\n",
    "def set_seeds(seed=1234):\n",
    "    \"\"\"Set seeds for reproducibility.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed) # multi-GPU\n",
    "\n",
    "# convert tensor to numpy\n",
    "tensor = torch.randint(10, size=(3, 3, 4))\n",
    "numpy_array = tensor.numpy()\n",
    "\n",
    "torch.zeros(3,4)    #create tensors with zeros\n",
    "torch.zeros_like(tensor)    #create tensors with zeros in the shape of tensor 'tensor'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensor Operations:\n",
    "\n",
    "# PyTorch provides a wide range of operations to manipulate tensors efficiently.\n",
    "# Element-wise operations: Addition, subtraction, multiplication, division, etc.\n",
    "# Reduction operations: Sum (torch.sum(a)), mean (torch.mean(a)), min, max, etc.\n",
    "# Matrix operations: Matrix multiplication (a @ b), dot product (torch.dot(a,b)), transpose (torch.transpose(a, 0, 1)), etc.\n",
    "# Indexing and slicing: Accessing specific elements or subsets of a tensor.\n",
    "# Broadcasting: Performing operations on tensors with different shapes.\n",
    "# Concatenation and stacking: Combining tensors along specified dimensions (torch.stack((a, b), dim=0 or 1); \n",
    "                                                                                # d = torch.cat((a, b.T), dim=1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU Acceleration\n",
    "\n",
    "# PyTorch supports GPU acceleration, which enables faster computations on compatible hardware.\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"GPU is available!\")\n",
    "    else:\n",
    "        print(\"GPU is not available.\")\n",
    "# You can move tensors to the GPU using .to(device) or .cuda() methods.\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # Checking GPU availability\n",
    "    x = torch.tensor([1, 2, 3]) # Creating a tensor\n",
    "    x = x.to(device)    # Moving the tensor to the GPU\n",
    "# Performing computations on the GPU can significantly speed up training deep learning models.\n",
    "    # Creating tensors on the GPU\n",
    "    a = torch.tensor([1, 2, 3], device=device)\n",
    "    b = torch.tensor([4, 5, 6], device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatic Differentiation:\n",
    "\n",
    "# PyTorch provides automatic differentiation, a powerful feature for training neural networks.\n",
    "# You can track the operations on tensors and compute gradients using the torch.autograd module.\n",
    "    # Creating a tensor with requires_grad=True\n",
    "    x = torch.tensor([2.0], requires_grad=True)\n",
    "# Gradients represent the derivative of a tensor with respect to another tensor.\n",
    "# Autograd allows you to compute gradients efficiently for backpropagation during training.\n",
    "\n",
    "\n",
    "\n",
    "#Torch provides a module, `autograd`, for automatically calculating the gradients of tensors. We can use it to \n",
    "# calculate the gradients of all our parameters with respect to the loss. Autograd works by keeping track of \n",
    "# operations performed on tensors, then going backwards through those operations, calculating gradients along the \n",
    "# way. To make sure PyTorch keeps track of operations on a tensor and calculates the gradients, you need to set \n",
    "# `requires_grad = True` on a tensor. You can do this at creation with the `requires_grad` keyword, or at any time \n",
    "# with `x.requires_grad_(True)`.\n",
    "x = torch.tensor([1.0, 2.0, 3.0], requires_grad=True)\n",
    "y = torch.tensor([4.0, 5.0, 6.0], requires_grad=True)\n",
    "z = x + y   # Perform operations\n",
    "output = torch.sum(z)\n",
    "#or\n",
    "x.requires_grad_(True)\n",
    "# Once you have computed the output tensor, you can call the backward() method on the output tensor to compute gradients\n",
    "output.backward()\n",
    "\n",
    "\n",
    "# You can turn off gradients for a block of code with the torch.no_grad()\n",
    ">>> with torch.no_grad():\n",
    "...     y = x * 2\n",
    ">>> y.requires_grad\n",
    "False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Tensor Manipulation:\n",
    "\n",
    "# PyTorch offers advanced tensor manipulation techniques for complex operations.\n",
    "# Reshaping tensors: Changing the shape or size of a tensor using view(), reshape(), or unsqueeze().\n",
    "# Tensor concatenation and splitting: Combining or splitting tensors along specified dimensions.\n",
    "    torch.cat(), torch.stack(), and torch.split()\n",
    "# Element-wise functions: Applying mathematical functions to each element of a tensor.\n",
    "    torch.sin(), torch.cos(), torch.exp(), torch.log()\n",
    "# Advanced indexing: Using advanced indexing techniques to access or modify specific elements of a tensor.\n",
    "# Broadcasting with advanced shapes: Handling tensors with different shapes during operations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network architecture by creating a custom model class that inherits from torch.nn.Module.\n",
    "\n",
    "# In the __init__ method, define the layers of your model using PyTorch's nn module. \n",
    "# This includes defining linear layers, activation functions, pooling layers, etc.\n",
    "\n",
    "# Implement the forward method to define the forward pass of your model. This method describes how the input flows \n",
    "# through the layers to produce an output.\n",
    "\n",
    "# NB:\n",
    "# Trainable Parameters in deep learning: weights, and biases\n",
    "#non-trainable parameters in deep learning: Hyperparameters, and Pretrained parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive') \n",
    "\n",
    "!pip install -q -r '/content/drive/MyDrive/Colab Notebooks/requirements.txt' "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.datasets import load_iris \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import KFold, train_test_split \n",
    "\n",
    "#deep learning libraries \n",
    "import torch.nn.init as init \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR \n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset \n",
    "import torchmetrics \n",
    "from torchsummary import summary "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sequential\n",
    "#Sequential API allows you to create a model by stacking layers on top of each other in a sequential manner\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#A simple feed-forward neural network with two hidden layers\n",
    "\n",
    "# Define the model architecture \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=64),  # First hidden layer\n",
    "    nn.ReLU(),                                    # Activation function\n",
    "    nn.Linear(in_features=64, out_features=32),   # Second hidden layer\n",
    "    nn.ReLU(),                                    # Activation function\n",
    "    nn.Linear(in_features=32, out_features=10)    # Output layer\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Custom function (Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paul_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(paul_model, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim \n",
    "        \n",
    "        self.fct1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fct2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.soft1 = nn.Softmax(dim=1)\n",
    "        self.fct3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fct1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fct2(x)\n",
    "        x = self.soft1(x)\n",
    "        x = self.fct3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the custom model\n",
    "input_dim = ...  # specify the input dimension\n",
    "hidden_dim = ...  # specify the hidden dimension\n",
    "output_dim = ...  # specify the output dimension\n",
    "model = paul_model(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# get output\n",
    "y_pred = model(X_train)\n",
    "\n",
    "model.state_dict()  #shows all the properties of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a custom function to define the model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom model class\n",
    "# Define your neural network architecture by creating a custom model class that inherits from torch.nn.Module.\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    # In the __init__ method, define the layers of your model using PyTorch's nn module. \n",
    "    # This includes defining linear layers, activation functions, pooling layers, etc.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  #first hidden layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   #second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)   #output layer\n",
    "\n",
    "# Implement the forward method to define the forward pass of your model. This method describes how the input flows \n",
    "# through the layers to produce an output.\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the custom model\n",
    "input_dim = ...  # specify the input dimension\n",
    "hidden_dim = ...  # specify the hidden dimension\n",
    "output_dim = ...  # specify the output dimension\n",
    "model = MyModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# get output\n",
    "y_pred = model(X_train)\n",
    "\n",
    "\n",
    "#Input features represents the number of features or variables in your input data\n",
    "#hidden features/dimensions represents the number of neurons in the hidden layers of your neural network\n",
    "#output dimension represents the number of neurons in the output layer of your neural network. \n",
    "    # If you have a multi-class classification problem with 10 classes, the output dimension would be 10\n",
    "    # In a regression dataset, the output dimension would typically be 1. This is because regression tasks \n",
    "        # involve predicting a continuous numerical value as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asides the Linear and ReLu, we have other subclasses of nn.Module that can be used to define different layers \n",
    "# and operations in your neural network\n",
    "\n",
    "# Convolutional Layers:\n",
    "nn.Conv1d: #1D convolutional layer for processing sequential data.\n",
    "nn.Conv2d: #2D convolutional layer for processing images or spatial data.\n",
    "nn.Conv3d: #3D convolutional layer for processing volumetric data.\n",
    "    \n",
    "# Pooling Layers:\n",
    "nn.MaxPool1d: #1D max pooling layer \n",
    "nn.MaxPool2d: #2D max pooling layer\n",
    "nn.MaxPool3d: #3D max pooling layer\n",
    "nn.AvgPool1d: #1D average pooling layer\n",
    "nn.AvgPool2d: #2D average pooling layer\n",
    "nn.AvgPool3d: #3D average pooling layer.\n",
    "    \n",
    "# Recurrent Layers:\n",
    "nn.RNN: #Basic RNN layer.\n",
    "nn.LSTM: #LSTM layer.\n",
    "nn.GRU: #GRU layer.\n",
    "\n",
    "# Normalization Layers:\n",
    "nn.BatchNorm1d: #Batch normalization layer for 1D inputs.\n",
    "nn.BatchNorm2d: #Batch normalization layer for 2D inputs.\n",
    "nn.BatchNorm3d: #Batch normalization layer for 3D inputs.\n",
    "\n",
    "# Dropout and Regularization:\n",
    "nn.Dropout: #Dropout layer for regularization.\n",
    "nn.Dropout2d: #2D dropout layer.\n",
    "nn.Dropout3d: #3D dropout layer.\n",
    "\n",
    "# Activation Functions:\n",
    "nn.Sigmoid: #Sigmoid activation function.\n",
    "nn.Tanh: #Hyperbolic tangent activation function.\n",
    "nn.Softmax: #Softmax activation function.\n",
    "nn.LeakyReLU: #Leaky ReLU activation function. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an example \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# Define the custom model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set the dimensions for the input, hidden, and output layers\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 3\n",
    "\n",
    "# Create an instance of the custom model\n",
    "model = CustomModel(input_dim, hidden_dim, output_dim)\n",
    "model.train()\n",
    "outputs = model(X_train) #\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid Function (nn.Sigmoid() or F.sigmoid())\n",
    "    # Range: (0, 1)\n",
    "    # Smooth, continuously differentiable function\n",
    "    # Used for 'binary classification' problems or when the output needs to be in the range of probabilities.\n",
    "    sigmoid_act = nn.Sigmoid()  #output = sigmoid_act(x)\n",
    "    output = F.sigmoid(x)\n",
    "\n",
    "# ReLU (Rectified Linear Unit) Function (nn.ReLU() or F.relu()) \n",
    "    # Range: [0, +∞)\n",
    "    # Simple and computationally efficient activation function\n",
    "    # Commonly used as a default choice for most deep learning models. often used for most hidden layers\n",
    "    relu = nn.ReLU()    #output = relu(x)\n",
    "    output = F.relu(x)\n",
    "\n",
    "# Leaky ReLU Function (nn.LeakyReLU() or F.leaky_relu())\n",
    "    # Range: (-∞, +∞)\n",
    "    # Similar to ReLU but allows small negative values for negative inputs\n",
    "    # Helps prevent \"dying ReLU\" problem by allowing a small gradient for negative inputs.\n",
    "    leaky_relu = nn.LeakyReLU(0.2)  # Set the negative slope.    output = leaky_relu(x)\n",
    "    output = F.leaky_relu(x, negative_slope=0.2)  # Set the negative slope\n",
    "\n",
    "# Tanh Function (nn.Tanh() or F.tanh())     - Hyperbolic tangent (tanh)\n",
    "    # Range: (-1, 1)\n",
    "    # S-shaped activation function that maps values between -1 and 1\n",
    "    # Used in some models as an alternative to sigmoid function.\n",
    "    tanh = nn.Tanh()    #output = tanh(x)\n",
    "    output = F.tanh(x)\n",
    "\n",
    "#Softmax Function (nn.Softmax(dim=) or F.softmax(dim=))\n",
    "    # Converts a vector of arbitrary real values into a probability distribution\n",
    "    # Typically used in the output layer for multi-class classification problems.\n",
    "    softmax = nn.Softmax(dim=1)  # Set the appropriate dimension\n",
    "    output = F.softmax(x, dim=1)  # Set the appropriate dimension\n",
    "    \n",
    "    #ln-softmax works fine on problems with a 'small number' of categories, \n",
    "    # or when categories are easily differentiable. But when categories are large, use log_softmax()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Hyperparameters/Metaparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common hyperparameters/metaparameters in deep learning include:\n",
    "\n",
    "#Model architecture:\n",
    "    # x \n",
    "\n",
    "# Learning rate: \n",
    "    # Determines the step size during gradient descent optimization and affects the convergence speed \n",
    "    # and accuracy of the model.\n",
    "\n",
    "# Number of hidden layers:  \n",
    "    # Determines the depth of the neural network architecture and influences the model's capacity to learn complex \n",
    "    # patterns.\n",
    "\n",
    "# Number of neurons per layer: \n",
    "    # Defines the width of the neural network architecture and affects the model's representational capacity and \n",
    "    # computational efficiency.\n",
    "\n",
    "# Activation functions: \n",
    "    # Determines the non-linear transformation applied to the output of each neuron, introducing non-linearity into \n",
    "    # the model.\n",
    "\n",
    "# Dropout rate: \n",
    "    # Controls the regularization technique of randomly dropping out a fraction of neurons during training, which\n",
    "    # helps prevent overfitting. \n",
    "\n",
    "# Batch size: \n",
    "    # Specifies the number of training samples propagated through the network before updating the model's weights.\n",
    "\n",
    "# Number of epochs: \n",
    "    # Specifies the number of times the entire training dataset is passed through the model during training.\n",
    "\n",
    "# Regularization techniques: \n",
    "    # Include methods like L1 and L2 regularization, which help prevent overfitting by adding penalties to the \n",
    "    # loss function. \n",
    "\n",
    "# Optimizer: \n",
    "    # Specifies the optimization algorithm used to update the model's weights during training, such as \n",
    "    # Stochastic Gradient Descent (SGD), Adam, or RMSprop.\n",
    "\n",
    "# Loss function: \n",
    "    # Defines the objective function used to measure the discrepancy between the predicted output and the \n",
    "    # true output during training.\n",
    "\n",
    "# Cross-validation sizes:\n",
    "    # x\n",
    "\n",
    "# Weight and Data Normalization:\n",
    "    # minmax normalization; z-score scaling\n",
    "    \n",
    "# Weight Initialization:\n",
    "    # xxxxx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Model Info/summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary \n",
    "\n",
    "model = ConvNet() #define your model\n",
    "summary(model, ( 28, 28))   #input tensor of size (28, 28) \n",
    "\n",
    "\n",
    "# Get the trainable parameters\n",
    "trainable_params = model.parameters()\n",
    "\n",
    "# Get the non-trainable parameters\n",
    "non_trainable_params = [p for p in model.parameters() if not p.requires_grad]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is a technique used in deep learning to prevent overfitting and improve the generalization ability \n",
    "# of the model.\n",
    "\n",
    "# Node Regularization: Modify the model (dropout)\n",
    "# Loss Regularization: Add a cost to the loss function (L1/2)\n",
    "# Data Regularization: Modify or add data (batch training, data augmentation, normalization) \n",
    "\n",
    "\n",
    "# L2 Regularization (Weight Decay):\n",
    "    # L2 regularization, also known as weight decay, adds a penalty term to the loss function that discourages large \n",
    "    # weights in the model.\n",
    "    # It helps prevent overfitting by encouraging the model to use smaller weights, effectively reducing the \n",
    "    # complexity of the model.\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.001)#Define the optimizer with weight decay\n",
    "\n",
    "# Dropout:\n",
    "    # Dropout is a regularization technique that randomly sets a fraction of the input units (nodes) to 0 during training.\n",
    "    # It helps prevent overfitting by introducing noise and reducing the interdependence of neurons.\n",
    "    #dropout is not a good idea for simple models or small data. \n",
    "    class MyModel(nn.Module):\n",
    "        def __init__(self):\n",
    "            super(MyModel, self).__init__()\n",
    "            self.fc1 = nn.Linear(64, 128)\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "            self.fc2 = nn.Linear(128, 10)\n",
    "        \n",
    "        def forward(self, x):\n",
    "            x = self.fc1(x)\n",
    "            x = self.dropout(x)  # Apply Dropout\n",
    "            x = torch.relu(x)\n",
    "            x = self.fc2(x)\n",
    "            return x \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.parameters(): This function returns an iterator over all the learnable parameters of the model. \n",
    "    # It is typically used when defining the optimizer to specify which parameters should be updated during training\n",
    "for param in model.parameters(): #for name, param in model.named_parameters(): #for both parameter name and tensor\n",
    "    print(param)\n",
    "\n",
    "# model.children(): This function returns an iterator over the immediate child modules of the model. It can be used, \n",
    "        # for example, to access and modify specific layers or modules within the model.\n",
    "for child in model.children():  #for name, child in model.named_children(): for both the child name and the module\n",
    "    print(child)\n",
    "\n",
    "# model.state_dict(): This function returns a dictionary containing the model's state, including the learnable \n",
    "        # parameters and buffers. It is commonly used for saving and loading model checkpoints.\n",
    "state_dict = model.state_dict()\n",
    "torch.save(state_dict, 'model_checkpoint.pth')\n",
    "\n",
    "# model.load_state_dict(): This method loads a state dictionary into the model, restoring the model's parameters \n",
    "        # and buffers from a saved checkpoint\n",
    "state_dict = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(state_dict)\n",
    "\n",
    "model.eval()    #sets the model to evaluation mode\n",
    "model.train()   #sets the model to training mode\n",
    "model.freeze()  #freezes all the parameters in the model, making them not trainable\n",
    "model.unfreeze()    #unfreezes all the parameters in the model, making them trainable\n",
    "\n",
    "#counting the number of parameters in the model:\n",
    "total_params = sum(p.numel() for p in model.parameters()) #.numel() is number of elements.\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "#Visualizing model architectures:\n",
    "from torchsummary import summary \n",
    "summary(model, input_size = ( 28, 28))   # Print a summary of the model architecture\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets and apply transformations\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='train_data/', transform=transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root='test_data/', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True, drop_last = False)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False, drop_last = False)\n",
    "                                        #drop_last wil drop the last batch if its not the same size as the rest"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> ANN Data (Use this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "inputs = data[:,:-1]\n",
    "labels = data[:,-1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels, random_state=23, train_size=0.9)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "# inputs \n",
    "X_train = torch.tensor(np.array(X_train), dtype=torch.float32)\n",
    "X_test = torch.tensor(np.array(X_test), dtype=torch.float32)\n",
    "y_train = torch.tensor(np.array(y_train), dtype=torch.long)\n",
    "y_test = torch.tensor(np.array(y_test), dtype=torch.long)\n",
    "\n",
    "#dataset \n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "#dataloader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "drop_last = True\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=test_dataset.tensors[0].shape[0]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "# From DataFrame  \n",
    "df = pd.read_csv('your_dataset.csv') #inputs, targets = load_iris(return_X_y = True, as_frame = True)\n",
    "\n",
    "# Extract the input features and target labels from the DataFrame\n",
    "inputs = df[['feature1', 'feature2', ...]].values\n",
    "targets = df['target'].values\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32, #use float for input data, and long for labels. \n",
    "                    device=None,    #can use 'cpu', or 'cuda'\n",
    "                    requires_grad=True) #or False\n",
    "targets = torch.tensor(targets, dtype=torch.long)   #or LongTensor for cuda\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(inputs, targets) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders in deep learning are utility classes or functions that help in efficiently loading and \n",
    "# processing training, validation, and testing data. They are an essential component of training deep learning models \n",
    "# and provide several benefits such as Data Batching, Data Shuffling, Data Augmentation, Data Transformation, \n",
    "# Efficient Memory Management, Parallel Data Loading.\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True) # Load the training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) #data loader \n",
    "                        # are used to move the input data and labels to a specified device (e.g., CPU or GPU) for computation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Folder Dataset\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.datasets import DatasetFolder\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensors\n",
    "])\n",
    "\n",
    "# Load the dataset from the image folders\n",
    "dataset = datasets.ImageFolder(root='Dataset/', transform=transform)\n",
    "\n",
    "\n",
    "# Create an instance of the DatasetFolder\n",
    "dataset = DatasetFolder(\n",
    "    root='Dataset/',\n",
    "    loader=torchvision.datasets.folder.default_loader,  # Use the default image loader\n",
    "    extensions=\".jpg\",  # Specify the file extensions of the images\n",
    "    transform=transform  # Apply the defined transformation pipeline\n",
    ")\n",
    "\n",
    "#CSV Dataset \n",
    "# Load the dataset from a CSV file\n",
    "dataset = datasets.CSVDataset(root='Dataset/', filename='data.csv', target_column='label',  # specify the CSV file and target column\n",
    "                            has_header=True,  # specify if the CSV file has a header row\n",
    "                            categorical_columns=[3, 4, 5],  # specify categorical columns (if any)\n",
    "                            continuous_columns=[0, 1, 2],  # specify continuous columns\n",
    "                            delimiter=',',  # specify the delimiter used in the CSV file\n",
    "                            transform=None)  # apply transformations if needed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom Dataset \n",
    "\n",
    "#Custom Dataset (from a root directory)\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Additional initialization logic goes here\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        # e.g., return len(self.data)\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve and preprocess a single sample from the dataset\n",
    "        # e.g., return self.transform(self.data[index]), self.labels[index]\n",
    "        pass\n",
    "\n",
    "dataset = CustomDataset(root_dir='Dataset/', transform=None)    # Create an instance of your custom dataset\n",
    "\n",
    "\n",
    "\n",
    "#Custom Dataset (from a dataframe with known input and output)\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_data = self.inputs[index]\n",
    "        label = self.labels[index]\n",
    "        return input_data, label\n",
    "\n",
    "# Assuming you have 'inputs' and 'labels' as your data\n",
    "dataset = CustomDataset(inputs, labels)\n",
    "\n",
    "# Create a data loader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data loader\n",
    "batch_size = 32\n",
    "shuffle = True\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last = False)\n",
    "\n",
    "#visualize the data in dataloader\n",
    "for inputs, labels in dataloader:\n",
    "    print(\"Inputs:\", inputs) \n",
    "    print(\"Labels:\", labels)\n",
    "    print()\n",
    "#or\n",
    "data_list = list(dataloader)\n",
    "print(data_list)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Scaling (Normalization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalization techniques are used to preprocess the input data in order to improve the training process and the \n",
    "# overall performance of deep learning models.\n",
    "\n",
    "#It is always a good idea to perform both data normalization (i.e z-score, minmax etc.) and batch normalization. \n",
    "\n",
    "# Batch Normalization:\n",
    "    # Batch Normalization is a technique that normalizes the inputs within each mini-batch during training.\n",
    "    #i.e It normalizes the activations within a mini-batch during training by adjusting the mean and standard deviation\n",
    "    # It helps stabilize and accelerate the training process by reducing internal covariate shift.\n",
    "    # should only be applied during training - use model.eval()\n",
    "    class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.bn1 = nn.BatchNorm1d(128)  # Batch Normalization\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)  # Apply Batch Normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x    \n",
    "    #  Batch Normalization requires a specified input size (e.g., nn.BatchNorm1d for 1D inputs, \n",
    "    # nn.BatchNorm2d for 2D inputs) depending on the dimensionality of the data\n",
    "\n",
    "\n",
    "#Layer Normalization:\n",
    "    # Layer Normalization is a technique that normalizes the inputs within each layer across the feature dimension.\n",
    "    # It helps improve the generalization ability of models and performs well on tasks with recurrent neural \n",
    "    # networks (RNNs).\n",
    "    class MyModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.ln1 = nn.LayerNorm(128)  # Layer Normalization\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.ln1(x)  # Apply Layer Normalization\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x    \n",
    "    # Layer Normalization normalizes across the feature dimension, so it doesn't require a specific input size\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function: \n",
    "    # Choose an appropriate loss function based on the problem you are solving. Common loss functions include \n",
    "        # mean squared error (MSE), binary cross-entropy, or categorical cross-entropy, depending on the task.\n",
    "    # Create an instance of the chosen loss function from torch.nn module.\n",
    "\n",
    "# Optimizer:\n",
    "    # Select an optimizer that will update the model's parameters during training. Popular choices include \n",
    "        # Stochastic Gradient Descent (SGD), Adam, or RMSprop.\n",
    "    # Initialize the optimizer by passing the model parameters and setting the learning rate and other hyperparameters.\n",
    "    \n",
    "# Training Loop:\n",
    "    # Iterate over the training dataset in batches.\n",
    "    # Zero the gradients of the model parameters to avoid accumulation.\n",
    "    # Pass the input batch through the model to obtain predictions.\n",
    "    # Calculate the loss between the predictions and the target values.\n",
    "    # Backpropagate the gradients by calling backward() on the loss tensor.\n",
    "    # Update the model parameters using the optimizer's step() function.\n",
    "    # Optionally, track and record metrics like accuracy or loss during training.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "#Regression \n",
    "loss_function = nn.MSELoss()  # Mean Squared Error loss: It is widely used in regression problems \n",
    "loss_function = nn.L1Loss() #Mean Absolute Error (MAE): It is often used in regression problems\n",
    "loss_function = nn.SmoothL1Loss() #Huber Loss: A robust loss function for regression problems that combines properties \n",
    "                                    #of both MSE and MAE. \n",
    "                                    \n",
    "#Classification \n",
    "loss_function = nn.CrossEntropyLoss( )  #Cross-Entropy Loss: It is commonly used in multi-class classification problems\n",
    "loss_function = nn.NLLLoss() #Negative log likelihood (NLL) loss: commonly used in multi-class classification problems\n",
    "            #The NLL loss is often used in combination with the nn.LogSoftmax() activation function for multi-class \n",
    "            # classification tasks.\n",
    "loss_function = nn.BCELoss() #Binary Cross-Entropy Loss: used in binary classification tasks, where the model's output \n",
    "                                #consists of probabilities instead of logits.\n",
    "loss_function = nn.BCEWithLogitsLoss() #Binary Cross-Entropy (BCE) Loss: It is commonly used in binary classification \n",
    "                                            #problems, where the model's output consists of logits \n",
    "                                            # (unbounded real numbers) rather than probabilities.\n",
    "                                            \n",
    "#Generative models\n",
    "loss_function = nn.KLDivLoss()  #Kullback-Leibler Divergence (KLD): Measuring the difference between two probability \n",
    "                                    #distributions, commonly used in generative models.\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_function(outputs, target_data)\n",
    "\n",
    "# Print the loss\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# Logits are the raw, unnormalized values produced by the model before applying any activation function like sigmoid or \n",
    "# softmax. They represent the model's predictions or scores for each class without being converted into probabilities \n",
    "# yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers play a crucial role in training neural networks by updating the model's parameters to minimize the \n",
    "# loss function\n",
    "\n",
    "# Stochastic Gradient Descent (SGD):\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# SGD is a classic optimization algorithm that updates the model parameters based on the gradients computed on \n",
    "# small subsets of the training data. It is great when all samples are similar to each other. \n",
    "# Stochastic gradient descent randomly picks a single sample to compute gradients and update parameters. \n",
    "\n",
    "# Adam:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Adam (Adaptive Moment Estimation) is an optimization algorithm that adapts the learning rate for each parameter \n",
    "# based on the estimates of the first and second moments of the gradients\n",
    "\n",
    "# Adagrad:\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "# Adagrad (Adaptive Gradient) is an optimization algorithm that adapts the learning rate for each parameter based on \n",
    "# the historical gradients for that parameter. It is often used in natural language processing tasks. \n",
    "\n",
    "#RMSprop:\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "# RMSprop (Root Mean Square Propagation) is an optimization algorithm that adapts the learning rate for each parameter \n",
    "# based on the moving average of squared gradients. It helps mitigate the diminishing learning rate problem\n",
    "\n",
    "# Adadelta:\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# Adadelta is an optimization algorithm that dynamically adapts the learning rate and accumulates only a limited \n",
    "# history of past gradients.\n",
    "\n",
    "\n",
    "#the most popular are: Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Commonly used optimizers in deep learning are based on gradient descents, with three types: batch, stochastic, \n",
    "# and mini-batch. Mini-batch is often used for a balance between the other two. Stochastic gradient descent is a \n",
    "# classic optimizer that applies mini-batch gradient descent with fixed step size.\n",
    "\n",
    "# Gradient descent has three types: batch, stochastic, and mini-batch.\n",
    "    # •\tBatch gradient descent uses the entire dataset to compute gradients and update parameters.\n",
    "    # •\tStochastic gradient descent randomly picks a single sample to compute gradients and update parameters.\n",
    "    # •\tMini-batch gradient descent randomly picks a subset of the dataset to compute gradients and update parameters.\n",
    "\n",
    "# NB:\t# Batch gradient descent considers entire database, slowing down training\n",
    "        # Stochastic gradient descent uses one sample, causing fluctuation and difficulty in reaching global minimum. \n",
    "        # Mini-batch gradient descent balances both.\n",
    "\n",
    "# Adaptive learning rates address issues with sparse data\n",
    "    # •\tLearning rate applied throughout training can be problematic\n",
    "    # •\tAdaptive learning rates scale based on inverse sum of squared gradient\n",
    "    \n",
    "#Adaptive learning rates are used in optimizers to dynamically adjust the step size (learning rate) during the training\n",
    "# process. The main reason for using adaptive learning rates is to improve the efficiency and effectiveness of the \n",
    "# optimization algorithm. They are beneficial because of their convergence speed, robustness to different scales, \n",
    "# handling sparse data, robustness to initial learning rate. AdaGrad, RMSprop and Adam use adaptive learning rate.\n",
    "\n",
    "# RMSprop(Root Mean Square Propagation) and Adam are popular optimization methods in deep learning\n",
    "    # •\tRMSprop scales the gradient and uses a moving average of the squared gradient\n",
    "    # •\tAdam combines AdaGrad(Adaptive Gradient Algorithm), RMSprop, and momentum methods into one\n",
    "\n",
    "# Comparison between ATOMS and RMSProp\n",
    "    # •\tATOMS corrects moment estimates for bias towards zero, making it easier to achieve good performance without \n",
    "        # tuning hyperparameters.\n",
    "    # •\tAdaptive learning methods like ATOMS are preferred for better convergence and results, but finding a good \n",
    "        # learning rate can also be effective.\n",
    "\n",
    "# Final Note: Adam and RMSprop are good starting points for adaptive learning weight methods.\n",
    "    # - Adam(Adaptive Moment Estimation) and RMSprop are extensions of SGD with momentum.\n",
    "    # - RMSprop is preferred for sparse data, while Adam is better for faster gradients.\n",
    "    # - Towards the end of optimization, Adam may be the best overall choice for adaptive learning weight methods. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop involves forward and backward pass to update weights\n",
    "    # Forward pass computes prediction\n",
    "    # Backward pass computes gradients, which are used to update weights\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# The training loop consists of two nested loops. The outer loop iterates over the specified number of epochs. \n",
    "# Inside the epoch loop, the model is set to train mode (model.train()) and the running loss is initialized. \n",
    "# Then, we iterate over the training data in batches using the train_loader.\n",
    "\n",
    "# For each batch, we perform the following steps:\n",
    "\n",
    "    # Zero the gradients using optimizer.zero_grad().\n",
    "    # Forward pass: Pass the input data through the model to obtain the predicted outputs.\n",
    "    # Compute the loss between the predicted outputs and the true labels.\n",
    "    # Backward pass: Compute the gradients of the loss with respect to the model parameters.\n",
    "    # Update the weights using the optimizer's step() method.\n",
    "    # Update the running loss by adding the current batch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model, optimizer, and loss function\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "train_epoch = 100\n",
    "losses = torch.zeros(train_epoch)\n",
    "ongoing_accuracy = [] \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(train_epoch):\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the running loss\n",
    "    running_loss = 0.0 \n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    # Iterate over the training dataset (training batches)\n",
    "    for inputs, labels in train_loader:\n",
    "\n",
    "        inputs, labels = inputs.to(device), labels.to(device) \n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        losses[epoch] = loss\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    accuracy = 100*torch.mean(((outputs>.5) == labels).float())   #Binary classification\n",
    "    ongoing_accuracy.append(accuracy) \n",
    "    print('Accuracy: ' accuracy) \n",
    "    # Print the loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{train_epoch}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Train classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binary classification\n",
    "    #see code above\n",
    "\n",
    "\n",
    "\n",
    "#multi-class classification\n",
    "\n",
    "# train the model \n",
    "loss_function = nn.CrossEntropyLoss()        # loss function \n",
    "optimizer =  torch.optim.SGD (model.parameters(), lr=0.05)  # optimizer\n",
    "train_epoch = 100\n",
    "losses = torch.zeros(train_epoch)\n",
    "ongoing_accuracy = [] \n",
    "\n",
    "# loop over the dataset multiple times\n",
    "for epoch in range(train_epoch):\n",
    "    running_loss = 0.0\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        inputs, labels = data\n",
    "        # inputs = inputs.float()  # Convert inputs to float type\n",
    "        # labels = labels.long()  # Convert labels to long type or LongTensor for cuda\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # zero the parameter gradients \n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        # forward + backward + optimize \n",
    "        y_pred = model(inputs) \n",
    "        loss = loss_function(y_pred, labels) \n",
    "        losses[epoch] = loss \n",
    "        \n",
    "        loss.backward() \n",
    "        optimizer.step() \n",
    "\n",
    "        running_loss += loss.item() \n",
    "        \n",
    "        # Calculate accuracy \n",
    "        predicted_labels = torch.argmax(y_pred, dim=1) \n",
    "        total_correct += torch.sum(predicted_labels == labels).item() \n",
    "        total_samples += labels.size(0) \n",
    "\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    ongoing_accuracy.append(accuracy)\n",
    "    print('Epoch: {}, Loss: {:.4f}, Accuracy: {:.2f}%'.format(epoch + 1, running_loss, accuracy))\n",
    "\n",
    "    print('Loss: {}'.format(running_loss) )\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "#y_pred_probabilities = F.softmax(y_pred, dim=1) #to view the probabilities by transforming into the softmax transfmtn.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Plots to visualize results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# report accuracy\n",
    "print('Model accuracy: %g%%' %accuracy)\n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(13,4))\n",
    "\n",
    "ax[0].plot(losses.detach())\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "ax[1].plot(ongoing_accuracy)\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_title('Accuracy')\n",
    "plt.show()\n",
    "# run training again to see whether this performance is consistent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Use this code for training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #define the model \n",
    "\n",
    "class iris_model(nn.Module):\n",
    "    \"\"\"Some Information about iris_model\"\"\"\n",
    "    def __init__(self, weight_init='default'):\n",
    "        super(iris_model, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 3)\n",
    "        \n",
    "        if weight_init == 'default':\n",
    "            pass  # Default weight initialization\n",
    "\n",
    "        elif weight_init == 'xavier_uniform':\n",
    "            self._init_weights_xavier_uniform()\n",
    "\n",
    "        elif weight_init == 'kaiming_normal':\n",
    "            self._init_weights_kaiming_normal()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.fc1(x))\n",
    "        # out = self.fca(out) \n",
    "        out = F.relu(self.fc2(out))\n",
    "        # out = self.fcb(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "\n",
    "    def _init_weights_xavier_uniform(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "    def _init_weights_kaiming_normal(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    init.constant_(m.bias, 0)\n",
    "\n",
    "model = iris_model()    #initializing the model  #model = iris_model(weight_init='xavier_uniform') \n",
    "\n",
    "\n",
    "\n",
    "# # A. create a class for the model \n",
    "# def create_model(nUnits, nLayers):\n",
    "#     class iris_model(nn.Module):\n",
    "#         def __init__(self):\n",
    "#             super().__init__()\n",
    "\n",
    "#             # create dictionary to store the layers\n",
    "#             self.layers = nn.ModuleDict()\n",
    "#             self.nLayers = nLayers \n",
    "\n",
    "#             ### input layer\n",
    "#             self.layers['input'] = nn.Linear(4, nUnits)\n",
    "            \n",
    "#             ### hidden layers\n",
    "#             for i in range(nLayers):\n",
    "#                 self.layers[f'hidden{i}'] = nn.Linear(nUnits, nUnits)\n",
    "\n",
    "#             ### output layer\n",
    "#             self.layers['output'] = nn.Linear(nUnits, 3)\n",
    "        \n",
    "#         # forward pass\n",
    "#         def forward(self, x):\n",
    "#             # input layer (note: the code in the video omits the relu after this layer)\n",
    "#             x = F.relu(self.layers['input'](x))\n",
    "\n",
    "#             # hidden layers\n",
    "#             for i in range(self.nLayers):\n",
    "#                 x = F.relu(self.layers[f'hidden{i}'](x))\n",
    "                \n",
    "#             # return output layer\n",
    "#             x = self.layers['output'](x)    #or x = F.sigmoid(self.layers['output](x)) for Binary classification \n",
    "#             return x \n",
    "        \n",
    "#     return iris_model() \n",
    "\n",
    "\n",
    "def create_model(nUnits, nLayers, weight_init):\n",
    "    class iris_model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # Create dictionary to store the layers\n",
    "            self.layers = nn.ModuleDict()\n",
    "            self.nLayers = nLayers \n",
    "            self.weight_init = weight_init\n",
    "\n",
    "            ### Input layer\n",
    "            self.layers['input'] = nn.Linear(4, nUnits)\n",
    "            self.layers['input_bn'] = nn.BatchNorm1d(nUnits)\n",
    "            self.layers['input_dropout'] = nn.Dropout(0.2)\n",
    "            \n",
    "            ### Hidden layers\n",
    "            for i in range(nLayers):\n",
    "                self.layers[f'hidden{i}'] = nn.Linear(nUnits, nUnits)\n",
    "                self.layers[f'hidden{i}_bn'] = nn.BatchNorm1d(nUnits)\n",
    "                self.layers[f'hidden{i}_dropout'] = nn.Dropout(0.2)\n",
    "\n",
    "            ### Output layer\n",
    "            self.layers['output'] = nn.Linear(nUnits, 3)\n",
    "        \n",
    "            # Initialize weights\n",
    "            self._initialize_weights()\n",
    "            \n",
    "        \n",
    "        # Forward pass\n",
    "        def forward(self, x):\n",
    "            # Input layer\n",
    "            x = self.layers['input'](x)\n",
    "            x = self.layers['input_bn'](x)\n",
    "            x = F.relu(x)\n",
    "            x = self.layers['input_dropout'](x)\n",
    "\n",
    "            # Hidden layers\n",
    "            for i in range(self.nLayers):\n",
    "                x = self.layers[f'hidden{i}'](x)\n",
    "                x = self.layers[f'hidden{i}_bn'](x)\n",
    "                x = F.relu(x)\n",
    "                x = self.layers[f'hidden{i}_dropout'](x)\n",
    "                \n",
    "            # Output layer\n",
    "            x = self.layers['output'](x)    #or x = F.sigmoid(self.layers['output](x)) for Binary classification \n",
    "            \n",
    "            return x \n",
    "        \n",
    "        def _initialize_weights(self):\n",
    "            for name, module in self.layers.items():\n",
    "                if isinstance(module, nn.Linear):\n",
    "                    weight_init = self.weight_init.get(name, 'default') #works well with sigmoid (uniform distribution)\n",
    "                    if weight_init == 'xavier_uniform':     #works well with sigmoid\n",
    "                        init.xavier_uniform_(module.weight)\n",
    "                    elif weight_init == 'kaiming_normal':   #works well with ReLU activation \n",
    "                        init.kaiming_normal_(module.weight)\n",
    "                    if module.bias is not None:\n",
    "                        init.constant_(module.bias, 0)\n",
    "    \n",
    "    return iris_model()\n",
    "\n",
    "nUnits = 64\n",
    "nLayers = 5\n",
    "weight_init = {\n",
    "    'input': 'default',\n",
    "    'hidden0': 'kaiming_normal',\n",
    "    'hidden1': 'kaiming_normal',\n",
    "    'hidden2': 'kaiming_normal',\n",
    "    # 'hidden3': 'kaiming_normal',\n",
    "    'output': 'default'\n",
    "}\n",
    "\n",
    "model = create_model(nUnits, nLayers, weight_init)    #initializing the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model training \n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "## metric = torchmetrics.Accuracy(task='multiclass', num_classes=num_classes)    (.Precision(), .Recall(), .F1Score(), .ConfusionMatrix())\n",
    "                #see doc. https://torchmetrics.readthedocs.io/en/stable/classification/accuracy.html#functional-interface \n",
    "                \n",
    "num_epochs = 300\n",
    "learning_rate = 0.01\n",
    "losses = torch.zeros(num_epochs)\n",
    "ongoing_accuracy = [] \n",
    "num_classes = 3 \n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Move the model and data to the appropriate device (e.g., GPU if available)\n",
    "model.to(device)\n",
    "\n",
    "# Loop over the dataset for multiple epochs \n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    batchAcc  = []\n",
    "    batchLoss = []\n",
    "    \n",
    "    # Iterate over the training dataloader (training batches)\n",
    "    for inputs, labels in train_dataloader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batchLoss.append(loss.item())\n",
    "\n",
    "        # Compute accuracy on the training set \n",
    "        predictions = torch.argmax(outputs, axis = 1) \n",
    "\n",
    "        accuracy = torchmetrics.functional.classification.accuracy(predictions, labels, task='multiclass', \n",
    "                                                            num_classes=num_classes) * 100 \n",
    "        # accuracy = torchmetrics.functional.classification.accuracy(predictions, labels, task='multiclass', num_classes=num_classes) \n",
    "        #                                                     (or metric(predictions, labels))\n",
    "        # accuracy = torchmetrics.functional.classification.binary_accuracy (predicted, labels, threshold = 0.5)   #for binary classification\n",
    "        # r2score = torchmetrics.functional.r2_score(preds, target) \n",
    "                \n",
    "        batchAcc.append(accuracy.item())\n",
    "        \n",
    "    # Update the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    ongoing_accuracy.append(np.mean(batchAcc)) \n",
    "    losses[epoch] = np.mean(batchLoss) \n",
    "\n",
    "    # Print loss and accuracy for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {np.mean(batchLoss):.4f}, Accuracy = {np.mean(batchAcc):.2f}%\") \n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# report accuracy\n",
    "print('Model accuracy: {:.2f}%'.format(ongoing_accuracy[-1]))  \n",
    "\n",
    "fig,ax = plt.subplots(1,2,figsize=(13,4))\n",
    "\n",
    "ax[0].plot(losses.detach())\n",
    "ax[0].set_ylabel('Loss')\n",
    "ax[0].set_xlabel('epoch')\n",
    "ax[0].set_title('Losses')\n",
    "\n",
    "ax[1].plot(ongoing_accuracy)\n",
    "ax[1].set_ylabel('accuracy')\n",
    "ax[1].set_xlabel('epoch')\n",
    "ax[1].set_title('Accuracy')\n",
    "plt.show()\n",
    "# run training again to see whether this performance is consistent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model evaluation\n",
    "\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Disable gradient computation for evaluation\n",
    "with torch.inference_mode():        #or torch.no_grad()\n",
    "    for data in test_dataloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Calculate predictions\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = torchmetrics.functional.classification.accuracy(predicted, labels, task='multiclass', num_classes=num_classes) \n",
    "\n",
    "print(f\"Accuracy on test set: {100 * accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from pathlib import Path \n",
    "\n",
    "# # Download helper functions from Learn PyTorch repo (if not already downloaded)\n",
    "# if Path(\"helper_functions.py\").is_file():\n",
    "#   print(\"helper_functions.py already exists, skipping download\")\n",
    "# else:\n",
    "#   print(\"Downloading helper_functions.py\")\n",
    "#   request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/helper_functions.py\")\n",
    "#   with open(\"helper_functions.py\", \"wb\") as f:\n",
    "#     f.write(request.content)\n",
    "\n",
    "# from helper_functions import plot_predictions, plot_decision_boundary "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Evaluating and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop:\n",
    "    # Evaluate the model on the validation dataset to monitor its performance and make any necessary adjustments.\n",
    "    # Pass the input batch through the model to obtain predictions.\n",
    "    # Calculate the validation loss and any desired evaluation metrics.\n",
    "    \n",
    "# Hyperparameter Tuning:\n",
    "    # Experiment with different learning rates, batch sizes, architectures, activation functions, \n",
    "        # regularization techniques, and optimizer settings.\n",
    "    # Use the validation set to evaluate different combinations of hyperparameters and choose the best-performing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "  for data, labels in test_loader:\n",
    "    # Forward pass\n",
    "    outputs = model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    # accuracy = (outputs.argmax(1) == labels).sum().item() / len(labels)\n",
    "    accuracy = 100*torch.mean(((outputs>.5) == labels).float())   #Binary classification\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to train mode\n",
    "model.train()\n",
    "\n",
    "best_loss = float('inf')  # Variable to track the best validation loss\n",
    "early_stopping_counter = 0  # Counter for early stopping\n",
    "early_stopping_patience = 3  # Number of epochs to wait before early stopping\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_accuracy = train_correct / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_accuracy = val_correct / len(val_dataset)\n",
    "\n",
    "    # Print epoch metrics\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the best model checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch > early_stopping_patience and val_loss >= best_loss:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter == early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "# Load the best model checkpoint\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Optimization (Hyperparameter Tuning)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto Tuning using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define your model\n",
    "class MyModel(nn.Module):\n",
    "    # Your model definition here\n",
    "\n",
    "# Define your objective function\n",
    "def objective(trial):\n",
    "    # Define your hyperparameters to be tuned\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1) \n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "    patience = trial.suggest_int('patience', 5, 20)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    patience = trial.suggest_int('patience', 5, 20)\n",
    "\n",
    "    # Define your model architecture with the hyperparameters\n",
    "    model = MyModel(input_size, hidden_size, num_layers, output_size, dropout_rate, activation)\n",
    "\n",
    "    # model = MyModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # if optimizer == 'adam':\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # else:\n",
    "    #     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    # Define your loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define your dataset and dataloader\n",
    "    train_dataset = MyDataset(train_data, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()   # Set the model to train mode\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, dim = 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    \n",
    "    # Report intermediate results to Optuna\n",
    "    trial.report(accuracy, epoch)\n",
    "\n",
    "    # Implement early stopping based on the patience parameter\n",
    "    if epoch - trial.best_trial.last_epoch > patience:\n",
    "        break\n",
    "\n",
    "    # Handle pruning based on the intermediate results\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    return accuracy\n",
    "\n",
    "# Define the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params     #or study.best_trial\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = MyModel(**best_params)\n",
    "# Train your model using the best hyperparameters\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "# Evaluate your best model on the test set\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", study.best_value)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Neurons vs Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A. create a class for the model \n",
    "def create_model(nUnits, nLayers):\n",
    "    class iris_model(nn.Module):\n",
    "        def __init__(self):\n",
    "            super().__init__()\n",
    "\n",
    "            # create dictionary to store the layers\n",
    "            self.layers = nn.ModuleDict()\n",
    "            self.nLayers = nLayers \n",
    "\n",
    "            ### input layer\n",
    "            self.layers['input'] = nn.Linear(4, nUnits)\n",
    "            \n",
    "            ### hidden layers\n",
    "            for i in range(nLayers):\n",
    "                self.layers[f'hidden{i}'] = nn.Linear(nUnits, nUnits)\n",
    "\n",
    "            ### output layer\n",
    "            self.layers['output'] = nn.Linear(nUnits, 3)\n",
    "        \n",
    "        # forward pass\n",
    "        def forward(self, x):\n",
    "            # input layer (note: the code in the video omits the relu after this layer)\n",
    "            x = F.relu(self.layers['input'](x))\n",
    "\n",
    "            # hidden layers\n",
    "            for i in range(self.nLayers):\n",
    "                x = F.relu(self.layers[f'hidden{i}'](x))\n",
    "                \n",
    "            # return output layer\n",
    "            x = self.layers['output'](x)\n",
    "            return x \n",
    "        \n",
    "    return iris_model() \n",
    "\n",
    "#B. Train the model (return 'final accuracy' and 'trainable parameters')\n",
    "def train_model():\n",
    "    loss_function = nn.CrossEntropyLoss()        # loss function \n",
    "    optimizer =  torch.optim.SGD (model.parameters(), lr=0.05)  # optimizer\n",
    "\n",
    "    # loop over the dataset multiple times\n",
    "    for epoch in range(train_epoch):\n",
    "        running_loss = 0.0\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        for i, data in enumerate(dataloader, 0):\n",
    "            inputs, labels = data\n",
    "            # inputs = inputs.float()  # Convert inputs to float type\n",
    "            # labels = labels.long()  # Convert labels to long type\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients \n",
    "            optimizer.zero_grad() \n",
    "\n",
    "            # forward + backward + optimize \n",
    "            y_pred = create_model(inputs) \n",
    "            loss = loss_function(y_pred, labels) \n",
    "            losses[epoch] = loss \n",
    "            \n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            running_loss += loss.item() \n",
    "            \n",
    "            # Calculate accuracy \n",
    "            predicted_labels = torch.argmax(y_pred, dim=1) \n",
    "            total_correct += torch.sum(predicted_labels == labels).item() \n",
    "            total_samples += labels.size(0) \n",
    "\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "    # total number of trainable parameters in the model\n",
    "    nParams = sum(p.numel() for p in theModel.parameters() if p.requires_grad)\n",
    "    \n",
    "    print('Finished Training')\n",
    "    return accuracy, nParams\n",
    "\n",
    "\n",
    "\n",
    "#C. describe the ranges of hyperparameters to be tesed\n",
    "numlayers = range(1,6)          # number of hidden layers\n",
    "numunits  = np.arange(4,101,3)  # number of nodes \n",
    "\n",
    "# initialize output matrices\n",
    "accuracies  = np.zeros((len(numunits),len(numlayers)))  #as a matrix\n",
    "totalparams = np.zeros((len(numunits),len(numlayers)))  #as a matrix\n",
    "\n",
    "# number of training epochs\n",
    "numepochs = 500\n",
    "\n",
    "\n",
    "#D start the experiment!\n",
    "for unitidx in range(len(numunits)):\n",
    "  for layeridx in range(len(numlayers)):\n",
    "\n",
    "    # create a fresh model instance\n",
    "    net = create_model(numunits[unitidx],numlayers[layeridx]) \n",
    "\n",
    "    # run the model and store the results\n",
    "    accuracy, nParams = train_model(net)\n",
    "    accuracies[unitidx,layeridx] = accuracy\n",
    "\n",
    "    # store the total number of parameters in the model\n",
    "    totalparams[unitidx,layeridx] = nParams\n",
    "\n",
    "\n",
    "#E. visualize \n",
    "# show accuracy as a function of model depth\n",
    "fig,ax = plt.subplots(1,figsize=(12,6))\n",
    "\n",
    "ax.plot(numunits,accuracies,'o-',markerfacecolor='w',markersize=9)\n",
    "ax.plot(numunits[[0,-1]],[33,33],'--',color=[.8,.8,.8])\n",
    "ax.plot(numunits[[0,-1]],[67,67],'--',color=[.8,.8,.8])\n",
    "ax.legend(numlayers)\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('Number of hidden units')\n",
    "ax.set_title('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Saving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Save the model checkpoint\n",
    "checkpoint = {\n",
    "    'epoch': 300,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss,\n",
    "    'learning_rate': optimizer.param_groups[0]['lr'],\n",
    "    'hyperparameters': {\n",
    "                'hidden_units': 64,\n",
    "                'batch_size': 32\n",
    "                        },\n",
    "    # 'other_info': 'Additional information about the checkpoint'\n",
    "}\n",
    "\n",
    "torch.save(checkpoint, 'model_checkpoint.pth')\n",
    "\n",
    "\n",
    "\n",
    "# Save the model without checkpoint information\n",
    "torch.save(model.state_dict(), 'model.pth')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you saved a checkpoint, you can load it as follows:\n",
    "\n",
    "checkpoint = torch.load('model_checkpoint.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "hidden_units = checkpoint['hyperparameters']['hidden_units']\n",
    "batch_size = checkpoint['hyperparameters']['batch_size']\n",
    "\n",
    "\n",
    "#If you saved just the model without the checkpoint information,\n",
    "model = MyModel()\n",
    "model.load_state_dict(torch.load('model.pth'))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paul_flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
