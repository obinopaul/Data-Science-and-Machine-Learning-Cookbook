{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your neural network architecture by creating a custom model class that inherits from torch.nn.Module.\n",
    "\n",
    "# In the __init__ method, define the layers of your model using PyTorch's nn module. \n",
    "# This includes defining linear layers, activation functions, pooling layers, etc.\n",
    "\n",
    "# Implement the forward method to define the forward pass of your model. This method describes how the input flows \n",
    "# through the layers to produce an output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sequential\n",
    "#Sequential API allows you to create a model by stacking layers on top of each other in a sequential manner\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "#A simple feed-forward neural network with two hidden layers\n",
    "\n",
    "# Define the model architecture \n",
    "model = nn.Sequential(\n",
    "    nn.Linear(in_features=784, out_features=64),  # First hidden layer\n",
    "    nn.ReLU(),                                    # Activation function\n",
    "    nn.Linear(in_features=64, out_features=32),   # Second hidden layer\n",
    "    nn.ReLU(),                                    # Activation function\n",
    "    nn.Linear(in_features=32, out_features=10)    # Output layer\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Custom function (Module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class paul_model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(paul_model, self).__init__()\n",
    "        self.fct1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fct2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.soft1 = nn.Softmax(dim=1)\n",
    "        self.fct3 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fct1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fct2(x)\n",
    "        x = self.soft1(x)\n",
    "        x = self.fct3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the custom model\n",
    "input_dim = ...  # specify the input dimension\n",
    "hidden_dim = ...  # specify the hidden dimension\n",
    "output_dim = ...  # specify the output dimension\n",
    "model = paul_model(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# get output\n",
    "y_pred = model(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a custom function to define the model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define a custom model class\n",
    "# Define your neural network architecture by creating a custom model class that inherits from torch.nn.Module.\n",
    "class MyModel(nn.Module):\n",
    "    \n",
    "    # In the __init__ method, define the layers of your model using PyTorch's nn module. \n",
    "    # This includes defining linear layers, activation functions, pooling layers, etc.\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)  #first hidden layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)   #second hidden layer\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)   #output layer\n",
    "\n",
    "# Implement the forward method to define the forward pass of your model. This method describes how the input flows \n",
    "# through the layers to produce an output.\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the custom model\n",
    "input_dim = ...  # specify the input dimension\n",
    "hidden_dim = ...  # specify the hidden dimension\n",
    "output_dim = ...  # specify the output dimension\n",
    "model = MyModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# get output\n",
    "y_pred = model(X_train)\n",
    "\n",
    "\n",
    "#Input features represents the number of features or variables in your input data\n",
    "#hidden features/dimensions represents the number of neurons in the hidden layers of your neural network\n",
    "#output dimension represents the number of neurons in the output layer of your neural network. \n",
    "    # If you have a multi-class classification problem with 10 classes, the output dimension would be 10\n",
    "    # In a regression dataset, the output dimension would typically be 1. This is because regression tasks \n",
    "        # involve predicting a continuous numerical value as the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#asides the Linear and ReLu, we have other subclasses of nn.Module that can be used to define different layers \n",
    "# and operations in your neural network\n",
    "\n",
    "# Convolutional Layers:\n",
    "nn.Conv1d: #1D convolutional layer for processing sequential data.\n",
    "nn.Conv2d: #2D convolutional layer for processing images or spatial data.\n",
    "nn.Conv3d: #3D convolutional layer for processing volumetric data.\n",
    "    \n",
    "# Pooling Layers:\n",
    "nn.MaxPool1d: #1D max pooling layer \n",
    "nn.MaxPool2d: #2D max pooling layer\n",
    "nn.MaxPool3d: #3D max pooling layer\n",
    "nn.AvgPool1d: #1D average pooling layer\n",
    "nn.AvgPool2d: #2D average pooling layer\n",
    "nn.AvgPool3d: #3D average pooling layer.\n",
    "    \n",
    "# Recurrent Layers:\n",
    "nn.RNN: #Basic RNN layer.\n",
    "nn.LSTM: #LSTM layer.\n",
    "nn.GRU: #GRU layer.\n",
    "\n",
    "# Normalization Layers:\n",
    "nn.BatchNorm1d: #Batch normalization layer for 1D inputs.\n",
    "nn.BatchNorm2d: #Batch normalization layer for 2D inputs.\n",
    "nn.BatchNorm3d: #Batch normalization layer for 3D inputs.\n",
    "\n",
    "# Dropout and Regularization:\n",
    "nn.Dropout: #Dropout layer for regularization.\n",
    "nn.Dropout2d: #2D dropout layer.\n",
    "nn.Dropout3d: #3D dropout layer.\n",
    "\n",
    "# Activation Functions:\n",
    "nn.Sigmoid: #Sigmoid activation function.\n",
    "nn.Tanh: #Hyperbolic tangent activation function.\n",
    "nn.Softmax: #Softmax activation function.\n",
    "nn.LeakyReLU: #Leaky ReLU activation function. \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#an example \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Preprocess the data\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "\n",
    "# Define the custom model\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n",
    "# Set the dimensions for the input, hidden, and output layers\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 16\n",
    "output_dim = 3\n",
    "\n",
    "# Create an instance of the custom model\n",
    "model = CustomModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# # Define the loss function and optimizer\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# # Training loop\n",
    "# num_epochs = 100\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     # Forward pass\n",
    "#     outputs = model(X_train)\n",
    "#     loss = criterion(outputs, y_train)\n",
    "    \n",
    "#     # Backward pass and optimization\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "    \n",
    "#     # Print the loss for every 10 epochs\n",
    "#     if (epoch+1) % 10 == 0:\n",
    "#         print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}\")\n",
    "\n",
    "# # Test the model\n",
    "# X_test = torch.Tensor(X_test)\n",
    "# y_test = torch.LongTensor(y_test)\n",
    "# with torch.no_grad():\n",
    "#     outputs = model(X_test)\n",
    "#     _, predicted = torch.max(outputs.data, 1)\n",
    "#     accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "#     print(f\"Test Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some common hyperparameters in deep learning include:\n",
    "\n",
    "# Learning rate: \n",
    "    # Determines the step size during gradient descent optimization and affects the convergence speed \n",
    "    # and accuracy of the model.\n",
    "\n",
    "# Number of hidden layers: \n",
    "    # Determines the depth of the neural network architecture and influences the model's capacity to learn complex \n",
    "    # patterns.\n",
    "\n",
    "# Number of neurons per layer: \n",
    "    # Defines the width of the neural network architecture and affects the model's representational capacity and \n",
    "    # computational efficiency.\n",
    "\n",
    "# Activation functions: \n",
    "    # Determines the non-linear transformation applied to the output of each neuron, introducing non-linearity into \n",
    "    # the model.\n",
    "\n",
    "# Dropout rate: \n",
    "    # Controls the regularization technique of randomly dropping out a fraction of neurons during training, which\n",
    "    # helps prevent overfitting. \n",
    "\n",
    "# Batch size: \n",
    "    # Specifies the number of training samples propagated through the network before updating the model's weights.\n",
    "\n",
    "# Number of epochs: \n",
    "    # Specifies the number of times the entire training dataset is passed through the model during training.\n",
    "\n",
    "# Regularization techniques: \n",
    "    # Include methods like L1 and L2 regularization, which help prevent overfitting by adding penalties to the \n",
    "    # loss function.\n",
    "\n",
    "# Optimizer: \n",
    "    # Specifies the optimization algorithm used to update the model's weights during training, such as \n",
    "    # Stochastic Gradient Descent (SGD), Adam, or RMSprop.\n",
    "\n",
    "# Loss function: \n",
    "    # Defines the objective function used to measure the discrepancy between the predicted output and the \n",
    "    # true output during training."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load datasets and apply transformations\n",
    "train_dataset = torchvision.datasets.ImageFolder(root='train_data/', transform=transform)\n",
    "test_dataset = torchvision.datasets.ImageFolder(root='test_data/', transform=transform)\n",
    "\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Custom Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset into a DataFrame\n",
    "df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Extract the input features and target labels from the DataFrame\n",
    "inputs = df[['feature1', 'feature2', ...]].values\n",
    "targets = df['target'].values\n",
    "\n",
    "# Convert the data to PyTorch tensors\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "targets = torch.tensor(targets, dtype=torch.long)\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders in deep learning are utility classes or functions that help in efficiently loading and \n",
    "# processing training, validation, and testing data. They are an essential component of training deep learning models \n",
    "# and provide several benefits such as Data Batching, Data Shuffling, Data Augmentation, Data Transformation, \n",
    "# Efficient Memory Management, Parallel Data Loading.\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True) # Load the training data\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True) #data loader \n",
    "                        # are used to move the input data and labels to a specified device (e.g., CPU or GPU) for computation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Folder Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the transformation to apply to the images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to a fixed size\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize image tensors\n",
    "])\n",
    "\n",
    "# Load the dataset from the image folders\n",
    "dataset = datasets.ImageFolder(root='Dataset/', transform=transform)\n",
    "\n",
    "# Use the dataset in your model or create a data loader\n",
    "\n",
    "\n",
    "\n",
    "#CSV Dataset\n",
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# Load the dataset from a CSV file\n",
    "dataset = datasets.CSVDataset(root='Dataset/', filename='data.csv', target_column='label',  # specify the CSV file and target column\n",
    "                              has_header=True,  # specify if the CSV file has a header row\n",
    "                              categorical_columns=[3, 4, 5],  # specify categorical columns (if any)\n",
    "                              continuous_columns=[0, 1, 2],  # specify continuous columns\n",
    "                              delimiter=',',  # specify the delimiter used in the CSV file\n",
    "                              transform=None)  # apply transformations if needed\n",
    "\n",
    "# Use the dataset in your model or create a data loader\n",
    "\n",
    "\n",
    "#Custom Dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        # Additional initialization logic goes here\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of samples in the dataset\n",
    "        # e.g., return len(self.data)\n",
    "        pass\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve and preprocess a single sample from the dataset\n",
    "        # e.g., return self.transform(self.data[index]), self.labels[index]\n",
    "        pass\n",
    "\n",
    "# Create an instance of your custom dataset\n",
    "dataset = CustomDataset(root_dir='Dataset/', transform=None)\n",
    "\n",
    "# Use the dataset in your model or create a data loader\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function: \n",
    "    # Choose an appropriate loss function based on the problem you are solving. Common loss functions include \n",
    "        # mean squared error (MSE), binary cross-entropy, or categorical cross-entropy, depending on the task.\n",
    "    # Create an instance of the chosen loss function from torch.nn module.\n",
    "\n",
    "# Optimizer:\n",
    "    # Select an optimizer that will update the model's parameters during training. Popular choices include \n",
    "        # Stochastic Gradient Descent (SGD), Adam, or RMSprop.\n",
    "    # Initialize the optimizer by passing the model parameters and setting the learning rate and other hyperparameters.\n",
    "    \n",
    "# Training Loop:\n",
    "    # Iterate over the training dataset in batches.\n",
    "    # Zero the gradients of the model parameters to avoid accumulation.\n",
    "    # Pass the input batch through the model to obtain predictions.\n",
    "    # Calculate the loss between the predictions and the target values.\n",
    "    # Backpropagate the gradients by calling backward() on the loss tensor.\n",
    "    # Update the model parameters using the optimizer's step() function.\n",
    "    # Optionally, track and record metrics like accuracy or loss during training.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "\n",
    "#Regression \n",
    "loss_function = nn.MSELoss()  # Mean Squared Error loss: It is widely used in regression problems \n",
    "loss_function = nn.L1Loss() #Mean Absolute Error (MAE): It is often used in regression problems\n",
    "loss_function = nn.SmoothL1Loss() #Huber Loss: A robust loss function for regression problems that combines properties \n",
    "                                    #of both MSE and MAE. \n",
    "                                    \n",
    "#Classification \n",
    "loss_function = nn.CrossEntropyLoss( )  #Cross-Entropy Loss: It is commonly used in multi-class classification problems\n",
    "loss_function = nn.BCELoss() #Binary Cross-Entropy Loss: used in binary classification tasks, where the model's output \n",
    "                                #consists of probabilities instead of logits.\n",
    "loss_function = nn.BCEWithLogitsLoss() #Binary Cross-Entropy (BCE) Loss: It is commonly used in binary classification \n",
    "                                            #problems, where the model's output consists of logits \n",
    "                                            # (unbounded real numbers) rather than probabilities.\n",
    "                                            \n",
    "#Generative models\n",
    "loss_function = nn.KLDivLoss()  #Kullback-Leibler Divergence (KLD): Measuring the difference between two probability \n",
    "                                    #distributions, commonly used in generative models.\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the loss\n",
    "loss = loss_function(outputs, target_data)\n",
    "\n",
    "# Print the loss\n",
    "print(\"Loss:\", loss.item())\n",
    "\n",
    "\n",
    "# Logits are the raw, unnormalized values produced by the model before applying any activation function like sigmoid or \n",
    "# softmax. They represent the model's predictions or scores for each class without being converted into probabilities \n",
    "# yet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers play a crucial role in training neural networks by updating the model's parameters to minimize the \n",
    "# loss function\n",
    "\n",
    "# Stochastic Gradient Descent (SGD):\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "# SGD is a classic optimization algorithm that updates the model parameters based on the gradients computed on \n",
    "# small subsets of the training data\n",
    "\n",
    "# Adam:\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "# Adam (Adaptive Moment Estimation) is an optimization algorithm that adapts the learning rate for each parameter \n",
    "# based on the estimates of the first and second moments of the gradients\n",
    "\n",
    "# Adagrad:\n",
    "optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "# Adagrad (Adaptive Gradient) is an optimization algorithm that adapts the learning rate for each parameter based on \n",
    "# the historical gradients for that parameter. It is often used in natural language processing tasks. \n",
    "\n",
    "#RMSprop:\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "# RMSprop (Root Mean Square Propagation) is an optimization algorithm that adapts the learning rate for each parameter \n",
    "# based on the moving average of squared gradients. It helps mitigate the diminishing learning rate problem\n",
    "\n",
    "# Adadelta:\n",
    "optimizer = torch.optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "# Adadelta is an optimization algorithm that dynamically adapts the learning rate and accumulates only a limited \n",
    "# history of past gradients.\n",
    "\n",
    "\n",
    "#the most popular are: Stochastic Gradient Descent (SGD), Adam, RMSprop, and Adagrad.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training loop consists of two nested loops. The outer loop iterates over the specified number of epochs. \n",
    "# Inside the epoch loop, the model is set to train mode (model.train()) and the running loss is initialized. \n",
    "# Then, we iterate over the training data in batches using the train_loader.\n",
    "\n",
    "# For each batch, we perform the following steps:\n",
    "\n",
    "    # Zero the gradients using optimizer.zero_grad().\n",
    "    # Forward pass: Pass the input data through the model to obtain the predicted outputs.\n",
    "    # Compute the loss between the predicted outputs and the true labels.\n",
    "    # Backward pass: Compute the gradients of the loss with respect to the model parameters.\n",
    "    # Update the weights using the optimizer's step() method.\n",
    "    # Update the running loss by adding the current batch loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your model, optimizer, and loss function\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    # Set the model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Initialize the running loss\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over the training dataset\n",
    "    for inputs, labels in train_loader:\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Compute the loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the running loss\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Compute the average loss for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    \n",
    "    # Print the loss for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Evaluating and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation Loop:\n",
    "    # Evaluate the model on the validation dataset to monitor its performance and make any necessary adjustments.\n",
    "    # Pass the input batch through the model to obtain predictions.\n",
    "    # Calculate the validation loss and any desired evaluation metrics.\n",
    "    \n",
    "# Hyperparameter Tuning:\n",
    "    # Experiment with different learning rates, batch sizes, architectures, activation functions, \n",
    "        # regularization techniques, and optimizer settings.\n",
    "    # Use the validation set to evaluate different combinations of hyperparameters and choose the best-performing ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model Evaluation\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "  for data, labels in test_loader:\n",
    "    # Forward pass\n",
    "    outputs = model(data)\n",
    "\n",
    "    # Calculate loss\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = (outputs.argmax(1) == labels).sum().item() / len(labels)\n",
    "\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to train mode\n",
    "model.train()\n",
    "\n",
    "best_loss = float('inf')  # Variable to track the best validation loss\n",
    "early_stopping_counter = 0  # Counter for early stopping\n",
    "early_stopping_patience = 3  # Number of epochs to wait before early stopping\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_dataset)\n",
    "    train_accuracy = train_correct / len(train_dataset)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    val_loss /= len(val_dataset)\n",
    "    val_accuracy = val_correct / len(val_dataset)\n",
    "\n",
    "    # Print epoch metrics\n",
    "    print(f\"Epoch: {epoch+1}/{num_epochs}\")\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Train Accuracy: {train_accuracy:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f} | Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Save the best model checkpoint\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    # Early stopping\n",
    "    if epoch > early_stopping_patience and val_loss >= best_loss:\n",
    "        early_stopping_counter += 1\n",
    "        if early_stopping_counter == early_stopping_patience:\n",
    "            print(\"Early stopping triggered!\")\n",
    "            break\n",
    "    else:\n",
    "        early_stopping_counter = 0\n",
    "\n",
    "# Load the best model checkpoint\n",
    "model.load_state_dict(torch.load('best_model.pt'))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Hyperparameter Tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Define your model\n",
    "class MyModel(nn.Module):\n",
    "    # Your model definition here\n",
    "\n",
    "# Define your objective function\n",
    "def objective(trial):\n",
    "    # Define your hyperparameters to be tuned\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    num_epochs = trial.suggest_int('num_epochs', 10, 30)\n",
    "    patience = trial.suggest_int('patience', 5, 20)\n",
    "    weight_decay = trial.suggest_loguniform('weight_decay', 1e-6, 1e-3)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [64, 128, 256])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    # batch_size = trial.suggest_categorical('batch_size', [16, 32, 64])\n",
    "    optimizer = trial.suggest_categorical('optimizer', ['adam', 'sgd'])\n",
    "    activation = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh'])\n",
    "    patience = trial.suggest_int('patience', 5, 20)\n",
    "\n",
    "    # Define your model architecture with the hyperparameters\n",
    "    model = MyModel(input_size, hidden_size, num_layers, output_size, dropout_rate, activation)\n",
    "\n",
    "    # model = MyModel()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # if optimizer == 'adam':\n",
    "    #     optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    # else:\n",
    "    #     optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "\n",
    "    # Define your loss function\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Define your dataset and dataloader\n",
    "    train_dataset = MyDataset(train_data, train_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()   # Set the model to train mode\n",
    "        for inputs, targets in train_loader:\n",
    "            optimizer.zero_grad()   # Zero the gradients\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_function(outputs, targets)\n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, dim = 1)\n",
    "            predictions.extend(predicted.tolist())\n",
    "\n",
    "    accuracy = accuracy_score(test_labels, predictions)\n",
    "    \n",
    "    # Report intermediate results to Optuna\n",
    "    trial.report(accuracy, epoch)\n",
    "\n",
    "    # Implement early stopping based on the patience parameter\n",
    "    if epoch - trial.best_trial.last_epoch > patience:\n",
    "        break\n",
    "\n",
    "    # Handle pruning based on the intermediate results\n",
    "    if trial.should_prune():\n",
    "        raise optuna.TrialPruned()\n",
    "    return accuracy\n",
    "\n",
    "# Define the study\n",
    "study = optuna.create_study(direction='maximize')\n",
    "\n",
    "# Run the optimization\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params     #or study.best_trial\n",
    "\n",
    "# Train the model with the best hyperparameters\n",
    "best_model = MyModel(**best_params)\n",
    "# Train your model using the best hyperparameters\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "# Evaluate your best model on the test set\n",
    "\n",
    "# Print the best hyperparameters and the best score\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "print(\"Best Score:\", study.best_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Saving and Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import resnet50  # or any other pre-defined model architecture you want to use \n",
    "\n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model_checkpoint.pth')\n",
    "\n",
    "# Load the model checkpoint\n",
    "model.load_state_dict(torch.load('model_checkpoint.pth'))\n",
    "#or\n",
    "model = resnet50(pretrained=True) \n",
    "model.load_state_dict(torch.load('model.pth'))\n",
    "\n",
    "\n",
    "# Load the pre-trained model\n",
    "model = torch.load('obinopaul/model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paul_flask",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
