{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f42406d0",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e164ac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd          # data analysis library for handling structured data\n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "# import pandas_profiling\n",
    "import time \n",
    "import math, copy\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "# plt.style.use('./deeplearning.mplstyle')\n",
    "# from lab_utils_uni import plt_house_x, plt_contour_wgrad, plt_divergence, plt_gradients \n",
    "\n",
    "# Machine Learning \n",
    "\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6746e081",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Cornel\\\\Documents\\\\5. Projects\\\\Smart Home\\\\Smart Home Dataset\\\\Smart Home Dataset with weather Information\\\\HomeC.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m smart_home \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39mr\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mCornel\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39m5. Projects\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSmart Home\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSmart Home Dataset\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mSmart Home Dataset with weather Information\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mHomeC.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, low_memory\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1733\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1734\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1735\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1736\u001b[0m     f,\n\u001b[0;32m   1737\u001b[0m     mode,\n\u001b[0;32m   1738\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1739\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1740\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1741\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1742\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1743\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1744\u001b[0m )\n\u001b[0;32m   1745\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\Cornel\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    851\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    852\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    853\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    854\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    855\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    857\u001b[0m             handle,\n\u001b[0;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    859\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    860\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    861\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    862\u001b[0m         )\n\u001b[0;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Cornel\\\\Documents\\\\5. Projects\\\\Smart Home\\\\Smart Home Dataset\\\\Smart Home Dataset with weather Information\\\\HomeC.csv'"
     ]
    }
   ],
   "source": [
    "smart_home = pd.read_csv(r\"C:\\Users\\Cornel\\Documents\\5. Projects\\Smart Home\\Smart Home Dataset\\Smart Home Dataset with weather Information\\HomeC.csv\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c276165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>use [kW]</th>\n",
       "      <th>gen [kW]</th>\n",
       "      <th>House overall [kW]</th>\n",
       "      <th>Dishwasher [kW]</th>\n",
       "      <th>Furnace 1 [kW]</th>\n",
       "      <th>Furnace 2 [kW]</th>\n",
       "      <th>Home office [kW]</th>\n",
       "      <th>Fridge [kW]</th>\n",
       "      <th>Wine cellar [kW]</th>\n",
       "      <th>Garage door [kW]</th>\n",
       "      <th>Kitchen 12 [kW]</th>\n",
       "      <th>Kitchen 14 [kW]</th>\n",
       "      <th>Kitchen 38 [kW]</th>\n",
       "      <th>Barn [kW]</th>\n",
       "      <th>Well [kW]</th>\n",
       "      <th>Microwave [kW]</th>\n",
       "      <th>Living room [kW]</th>\n",
       "      <th>Solar [kW]</th>\n",
       "      <th>temperature</th>\n",
       "      <th>icon</th>\n",
       "      <th>humidity</th>\n",
       "      <th>visibility</th>\n",
       "      <th>summary</th>\n",
       "      <th>apparentTemperature</th>\n",
       "      <th>pressure</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>cloudCover</th>\n",
       "      <th>windBearing</th>\n",
       "      <th>precipIntensity</th>\n",
       "      <th>dewPoint</th>\n",
       "      <th>precipProbability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1451624400</td>\n",
       "      <td>0.932833</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>0.932833</td>\n",
       "      <td>0.000033</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.061917</td>\n",
       "      <td>0.442633</td>\n",
       "      <td>0.124150</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031350</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.001517</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>36.14</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>29.26</td>\n",
       "      <td>1016.91</td>\n",
       "      <td>9.18</td>\n",
       "      <td>cloudCover</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1451624401</td>\n",
       "      <td>0.934333</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.934333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020717</td>\n",
       "      <td>0.063817</td>\n",
       "      <td>0.444067</td>\n",
       "      <td>0.124000</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.013117</td>\n",
       "      <td>0.000417</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>36.14</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>29.26</td>\n",
       "      <td>1016.91</td>\n",
       "      <td>9.18</td>\n",
       "      <td>cloudCover</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1451624402</td>\n",
       "      <td>0.931817</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>0.931817</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.020700</td>\n",
       "      <td>0.062317</td>\n",
       "      <td>0.446067</td>\n",
       "      <td>0.123533</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.013083</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000167</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.031517</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.003467</td>\n",
       "      <td>36.14</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>29.26</td>\n",
       "      <td>1016.91</td>\n",
       "      <td>9.18</td>\n",
       "      <td>cloudCover</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1451624403</td>\n",
       "      <td>1.022050</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>1.022050</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.106900</td>\n",
       "      <td>0.068517</td>\n",
       "      <td>0.446583</td>\n",
       "      <td>0.123133</td>\n",
       "      <td>0.006983</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.000433</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031500</td>\n",
       "      <td>0.001017</td>\n",
       "      <td>0.004067</td>\n",
       "      <td>0.001617</td>\n",
       "      <td>0.003483</td>\n",
       "      <td>36.14</td>\n",
       "      <td>clear-night</td>\n",
       "      <td>0.62</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Clear</td>\n",
       "      <td>29.26</td>\n",
       "      <td>1016.91</td>\n",
       "      <td>9.18</td>\n",
       "      <td>cloudCover</td>\n",
       "      <td>282.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>24.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         time  use [kW]  gen [kW]  House overall [kW]  Dishwasher [kW]  Furnace 1 [kW]  Furnace 2 [kW]  Home office [kW]  Fridge [kW]  Wine cellar [kW]  Garage door [kW]  Kitchen 12 [kW]  Kitchen 14 [kW]  Kitchen 38 [kW]  Barn [kW]  Well [kW]  Microwave [kW]  Living room [kW]  Solar [kW]  temperature         icon  humidity  visibility summary  apparentTemperature  pressure  windSpeed  cloudCover  windBearing  precipIntensity  dewPoint  precipProbability\n",
       "0  1451624400  0.932833  0.003483            0.932833         0.000033        0.020700        0.061917          0.442633     0.124150          0.006983          0.013083         0.000417         0.000150         0.000000   0.031350   0.001017        0.004067          0.001517    0.003483        36.14  clear-night      0.62        10.0   Clear                29.26   1016.91       9.18  cloudCover        282.0              0.0      24.4                0.0\n",
       "1  1451624401  0.934333  0.003467            0.934333         0.000000        0.020717        0.063817          0.444067     0.124000          0.006983          0.013117         0.000417         0.000150         0.000000   0.031500   0.001017        0.004067          0.001650    0.003467        36.14  clear-night      0.62        10.0   Clear                29.26   1016.91       9.18  cloudCover        282.0              0.0      24.4                0.0\n",
       "2  1451624402  0.931817  0.003467            0.931817         0.000017        0.020700        0.062317          0.446067     0.123533          0.006983          0.013083         0.000433         0.000167         0.000017   0.031517   0.001000        0.004067          0.001650    0.003467        36.14  clear-night      0.62        10.0   Clear                29.26   1016.91       9.18  cloudCover        282.0              0.0      24.4                0.0\n",
       "3  1451624403  1.022050  0.003483            1.022050         0.000017        0.106900        0.068517          0.446583     0.123133          0.006983          0.013000         0.000433         0.000217         0.000000   0.031500   0.001017        0.004067          0.001617    0.003483        36.14  clear-night      0.62        10.0   Clear                29.26   1016.91       9.18  cloudCover        282.0              0.0      24.4                0.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smart_home.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7d0dbb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if df[col].dtype == 'object':\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    return df\n",
    "\n",
    "smart_home = replace_non_numeric(smart_home, columns= ['time', 'use [kW]', 'gen [kW]', 'House overall [kW]', 'Dishwasher [kW]',\n",
    "       'Furnace 1 [kW]', 'Furnace 2 [kW]', 'Home office [kW]', 'Fridge [kW]',\n",
    "       'Wine cellar [kW]', 'Garage door [kW]', 'Kitchen 12 [kW]',\n",
    "       'Kitchen 14 [kW]', 'Kitchen 38 [kW]', 'Barn [kW]', 'Well [kW]',\n",
    "       'Microwave [kW]', 'Living room [kW]', 'Solar [kW]', 'temperature',\n",
    "       'humidity', 'visibility', 'apparentTemperature',\n",
    "       'pressure', 'windSpeed', 'cloudCover', 'windBearing', 'precipIntensity',\n",
    "       'dewPoint', 'precipProbability'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7a180009",
   "metadata": {},
   "source": [
    "### Scikit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a9daba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Built-in datasets. \n",
    "from sklearn.datasets import load_boston #Sklearn built-in dataset (load_boston, load_breast_cancer etc.)\n",
    "X, y = load_boston (return_X_y= True)\n",
    "\n",
    "#Sklearn Models\n",
    "#from sklearn.\"sub-module\" import \"model\"\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "    #steps  \n",
    "    1. create your model object\n",
    "        mod = KNeighborsRegressor() \n",
    "    2. Learn from your Data\n",
    "        mod.fit(X, y)   \n",
    "    3. Predict\n",
    "        pred = mod.predict(X) \n",
    "    \n",
    "    #Or (you may need to perform feature scaaling on your model in order to achieve better results) \n",
    "from sklearn.preprocessing import StandardScaler #this model can perform scaling\n",
    "from sklearn.pipeline import Pipeline #pipeline object - allows you to chain processing steps after each other\n",
    "    4. Pipeline\n",
    "        pipe = Pipeline( [                              #it needs a list of tuples, and its a pair of a name and a step\n",
    "            (\"scale\", StandardScaler()),\n",
    "            (\"model\", KNeighborsRegressor()) \n",
    "            ]) \n",
    "    5. then learn from the Data\n",
    "        pipe.fit(X, y)\n",
    "    6. Predict\n",
    "        pred = pipe.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba2f634",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scikit-Learn Sub-modules\n",
    "\n",
    "# Scikit-Learn library is organized into several sub-modules, each of which contains a set of related functions and classes. \n",
    "# Here are the main sub-modules in scikit-learn:\n",
    "\n",
    "# sklearn.datasets: This sub-module provides a set of standard datasets for machine learning, including iris, \n",
    "#     digits, and breast cancer.\n",
    "\n",
    "# sklearn.model_selection: This sub-module contains functions for model selection, such as splitting data into \n",
    "#     training and test sets, cross-validation, and grid search.\n",
    "\n",
    "# sklearn.preprocessing: This sub-module provides functions for preprocessing data, such as scaling, normalization, \n",
    "#     and encoding categorical variables.\n",
    "\n",
    "# sklearn.feature_extraction: This sub-module contains functions for feature extraction from raw data, \n",
    "#     such as text data, including Bag of Words, CountVectorizer, and TfidfVectorizer.\n",
    "\n",
    "# sklearn.metrics: This sub-module provides functions for evaluating the performance of machine learning models, \n",
    "#     such as accuracy, precision, recall, and F1 score.\n",
    "\n",
    "# sklearn.pipeline: This sub-module provides tools for building machine learning pipelines, \n",
    "#     which allows you to chain together multiple steps, such as feature extraction, preprocessing, and model selection.\n",
    "\n",
    "# sklearn.decomposition: This sub-module provides classes for matrix factorization and decomposition, \n",
    "#     such as Principal Component Analysis (PCA), Non-negative Matrix Factorization (NMF), \n",
    "#     and Latent Dirichlet Allocation (LDA).\n",
    "\n",
    "# sklearn.discriminant_analysis: This sub-module provides classes for linear and quadratic discriminant analysis, \n",
    "#     which are used for supervised classification tasks.\n",
    "\n",
    "# sklearn.covariance: This sub-module provides classes for covariance estimation, such as Empirical Covariance and \n",
    "#     Shrunk Covariance.\n",
    "\n",
    "# sklearn.exceptions: This sub-module contains custom exceptions raised by scikit-learn, such as NotFittedError and \n",
    "#     ConvergenceWarning.\n",
    "\n",
    "\n",
    "#Models: \n",
    "\n",
    "\n",
    "# sklearn.linear_model: This sub-module contains classes for linear models, such as linear regression, \n",
    "#     logistic regression, and ridge regression.\n",
    "\n",
    "# sklearn.tree: This sub-module provides classes for decision trees, such as DecisionTreeClassifier and \n",
    "#     DecisionTreeRegressor.\n",
    "\n",
    "# sklearn.ensemble: This sub-module contains classes for ensemble models, such as random forests, AdaBoost, \n",
    "#     and Gradient Boosting.\n",
    "\n",
    "# sklearn.cluster: This sub-module provides classes for clustering, such as KMeans and Hierarchical Clustering.\n",
    "\n",
    "# sklearn.neural_network: This sub-module contains classes for neural networks, such as Multi-Layer Perceptron (MLP) \n",
    "#     and Convolutional Neural Networks (CNNs).\n",
    "\n",
    "# sklearn.svm: This sub-module contains classes for Support Vector Machines (SVMs), such as SVM classifier and regression.\n",
    "\n",
    "# sklearn.manifold: This sub-module provides classes for manifold learning, such as t-SNE and Isomap.\n",
    "\n",
    "# sklearn.naive_bayes: This sub-module provides classes for Naive Bayes models, such as Gaussian Naive Bayes and \n",
    "#     Multinomial Naive Bayes.\n",
    "\n",
    "# sklearn.neighbors: This sub-module provides classes for k-Nearest Neighbors (k-NN) models, \n",
    "#     such as KNeighborsClassifier and KNeighborsRegressor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa3dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fcba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77025d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data.data, columns=data.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779e2117",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>0.9053</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>0.006399</td>\n",
       "      <td>0.04904</td>\n",
       "      <td>0.05373</td>\n",
       "      <td>0.01587</td>\n",
       "      <td>0.03003</td>\n",
       "      <td>0.006193</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>0.7339</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>0.005225</td>\n",
       "      <td>0.01308</td>\n",
       "      <td>0.01860</td>\n",
       "      <td>0.01340</td>\n",
       "      <td>0.01389</td>\n",
       "      <td>0.003532</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>0.7869</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>0.006150</td>\n",
       "      <td>0.04006</td>\n",
       "      <td>0.03832</td>\n",
       "      <td>0.02058</td>\n",
       "      <td>0.02250</td>\n",
       "      <td>0.004571</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>1.1560</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>0.009110</td>\n",
       "      <td>0.07458</td>\n",
       "      <td>0.05661</td>\n",
       "      <td>0.01867</td>\n",
       "      <td>0.05963</td>\n",
       "      <td>0.009208</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>0.7813</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>0.011490</td>\n",
       "      <td>0.02461</td>\n",
       "      <td>0.05688</td>\n",
       "      <td>0.01885</td>\n",
       "      <td>0.01756</td>\n",
       "      <td>0.005115</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>1.2560</td>\n",
       "      <td>7.673</td>\n",
       "      <td>158.70</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.02891</td>\n",
       "      <td>0.05198</td>\n",
       "      <td>0.02454</td>\n",
       "      <td>0.01114</td>\n",
       "      <td>0.004239</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>2.4630</td>\n",
       "      <td>5.203</td>\n",
       "      <td>99.04</td>\n",
       "      <td>0.005769</td>\n",
       "      <td>0.02423</td>\n",
       "      <td>0.03950</td>\n",
       "      <td>0.01678</td>\n",
       "      <td>0.01898</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>1.0750</td>\n",
       "      <td>3.425</td>\n",
       "      <td>48.55</td>\n",
       "      <td>0.005903</td>\n",
       "      <td>0.03731</td>\n",
       "      <td>0.04730</td>\n",
       "      <td>0.01557</td>\n",
       "      <td>0.01318</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>1.5950</td>\n",
       "      <td>5.772</td>\n",
       "      <td>86.22</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>0.06158</td>\n",
       "      <td>0.07117</td>\n",
       "      <td>0.01664</td>\n",
       "      <td>0.02324</td>\n",
       "      <td>0.006185</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>1.4280</td>\n",
       "      <td>2.548</td>\n",
       "      <td>19.15</td>\n",
       "      <td>0.007189</td>\n",
       "      <td>0.00466</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.02676</td>\n",
       "      <td>0.002783</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter  worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "0          17.99         10.38          122.80     1001.0          0.11840           0.27760         0.30010              0.14710         0.2419                 0.07871        1.0950         0.9053            8.589      153.40          0.006399            0.04904          0.05373               0.01587         0.03003                 0.006193        25.380          17.33           184.60      2019.0           0.16220            0.66560           0.7119                0.2654          0.4601                  0.11890\n",
       "1          20.57         17.77          132.90     1326.0          0.08474           0.07864         0.08690              0.07017         0.1812                 0.05667        0.5435         0.7339            3.398       74.08          0.005225            0.01308          0.01860               0.01340         0.01389                 0.003532        24.990          23.41           158.80      1956.0           0.12380            0.18660           0.2416                0.1860          0.2750                  0.08902\n",
       "2          19.69         21.25          130.00     1203.0          0.10960           0.15990         0.19740              0.12790         0.2069                 0.05999        0.7456         0.7869            4.585       94.03          0.006150            0.04006          0.03832               0.02058         0.02250                 0.004571        23.570          25.53           152.50      1709.0           0.14440            0.42450           0.4504                0.2430          0.3613                  0.08758\n",
       "3          11.42         20.38           77.58      386.1          0.14250           0.28390         0.24140              0.10520         0.2597                 0.09744        0.4956         1.1560            3.445       27.23          0.009110            0.07458          0.05661               0.01867         0.05963                 0.009208        14.910          26.50            98.87       567.7           0.20980            0.86630           0.6869                0.2575          0.6638                  0.17300\n",
       "4          20.29         14.34          135.10     1297.0          0.10030           0.13280         0.19800              0.10430         0.1809                 0.05883        0.7572         0.7813            5.438       94.44          0.011490            0.02461          0.05688               0.01885         0.01756                 0.005115        22.540          16.67           152.20      1575.0           0.13740            0.20500           0.4000                0.1625          0.2364                  0.07678\n",
       "..           ...           ...             ...        ...              ...               ...             ...                  ...            ...                     ...           ...            ...              ...         ...               ...                ...              ...                   ...             ...                      ...           ...            ...              ...         ...               ...                ...              ...                   ...             ...                      ...\n",
       "564        21.56         22.39          142.00     1479.0          0.11100           0.11590         0.24390              0.13890         0.1726                 0.05623        1.1760         1.2560            7.673      158.70          0.010300            0.02891          0.05198               0.02454         0.01114                 0.004239        25.450          26.40           166.10      2027.0           0.14100            0.21130           0.4107                0.2216          0.2060                  0.07115\n",
       "565        20.13         28.25          131.20     1261.0          0.09780           0.10340         0.14400              0.09791         0.1752                 0.05533        0.7655         2.4630            5.203       99.04          0.005769            0.02423          0.03950               0.01678         0.01898                 0.002498        23.690          38.25           155.00      1731.0           0.11660            0.19220           0.3215                0.1628          0.2572                  0.06637\n",
       "566        16.60         28.08          108.30      858.1          0.08455           0.10230         0.09251              0.05302         0.1590                 0.05648        0.4564         1.0750            3.425       48.55          0.005903            0.03731          0.04730               0.01557         0.01318                 0.003892        18.980          34.12           126.70      1124.0           0.11390            0.30940           0.3403                0.1418          0.2218                  0.07820\n",
       "567        20.60         29.33          140.10     1265.0          0.11780           0.27700         0.35140              0.15200         0.2397                 0.07016        0.7260         1.5950            5.772       86.22          0.006522            0.06158          0.07117               0.01664         0.02324                 0.006185        25.740          39.42           184.60      1821.0           0.16500            0.86810           0.9387                0.2650          0.4087                  0.12400\n",
       "568         7.76         24.54           47.92      181.0          0.05263           0.04362         0.00000              0.00000         0.1587                 0.05884        0.3857         1.4280            2.548       19.15          0.007189            0.00466          0.00000               0.00000         0.02676                 0.002783         9.456          30.37            59.16       268.6           0.08996            0.06444           0.0000                0.0000          0.2871                  0.07039\n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af831349",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "iris_data = load_iris()\n",
    "iris_features = iris_data.data \n",
    "iris_target = iris_data.target\n",
    "\n",
    "# Convert the data to a DataFrame\n",
    "df = pd.DataFrame(iris_features, columns=iris_data.feature_names)\n",
    "\n",
    "# Add the target variable to the DataFrame\n",
    "df['target'] = iris_target "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d193cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  target\n",
       "0                  5.1               3.5                1.4               0.2       0\n",
       "1                  4.9               3.0                1.4               0.2       0\n",
       "2                  4.7               3.2                1.3               0.2       0\n",
       "3                  4.6               3.1                1.5               0.2       0\n",
       "4                  5.0               3.6                1.4               0.2       0\n",
       "..                 ...               ...                ...               ...     ...\n",
       "145                6.7               3.0                5.2               2.3       2\n",
       "146                6.3               2.5                5.0               1.9       2\n",
       "147                6.5               3.0                5.2               2.0       2\n",
       "148                6.2               3.4                5.4               2.3       2\n",
       "149                5.9               3.0                5.1               1.8       2\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004f5fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _iris_dataset:\n",
      "\n",
      "Iris plants dataset\n",
      "--------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 150 (50 in each of three classes)\n",
      "    :Number of Attributes: 4 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      "        - sepal length in cm\n",
      "        - sepal width in cm\n",
      "        - petal length in cm\n",
      "        - petal width in cm\n",
      "        - class:\n",
      "                - Iris-Setosa\n",
      "                - Iris-Versicolour\n",
      "                - Iris-Virginica\n",
      "                \n",
      "    :Summary Statistics:\n",
      "\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "                    Min  Max   Mean    SD   Class Correlation\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "    sepal length:   4.3  7.9   5.84   0.83    0.7826\n",
      "    sepal width:    2.0  4.4   3.05   0.43   -0.4194\n",
      "    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\n",
      "    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\n",
      "    ============== ==== ==== ======= ===== ====================\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: 33.3% for each of 3 classes.\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "The famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\n",
      "from Fisher's paper. Note that it's the same as in R, but not as in the UCI\n",
      "Machine Learning Repository, which has two wrong data points.\n",
      "\n",
      "This is perhaps the best known database to be found in the\n",
      "pattern recognition literature.  Fisher's paper is a classic in the field and\n",
      "is referenced frequently to this day.  (See Duda & Hart, for example.)  The\n",
      "data set contains 3 classes of 50 instances each, where each class refers to a\n",
      "type of iris plant.  One class is linearly separable from the other 2; the\n",
      "latter are NOT linearly separable from each other.\n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\n",
      "     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\n",
      "     Mathematical Statistics\" (John Wiley, NY, 1950).\n",
      "   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\n",
      "     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\n",
      "   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\n",
      "     Structure and Classification Rule for Recognition in Partially Exposed\n",
      "     Environments\".  IEEE Transactions on Pattern Analysis and Machine\n",
      "     Intelligence, Vol. PAMI-2, No. 1, 67-71.\n",
      "   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\n",
      "     on Information Theory, May 1972, 431-433.\n",
      "   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\n",
      "     conceptual clustering system finds 3 classes in the data.\n",
      "   - Many, many more ...\n"
     ]
    }
   ],
   "source": [
    "# iris_data.DESCR\n",
    "print(iris_data.DESCR)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "55f0aee4",
   "metadata": {},
   "source": [
    "### Linear Regression and Multiple Linear Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4b31424b",
   "metadata": {},
   "source": [
    "> Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25162c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization is a technique used in machine learning and other computational tasks to speed up computations \n",
    "# by performing them on arrays of data instead of iterating over each individual element. \n",
    "# NumPy is a popular Python library for numerical computations and it provides many functions and methods for vectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd222b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.14\n"
     ]
    }
   ],
   "source": [
    "#to compute the Multiple Linear regression values\n",
    "# f(x) = W.X + b\n",
    "# W = [w1, w2, w3, .... wn]     #a row vector of W (slope)\n",
    "# X = [x1, x2, x3, .... xn]     # a row vector of X (features)\n",
    "W = np.array([1.0, 2.5, -3.3])\n",
    "b = 4\n",
    "X = np.array([10,20,30])\n",
    "\n",
    "#use\n",
    "f = np.dot(W,X) + b     #Vectorization\n",
    "\n",
    "#or a for-loop, which doesn't work as fast\n",
    "f = 0 \n",
    "for j in range(3):\n",
    "    # f[j] = w[j] * x[j] \n",
    "    f = f + W[j] * W[j]\n",
    "f = f + b\n",
    "#NB: Vectorization makes the code shorter, faster and more efficient. \n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb084eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-35.0\n",
      "Elapsed time:  0.0010428428649902344 seconds\n"
     ]
    }
   ],
   "source": [
    "#Checking the time it takes to perform Vectorization, and comparing it with for-loop\n",
    "W = np.array([1.0, 2.5, -3.3])\n",
    "b = 4\n",
    "X = np.array([10,20,30])\n",
    "\n",
    "#vectorization\n",
    "start_time = time.time()\n",
    "\n",
    "f = np.dot(W,X) + b     #Vectorization\n",
    "#or a for-loop, which doesn't work as fast\n",
    "print(f) \n",
    "\n",
    "end_time = time.time() \n",
    "\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time: \", elapsed_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1fa69d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-35.0\n",
      "Elapsed time:  0.0009684562683105469 seconds\n"
     ]
    }
   ],
   "source": [
    "#using for-loop\n",
    "\n",
    "W = np.array([1.0, 2.5, -3.3])\n",
    "b = 4\n",
    "X = np.array([10,20,30])\n",
    "\n",
    "#for-loop\n",
    "start_time = time.time()\n",
    "\n",
    "f = 0 \n",
    "for j in range(3):\n",
    "    f = f + W[j] * X[j]\n",
    "f = f + b\n",
    "print(f) \n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(\"Elapsed time: \", elapsed_time, \"seconds\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1dcc1680",
   "metadata": {},
   "source": [
    "> Understanding Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0544de2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.14331378, 0.50623554, 0.36266187, 0.56075036],\n",
       "        [0.75644037, 0.00317005, 0.28006657, 0.27545594],\n",
       "        [0.35731577, 0.63558213, 0.15984875, 0.92458012]],\n",
       "\n",
       "       [[0.54956668, 0.01727769, 0.36649432, 0.58258039],\n",
       "        [0.45841122, 0.00759815, 0.41263046, 0.88569144],\n",
       "        [0.66353197, 0.13433911, 0.31030726, 0.79395491]]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "arr = np.random.rand(2, 3, 4)\n",
    "\n",
    "row_means = np.mean(arr, axis=0)\n",
    "#it performs the mean on each row (just like axis = 2), unless there are multiple dimensions,\n",
    "#then it adds the value of each [ith] element in each dimension and divides by total no of values in that dimension.\n",
    "\n",
    "row_means = np.mean(arr, axis=1)\n",
    "print(row_means)\n",
    "#it sums the values of each column, and divides it by the number of elements in that column. \n",
    "# If there are multiple dimensions present, it does it for each dimension (1D, 2D, 3D etc.)\n",
    "\n",
    "col_means = np.mean(arr, axis=2)\n",
    "print(col_means)\n",
    "#it sums the values of each row, and divides it by the number of elements in that row. \n",
    "# If there are multiple dimensions present, it does it for each dimension (1D, 2D, 3D etc.)\n",
    "\n",
    "col_means = np.mean(arr, axis=(2,1))\n",
    "#it first performs the mean on axis = 1(columns), and then performs another mean on the previous mean on axis = 2(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063cd05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning Model \n",
    "'''\n",
    "An ML model is a mathematical representatin of a system that is used to make predictions or decisions. In ML, a model \n",
    "is typically a set of parameters that can be tunes/adjusted to fit a given dataset\n",
    "'''\n",
    "\n",
    "#Cost Function (measure of error or loss of the model)\n",
    "'''\n",
    "A cost function is a measure of how well a model fits the training data. The goal of machine learning is to find the \n",
    "set of parameters that minimize the cost function. The cost function can be seen as a metric of how accurate the models\n",
    "predictions are. Some common examples include: Mean Squared Error (MSE), Binary Cross-Entropy, Categorical Cross-\n",
    "Entropy.\n",
    "'''\n",
    "\n",
    "#Gradient Descent\n",
    "'''\n",
    "Gradient descent is an optimization algorithm commonly used in machine earning for finding the minimum of a cost \n",
    "fnction by iteratively adjusting the values of its parameters. Variations of gradient descent include: Batch gradient\n",
    "descent, stochastic gradient descent (SGD), Mini-batch gradient descent, momentum-based gradient descent, AdaGrad,\n",
    "Adam. \n",
    "'''\n",
    "\n",
    "#Gradient of a cost function (d/dw, d/db)\n",
    "'''\n",
    "The gradient of a cost function wrt w is the average of the product of the residual error and input features.\n",
    "The gradient of a cost function wrt b is the average of the residual error\n",
    "'''\n",
    "\n",
    "#Learning Rate (alpha)\n",
    "'''\n",
    "This is a hyperparameter that determines how much the model parameters are updated at each step of the optimization \n",
    "process. \n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "23d114cf",
   "metadata": {},
   "source": [
    "#### Linear Model, Cost function, Gradient Descent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dac91b5c",
   "metadata": {},
   "source": [
    ">> Simple Linear Regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e209a75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looping through the dataset\n",
    "\n",
    "\n",
    "# Load our data set\n",
    "x_train = np.array([1.0, 2.0])   #features\n",
    "y_train = np.array([300.0, 500.0])   #target value\n",
    "\n",
    "\n",
    "\n",
    "#Function to calculate the cost\n",
    "def compute_cost(x, y, w, b):\n",
    "   \n",
    "    m = x.shape[0] \n",
    "    cost = 0\n",
    "    \n",
    "    for i in range(m):\n",
    "        f_wb = w * x[i] + b #model output\n",
    "        cost = cost + (f_wb - y[i])**2\n",
    "    total_cost = 1 / (2 * m) * cost\n",
    "\n",
    "    return total_cost\n",
    "\n",
    "\n",
    "def compute_gradient(x, y, w, b): #computes d/dw and d/db\n",
    "    \"\"\"\n",
    "    Computes the gradient for linear regression \n",
    "    Args:\n",
    "      x (ndarray (m,)): Data, m examples \n",
    "      y (ndarray (m,)): target values\n",
    "      w,b (scalar)    : model parameters  \n",
    "    Returns\n",
    "      dj_dw (scalar): The gradient of the cost w.r.t. the parameters w\n",
    "      dj_db (scalar): The gradient of the cost w.r.t. the parameter b     \n",
    "     \"\"\"\n",
    "    \n",
    "    # Number of training examples\n",
    "    m = x.shape[0]    \n",
    "    dj_dw = 0\n",
    "    dj_db = 0\n",
    "    \n",
    "    for i in range(m):  \n",
    "        f_wb = w * x[i] + b #model\n",
    "        dj_dw_i = (f_wb - y[i]) * x[i] \n",
    "        dj_db_i = f_wb - y[i] \n",
    "        dj_db += dj_db_i\n",
    "        dj_dw += dj_dw_i \n",
    "    dj_dw = dj_dw / m \n",
    "    dj_db = dj_db / m \n",
    "        \n",
    "    return dj_dw, dj_db\n",
    "\n",
    "def gradient_descent(x, y, w_in, b_in, alpha, num_iters, cost_function, gradient_function): \n",
    "    \"\"\"\n",
    "    Performs gradient descent to fit w,b. Updates w,b by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      x (ndarray (m,))  : Data, m examples \n",
    "      y (ndarray (m,))  : target values\n",
    "      w_in,b_in (scalar): initial values of model parameters  \n",
    "      alpha (float):     Learning rate\n",
    "      num_iters (int):   number of iterations to run gradient descent\n",
    "      cost_function:     function to call to produce cost\n",
    "      gradient_function: function to call to produce gradient\n",
    "      \n",
    "    Returns:\n",
    "      w (scalar): Updated value of parameter after running gradient descent\n",
    "      b (scalar): Updated value of parameter after running gradient descent\n",
    "      J_history (List): History of cost values\n",
    "      p_history (list): History of parameters [w,b] \n",
    "      \"\"\" \n",
    "    \n",
    "    w = copy.deepcopy(w_in) # avoid modifying global w_in\n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    p_history = []\n",
    "    b = b_in\n",
    "    w = w_in\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "        # Calculate the gradient and update the parameters using gradient_function\n",
    "        dj_dw, dj_db = gradient_function(x, y, w , b)     \n",
    "\n",
    "        # Update Parameters using equation (3) above\n",
    "        b = b - alpha * dj_db                            \n",
    "        w = w - alpha * dj_dw                            \n",
    "\n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            J_history.append( cost_function(x, y, w , b))\n",
    "            p_history.append([w,b])\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0:\n",
    "            print(f\"Iteration {i:4}: Cost {J_history[-1]:0.2e} \",\n",
    "                  f\"dj_dw: {dj_dw: 0.3e}, dj_db: {dj_db: 0.3e}  \",\n",
    "                  f\"w: {w: 0.3e}, b:{b: 0.5e}\")\n",
    " \n",
    "    return w, b, J_history, p_history #return w and J,w history for graphing\n",
    "\n",
    "# plt_gradients(x_train,y_train, compute_cost, compute_gradient)\n",
    "# plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bcd99bb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration    0: Cost 7.93e+04  dj_dw: -6.500e+02, dj_db: -4.000e+02   w:  6.500e+00, b: 4.00000e+00\n",
      "Iteration 1000: Cost 3.41e+00  dj_dw: -3.712e-01, dj_db:  6.007e-01   w:  1.949e+02, b: 1.08228e+02\n",
      "Iteration 2000: Cost 7.93e-01  dj_dw: -1.789e-01, dj_db:  2.895e-01   w:  1.975e+02, b: 1.03966e+02\n",
      "Iteration 3000: Cost 1.84e-01  dj_dw: -8.625e-02, dj_db:  1.396e-01   w:  1.988e+02, b: 1.01912e+02\n",
      "Iteration 4000: Cost 4.28e-02  dj_dw: -4.158e-02, dj_db:  6.727e-02   w:  1.994e+02, b: 1.00922e+02\n",
      "Iteration 5000: Cost 9.95e-03  dj_dw: -2.004e-02, dj_db:  3.243e-02   w:  1.997e+02, b: 1.00444e+02\n",
      "Iteration 6000: Cost 2.31e-03  dj_dw: -9.660e-03, dj_db:  1.563e-02   w:  1.999e+02, b: 1.00214e+02\n",
      "Iteration 7000: Cost 5.37e-04  dj_dw: -4.657e-03, dj_db:  7.535e-03   w:  1.999e+02, b: 1.00103e+02\n",
      "Iteration 8000: Cost 1.25e-04  dj_dw: -2.245e-03, dj_db:  3.632e-03   w:  2.000e+02, b: 1.00050e+02\n",
      "Iteration 9000: Cost 2.90e-05  dj_dw: -1.082e-03, dj_db:  1.751e-03   w:  2.000e+02, b: 1.00024e+02\n",
      "(w,b) found by gradient descent: (199.9929,100.0116)\n"
     ]
    }
   ],
   "source": [
    "# initialize parameters\n",
    "w_init = 0\n",
    "b_init = 0\n",
    "# some gradient descent settings\n",
    "iterations = 10000\n",
    "tmp_alpha = 1.0e-2\n",
    "# run gradient descent\n",
    "w_final, b_final, J_hist, p_hist = gradient_descent(x_train ,y_train, w_init, b_init, tmp_alpha, \n",
    "                                                    iterations, compute_cost, compute_gradient)\n",
    "print(f\"(w,b) found by gradient descent: ({w_final:8.4f},{b_final:8.4f})\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c06f95f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 sqft house prediction 300.0 Thousand dollars\n",
      "1200 sqft house prediction 340.0 Thousand dollars\n",
      "2000 sqft house prediction 500.0 Thousand dollars\n"
     ]
    }
   ],
   "source": [
    "#Prediction\n",
    "\n",
    "print(f\"1000 sqft house prediction {w_final*1.0 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"1200 sqft house prediction {w_final*1.2 + b_final:0.1f} Thousand dollars\")\n",
    "print(f\"2000 sqft house prediction {w_final*2.0 + b_final:0.1f} Thousand dollars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8110f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using vectorization  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e836a125",
   "metadata": {},
   "source": [
    ">> Multiple Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82951be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_x = 3 # The algorithm starts at x=3  \n",
    "rate = 0.01 # Learning rate  \n",
    "precision = 0.000001 #This tells us when to stop the algorithm  \n",
    "previous_step_size = 1 #  \n",
    "max_iters = 10000 # maximum number of iterations  \n",
    "iters = 0 #iteration counter \n",
    "df = lambda x: 2*(x+5) #Gradient of our function  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88fb02d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1 \n",
      "X value is 2.84\n",
      "Iteration 2 \n",
      "X value is 2.6832\n",
      "Iteration 3 \n",
      "X value is 2.529536\n",
      "Iteration 4 \n",
      "X value is 2.37894528\n",
      "Iteration 5 \n",
      "X value is 2.2313663744\n",
      "Iteration 6 \n",
      "X value is 2.0867390469119997\n",
      "Iteration 7 \n",
      "X value is 1.9450042659737599\n",
      "Iteration 8 \n",
      "X value is 1.8061041806542846\n",
      "Iteration 9 \n",
      "X value is 1.669982097041199\n",
      "Iteration 10 \n",
      "X value is 1.5365824551003748\n",
      "Iteration 11 \n",
      "X value is 1.4058508059983674\n",
      "Iteration 12 \n",
      "X value is 1.2777337898784\n",
      "Iteration 13 \n",
      "X value is 1.152179114080832\n",
      "Iteration 14 \n",
      "X value is 1.0291355317992152\n",
      "Iteration 15 \n",
      "X value is 0.9085528211632309\n",
      "Iteration 16 \n",
      "X value is 0.7903817647399662\n",
      "Iteration 17 \n",
      "X value is 0.6745741294451669\n",
      "Iteration 18 \n",
      "X value is 0.5610826468562635\n",
      "Iteration 19 \n",
      "X value is 0.44986099391913825\n",
      "Iteration 20 \n",
      "X value is 0.3408637740407555\n",
      "Iteration 21 \n",
      "X value is 0.23404649855994042\n",
      "Iteration 22 \n",
      "X value is 0.1293655685887416\n",
      "Iteration 23 \n",
      "X value is 0.026778257216966764\n",
      "Iteration 24 \n",
      "X value is -0.07375730792737258\n",
      "Iteration 25 \n",
      "X value is -0.1722821617688251\n",
      "Iteration 26 \n",
      "X value is -0.2688365185334486\n",
      "Iteration 27 \n",
      "X value is -0.36345978816277963\n",
      "Iteration 28 \n",
      "X value is -0.45619059239952403\n",
      "Iteration 29 \n",
      "X value is -0.5470667805515336\n",
      "Iteration 30 \n",
      "X value is -0.6361254449405029\n",
      "Iteration 31 \n",
      "X value is -0.7234029360416929\n",
      "Iteration 32 \n",
      "X value is -0.8089348773208591\n",
      "Iteration 33 \n",
      "X value is -0.8927561797744419\n",
      "Iteration 34 \n",
      "X value is -0.9749010561789531\n",
      "Iteration 35 \n",
      "X value is -1.055403035055374\n",
      "Iteration 36 \n",
      "X value is -1.1342949743542665\n",
      "Iteration 37 \n",
      "X value is -1.2116090748671813\n",
      "Iteration 38 \n",
      "X value is -1.2873768933698377\n",
      "Iteration 39 \n",
      "X value is -1.361629355502441\n",
      "Iteration 40 \n",
      "X value is -1.4343967683923922\n",
      "Iteration 41 \n",
      "X value is -1.5057088330245443\n",
      "Iteration 42 \n",
      "X value is -1.5755946563640535\n",
      "Iteration 43 \n",
      "X value is -1.6440827632367725\n",
      "Iteration 44 \n",
      "X value is -1.711201107972037\n",
      "Iteration 45 \n",
      "X value is -1.7769770858125964\n",
      "Iteration 46 \n",
      "X value is -1.8414375440963444\n",
      "Iteration 47 \n",
      "X value is -1.9046087932144176\n",
      "Iteration 48 \n",
      "X value is -1.9665166173501292\n",
      "Iteration 49 \n",
      "X value is -2.0271862850031264\n",
      "Iteration 50 \n",
      "X value is -2.0866425593030637\n",
      "Iteration 51 \n",
      "X value is -2.1449097081170025\n",
      "Iteration 52 \n",
      "X value is -2.2020115139546625\n",
      "Iteration 53 \n",
      "X value is -2.257971283675569\n",
      "Iteration 54 \n",
      "X value is -2.312811858002058\n",
      "Iteration 55 \n",
      "X value is -2.3665556208420164\n",
      "Iteration 56 \n",
      "X value is -2.419224508425176\n",
      "Iteration 57 \n",
      "X value is -2.4708400182566725\n",
      "Iteration 58 \n",
      "X value is -2.521423217891539\n",
      "Iteration 59 \n",
      "X value is -2.570994753533708\n",
      "Iteration 60 \n",
      "X value is -2.619574858463034\n",
      "Iteration 61 \n",
      "X value is -2.667183361293773\n",
      "Iteration 62 \n",
      "X value is -2.713839694067898\n",
      "Iteration 63 \n",
      "X value is -2.75956290018654\n",
      "Iteration 64 \n",
      "X value is -2.804371642182809\n",
      "Iteration 65 \n",
      "X value is -2.8482842093391527\n",
      "Iteration 66 \n",
      "X value is -2.8913185251523696\n",
      "Iteration 67 \n",
      "X value is -2.9334921546493224\n",
      "Iteration 68 \n",
      "X value is -2.974822311556336\n",
      "Iteration 69 \n",
      "X value is -3.015325865325209\n",
      "Iteration 70 \n",
      "X value is -3.055019348018705\n",
      "Iteration 71 \n",
      "X value is -3.093918961058331\n",
      "Iteration 72 \n",
      "X value is -3.1320405818371646\n",
      "Iteration 73 \n",
      "X value is -3.1693997702004215\n",
      "Iteration 74 \n",
      "X value is -3.206011774796413\n",
      "Iteration 75 \n",
      "X value is -3.2418915393004846\n",
      "Iteration 76 \n",
      "X value is -3.277053708514475\n",
      "Iteration 77 \n",
      "X value is -3.3115126343441856\n",
      "Iteration 78 \n",
      "X value is -3.345282381657302\n",
      "Iteration 79 \n",
      "X value is -3.378376734024156\n",
      "Iteration 80 \n",
      "X value is -3.4108091993436727\n",
      "Iteration 81 \n",
      "X value is -3.4425930153567994\n",
      "Iteration 82 \n",
      "X value is -3.4737411550496633\n",
      "Iteration 83 \n",
      "X value is -3.50426633194867\n",
      "Iteration 84 \n",
      "X value is -3.534181005309697\n",
      "Iteration 85 \n",
      "X value is -3.563497385203503\n",
      "Iteration 86 \n",
      "X value is -3.5922274374994325\n",
      "Iteration 87 \n",
      "X value is -3.620382888749444\n",
      "Iteration 88 \n",
      "X value is -3.6479752309744553\n",
      "Iteration 89 \n",
      "X value is -3.675015726354966\n",
      "Iteration 90 \n",
      "X value is -3.7015154118278666\n",
      "Iteration 91 \n",
      "X value is -3.7274851035913095\n",
      "Iteration 92 \n",
      "X value is -3.7529354015194833\n",
      "Iteration 93 \n",
      "X value is -3.7778766934890937\n",
      "Iteration 94 \n",
      "X value is -3.8023191596193118\n",
      "Iteration 95 \n",
      "X value is -3.8262727764269258\n",
      "Iteration 96 \n",
      "X value is -3.8497473208983872\n",
      "Iteration 97 \n",
      "X value is -3.8727523744804193\n",
      "Iteration 98 \n",
      "X value is -3.895297326990811\n",
      "Iteration 99 \n",
      "X value is -3.917391380450995\n",
      "Iteration 100 \n",
      "X value is -3.939043552841975\n",
      "Iteration 101 \n",
      "X value is -3.9602626817851356\n",
      "Iteration 102 \n",
      "X value is -3.981057428149433\n",
      "Iteration 103 \n",
      "X value is -4.001436279586445\n",
      "Iteration 104 \n",
      "X value is -4.021407553994716\n",
      "Iteration 105 \n",
      "X value is -4.040979402914822\n",
      "Iteration 106 \n",
      "X value is -4.060159814856525\n",
      "Iteration 107 \n",
      "X value is -4.078956618559395\n",
      "Iteration 108 \n",
      "X value is -4.097377486188207\n",
      "Iteration 109 \n",
      "X value is -4.115429936464443\n",
      "Iteration 110 \n",
      "X value is -4.133121337735154\n",
      "Iteration 111 \n",
      "X value is -4.150458910980451\n",
      "Iteration 112 \n",
      "X value is -4.167449732760842\n",
      "Iteration 113 \n",
      "X value is -4.1841007381056246\n",
      "Iteration 114 \n",
      "X value is -4.200418723343512\n",
      "Iteration 115 \n",
      "X value is -4.216410348876642\n",
      "Iteration 116 \n",
      "X value is -4.2320821418991095\n",
      "Iteration 117 \n",
      "X value is -4.247440499061128\n",
      "Iteration 118 \n",
      "X value is -4.262491689079905\n",
      "Iteration 119 \n",
      "X value is -4.277241855298307\n",
      "Iteration 120 \n",
      "X value is -4.291697018192341\n",
      "Iteration 121 \n",
      "X value is -4.305863077828494\n",
      "Iteration 122 \n",
      "X value is -4.319745816271924\n",
      "Iteration 123 \n",
      "X value is -4.333350899946486\n",
      "Iteration 124 \n",
      "X value is -4.3466838819475555\n",
      "Iteration 125 \n",
      "X value is -4.359750204308605\n",
      "Iteration 126 \n",
      "X value is -4.372555200222433\n",
      "Iteration 127 \n",
      "X value is -4.385104096217984\n",
      "Iteration 128 \n",
      "X value is -4.3974020142936245\n",
      "Iteration 129 \n",
      "X value is -4.409453974007752\n",
      "Iteration 130 \n",
      "X value is -4.421264894527597\n",
      "Iteration 131 \n",
      "X value is -4.432839596637045\n",
      "Iteration 132 \n",
      "X value is -4.444182804704305\n",
      "Iteration 133 \n",
      "X value is -4.4552991486102185\n",
      "Iteration 134 \n",
      "X value is -4.466193165638014\n",
      "Iteration 135 \n",
      "X value is -4.4768693023252535\n",
      "Iteration 136 \n",
      "X value is -4.487331916278748\n",
      "Iteration 137 \n",
      "X value is -4.497585277953173\n",
      "Iteration 138 \n",
      "X value is -4.50763357239411\n",
      "Iteration 139 \n",
      "X value is -4.517480900946228\n",
      "Iteration 140 \n",
      "X value is -4.527131282927304\n",
      "Iteration 141 \n",
      "X value is -4.536588657268758\n",
      "Iteration 142 \n",
      "X value is -4.545856884123382\n",
      "Iteration 143 \n",
      "X value is -4.5549397464409145\n",
      "Iteration 144 \n",
      "X value is -4.563840951512097\n",
      "Iteration 145 \n",
      "X value is -4.572564132481855\n",
      "Iteration 146 \n",
      "X value is -4.581112849832218\n",
      "Iteration 147 \n",
      "X value is -4.589490592835574\n",
      "Iteration 148 \n",
      "X value is -4.597700780978863\n",
      "Iteration 149 \n",
      "X value is -4.605746765359285\n",
      "Iteration 150 \n",
      "X value is -4.6136318300521\n",
      "Iteration 151 \n",
      "X value is -4.621359193451058\n",
      "Iteration 152 \n",
      "X value is -4.628932009582036\n",
      "Iteration 153 \n",
      "X value is -4.636353369390395\n",
      "Iteration 154 \n",
      "X value is -4.643626302002588\n",
      "Iteration 155 \n",
      "X value is -4.650753775962536\n",
      "Iteration 156 \n",
      "X value is -4.657738700443285\n",
      "Iteration 157 \n",
      "X value is -4.664583926434419\n",
      "Iteration 158 \n",
      "X value is -4.671292247905731\n",
      "Iteration 159 \n",
      "X value is -4.6778664029476165\n",
      "Iteration 160 \n",
      "X value is -4.684309074888664\n",
      "Iteration 161 \n",
      "X value is -4.6906228933908904\n",
      "Iteration 162 \n",
      "X value is -4.696810435523073\n",
      "Iteration 163 \n",
      "X value is -4.702874226812612\n",
      "Iteration 164 \n",
      "X value is -4.708816742276359\n",
      "Iteration 165 \n",
      "X value is -4.714640407430832\n",
      "Iteration 166 \n",
      "X value is -4.720347599282215\n",
      "Iteration 167 \n",
      "X value is -4.725940647296571\n",
      "Iteration 168 \n",
      "X value is -4.731421834350639\n",
      "Iteration 169 \n",
      "X value is -4.736793397663627\n",
      "Iteration 170 \n",
      "X value is -4.742057529710355\n",
      "Iteration 171 \n",
      "X value is -4.747216379116147\n",
      "Iteration 172 \n",
      "X value is -4.752272051533824\n",
      "Iteration 173 \n",
      "X value is -4.757226610503148\n",
      "Iteration 174 \n",
      "X value is -4.762082078293084\n",
      "Iteration 175 \n",
      "X value is -4.766840436727223\n",
      "Iteration 176 \n",
      "X value is -4.771503627992678\n",
      "Iteration 177 \n",
      "X value is -4.776073555432824\n",
      "Iteration 178 \n",
      "X value is -4.780552084324168\n",
      "Iteration 179 \n",
      "X value is -4.784941042637685\n",
      "Iteration 180 \n",
      "X value is -4.7892422217849315\n",
      "Iteration 181 \n",
      "X value is -4.793457377349233\n",
      "Iteration 182 \n",
      "X value is -4.7975882298022485\n",
      "Iteration 183 \n",
      "X value is -4.801636465206204\n",
      "Iteration 184 \n",
      "X value is -4.805603735902079\n",
      "Iteration 185 \n",
      "X value is -4.809491661184038\n",
      "Iteration 186 \n",
      "X value is -4.813301827960357\n",
      "Iteration 187 \n",
      "X value is -4.81703579140115\n",
      "Iteration 188 \n",
      "X value is -4.820695075573127\n",
      "Iteration 189 \n",
      "X value is -4.824281174061665\n",
      "Iteration 190 \n",
      "X value is -4.827795550580431\n",
      "Iteration 191 \n",
      "X value is -4.831239639568823\n",
      "Iteration 192 \n",
      "X value is -4.834614846777447\n",
      "Iteration 193 \n",
      "X value is -4.837922549841898\n",
      "Iteration 194 \n",
      "X value is -4.84116409884506\n",
      "Iteration 195 \n",
      "X value is -4.844340816868159\n",
      "Iteration 196 \n",
      "X value is -4.847454000530796\n",
      "Iteration 197 \n",
      "X value is -4.85050492052018\n",
      "Iteration 198 \n",
      "X value is -4.853494822109776\n",
      "Iteration 199 \n",
      "X value is -4.85642492566758\n",
      "Iteration 200 \n",
      "X value is -4.859296427154229\n",
      "Iteration 201 \n",
      "X value is -4.862110498611145\n",
      "Iteration 202 \n",
      "X value is -4.864868288638922\n",
      "Iteration 203 \n",
      "X value is -4.867570922866143\n",
      "Iteration 204 \n",
      "X value is -4.87021950440882\n",
      "Iteration 205 \n",
      "X value is -4.872815114320644\n",
      "Iteration 206 \n",
      "X value is -4.875358812034231\n",
      "Iteration 207 \n",
      "X value is -4.877851635793546\n",
      "Iteration 208 \n",
      "X value is -4.880294603077676\n",
      "Iteration 209 \n",
      "X value is -4.882688711016122\n",
      "Iteration 210 \n",
      "X value is -4.8850349367958\n",
      "Iteration 211 \n",
      "X value is -4.887334238059884\n",
      "Iteration 212 \n",
      "X value is -4.8895875532986866\n",
      "Iteration 213 \n",
      "X value is -4.891795802232712\n",
      "Iteration 214 \n",
      "X value is -4.893959886188058\n",
      "Iteration 215 \n",
      "X value is -4.896080688464297\n",
      "Iteration 216 \n",
      "X value is -4.898159074695011\n",
      "Iteration 217 \n",
      "X value is -4.9001958932011105\n",
      "Iteration 218 \n",
      "X value is -4.902191975337089\n",
      "Iteration 219 \n",
      "X value is -4.904148135830347\n",
      "Iteration 220 \n",
      "X value is -4.90606517311374\n",
      "Iteration 221 \n",
      "X value is -4.907943869651465\n",
      "Iteration 222 \n",
      "X value is -4.909784992258436\n",
      "Iteration 223 \n",
      "X value is -4.911589292413267\n",
      "Iteration 224 \n",
      "X value is -4.913357506565002\n",
      "Iteration 225 \n",
      "X value is -4.915090356433702\n",
      "Iteration 226 \n",
      "X value is -4.9167885493050285\n",
      "Iteration 227 \n",
      "X value is -4.918452778318928\n",
      "Iteration 228 \n",
      "X value is -4.920083722752549\n",
      "Iteration 229 \n",
      "X value is -4.921682048297498\n",
      "Iteration 230 \n",
      "X value is -4.923248407331548\n",
      "Iteration 231 \n",
      "X value is -4.9247834391849175\n",
      "Iteration 232 \n",
      "X value is -4.926287770401219\n",
      "Iteration 233 \n",
      "X value is -4.927762014993195\n",
      "Iteration 234 \n",
      "X value is -4.929206774693331\n",
      "Iteration 235 \n",
      "X value is -4.930622639199464\n",
      "Iteration 236 \n",
      "X value is -4.932010186415474\n",
      "Iteration 237 \n",
      "X value is -4.933369982687164\n",
      "Iteration 238 \n",
      "X value is -4.934702583033421\n",
      "Iteration 239 \n",
      "X value is -4.936008531372753\n",
      "Iteration 240 \n",
      "X value is -4.937288360745298\n",
      "Iteration 241 \n",
      "X value is -4.938542593530392\n",
      "Iteration 242 \n",
      "X value is -4.939771741659784\n",
      "Iteration 243 \n",
      "X value is -4.940976306826588\n",
      "Iteration 244 \n",
      "X value is -4.942156780690056\n",
      "Iteration 245 \n",
      "X value is -4.943313645076255\n",
      "Iteration 246 \n",
      "X value is -4.94444737217473\n",
      "Iteration 247 \n",
      "X value is -4.945558424731236\n",
      "Iteration 248 \n",
      "X value is -4.946647256236611\n",
      "Iteration 249 \n",
      "X value is -4.947714311111879\n",
      "Iteration 250 \n",
      "X value is -4.9487600248896415\n",
      "Iteration 251 \n",
      "X value is -4.949784824391848\n",
      "Iteration 252 \n",
      "X value is -4.950789127904011\n",
      "Iteration 253 \n",
      "X value is -4.951773345345931\n",
      "Iteration 254 \n",
      "X value is -4.952737878439012\n",
      "Iteration 255 \n",
      "X value is -4.953683120870232\n",
      "Iteration 256 \n",
      "X value is -4.954609458452827\n",
      "Iteration 257 \n",
      "X value is -4.955517269283771\n",
      "Iteration 258 \n",
      "X value is -4.956406923898095\n",
      "Iteration 259 \n",
      "X value is -4.957278785420133\n",
      "Iteration 260 \n",
      "X value is -4.958133209711731\n",
      "Iteration 261 \n",
      "X value is -4.958970545517496\n",
      "Iteration 262 \n",
      "X value is -4.959791134607146\n",
      "Iteration 263 \n",
      "X value is -4.960595311915003\n",
      "Iteration 264 \n",
      "X value is -4.9613834056767026\n",
      "Iteration 265 \n",
      "X value is -4.962155737563169\n",
      "Iteration 266 \n",
      "X value is -4.962912622811905\n",
      "Iteration 267 \n",
      "X value is -4.963654370355667\n",
      "Iteration 268 \n",
      "X value is -4.964381282948554\n",
      "Iteration 269 \n",
      "X value is -4.965093657289583\n",
      "Iteration 270 \n",
      "X value is -4.965791784143791\n",
      "Iteration 271 \n",
      "X value is -4.966475948460915\n",
      "Iteration 272 \n",
      "X value is -4.967146429491697\n",
      "Iteration 273 \n",
      "X value is -4.967803500901863\n",
      "Iteration 274 \n",
      "X value is -4.968447430883826\n",
      "Iteration 275 \n",
      "X value is -4.969078482266149\n",
      "Iteration 276 \n",
      "X value is -4.969696912620826\n",
      "Iteration 277 \n",
      "X value is -4.970302974368409\n",
      "Iteration 278 \n",
      "X value is -4.970896914881041\n",
      "Iteration 279 \n",
      "X value is -4.97147897658342\n",
      "Iteration 280 \n",
      "X value is -4.972049397051752\n",
      "Iteration 281 \n",
      "X value is -4.972608409110717\n",
      "Iteration 282 \n",
      "X value is -4.973156240928502\n",
      "Iteration 283 \n",
      "X value is -4.973693116109932\n",
      "Iteration 284 \n",
      "X value is -4.974219253787734\n",
      "Iteration 285 \n",
      "X value is -4.974734868711979\n",
      "Iteration 286 \n",
      "X value is -4.975240171337739\n",
      "Iteration 287 \n",
      "X value is -4.975735367910985\n",
      "Iteration 288 \n",
      "X value is -4.976220660552765\n",
      "Iteration 289 \n",
      "X value is -4.976696247341709\n",
      "Iteration 290 \n",
      "X value is -4.977162322394875\n",
      "Iteration 291 \n",
      "X value is -4.977619075946977\n",
      "Iteration 292 \n",
      "X value is -4.978066694428038\n",
      "Iteration 293 \n",
      "X value is -4.978505360539477\n",
      "Iteration 294 \n",
      "X value is -4.978935253328687\n",
      "Iteration 295 \n",
      "X value is -4.979356548262113\n",
      "Iteration 296 \n",
      "X value is -4.979769417296871\n",
      "Iteration 297 \n",
      "X value is -4.980174028950934\n",
      "Iteration 298 \n",
      "X value is -4.980570548371915\n",
      "Iteration 299 \n",
      "X value is -4.980959137404477\n",
      "Iteration 300 \n",
      "X value is -4.981339954656387\n",
      "Iteration 301 \n",
      "X value is -4.981713155563259\n",
      "Iteration 302 \n",
      "X value is -4.982078892451994\n",
      "Iteration 303 \n",
      "X value is -4.9824373146029535\n",
      "Iteration 304 \n",
      "X value is -4.982788568310895\n",
      "Iteration 305 \n",
      "X value is -4.983132796944677\n",
      "Iteration 306 \n",
      "X value is -4.983470141005784\n",
      "Iteration 307 \n",
      "X value is -4.983800738185668\n",
      "Iteration 308 \n",
      "X value is -4.984124723421955\n",
      "Iteration 309 \n",
      "X value is -4.984442228953515\n",
      "Iteration 310 \n",
      "X value is -4.984753384374445\n",
      "Iteration 311 \n",
      "X value is -4.985058316686956\n",
      "Iteration 312 \n",
      "X value is -4.9853571503532175\n",
      "Iteration 313 \n",
      "X value is -4.985650007346153\n",
      "Iteration 314 \n",
      "X value is -4.9859370071992295\n",
      "Iteration 315 \n",
      "X value is -4.986218267055245\n",
      "Iteration 316 \n",
      "X value is -4.98649390171414\n",
      "Iteration 317 \n",
      "X value is -4.986764023679857\n",
      "Iteration 318 \n",
      "X value is -4.98702874320626\n",
      "Iteration 319 \n",
      "X value is -4.987288168342134\n",
      "Iteration 320 \n",
      "X value is -4.987542404975292\n",
      "Iteration 321 \n",
      "X value is -4.987791556875786\n",
      "Iteration 322 \n",
      "X value is -4.98803572573827\n",
      "Iteration 323 \n",
      "X value is -4.988275011223505\n",
      "Iteration 324 \n",
      "X value is -4.988509510999035\n",
      "Iteration 325 \n",
      "X value is -4.988739320779054\n",
      "Iteration 326 \n",
      "X value is -4.988964534363473\n",
      "Iteration 327 \n",
      "X value is -4.989185243676204\n",
      "Iteration 328 \n",
      "X value is -4.98940153880268\n",
      "Iteration 329 \n",
      "X value is -4.989613508026626\n",
      "Iteration 330 \n",
      "X value is -4.989821237866094\n",
      "Iteration 331 \n",
      "X value is -4.990024813108772\n",
      "Iteration 332 \n",
      "X value is -4.9902243168465965\n",
      "Iteration 333 \n",
      "X value is -4.990419830509665\n",
      "Iteration 334 \n",
      "X value is -4.990611433899471\n",
      "Iteration 335 \n",
      "X value is -4.990799205221482\n",
      "Iteration 336 \n",
      "X value is -4.990983221117052\n",
      "Iteration 337 \n",
      "X value is -4.991163556694711\n",
      "Iteration 338 \n",
      "X value is -4.991340285560817\n",
      "Iteration 339 \n",
      "X value is -4.9915134798496\n",
      "Iteration 340 \n",
      "X value is -4.991683210252608\n",
      "Iteration 341 \n",
      "X value is -4.991849546047556\n",
      "Iteration 342 \n",
      "X value is -4.992012555126605\n",
      "Iteration 343 \n",
      "X value is -4.992172304024073\n",
      "Iteration 344 \n",
      "X value is -4.992328857943591\n",
      "Iteration 345 \n",
      "X value is -4.99248228078472\n",
      "Iteration 346 \n",
      "X value is -4.992632635169025\n",
      "Iteration 347 \n",
      "X value is -4.9927799824656445\n",
      "Iteration 348 \n",
      "X value is -4.992924382816332\n",
      "Iteration 349 \n",
      "X value is -4.993065895160005\n",
      "Iteration 350 \n",
      "X value is -4.993204577256805\n",
      "Iteration 351 \n",
      "X value is -4.993340485711669\n",
      "Iteration 352 \n",
      "X value is -4.993473675997436\n",
      "Iteration 353 \n",
      "X value is -4.993604202477487\n",
      "Iteration 354 \n",
      "X value is -4.993732118427937\n",
      "Iteration 355 \n",
      "X value is -4.993857476059379\n",
      "Iteration 356 \n",
      "X value is -4.993980326538191\n",
      "Iteration 357 \n",
      "X value is -4.9941007200074266\n",
      "Iteration 358 \n",
      "X value is -4.994218705607278\n",
      "Iteration 359 \n",
      "X value is -4.994334331495133\n",
      "Iteration 360 \n",
      "X value is -4.994447644865231\n",
      "Iteration 361 \n",
      "X value is -4.994558691967926\n",
      "Iteration 362 \n",
      "X value is -4.994667518128567\n",
      "Iteration 363 \n",
      "X value is -4.994774167765996\n",
      "Iteration 364 \n",
      "X value is -4.9948786844106765\n",
      "Iteration 365 \n",
      "X value is -4.994981110722463\n",
      "Iteration 366 \n",
      "X value is -4.995081488508014\n",
      "Iteration 367 \n",
      "X value is -4.995179858737854\n",
      "Iteration 368 \n",
      "X value is -4.995276261563097\n",
      "Iteration 369 \n",
      "X value is -4.995370736331835\n",
      "Iteration 370 \n",
      "X value is -4.9954633216051985\n",
      "Iteration 371 \n",
      "X value is -4.995554055173095\n",
      "Iteration 372 \n",
      "X value is -4.995642974069633\n",
      "Iteration 373 \n",
      "X value is -4.99573011458824\n",
      "Iteration 374 \n",
      "X value is -4.995815512296476\n",
      "Iteration 375 \n",
      "X value is -4.995899202050547\n",
      "Iteration 376 \n",
      "X value is -4.995981218009535\n",
      "Iteration 377 \n",
      "X value is -4.996061593649345\n",
      "Iteration 378 \n",
      "X value is -4.996140361776358\n",
      "Iteration 379 \n",
      "X value is -4.996217554540831\n",
      "Iteration 380 \n",
      "X value is -4.996293203450014\n",
      "Iteration 381 \n",
      "X value is -4.996367339381013\n",
      "Iteration 382 \n",
      "X value is -4.996439992593393\n",
      "Iteration 383 \n",
      "X value is -4.996511192741525\n",
      "Iteration 384 \n",
      "X value is -4.996580968886694\n",
      "Iteration 385 \n",
      "X value is -4.99664934950896\n",
      "Iteration 386 \n",
      "X value is -4.9967163625187805\n",
      "Iteration 387 \n",
      "X value is -4.996782035268405\n",
      "Iteration 388 \n",
      "X value is -4.996846394563037\n",
      "Iteration 389 \n",
      "X value is -4.996909466671776\n",
      "Iteration 390 \n",
      "X value is -4.996971277338341\n",
      "Iteration 391 \n",
      "X value is -4.997031851791574\n",
      "Iteration 392 \n",
      "X value is -4.997091214755742\n",
      "Iteration 393 \n",
      "X value is -4.997149390460628\n",
      "Iteration 394 \n",
      "X value is -4.997206402651415\n",
      "Iteration 395 \n",
      "X value is -4.997262274598387\n",
      "Iteration 396 \n",
      "X value is -4.997317029106419\n",
      "Iteration 397 \n",
      "X value is -4.997370688524291\n",
      "Iteration 398 \n",
      "X value is -4.997423274753805\n",
      "Iteration 399 \n",
      "X value is -4.997474809258729\n",
      "Iteration 400 \n",
      "X value is -4.997525313073554\n",
      "Iteration 401 \n",
      "X value is -4.997574806812083\n",
      "Iteration 402 \n",
      "X value is -4.997623310675841\n",
      "Iteration 403 \n",
      "X value is -4.997670844462324\n",
      "Iteration 404 \n",
      "X value is -4.997717427573078\n",
      "Iteration 405 \n",
      "X value is -4.997763079021617\n",
      "Iteration 406 \n",
      "X value is -4.997807817441185\n",
      "Iteration 407 \n",
      "X value is -4.997851661092361\n",
      "Iteration 408 \n",
      "X value is -4.997894627870514\n",
      "Iteration 409 \n",
      "X value is -4.997936735313104\n",
      "Iteration 410 \n",
      "X value is -4.9979780006068415\n",
      "Iteration 411 \n",
      "X value is -4.998018440594705\n",
      "Iteration 412 \n",
      "X value is -4.998058071782811\n",
      "Iteration 413 \n",
      "X value is -4.998096910347155\n",
      "Iteration 414 \n",
      "X value is -4.998134972140212\n",
      "Iteration 415 \n",
      "X value is -4.998172272697408\n",
      "Iteration 416 \n",
      "X value is -4.9982088272434595\n",
      "Iteration 417 \n",
      "X value is -4.998244650698591\n",
      "Iteration 418 \n",
      "X value is -4.998279757684619\n",
      "Iteration 419 \n",
      "X value is -4.998314162530927\n",
      "Iteration 420 \n",
      "X value is -4.998347879280309\n",
      "Iteration 421 \n",
      "X value is -4.998380921694703\n",
      "Iteration 422 \n",
      "X value is -4.998413303260809\n",
      "Iteration 423 \n",
      "X value is -4.998445037195593\n",
      "Iteration 424 \n",
      "X value is -4.998476136451681\n",
      "Iteration 425 \n",
      "X value is -4.998506613722648\n",
      "Iteration 426 \n",
      "X value is -4.998536481448195\n",
      "Iteration 427 \n",
      "X value is -4.998565751819231\n",
      "Iteration 428 \n",
      "X value is -4.998594436782846\n",
      "Iteration 429 \n",
      "X value is -4.998622548047189\n",
      "Iteration 430 \n",
      "X value is -4.998650097086245\n",
      "Iteration 431 \n",
      "X value is -4.9986770951445205\n",
      "Iteration 432 \n",
      "X value is -4.99870355324163\n",
      "Iteration 433 \n",
      "X value is -4.998729482176797\n",
      "Iteration 434 \n",
      "X value is -4.998754892533261\n",
      "Iteration 435 \n",
      "X value is -4.998779794682596\n",
      "Iteration 436 \n",
      "X value is -4.998804198788944\n",
      "Iteration 437 \n",
      "X value is -4.998828114813166\n",
      "Iteration 438 \n",
      "X value is -4.998851552516903\n",
      "Iteration 439 \n",
      "X value is -4.998874521466565\n",
      "Iteration 440 \n",
      "X value is -4.998897031037234\n",
      "Iteration 441 \n",
      "X value is -4.998919090416489\n",
      "Iteration 442 \n",
      "X value is -4.99894070860816\n",
      "Iteration 443 \n",
      "X value is -4.998961894435997\n",
      "Iteration 444 \n",
      "X value is -4.998982656547277\n",
      "Iteration 445 \n",
      "X value is -4.999003003416331\n",
      "Iteration 446 \n",
      "X value is -4.999022943348004\n",
      "Iteration 447 \n",
      "X value is -4.999042484481044\n",
      "Iteration 448 \n",
      "X value is -4.999061634791423\n",
      "Iteration 449 \n",
      "X value is -4.999080402095594\n",
      "Iteration 450 \n",
      "X value is -4.999098794053682\n",
      "Iteration 451 \n",
      "X value is -4.999116818172609\n",
      "Iteration 452 \n",
      "X value is -4.999134481809157\n",
      "Iteration 453 \n",
      "X value is -4.999151792172974\n",
      "Iteration 454 \n",
      "X value is -4.999168756329515\n",
      "Iteration 455 \n",
      "X value is -4.999185381202924\n",
      "Iteration 456 \n",
      "X value is -4.999201673578866\n",
      "Iteration 457 \n",
      "X value is -4.999217640107289\n",
      "Iteration 458 \n",
      "X value is -4.999233287305143\n",
      "Iteration 459 \n",
      "X value is -4.9992486215590395\n",
      "Iteration 460 \n",
      "X value is -4.999263649127859\n",
      "Iteration 461 \n",
      "X value is -4.999278376145302\n",
      "Iteration 462 \n",
      "X value is -4.999292808622396\n",
      "Iteration 463 \n",
      "X value is -4.999306952449948\n",
      "Iteration 464 \n",
      "X value is -4.999320813400949\n",
      "Iteration 465 \n",
      "X value is -4.99933439713293\n",
      "Iteration 466 \n",
      "X value is -4.999347709190272\n",
      "Iteration 467 \n",
      "X value is -4.9993607550064665\n",
      "Iteration 468 \n",
      "X value is -4.999373539906337\n",
      "Iteration 469 \n",
      "X value is -4.99938606910821\n",
      "Iteration 470 \n",
      "X value is -4.9993983477260455\n",
      "Iteration 471 \n",
      "X value is -4.999410380771525\n",
      "Iteration 472 \n",
      "X value is -4.999422173156094\n",
      "Iteration 473 \n",
      "X value is -4.9994337296929725\n",
      "Iteration 474 \n",
      "X value is -4.999445055099113\n",
      "Iteration 475 \n",
      "X value is -4.999456153997131\n",
      "Iteration 476 \n",
      "X value is -4.999467030917188\n",
      "Iteration 477 \n",
      "X value is -4.9994776902988445\n",
      "Iteration 478 \n",
      "X value is -4.999488136492867\n",
      "Iteration 479 \n",
      "X value is -4.99949837376301\n",
      "Iteration 480 \n",
      "X value is -4.99950840628775\n",
      "Iteration 481 \n",
      "X value is -4.999518238161995\n",
      "Iteration 482 \n",
      "X value is -4.999527873398756\n",
      "Iteration 483 \n",
      "X value is -4.99953731593078\n",
      "Iteration 484 \n",
      "X value is -4.999546569612165\n",
      "Iteration 485 \n",
      "X value is -4.999555638219921\n",
      "Iteration 486 \n",
      "X value is -4.999564525455523\n",
      "Iteration 487 \n",
      "X value is -4.999573234946412\n",
      "Iteration 488 \n",
      "X value is -4.9995817702474845\n",
      "Iteration 489 \n",
      "X value is -4.999590134842535\n",
      "Iteration 490 \n",
      "X value is -4.999598332145684\n",
      "Iteration 491 \n",
      "X value is -4.99960636550277\n",
      "Iteration 492 \n",
      "X value is -4.999614238192715\n",
      "Iteration 493 \n",
      "X value is -4.999621953428861\n",
      "Iteration 494 \n",
      "X value is -4.999629514360284\n",
      "Iteration 495 \n",
      "X value is -4.999636924073078\n",
      "Iteration 496 \n",
      "X value is -4.999644185591617\n",
      "Iteration 497 \n",
      "X value is -4.999651301879784\n",
      "Iteration 498 \n",
      "X value is -4.999658275842188\n",
      "Iteration 499 \n",
      "X value is -4.999665110325345\n",
      "Iteration 500 \n",
      "X value is -4.999671808118838\n",
      "Iteration 501 \n",
      "X value is -4.9996783719564615\n",
      "Iteration 502 \n",
      "X value is -4.999684804517332\n",
      "Iteration 503 \n",
      "X value is -4.999691108426985\n",
      "Iteration 504 \n",
      "X value is -4.999697286258446\n",
      "Iteration 505 \n",
      "X value is -4.9997033405332765\n",
      "Iteration 506 \n",
      "X value is -4.999709273722611\n",
      "Iteration 507 \n",
      "X value is -4.999715088248159\n",
      "Iteration 508 \n",
      "X value is -4.999720786483196\n",
      "Iteration 509 \n",
      "X value is -4.999726370753532\n",
      "Iteration 510 \n",
      "X value is -4.999731843338461\n",
      "Iteration 511 \n",
      "X value is -4.999737206471692\n",
      "Iteration 512 \n",
      "X value is -4.999742462342258\n",
      "Iteration 513 \n",
      "X value is -4.999747613095413\n",
      "Iteration 514 \n",
      "X value is -4.999752660833504\n",
      "Iteration 515 \n",
      "X value is -4.999757607616834\n",
      "Iteration 516 \n",
      "X value is -4.999762455464498\n",
      "Iteration 517 \n",
      "X value is -4.999767206355208\n",
      "Iteration 518 \n",
      "X value is -4.999771862228104\n",
      "Iteration 519 \n",
      "X value is -4.999776424983542\n",
      "Iteration 520 \n",
      "X value is -4.9997808964838715\n",
      "Iteration 521 \n",
      "X value is -4.999785278554194\n",
      "Iteration 522 \n",
      "X value is -4.9997895729831106\n",
      "Iteration 523 \n",
      "X value is -4.999793781523448\n",
      "Iteration 524 \n",
      "X value is -4.999797905892979\n",
      "Iteration 525 \n",
      "X value is -4.999801947775119\n",
      "Iteration 526 \n",
      "X value is -4.999805908819617\n",
      "Iteration 527 \n",
      "X value is -4.999809790643225\n",
      "Iteration 528 \n",
      "X value is -4.99981359483036\n",
      "Iteration 529 \n",
      "X value is -4.999817322933753\n",
      "Iteration 530 \n",
      "X value is -4.999820976475077\n",
      "Iteration 531 \n",
      "X value is -4.999824556945576\n",
      "Iteration 532 \n",
      "X value is -4.999828065806665\n",
      "Iteration 533 \n",
      "X value is -4.9998315044905315\n",
      "Iteration 534 \n",
      "X value is -4.999834874400721\n",
      "Iteration 535 \n",
      "X value is -4.999838176912706\n",
      "Iteration 536 \n",
      "X value is -4.999841413374452\n",
      "Iteration 537 \n",
      "X value is -4.999844585106963\n",
      "Iteration 538 \n",
      "X value is -4.999847693404824\n",
      "Iteration 539 \n",
      "X value is -4.999850739536727\n",
      "Iteration 540 \n",
      "X value is -4.999853724745993\n",
      "Iteration 541 \n",
      "X value is -4.999856650251073\n",
      "Iteration 542 \n",
      "X value is -4.999859517246051\n",
      "Iteration 543 \n",
      "X value is -4.99986232690113\n",
      "Iteration 544 \n",
      "X value is -4.999865080363108\n",
      "Iteration 545 \n",
      "X value is -4.999867778755846\n",
      "Iteration 546 \n",
      "X value is -4.999870423180729\n",
      "Iteration 547 \n",
      "X value is -4.999873014717115\n",
      "Iteration 548 \n",
      "X value is -4.999875554422772\n",
      "Iteration 549 \n",
      "X value is -4.999878043334316\n",
      "Iteration 550 \n",
      "X value is -4.99988048246763\n",
      "Iteration 551 \n",
      "X value is -4.999882872818278\n",
      "Iteration 552 \n",
      "X value is -4.999885215361912\n",
      "Iteration 553 \n",
      "X value is -4.999887511054674\n",
      "Iteration 554 \n",
      "X value is -4.999889760833581\n",
      "Iteration 555 \n",
      "X value is -4.999891965616909\n",
      "Iteration 556 \n",
      "X value is -4.999894126304571\n",
      "Iteration 557 \n",
      "X value is -4.999896243778479\n",
      "Iteration 558 \n",
      "X value is -4.999898318902909\n",
      "Iteration 559 \n",
      "X value is -4.999900352524851\n",
      "Iteration 560 \n",
      "X value is -4.9999023454743545\n",
      "Iteration 561 \n",
      "X value is -4.999904298564868\n",
      "Iteration 562 \n",
      "X value is -4.9999062125935705\n",
      "Iteration 563 \n",
      "X value is -4.999908088341699\n",
      "Iteration 564 \n",
      "X value is -4.9999099265748645\n",
      "Iteration 565 \n",
      "X value is -4.999911728043367\n",
      "Iteration 566 \n",
      "X value is -4.9999134934825\n",
      "Iteration 567 \n",
      "X value is -4.99991522361285\n",
      "Iteration 568 \n",
      "X value is -4.999916919140593\n",
      "Iteration 569 \n",
      "X value is -4.999918580757781\n",
      "Iteration 570 \n",
      "X value is -4.999920209142625\n",
      "Iteration 571 \n",
      "X value is -4.999921804959773\n",
      "Iteration 572 \n",
      "X value is -4.9999233688605775\n",
      "Iteration 573 \n",
      "X value is -4.999924901483366\n",
      "Iteration 574 \n",
      "X value is -4.999926403453699\n",
      "Iteration 575 \n",
      "X value is -4.999927875384625\n",
      "Iteration 576 \n",
      "X value is -4.999929317876933\n",
      "Iteration 577 \n",
      "X value is -4.999930731519394\n",
      "Iteration 578 \n",
      "X value is -4.999932116889006\n",
      "Iteration 579 \n",
      "X value is -4.999933474551226\n",
      "Iteration 580 \n",
      "X value is -4.999934805060202\n",
      "Iteration 581 \n",
      "X value is -4.999936108958998\n",
      "Iteration 582 \n",
      "X value is -4.999937386779818\n",
      "Iteration 583 \n",
      "X value is -4.999938639044221\n",
      "Iteration 584 \n",
      "X value is -4.999939866263337\n",
      "Iteration 585 \n",
      "X value is -4.99994106893807\n",
      "Iteration 586 \n",
      "X value is -4.999942247559309\n",
      "Iteration 587 \n",
      "X value is -4.999943402608123\n",
      "Iteration 588 \n",
      "X value is -4.9999445345559606\n",
      "Iteration 589 \n",
      "X value is -4.999945643864842\n",
      "Iteration 590 \n",
      "X value is -4.999946730987545\n",
      "Iteration 591 \n",
      "X value is -4.999947796367794\n",
      "Iteration 592 \n",
      "X value is -4.999948840440438\n",
      "Iteration 593 \n",
      "X value is -4.999949863631629\n",
      "Iteration 594 \n",
      "X value is -4.999950866358997\n",
      "Iteration 595 \n",
      "X value is -4.9999518490318176\n",
      "The local minimum occurs at -4.9999518490318176\n"
     ]
    }
   ],
   "source": [
    "while previous_step_size > precision and iters < max_iters:  \n",
    "    prev_x = cur_x #Store current x value in prev_x  \n",
    "    cur_x = cur_x - rate * df(prev_x) #Grad descent  \n",
    "    previous_step_size = abs(cur_x - prev_x) #Change in x  \n",
    "    iters = iters+1 #iteration count  \n",
    "    print(\"Iteration\",iters,\"\\nX value is\",cur_x) #Print iterations  \n",
    "    \n",
    "print(\"The local minimum occurs at\", cur_x)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a9f4a944",
   "metadata": {},
   "source": [
    "## Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "53b77abd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "load_breast_cancer = load_breast_cancer()\n",
    "X = load_breast_cancer.data\n",
    "y = load_breast_cancer.target\n",
    "\n",
    "df = pd.DataFrame(X, columns=load_breast_cancer.feature_names)\n",
    "df['target'] = y\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1),\n",
    "                                                    df['target'],\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=0) \n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "127fa167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "      <td>398.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.185500</td>\n",
       "      <td>19.192839</td>\n",
       "      <td>92.335503</td>\n",
       "      <td>661.859045</td>\n",
       "      <td>0.096475</td>\n",
       "      <td>0.104080</td>\n",
       "      <td>0.088794</td>\n",
       "      <td>0.049479</td>\n",
       "      <td>0.180698</td>\n",
       "      <td>0.062646</td>\n",
       "      <td>0.409096</td>\n",
       "      <td>1.204458</td>\n",
       "      <td>2.866619</td>\n",
       "      <td>41.384364</td>\n",
       "      <td>0.006984</td>\n",
       "      <td>0.025099</td>\n",
       "      <td>0.031226</td>\n",
       "      <td>0.011678</td>\n",
       "      <td>0.020402</td>\n",
       "      <td>0.003735</td>\n",
       "      <td>16.359621</td>\n",
       "      <td>25.534523</td>\n",
       "      <td>107.736030</td>\n",
       "      <td>894.246985</td>\n",
       "      <td>0.132529</td>\n",
       "      <td>0.253369</td>\n",
       "      <td>0.268531</td>\n",
       "      <td>0.115221</td>\n",
       "      <td>0.289187</td>\n",
       "      <td>0.083718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.598618</td>\n",
       "      <td>4.190476</td>\n",
       "      <td>24.769334</td>\n",
       "      <td>366.558262</td>\n",
       "      <td>0.013916</td>\n",
       "      <td>0.051031</td>\n",
       "      <td>0.078154</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.028057</td>\n",
       "      <td>0.006861</td>\n",
       "      <td>0.293965</td>\n",
       "      <td>0.551973</td>\n",
       "      <td>2.109921</td>\n",
       "      <td>49.830675</td>\n",
       "      <td>0.002861</td>\n",
       "      <td>0.016835</td>\n",
       "      <td>0.028576</td>\n",
       "      <td>0.005862</td>\n",
       "      <td>0.008181</td>\n",
       "      <td>0.002638</td>\n",
       "      <td>4.965170</td>\n",
       "      <td>6.147466</td>\n",
       "      <td>34.460647</td>\n",
       "      <td>597.853882</td>\n",
       "      <td>0.022989</td>\n",
       "      <td>0.153653</td>\n",
       "      <td>0.193250</td>\n",
       "      <td>0.065688</td>\n",
       "      <td>0.063439</td>\n",
       "      <td>0.017957</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>0.111500</td>\n",
       "      <td>0.362800</td>\n",
       "      <td>0.757000</td>\n",
       "      <td>7.228000</td>\n",
       "      <td>0.001713</td>\n",
       "      <td>0.002252</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007882</td>\n",
       "      <td>0.000895</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.710000</td>\n",
       "      <td>16.032500</td>\n",
       "      <td>75.310000</td>\n",
       "      <td>420.350000</td>\n",
       "      <td>0.086740</td>\n",
       "      <td>0.066805</td>\n",
       "      <td>0.030385</td>\n",
       "      <td>0.020710</td>\n",
       "      <td>0.161525</td>\n",
       "      <td>0.057685</td>\n",
       "      <td>0.232325</td>\n",
       "      <td>0.828050</td>\n",
       "      <td>1.596250</td>\n",
       "      <td>17.850000</td>\n",
       "      <td>0.005180</td>\n",
       "      <td>0.013628</td>\n",
       "      <td>0.016055</td>\n",
       "      <td>0.007932</td>\n",
       "      <td>0.014950</td>\n",
       "      <td>0.002238</td>\n",
       "      <td>13.052500</td>\n",
       "      <td>20.865000</td>\n",
       "      <td>84.122500</td>\n",
       "      <td>516.425000</td>\n",
       "      <td>0.116300</td>\n",
       "      <td>0.148600</td>\n",
       "      <td>0.119100</td>\n",
       "      <td>0.065320</td>\n",
       "      <td>0.247850</td>\n",
       "      <td>0.071462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.310000</td>\n",
       "      <td>18.725000</td>\n",
       "      <td>86.140000</td>\n",
       "      <td>548.450000</td>\n",
       "      <td>0.096035</td>\n",
       "      <td>0.094035</td>\n",
       "      <td>0.062650</td>\n",
       "      <td>0.033870</td>\n",
       "      <td>0.179150</td>\n",
       "      <td>0.061715</td>\n",
       "      <td>0.321750</td>\n",
       "      <td>1.080000</td>\n",
       "      <td>2.230000</td>\n",
       "      <td>24.190000</td>\n",
       "      <td>0.006266</td>\n",
       "      <td>0.020500</td>\n",
       "      <td>0.026245</td>\n",
       "      <td>0.010800</td>\n",
       "      <td>0.018610</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>14.915000</td>\n",
       "      <td>25.155000</td>\n",
       "      <td>97.455000</td>\n",
       "      <td>682.000000</td>\n",
       "      <td>0.132250</td>\n",
       "      <td>0.216000</td>\n",
       "      <td>0.229850</td>\n",
       "      <td>0.098855</td>\n",
       "      <td>0.280950</td>\n",
       "      <td>0.080075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.772500</td>\n",
       "      <td>21.597500</td>\n",
       "      <td>104.475000</td>\n",
       "      <td>779.125000</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.128750</td>\n",
       "      <td>0.128775</td>\n",
       "      <td>0.075022</td>\n",
       "      <td>0.195400</td>\n",
       "      <td>0.065735</td>\n",
       "      <td>0.474900</td>\n",
       "      <td>1.464500</td>\n",
       "      <td>3.295750</td>\n",
       "      <td>45.120000</td>\n",
       "      <td>0.008161</td>\n",
       "      <td>0.032060</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>0.014417</td>\n",
       "      <td>0.022937</td>\n",
       "      <td>0.004502</td>\n",
       "      <td>19.005000</td>\n",
       "      <td>29.452500</td>\n",
       "      <td>125.775000</td>\n",
       "      <td>1093.250000</td>\n",
       "      <td>0.145275</td>\n",
       "      <td>0.328975</td>\n",
       "      <td>0.378750</td>\n",
       "      <td>0.161300</td>\n",
       "      <td>0.318275</td>\n",
       "      <td>0.091853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>33.810000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.144700</td>\n",
       "      <td>0.311400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>2.873000</td>\n",
       "      <td>4.885000</td>\n",
       "      <td>21.980000</td>\n",
       "      <td>542.200000</td>\n",
       "      <td>0.023330</td>\n",
       "      <td>0.106400</td>\n",
       "      <td>0.396000</td>\n",
       "      <td>0.052790</td>\n",
       "      <td>0.061460</td>\n",
       "      <td>0.029840</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.105000</td>\n",
       "      <td>0.290300</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter    mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter   worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "count   398.000000    398.000000      398.000000   398.000000       398.000000        398.000000      398.000000           398.000000     398.000000              398.000000    398.000000     398.000000       398.000000  398.000000        398.000000         398.000000       398.000000            398.000000      398.000000               398.000000    398.000000     398.000000       398.000000   398.000000        398.000000         398.000000       398.000000            398.000000      398.000000               398.000000\n",
       "mean     14.185500     19.192839       92.335503   661.859045         0.096475          0.104080        0.088794             0.049479       0.180698                0.062646      0.409096       1.204458         2.866619   41.384364          0.006984           0.025099         0.031226              0.011678        0.020402                 0.003735     16.359621      25.534523       107.736030   894.246985          0.132529           0.253369         0.268531              0.115221        0.289187                 0.083718\n",
       "std       3.598618      4.190476       24.769334   366.558262         0.013916          0.051031        0.078154             0.039216       0.028057                0.006861      0.293965       0.551973         2.109921   49.830675          0.002861           0.016835         0.028576              0.005862        0.008181                 0.002638      4.965170       6.147466        34.460647   597.853882          0.022989           0.153653         0.193250              0.065688        0.063439                 0.017957\n",
       "min       6.981000      9.710000       43.790000   143.500000         0.052630          0.019380        0.000000             0.000000       0.106000                0.049960      0.111500       0.362800         0.757000    7.228000          0.001713           0.002252         0.000000              0.000000        0.007882                 0.000895      7.930000      12.020000        50.410000   185.200000          0.071170           0.027290         0.000000              0.000000        0.156500                 0.055040\n",
       "25%      11.710000     16.032500       75.310000   420.350000         0.086740          0.066805        0.030385             0.020710       0.161525                0.057685      0.232325       0.828050         1.596250   17.850000          0.005180           0.013628         0.016055              0.007932        0.014950                 0.002238     13.052500      20.865000        84.122500   516.425000          0.116300           0.148600         0.119100              0.065320        0.247850                 0.071462\n",
       "50%      13.310000     18.725000       86.140000   548.450000         0.096035          0.094035        0.062650             0.033870       0.179150                0.061715      0.321750       1.080000         2.230000   24.190000          0.006266           0.020500         0.026245              0.010800        0.018610                 0.003110     14.915000      25.155000        97.455000   682.000000          0.132250           0.216000         0.229850              0.098855        0.280950                 0.080075\n",
       "75%      15.772500     21.597500      104.475000   779.125000         0.105400          0.128750        0.128775             0.075022       0.195400                0.065735      0.474900       1.464500         3.295750   45.120000          0.008161           0.032060         0.040000              0.014417        0.022937                 0.004502     19.005000      29.452500       125.775000  1093.250000          0.145275           0.328975         0.378750              0.161300        0.318275                 0.091853\n",
       "max      28.110000     33.810000      188.500000  2501.000000         0.144700          0.311400        0.426800             0.201200       0.304000                0.097440      2.873000       4.885000        21.980000  542.200000          0.023330           0.106400         0.396000              0.052790        0.061460                 0.029840     36.040000      49.540000       251.200000  4254.000000          0.222600           1.058000         1.105000              0.290300        0.663800                 0.207500"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd37a388",
   "metadata": {},
   "source": [
    "> Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfb5bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zscore_normalize_features(X):\n",
    "    \"\"\"\n",
    "    computes  X, zcore normalized by column\n",
    "    \n",
    "    Args:\n",
    "      X (ndarray (m,n))     : input data, m examples, n features\n",
    "      \n",
    "    Returns:\n",
    "      X_norm (ndarray (m,n)): input normalized by column\n",
    "      mu (ndarray (n,))     : mean of each feature\n",
    "      sigma (ndarray (n,))  : standard deviation of each feature\n",
    "    \"\"\"\n",
    "    # find the mean of each column/feature\n",
    "    mu     = np.mean(X, axis=0)                 # mu will have shape (n,)\n",
    "    # find the standard deviation of each column/feature\n",
    "    sigma  = np.std(X, axis=0)                  # sigma will have shape (n,)\n",
    "    # element-wise, subtract mu for that column from each example, divide by std for that column\n",
    "    X_norm = (X - mu) / sigma      \n",
    "\n",
    "    return (X_norm, mu, sigma)\n",
    "\n",
    "zscore_normalize_features(X_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "d59b1408",
   "metadata": {},
   "outputs": [],
   "source": [
    "#standardize\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# set up the scaler \n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit the scaler to the train set, it will learn the parameters\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# transform train and test sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# We fit the StandardScaler to only the training set so that it learns the mean and standard deviation of the training \n",
    "# set only. Then, we use those learned parameters to transform both the training and testing sets. This is done to \n",
    "# avoid any data leakage from the testing set into the training set, which could result in overly optimistic \n",
    "# performance estimates on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "ed1a97c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.749980</td>\n",
       "      <td>-1.099787</td>\n",
       "      <td>-0.741586</td>\n",
       "      <td>-0.701887</td>\n",
       "      <td>0.584593</td>\n",
       "      <td>-0.427726</td>\n",
       "      <td>-0.457550</td>\n",
       "      <td>-0.760550</td>\n",
       "      <td>-0.099860</td>\n",
       "      <td>0.451444</td>\n",
       "      <td>-0.700612</td>\n",
       "      <td>-0.069762</td>\n",
       "      <td>-0.616731</td>\n",
       "      <td>-0.543408</td>\n",
       "      <td>-0.709153</td>\n",
       "      <td>-0.235489</td>\n",
       "      <td>0.36209</td>\n",
       "      <td>-0.621777</td>\n",
       "      <td>-0.241390</td>\n",
       "      <td>-0.045963</td>\n",
       "      <td>-0.798483</td>\n",
       "      <td>-0.591967</td>\n",
       "      <td>-0.746602</td>\n",
       "      <td>-0.714529</td>\n",
       "      <td>0.116328</td>\n",
       "      <td>-0.341255</td>\n",
       "      <td>-0.046272</td>\n",
       "      <td>-0.623597</td>\n",
       "      <td>0.077542</td>\n",
       "      <td>0.450628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.028214</td>\n",
       "      <td>-0.139262</td>\n",
       "      <td>-1.029804</td>\n",
       "      <td>-0.894732</td>\n",
       "      <td>0.742882</td>\n",
       "      <td>-0.731843</td>\n",
       "      <td>-0.843301</td>\n",
       "      <td>-0.808805</td>\n",
       "      <td>-1.159759</td>\n",
       "      <td>0.489386</td>\n",
       "      <td>-0.887604</td>\n",
       "      <td>0.650381</td>\n",
       "      <td>-0.869191</td>\n",
       "      <td>-0.629005</td>\n",
       "      <td>0.661884</td>\n",
       "      <td>-0.936002</td>\n",
       "      <td>-0.46060</td>\n",
       "      <td>-0.423483</td>\n",
       "      <td>-0.305031</td>\n",
       "      <td>-0.158707</td>\n",
       "      <td>-1.068703</td>\n",
       "      <td>-0.161981</td>\n",
       "      <td>-1.074343</td>\n",
       "      <td>-0.868941</td>\n",
       "      <td>0.382001</td>\n",
       "      <td>-0.970737</td>\n",
       "      <td>-0.954894</td>\n",
       "      <td>-0.761238</td>\n",
       "      <td>-1.071453</td>\n",
       "      <td>-0.295414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.538522</td>\n",
       "      <td>-0.299349</td>\n",
       "      <td>-0.568574</td>\n",
       "      <td>-0.550561</td>\n",
       "      <td>-0.714814</td>\n",
       "      <td>-0.731647</td>\n",
       "      <td>-0.639856</td>\n",
       "      <td>-0.668125</td>\n",
       "      <td>0.581758</td>\n",
       "      <td>-0.053477</td>\n",
       "      <td>-0.644071</td>\n",
       "      <td>-0.402987</td>\n",
       "      <td>-0.656119</td>\n",
       "      <td>-0.499806</td>\n",
       "      <td>-0.512840</td>\n",
       "      <td>-0.563799</td>\n",
       "      <td>-0.39543</td>\n",
       "      <td>-0.641247</td>\n",
       "      <td>-0.295240</td>\n",
       "      <td>-0.474919</td>\n",
       "      <td>-0.558512</td>\n",
       "      <td>-0.051227</td>\n",
       "      <td>-0.614110</td>\n",
       "      <td>-0.552748</td>\n",
       "      <td>-0.471637</td>\n",
       "      <td>-0.485919</td>\n",
       "      <td>-0.384602</td>\n",
       "      <td>-0.504704</td>\n",
       "      <td>0.349008</td>\n",
       "      <td>-0.133716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter  worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "0    -0.749980     -1.099787       -0.741586  -0.701887         0.584593         -0.427726       -0.457550            -0.760550      -0.099860                0.451444     -0.700612      -0.069762        -0.616731   -0.543408         -0.709153          -0.235489          0.36209             -0.621777       -0.241390                -0.045963     -0.798483      -0.591967        -0.746602   -0.714529          0.116328          -0.341255        -0.046272             -0.623597        0.077542                 0.450628\n",
       "1    -1.028214     -0.139262       -1.029804  -0.894732         0.742882         -0.731843       -0.843301            -0.808805      -1.159759                0.489386     -0.887604       0.650381        -0.869191   -0.629005          0.661884          -0.936002         -0.46060             -0.423483       -0.305031                -0.158707     -1.068703      -0.161981        -1.074343   -0.868941          0.382001          -0.970737        -0.954894             -0.761238       -1.071453                -0.295414\n",
       "2    -0.538522     -0.299349       -0.568574  -0.550561        -0.714814         -0.731647       -0.639856            -0.668125       0.581758               -0.053477     -0.644071      -0.402987        -0.656119   -0.499806         -0.512840          -0.563799         -0.39543             -0.641247       -0.295240                -0.474919     -0.558512      -0.051227        -0.614110   -0.552748         -0.471637          -0.485919        -0.384602             -0.504704        0.349008                -0.133716"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "293fcd6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "      <td>3.980000e+02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-2.289626e-15</td>\n",
       "      <td>-1.606755e-16</td>\n",
       "      <td>2.392280e-15</td>\n",
       "      <td>-6.516284e-16</td>\n",
       "      <td>-1.385826e-15</td>\n",
       "      <td>1.696019e-16</td>\n",
       "      <td>1.325573e-15</td>\n",
       "      <td>5.891435e-16</td>\n",
       "      <td>-3.807117e-15</td>\n",
       "      <td>-1.689324e-15</td>\n",
       "      <td>-2.845295e-16</td>\n",
       "      <td>1.762967e-16</td>\n",
       "      <td>-4.686369e-16</td>\n",
       "      <td>1.115802e-17</td>\n",
       "      <td>2.144572e-15</td>\n",
       "      <td>5.445114e-16</td>\n",
       "      <td>4.418576e-16</td>\n",
       "      <td>3.168878e-16</td>\n",
       "      <td>-1.472859e-16</td>\n",
       "      <td>1.162666e-15</td>\n",
       "      <td>1.008685e-15</td>\n",
       "      <td>-7.141133e-16</td>\n",
       "      <td>-1.887937e-15</td>\n",
       "      <td>-1.696019e-16</td>\n",
       "      <td>3.586188e-15</td>\n",
       "      <td>-8.346199e-16</td>\n",
       "      <td>-3.436670e-16</td>\n",
       "      <td>-7.141133e-17</td>\n",
       "      <td>-5.500904e-16</td>\n",
       "      <td>3.874065e-15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "      <td>1.001259e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-2.004538e+00</td>\n",
       "      <td>-2.265799e+00</td>\n",
       "      <td>-1.962370e+00</td>\n",
       "      <td>-1.415904e+00</td>\n",
       "      <td>-3.154619e+00</td>\n",
       "      <td>-1.661853e+00</td>\n",
       "      <td>-1.137578e+00</td>\n",
       "      <td>-1.263267e+00</td>\n",
       "      <td>-2.665744e+00</td>\n",
       "      <td>-1.851347e+00</td>\n",
       "      <td>-1.013628e+00</td>\n",
       "      <td>-1.526736e+00</td>\n",
       "      <td>-1.001115e+00</td>\n",
       "      <td>-6.863113e-01</td>\n",
       "      <td>-1.844334e+00</td>\n",
       "      <td>-1.358879e+00</td>\n",
       "      <td>-1.094086e+00</td>\n",
       "      <td>-1.994631e+00</td>\n",
       "      <td>-1.532310e+00</td>\n",
       "      <td>-1.078191e+00</td>\n",
       "      <td>-1.699888e+00</td>\n",
       "      <td>-2.201156e+00</td>\n",
       "      <td>-1.665615e+00</td>\n",
       "      <td>-1.187480e+00</td>\n",
       "      <td>-2.672370e+00</td>\n",
       "      <td>-1.473214e+00</td>\n",
       "      <td>-1.391304e+00</td>\n",
       "      <td>-1.756275e+00</td>\n",
       "      <td>-2.094184e+00</td>\n",
       "      <td>-1.599036e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-6.887687e-01</td>\n",
       "      <td>-7.551212e-01</td>\n",
       "      <td>-6.882273e-01</td>\n",
       "      <td>-6.596851e-01</td>\n",
       "      <td>-7.004241e-01</td>\n",
       "      <td>-7.313526e-01</td>\n",
       "      <td>-7.483043e-01</td>\n",
       "      <td>-7.345076e-01</td>\n",
       "      <td>-6.842324e-01</td>\n",
       "      <td>-7.240302e-01</td>\n",
       "      <td>-6.020917e-01</td>\n",
       "      <td>-6.827902e-01</td>\n",
       "      <td>-6.028507e-01</td>\n",
       "      <td>-4.728811e-01</td>\n",
       "      <td>-6.312051e-01</td>\n",
       "      <td>-6.823056e-01</td>\n",
       "      <td>-5.315521e-01</td>\n",
       "      <td>-6.398811e-01</td>\n",
       "      <td>-6.672911e-01</td>\n",
       "      <td>-5.683025e-01</td>\n",
       "      <td>-6.669023e-01</td>\n",
       "      <td>-7.605410e-01</td>\n",
       "      <td>-6.860942e-01</td>\n",
       "      <td>-6.327592e-01</td>\n",
       "      <td>-7.068235e-01</td>\n",
       "      <td>-6.827132e-01</td>\n",
       "      <td>-7.742265e-01</td>\n",
       "      <td>-7.606279e-01</td>\n",
       "      <td>-6.524167e-01</td>\n",
       "      <td>-6.833502e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-2.435940e-01</td>\n",
       "      <td>-1.117840e-01</td>\n",
       "      <td>-2.504428e-01</td>\n",
       "      <td>-3.097783e-01</td>\n",
       "      <td>-3.165413e-02</td>\n",
       "      <td>-1.970875e-01</td>\n",
       "      <td>-3.349449e-01</td>\n",
       "      <td>-3.985115e-01</td>\n",
       "      <td>-5.525183e-02</td>\n",
       "      <td>-1.359284e-01</td>\n",
       "      <td>-2.975057e-01</td>\n",
       "      <td>-2.257626e-01</td>\n",
       "      <td>-3.021060e-01</td>\n",
       "      <td>-3.454901e-01</td>\n",
       "      <td>-2.509158e-01</td>\n",
       "      <td>-2.735541e-01</td>\n",
       "      <td>-1.745157e-01</td>\n",
       "      <td>-1.500396e-01</td>\n",
       "      <td>-2.193611e-01</td>\n",
       "      <td>-2.370954e-01</td>\n",
       "      <td>-2.913171e-01</td>\n",
       "      <td>-6.181413e-02</td>\n",
       "      <td>-2.987167e-01</td>\n",
       "      <td>-3.554617e-01</td>\n",
       "      <td>-1.215325e-02</td>\n",
       "      <td>-2.435097e-01</td>\n",
       "      <td>-2.004118e-01</td>\n",
       "      <td>-2.494669e-01</td>\n",
       "      <td>-1.300027e-01</td>\n",
       "      <td>-2.031343e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.415577e-01</td>\n",
       "      <td>5.745618e-01</td>\n",
       "      <td>4.907188e-01</td>\n",
       "      <td>3.203135e-01</td>\n",
       "      <td>6.421523e-01</td>\n",
       "      <td>4.840367e-01</td>\n",
       "      <td>5.122082e-01</td>\n",
       "      <td>6.521782e-01</td>\n",
       "      <td>5.246593e-01</td>\n",
       "      <td>4.507140e-01</td>\n",
       "      <td>2.241309e-01</td>\n",
       "      <td>4.717057e-01</td>\n",
       "      <td>2.036434e-01</td>\n",
       "      <td>7.506095e-02</td>\n",
       "      <td>4.120316e-01</td>\n",
       "      <td>4.139931e-01</td>\n",
       "      <td>3.074308e-01</td>\n",
       "      <td>4.678132e-01</td>\n",
       "      <td>3.102610e-01</td>\n",
       "      <td>2.913176e-01</td>\n",
       "      <td>5.334579e-01</td>\n",
       "      <td>6.381342e-01</td>\n",
       "      <td>5.241247e-01</td>\n",
       "      <td>3.332813e-01</td>\n",
       "      <td>5.551245e-01</td>\n",
       "      <td>4.926776e-01</td>\n",
       "      <td>5.710645e-01</td>\n",
       "      <td>7.023581e-01</td>\n",
       "      <td>4.590940e-01</td>\n",
       "      <td>4.535557e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>3.874272e+00</td>\n",
       "      <td>3.492577e+00</td>\n",
       "      <td>3.887288e+00</td>\n",
       "      <td>5.023637e+00</td>\n",
       "      <td>3.469765e+00</td>\n",
       "      <td>4.067714e+00</td>\n",
       "      <td>4.330322e+00</td>\n",
       "      <td>3.873693e+00</td>\n",
       "      <td>4.400250e+00</td>\n",
       "      <td>5.077455e+00</td>\n",
       "      <td>8.392180e+00</td>\n",
       "      <td>6.676362e+00</td>\n",
       "      <td>9.070214e+00</td>\n",
       "      <td>1.006300e+01</td>\n",
       "      <td>5.720160e+00</td>\n",
       "      <td>4.835468e+00</td>\n",
       "      <td>1.278093e+01</td>\n",
       "      <td>7.021664e+00</td>\n",
       "      <td>5.024846e+00</td>\n",
       "      <td>9.909610e+00</td>\n",
       "      <td>3.968676e+00</td>\n",
       "      <td>3.909853e+00</td>\n",
       "      <td>4.168365e+00</td>\n",
       "      <td>5.626762e+00</td>\n",
       "      <td>3.922860e+00</td>\n",
       "      <td>5.243276e+00</td>\n",
       "      <td>4.333890e+00</td>\n",
       "      <td>2.668655e+00</td>\n",
       "      <td>5.912480e+00</td>\n",
       "      <td>6.901830e+00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        mean radius  mean texture  mean perimeter     mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error    area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter    worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "count  3.980000e+02  3.980000e+02    3.980000e+02  3.980000e+02     3.980000e+02      3.980000e+02    3.980000e+02         3.980000e+02   3.980000e+02            3.980000e+02  3.980000e+02   3.980000e+02     3.980000e+02  3.980000e+02      3.980000e+02       3.980000e+02     3.980000e+02          3.980000e+02    3.980000e+02             3.980000e+02  3.980000e+02   3.980000e+02     3.980000e+02  3.980000e+02      3.980000e+02       3.980000e+02     3.980000e+02          3.980000e+02    3.980000e+02             3.980000e+02\n",
       "mean  -2.289626e-15 -1.606755e-16    2.392280e-15 -6.516284e-16    -1.385826e-15      1.696019e-16    1.325573e-15         5.891435e-16  -3.807117e-15           -1.689324e-15 -2.845295e-16   1.762967e-16    -4.686369e-16  1.115802e-17      2.144572e-15       5.445114e-16     4.418576e-16          3.168878e-16   -1.472859e-16             1.162666e-15  1.008685e-15  -7.141133e-16    -1.887937e-15 -1.696019e-16      3.586188e-15      -8.346199e-16    -3.436670e-16         -7.141133e-17   -5.500904e-16             3.874065e-15\n",
       "std    1.001259e+00  1.001259e+00    1.001259e+00  1.001259e+00     1.001259e+00      1.001259e+00    1.001259e+00         1.001259e+00   1.001259e+00            1.001259e+00  1.001259e+00   1.001259e+00     1.001259e+00  1.001259e+00      1.001259e+00       1.001259e+00     1.001259e+00          1.001259e+00    1.001259e+00             1.001259e+00  1.001259e+00   1.001259e+00     1.001259e+00  1.001259e+00      1.001259e+00       1.001259e+00     1.001259e+00          1.001259e+00    1.001259e+00             1.001259e+00\n",
       "min   -2.004538e+00 -2.265799e+00   -1.962370e+00 -1.415904e+00    -3.154619e+00     -1.661853e+00   -1.137578e+00        -1.263267e+00  -2.665744e+00           -1.851347e+00 -1.013628e+00  -1.526736e+00    -1.001115e+00 -6.863113e-01     -1.844334e+00      -1.358879e+00    -1.094086e+00         -1.994631e+00   -1.532310e+00            -1.078191e+00 -1.699888e+00  -2.201156e+00    -1.665615e+00 -1.187480e+00     -2.672370e+00      -1.473214e+00    -1.391304e+00         -1.756275e+00   -2.094184e+00            -1.599036e+00\n",
       "25%   -6.887687e-01 -7.551212e-01   -6.882273e-01 -6.596851e-01    -7.004241e-01     -7.313526e-01   -7.483043e-01        -7.345076e-01  -6.842324e-01           -7.240302e-01 -6.020917e-01  -6.827902e-01    -6.028507e-01 -4.728811e-01     -6.312051e-01      -6.823056e-01    -5.315521e-01         -6.398811e-01   -6.672911e-01            -5.683025e-01 -6.669023e-01  -7.605410e-01    -6.860942e-01 -6.327592e-01     -7.068235e-01      -6.827132e-01    -7.742265e-01         -7.606279e-01   -6.524167e-01            -6.833502e-01\n",
       "50%   -2.435940e-01 -1.117840e-01   -2.504428e-01 -3.097783e-01    -3.165413e-02     -1.970875e-01   -3.349449e-01        -3.985115e-01  -5.525183e-02           -1.359284e-01 -2.975057e-01  -2.257626e-01    -3.021060e-01 -3.454901e-01     -2.509158e-01      -2.735541e-01    -1.745157e-01         -1.500396e-01   -2.193611e-01            -2.370954e-01 -2.913171e-01  -6.181413e-02    -2.987167e-01 -3.554617e-01     -1.215325e-02      -2.435097e-01    -2.004118e-01         -2.494669e-01   -1.300027e-01            -2.031343e-01\n",
       "75%    4.415577e-01  5.745618e-01    4.907188e-01  3.203135e-01     6.421523e-01      4.840367e-01    5.122082e-01         6.521782e-01   5.246593e-01            4.507140e-01  2.241309e-01   4.717057e-01     2.036434e-01  7.506095e-02      4.120316e-01       4.139931e-01     3.074308e-01          4.678132e-01    3.102610e-01             2.913176e-01  5.334579e-01   6.381342e-01     5.241247e-01  3.332813e-01      5.551245e-01       4.926776e-01     5.710645e-01          7.023581e-01    4.590940e-01             4.535557e-01\n",
       "max    3.874272e+00  3.492577e+00    3.887288e+00  5.023637e+00     3.469765e+00      4.067714e+00    4.330322e+00         3.873693e+00   4.400250e+00            5.077455e+00  8.392180e+00   6.676362e+00     9.070214e+00  1.006300e+01      5.720160e+00       4.835468e+00     1.278093e+01          7.021664e+00    5.024846e+00             9.909610e+00  3.968676e+00   3.909853e+00     4.168365e+00  5.626762e+00      3.922860e+00       5.243276e+00     4.333890e+00          2.668655e+00    5.912480e+00             6.901830e+00"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_scaled.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0d4a09e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# let's compare the variable distributions before and after scaling\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12, 5))\n",
    "\n",
    "# before scaling\n",
    "ax1.set_title('Before Scaling')\n",
    "sns.kdeplot(X_train['mean radius'], ax=ax1)\n",
    "sns.kdeplot(X_train['mean perimeter'], ax=ax1)\n",
    "sns.kdeplot(X_train['mean area'], ax=ax1)\n",
    "\n",
    "# after scaling\n",
    "ax2.set_title('After Standard Scaling')\n",
    "sns.kdeplot(X_train_scaled['mean radius'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['mean perimeter'], ax=ax2)\n",
    "sns.kdeplot(X_train_scaled['mean area'], ax=ax2)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc5eb28d",
   "metadata": {},
   "source": [
    "> Mean Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "8c0e3262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "\n        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import numpy as np\\nfrom sklearn.preprocessing import RobustScaler\\nimport sklearn'); }\n    ",
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def mean_normalize(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Perform mean normalization on both the training and testing sets.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy.ndarray\n",
    "        The training set features as a 2D array.\n",
    "\n",
    "    X_test: numpy.ndarray\n",
    "        The testing set features as a 2D array.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_norm: numpy.ndarray\n",
    "        The mean-normalized training set features as a 2D array.\n",
    "\n",
    "    X_test_norm: numpy.ndarray\n",
    "        The mean-normalized testing set features as a 2D array.\n",
    "    \"\"\"\n",
    "    scaler_mean = StandardScaler(with_mean=True, with_std=False) # set up the scaler\n",
    "    scaler_minmax = RobustScaler(with_centering = False, with_scaling = True,\n",
    "                                 quantile_range = (0,100))\n",
    "    \n",
    "    scaler_mean.fit(X_train) # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler_minmax.fit(X_train) #fit the scaler to the train set, it will learn the parameters\n",
    "    \n",
    "    X_train_norm = scaler_minmax.transform(scaler_mean.transform(X_train)) # transform train set\n",
    "    X_test_norm = scaler_minmax.transform(scaler_mean.transform(X_test)) # transform test set\n",
    "    return X_train_norm, X_test_norm \n",
    "\n",
    "X_train_norm, X_test_norm = mean_normalize(X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bfae9ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n",
    "X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "87e5ebd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.2</td>\n",
       "      <td>19.2</td>\n",
       "      <td>92.3</td>\n",
       "      <td>661.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2.9</td>\n",
       "      <td>41.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.4</td>\n",
       "      <td>25.5</td>\n",
       "      <td>107.7</td>\n",
       "      <td>894.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.6</td>\n",
       "      <td>4.2</td>\n",
       "      <td>24.8</td>\n",
       "      <td>366.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>49.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>34.5</td>\n",
       "      <td>597.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>7.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>43.8</td>\n",
       "      <td>143.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.8</td>\n",
       "      <td>7.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.9</td>\n",
       "      <td>12.0</td>\n",
       "      <td>50.4</td>\n",
       "      <td>185.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.7</td>\n",
       "      <td>16.0</td>\n",
       "      <td>75.3</td>\n",
       "      <td>420.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>17.8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.1</td>\n",
       "      <td>20.9</td>\n",
       "      <td>84.1</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.3</td>\n",
       "      <td>18.7</td>\n",
       "      <td>86.1</td>\n",
       "      <td>548.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>24.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14.9</td>\n",
       "      <td>25.2</td>\n",
       "      <td>97.5</td>\n",
       "      <td>682.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.8</td>\n",
       "      <td>21.6</td>\n",
       "      <td>104.5</td>\n",
       "      <td>779.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>3.3</td>\n",
       "      <td>45.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>29.5</td>\n",
       "      <td>125.8</td>\n",
       "      <td>1093.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.1</td>\n",
       "      <td>33.8</td>\n",
       "      <td>188.5</td>\n",
       "      <td>2501.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>4.9</td>\n",
       "      <td>22.0</td>\n",
       "      <td>542.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>49.5</td>\n",
       "      <td>251.2</td>\n",
       "      <td>4254.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.1</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter  mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter  worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "count        398.0         398.0           398.0      398.0            398.0             398.0           398.0                398.0          398.0                   398.0         398.0          398.0            398.0       398.0             398.0              398.0            398.0                 398.0           398.0                    398.0         398.0          398.0            398.0       398.0             398.0              398.0            398.0                 398.0           398.0                    398.0\n",
       "mean          14.2          19.2            92.3      661.9              0.1               0.1             0.1                  0.0            0.2                     0.1           0.4            1.2              2.9        41.4               0.0                0.0              0.0                   0.0             0.0                      0.0          16.4           25.5            107.7       894.2               0.1                0.3              0.3                   0.1             0.3                      0.1\n",
       "std            3.6           4.2            24.8      366.6              0.0               0.1             0.1                  0.0            0.0                     0.0           0.3            0.6              2.1        49.8               0.0                0.0              0.0                   0.0             0.0                      0.0           5.0            6.1             34.5       597.9               0.0                0.2              0.2                   0.1             0.1                      0.0\n",
       "min            7.0           9.7            43.8      143.5              0.1               0.0             0.0                  0.0            0.1                     0.0           0.1            0.4              0.8         7.2               0.0                0.0              0.0                   0.0             0.0                      0.0           7.9           12.0             50.4       185.2               0.1                0.0              0.0                   0.0             0.2                      0.1\n",
       "25%           11.7          16.0            75.3      420.4              0.1               0.1             0.0                  0.0            0.2                     0.1           0.2            0.8              1.6        17.8               0.0                0.0              0.0                   0.0             0.0                      0.0          13.1           20.9             84.1       516.4               0.1                0.1              0.1                   0.1             0.2                      0.1\n",
       "50%           13.3          18.7            86.1      548.4              0.1               0.1             0.1                  0.0            0.2                     0.1           0.3            1.1              2.2        24.2               0.0                0.0              0.0                   0.0             0.0                      0.0          14.9           25.2             97.5       682.0               0.1                0.2              0.2                   0.1             0.3                      0.1\n",
       "75%           15.8          21.6           104.5      779.1              0.1               0.1             0.1                  0.1            0.2                     0.1           0.5            1.5              3.3        45.1               0.0                0.0              0.0                   0.0             0.0                      0.0          19.0           29.5            125.8      1093.2               0.1                0.3              0.4                   0.2             0.3                      0.1\n",
       "max           28.1          33.8           188.5     2501.0              0.1               0.3             0.4                  0.2            0.3                     0.1           2.9            4.9             22.0       542.2               0.0                0.1              0.4                   0.1             0.1                      0.0          36.0           49.5            251.2      4254.0               0.2                1.1              1.1                   0.3             0.7                      0.2"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(X_train.describe(), 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "8abb2490",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter  mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter  worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "count        398.0         398.0           398.0      398.0            398.0             398.0           398.0                398.0          398.0                   398.0         398.0          398.0            398.0       398.0             398.0              398.0            398.0                 398.0           398.0                    398.0         398.0          398.0            398.0       398.0             398.0              398.0            398.0                 398.0           398.0                    398.0\n",
       "mean          -0.0          -0.0             0.0       -0.0             -0.0               0.0             0.0                  0.0           -0.0                    -0.0          -0.0            0.0             -0.0         0.0               0.0                0.0              0.0                   0.0            -0.0                      0.0           0.0           -0.0             -0.0        -0.0               0.0               -0.0             -0.0                  -0.0            -0.0                      0.0\n",
       "std            0.2           0.2             0.2        0.2              0.2               0.2             0.2                  0.2            0.1                     0.1           0.1            0.1              0.1         0.1               0.1                0.2              0.1                   0.1             0.2                      0.1           0.2            0.2              0.2         0.1               0.2                0.1              0.2                   0.2             0.1                      0.1\n",
       "min           -0.3          -0.4            -0.3       -0.2             -0.5              -0.3            -0.2                 -0.2           -0.4                    -0.3          -0.1           -0.2             -0.1        -0.1              -0.2               -0.2             -0.1                  -0.2            -0.2                     -0.1          -0.3           -0.4             -0.3        -0.2              -0.4               -0.2             -0.2                  -0.4            -0.3                     -0.2\n",
       "25%           -0.1          -0.1            -0.1       -0.1             -0.1              -0.1            -0.1                 -0.1           -0.1                    -0.1          -0.1           -0.1             -0.1        -0.0              -0.1               -0.1             -0.0                  -0.1            -0.1                     -0.1          -0.1           -0.1             -0.1        -0.1              -0.1               -0.1             -0.1                  -0.2            -0.1                     -0.1\n",
       "50%           -0.0          -0.0            -0.0       -0.0             -0.0              -0.0            -0.1                 -0.1           -0.0                    -0.0          -0.0           -0.0             -0.0        -0.0              -0.0               -0.0             -0.0                  -0.0            -0.0                     -0.0          -0.1           -0.0             -0.1        -0.1              -0.0               -0.0             -0.0                  -0.1            -0.0                     -0.0\n",
       "75%            0.1           0.1             0.1        0.0              0.1               0.1             0.1                  0.1            0.1                     0.1           0.0            0.1              0.0         0.0               0.1                0.1              0.0                   0.1             0.0                      0.0           0.1            0.1              0.1         0.0               0.1                0.1              0.1                   0.2             0.1                      0.1\n",
       "max            0.7           0.6             0.7        0.8              0.5               0.7             0.8                  0.8            0.6                     0.7           0.9            0.8              0.9         0.9               0.8                0.8              0.9                   0.8             0.8                      0.9           0.7            0.6              0.7         0.8               0.6                0.8              0.8                   0.6             0.7                      0.8"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(X_train_norm.describe(), 1) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf6dd42b",
   "metadata": {},
   "source": [
    "> Scaling to Maximum and Minimum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "d8820323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def scale_min_max(X_train, X_test):\n",
    "    \"\"\"\n",
    "    Scales the features in X_train and X_test to the range [0, 1] using MinMaxScaler.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train: numpy array\n",
    "        Training data features\n",
    "        \n",
    "    X_test: numpy array\n",
    "        Test data features\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train_scaled: numpy array\n",
    "        Scaled training data features\n",
    "        \n",
    "    X_test_scaled: numpy array\n",
    "        Scaled test data features\n",
    "    \"\"\"\n",
    "    # set up the scaler\n",
    "    scaler = MinMaxScaler()\n",
    "    \n",
    "    # fit the scaler to the train set, it will learn the parameters\n",
    "    scaler.fit(X_train)\n",
    "    \n",
    "    # transform train and test sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "X_train_scaled, X_test_scaled = scale_min_max(X_train, X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "2c1ab93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns = X_train.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_norm, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "03073ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>radius error</th>\n",
       "      <th>texture error</th>\n",
       "      <th>perimeter error</th>\n",
       "      <th>area error</th>\n",
       "      <th>smoothness error</th>\n",
       "      <th>compactness error</th>\n",
       "      <th>concavity error</th>\n",
       "      <th>concave points error</th>\n",
       "      <th>symmetry error</th>\n",
       "      <th>fractal dimension error</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "      <td>398.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       mean radius  mean texture  mean perimeter  mean area  mean smoothness  mean compactness  mean concavity  mean concave points  mean symmetry  mean fractal dimension  radius error  texture error  perimeter error  area error  smoothness error  compactness error  concavity error  concave points error  symmetry error  fractal dimension error  worst radius  worst texture  worst perimeter  worst area  worst smoothness  worst compactness  worst concavity  worst concave points  worst symmetry  worst fractal dimension\n",
       "count        398.0         398.0           398.0      398.0            398.0             398.0           398.0                398.0          398.0                   398.0         398.0          398.0            398.0       398.0             398.0              398.0            398.0                 398.0           398.0                    398.0         398.0          398.0            398.0       398.0             398.0              398.0            398.0                 398.0           398.0                    398.0\n",
       "mean           0.3           0.4             0.3        0.2              0.5               0.3             0.2                  0.2            0.4                     0.3           0.1            0.2              0.1         0.1               0.2                0.2              0.1                   0.2             0.2                      0.1           0.3            0.4              0.3         0.2               0.4                0.2              0.2                   0.4             0.3                      0.2\n",
       "std            0.2           0.2             0.2        0.2              0.2               0.2             0.2                  0.2            0.1                     0.1           0.1            0.1              0.1         0.1               0.1                0.2              0.1                   0.1             0.2                      0.1           0.2            0.2              0.2         0.1               0.2                0.1              0.2                   0.2             0.1                      0.1\n",
       "min            0.0           0.0             0.0        0.0              0.0               0.0             0.0                  0.0            0.0                     0.0           0.0            0.0              0.0         0.0               0.0                0.0              0.0                   0.0             0.0                      0.0           0.0            0.0              0.0         0.0               0.0                0.0              0.0                   0.0             0.0                      0.0\n",
       "25%            0.2           0.3             0.2        0.1              0.4               0.2             0.1                  0.1            0.3                     0.2           0.0            0.1              0.0         0.0               0.2                0.1              0.0                   0.2             0.1                      0.0           0.2            0.2              0.2         0.1               0.3                0.1              0.1                   0.2             0.2                      0.1\n",
       "50%            0.3           0.4             0.3        0.2              0.5               0.3             0.1                  0.2            0.4                     0.2           0.1            0.2              0.1         0.0               0.2                0.2              0.1                   0.2             0.2                      0.1           0.2            0.4              0.2         0.1               0.4                0.2              0.2                   0.3             0.2                      0.2\n",
       "75%            0.4           0.5             0.4        0.3              0.6               0.4             0.3                  0.4            0.5                     0.3           0.1            0.2              0.1         0.1               0.3                0.3              0.1                   0.3             0.3                      0.1           0.4            0.5              0.4         0.2               0.5                0.3              0.3                   0.6             0.3                      0.2\n",
       "max            1.0           1.0             1.0        1.0              1.0               1.0             1.0                  1.0            1.0                     1.0           1.0            1.0              1.0         1.0               1.0                1.0              1.0                   1.0             1.0                      1.0           1.0            1.0              1.0         1.0               1.0                1.0              1.0                   1.0             1.0                      1.0"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.round(X_train_scaled.describe(), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0db13095",
   "metadata": {},
   "source": [
    "### Next thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "26905821",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "import numpy as np\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "5bf41d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_wine()\n",
    "features = load_wine().data\n",
    "target = load_wine().target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1ad2f46c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _wine_dataset:\n",
      "\n",
      "Wine recognition dataset\n",
      "------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    :Number of Instances: 178\n",
      "    :Number of Attributes: 13 numeric, predictive attributes and the class\n",
      "    :Attribute Information:\n",
      " \t\t- Alcohol\n",
      " \t\t- Malic acid\n",
      " \t\t- Ash\n",
      "\t\t- Alcalinity of ash  \n",
      " \t\t- Magnesium\n",
      "\t\t- Total phenols\n",
      " \t\t- Flavanoids\n",
      " \t\t- Nonflavanoid phenols\n",
      " \t\t- Proanthocyanins\n",
      "\t\t- Color intensity\n",
      " \t\t- Hue\n",
      " \t\t- OD280/OD315 of diluted wines\n",
      " \t\t- Proline\n",
      "\n",
      "    - class:\n",
      "            - class_0\n",
      "            - class_1\n",
      "            - class_2\n",
      "\t\t\n",
      "    :Summary Statistics:\n",
      "    \n",
      "    ============================= ==== ===== ======= =====\n",
      "                                   Min   Max   Mean     SD\n",
      "    ============================= ==== ===== ======= =====\n",
      "    Alcohol:                      11.0  14.8    13.0   0.8\n",
      "    Malic Acid:                   0.74  5.80    2.34  1.12\n",
      "    Ash:                          1.36  3.23    2.36  0.27\n",
      "    Alcalinity of Ash:            10.6  30.0    19.5   3.3\n",
      "    Magnesium:                    70.0 162.0    99.7  14.3\n",
      "    Total Phenols:                0.98  3.88    2.29  0.63\n",
      "    Flavanoids:                   0.34  5.08    2.03  1.00\n",
      "    Nonflavanoid Phenols:         0.13  0.66    0.36  0.12\n",
      "    Proanthocyanins:              0.41  3.58    1.59  0.57\n",
      "    Colour Intensity:              1.3  13.0     5.1   2.3\n",
      "    Hue:                          0.48  1.71    0.96  0.23\n",
      "    OD280/OD315 of diluted wines: 1.27  4.00    2.61  0.71\n",
      "    Proline:                       278  1680     746   315\n",
      "    ============================= ==== ===== ======= =====\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "    :Class Distribution: class_0 (59), class_1 (71), class_2 (48)\n",
      "    :Creator: R.A. Fisher\n",
      "    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n",
      "    :Date: July, 1988\n",
      "\n",
      "This is a copy of UCI ML Wine recognition datasets.\n",
      "https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data\n",
      "\n",
      "The data is the results of a chemical analysis of wines grown in the same\n",
      "region in Italy by three different cultivators. There are thirteen different\n",
      "measurements taken for different constituents found in the three types of\n",
      "wine.\n",
      "\n",
      "Original Owners: \n",
      "\n",
      "Forina, M. et al, PARVUS - \n",
      "An Extendible Package for Data Exploration, Classification and Correlation. \n",
      "Institute of Pharmaceutical and Food Analysis and Technologies,\n",
      "Via Brigata Salerno, 16147 Genoa, Italy.\n",
      "\n",
      "Citation:\n",
      "\n",
      "Lichman, M. (2013). UCI Machine Learning Repository\n",
      "[https://archive.ics.uci.edu/ml]. Irvine, CA: University of California,\n",
      "School of Information and Computer Science. \n",
      "\n",
      ".. topic:: References\n",
      "\n",
      "  (1) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  Comparison of Classifiers in High Dimensional Settings, \n",
      "  Tech. Rep. no. 92-02, (1992), Dept. of Computer Science and Dept. of  \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Technometrics). \n",
      "\n",
      "  The data was used with many others for comparing various \n",
      "  classifiers. The classes are separable, though only RDA \n",
      "  has achieved 100% correct classification. \n",
      "  (RDA : 100%, QDA 99.4%, LDA 98.9%, 1NN 96.1% (z-transformed data)) \n",
      "  (All results using the leave-one-out technique) \n",
      "\n",
      "  (2) S. Aeberhard, D. Coomans and O. de Vel, \n",
      "  \"THE CLASSIFICATION PERFORMANCE OF RDA\" \n",
      "  Tech. Rep. no. 92-01, (1992), Dept. of Computer Science and Dept. of \n",
      "  Mathematics and Statistics, James Cook University of North Queensland. \n",
      "  (Also submitted to Journal of Chemometrics).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(data.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e336cda1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178, 13), (178,))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9de798c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.feature_names\n",
    "df = pd.DataFrame(features, columns=data.feature_names)\n",
    "df[\"target\"] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5608cd19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>13.71</td>\n",
       "      <td>5.65</td>\n",
       "      <td>2.45</td>\n",
       "      <td>20.5</td>\n",
       "      <td>95.0</td>\n",
       "      <td>1.68</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.52</td>\n",
       "      <td>1.06</td>\n",
       "      <td>7.70</td>\n",
       "      <td>0.64</td>\n",
       "      <td>1.74</td>\n",
       "      <td>740.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>13.40</td>\n",
       "      <td>3.91</td>\n",
       "      <td>2.48</td>\n",
       "      <td>23.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.41</td>\n",
       "      <td>7.30</td>\n",
       "      <td>0.70</td>\n",
       "      <td>1.56</td>\n",
       "      <td>750.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175</th>\n",
       "      <td>13.27</td>\n",
       "      <td>4.28</td>\n",
       "      <td>2.26</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.69</td>\n",
       "      <td>0.43</td>\n",
       "      <td>1.35</td>\n",
       "      <td>10.20</td>\n",
       "      <td>0.59</td>\n",
       "      <td>1.56</td>\n",
       "      <td>835.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>13.17</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.37</td>\n",
       "      <td>20.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1.65</td>\n",
       "      <td>0.68</td>\n",
       "      <td>0.53</td>\n",
       "      <td>1.46</td>\n",
       "      <td>9.30</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.62</td>\n",
       "      <td>840.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>14.13</td>\n",
       "      <td>4.10</td>\n",
       "      <td>2.74</td>\n",
       "      <td>24.5</td>\n",
       "      <td>96.0</td>\n",
       "      <td>2.05</td>\n",
       "      <td>0.76</td>\n",
       "      <td>0.56</td>\n",
       "      <td>1.35</td>\n",
       "      <td>9.20</td>\n",
       "      <td>0.61</td>\n",
       "      <td>1.60</td>\n",
       "      <td>560.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>178 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  od280/od315_of_diluted_wines  proline  target\n",
       "0      14.23        1.71  2.43               15.6      127.0           2.80        3.06                  0.28             2.29             5.64  1.04                          3.92   1065.0       0\n",
       "1      13.20        1.78  2.14               11.2      100.0           2.65        2.76                  0.26             1.28             4.38  1.05                          3.40   1050.0       0\n",
       "2      13.16        2.36  2.67               18.6      101.0           2.80        3.24                  0.30             2.81             5.68  1.03                          3.17   1185.0       0\n",
       "3      14.37        1.95  2.50               16.8      113.0           3.85        3.49                  0.24             2.18             7.80  0.86                          3.45   1480.0       0\n",
       "4      13.24        2.59  2.87               21.0      118.0           2.80        2.69                  0.39             1.82             4.32  1.04                          2.93    735.0       0\n",
       "..       ...         ...   ...                ...        ...            ...         ...                   ...              ...              ...   ...                           ...      ...     ...\n",
       "173    13.71        5.65  2.45               20.5       95.0           1.68        0.61                  0.52             1.06             7.70  0.64                          1.74    740.0       2\n",
       "174    13.40        3.91  2.48               23.0      102.0           1.80        0.75                  0.43             1.41             7.30  0.70                          1.56    750.0       2\n",
       "175    13.27        4.28  2.26               20.0      120.0           1.59        0.69                  0.43             1.35            10.20  0.59                          1.56    835.0       2\n",
       "176    13.17        2.59  2.37               20.0      120.0           1.65        0.68                  0.53             1.46             9.30  0.60                          1.62    840.0       2\n",
       "177    14.13        4.10  2.74               24.5       96.0           2.05        0.76                  0.56             1.35             9.20  0.61                          1.60    560.0       2\n",
       "\n",
       "[178 rows x 14 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6eca255e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"target\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e80ed4c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['class_0', 'class_1', 'class_2'], dtype='<U7')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.target_names"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
