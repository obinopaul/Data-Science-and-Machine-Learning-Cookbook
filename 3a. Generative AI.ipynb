{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NLP and LLM Terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# corpus\n",
    "    # A collection of text documents used to train a language model. The corpus can be a collection of books, articles, or any text data.\n",
    "    \n",
    "    \n",
    "# Vocabulary\n",
    "    # The set of unique tokens (words, sub-words, or characters) that a model can understand. The vocabulary is typically derived from\n",
    "    # the training corpus and includes common words and special tokens like [PAD], [UNK], [CLS], and [SEP].\n",
    "    \n",
    "    \n",
    "# Attention Mechanism\n",
    "    # A mechanism that allows the model to focus on important words in a sequence, enabling the model to handle long-range dependencies \n",
    "    # and capture context.\n",
    "\n",
    "\n",
    "# Tokens\n",
    "    # The smallest unit of input or output the model processes. Tokens can represent words, sub-words, or even characters, depending \n",
    "    # on the tokenization strategy.\n",
    "    \n",
    "    \n",
    "# Tokenization\n",
    "    # The process of converting raw text into tokens (usually words, sub-words, or characters) that the model can process. \n",
    "    # Tokenizers break down text based on a model's vocabulary (e.g., Byte Pair Encoding or WordPiece).\n",
    "    # Byte Pair Encoding (BPE):\n",
    "        # A tokenization algorithm that iteratively merges the most frequent pairs of characters in a corpus to create a vocabulary of \n",
    "        # variable-length tokens. BPE is widely used in NLP tasks, including machine translation and text generation.\n",
    "\n",
    "\n",
    "# Embeddings\n",
    "    # Dense vector representations of tokens (words/sub-words) that capture semantic meaning. Used in LLMs to map input tokens into \n",
    "    # a continuous vector space where similar meanings are close together. Instead of treating each word as a unique, isolated token, \n",
    "    # embeddings allow words with similar meanings to be represented by vectors (arrays of numbers) that are close together \n",
    "    # in a multi-dimensional space.\n",
    "    \n",
    "    # example: Word2Vec, GloVe, FastText, BERT embeddings.\n",
    "        # Imagine you have the words: \n",
    "        # \"dog\", \"cat\", \"apple\", \"banana\" \n",
    "        # \"dog\" -> [0.1, 0.2, 0.3, 0.4], \n",
    "        # \"cat\" -> [0.2, 0.3, 0.4, 0.5], \n",
    "        # \"apple\" -> [0.3, 0.4, 0.5, 0.6], \n",
    "        # \"banana\" -> [0.4, 0.5, 0.6, 0.7]\n",
    "        # The embeddings for \"dog\" and \"cat\" are closer together than \"dog\" and \"apple\" because \"dog\" and \"cat\" are semantically\n",
    "        # similar (both animals) compared to \"dog\" and \"apple\" (different categories).\n",
    "        \n",
    "        \n",
    "# Part-of-Speech (POS) Tagging:\n",
    "    # Assigning each word in a sentence a grammatical category (e.g., noun, verb, adjective). Helps the model understand the \n",
    "    # structure of sentences and the role of each word, which is useful for tasks like parsing, translation, and question answering\n",
    "\n",
    "\n",
    "# Named Entity Recognition (NER):\n",
    "    # Identifying and categorizing entities (names, dates, locations, organizations, etc.) in text.\n",
    "\n",
    "\n",
    "# Stemming or Lemmatization:\n",
    "    # Reducing words to their base or root form. Stemming is a rule-based process that removes prefixes or suffixes, while lemmatization \n",
    "    # uses a vocabulary and morphological analysis to return the base form of a word.\n",
    "    \n",
    "\n",
    "# Sampling Techniques\n",
    "    # Methods used to generate outputs from a model, such as greedy search (selecting the most likely next token), beam search (exploring \n",
    "    # multiple token sequences), and temperature sampling (introducing randomness to outputs).\n",
    "\n",
    "\n",
    "# Beam Search\n",
    "    # A search strategy used during text generation to explore multiple possible token sequences and select the most likely ones. \n",
    "    # It reduces the likelihood of poor-quality outputs compared to greedy search.\n",
    "    \n",
    "    \n",
    "# Greedy Search\n",
    "    # A simpler search method where the model always selects the most probable next token. It is fast but may lead to less coherent \n",
    "    # or repetitive outputs.\n",
    "\n",
    "\n",
    "# Autoregressive Models\n",
    "    # LLMs like GPT, which generate text one token at a time, predicting the next token based on previously generated tokens. \n",
    "    # This type of model is suitable for tasks like text generation.\n",
    "    \n",
    "    \n",
    "# Masked Language Models (MLM)\n",
    "    # Models like BERT that learn by predicting masked-out tokens in a sentence, using the surrounding context. These models are \n",
    "    # bidirectional, meaning they consider context from both directions.\n",
    "    \n",
    "    \n",
    "# Zero-Shot Learning\n",
    "    # The model’s ability to perform tasks without explicit examples in the training data. For example, a zero-shot LLM can classify \n",
    "    # text without having seen labeled examples for that specific task.\n",
    "    \n",
    "    \n",
    "# Few-Shot Learning\n",
    "    # The model can generalize from only a few examples during inference. For instance, by providing the model a few sample questions \n",
    "    # and answers, it can handle similar tasks effectively.\n",
    "    \n",
    "    \n",
    "# Fine-Tuning vs. Transfer Learning\n",
    "    # Fine-Tuning: The process of adapting a pretrained LLM to a specific task (e.g., classification, question answering) by training \n",
    "        # it further on task-specific labeled data.\n",
    "    # Transfer Learning: Leveraging knowledge from a pretrained model and applying it to a new but related task, without needing to \n",
    "        # retrain from scratch.\n",
    "\n",
    "\n",
    "# Temperature Sampling\n",
    "    # A technique used during text generation to control the randomness of the output. Higher temperatures (e.g., 1.0) result in \n",
    "    # more diverse outputs, while lower temperatures (e.g., 0.2) make the model more deterministic.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PEFT + LoRA (Parameter Efficient Fine-tuning + Low-Rank Adaptation)\n",
    "    # Description: Fine-tunes only a small adapter layer added on top of a pre-trained model, conserving memory and improving efficiency.\n",
    "    # Use Case: Helps in training large models by keeping the original model frozen and updating only small parts.\n",
    "\n",
    "\n",
    "# 2. Quantization-Aware Training (QAT)\n",
    "    # Description: Reduces model size by converting high-precision weights (e.g., FP32) to lower precision formats (e.g., FP16 or INT8).\n",
    "    # Benefits: Saves memory and reduces training time but may affect model accuracy.\n",
    "    # Challenges: Requires careful monitoring to ensure model quality isn’t degraded.\n",
    "\n",
    "\n",
    "# 3. Gradient Checkpointing\n",
    "    # Description: Saves memory by storing only certain intermediate values during backpropagation.\n",
    "    # Use Case: Reduces memory usage but slows down training.\n",
    "\n",
    "\n",
    "# 4. Distributed Training\n",
    "    # Description: Splits the model and data across multiple devices or nodes for faster training.\n",
    "    # Key Techniques:\n",
    "        # FSDP (Fully Sharded Data Parallel): Shards model weights and optimizer states across devices.\n",
    "        # Deepspeed Zero Redundancy Optimizer (ZeRO): Distributes model parameters to save memory and optimize training efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> LLM Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Post-Training Quantization (PTQ)\n",
    "    # Description: Quantizes a model’s weights and activations after training to reduce memory usage.\n",
    "    # Use Case: Reduces memory footprint for serving models at lower precision (e.g., FP32 → INT8).\n",
    "\n",
    "\n",
    "# 2. Distributed Inference\n",
    "    # Description: Partitioning model weights across multiple devices to handle large models.\n",
    "    # Techniques:\n",
    "    # Model Partitioning: Divides a large model across multiple GPUs or nodes for more efficient computation.\n",
    "    # In-flight Batching: Enables the processing of new requests while others are still being computed, improving GPU utilization.\n",
    "\n",
    "\n",
    "# 3. Dynamic Batching & Continuous Batching\n",
    "    # Description: Dynamically adjusts batch sizes during inference to maximize GPU utilization, reducing latency.\n",
    "    # Benefits: Ensures high throughput and efficiency, especially for models with varying input lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Optimization Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. TensorRT-LLM\n",
    "    # Description: Optimizes models with kernel fusion and memory techniques like KV caching, Paged Attention, and FlashAttention.\n",
    "    # Benefits: Improves performance but requires conversion into TensorRT format for use.\n",
    "\n",
    "\n",
    "# 2. vLLM\n",
    "    # Description: An inference engine that uses Paged Attention to reduce resource wastage, optimizing memory usage and improving throughput.\n",
    "    # Benefits: High efficiency in processing tokens compared to traditional methods.\n",
    "\n",
    "\n",
    "# 3. DeepSpeed-Fastgen\n",
    "    # Description: Combines DeepSpeed's training and inference capabilities for fast, efficient model serving.\n",
    "    # Key Features: Supports Dynamic Splitfuse batching, improving latency and throughput for large models.\n",
    "    \n",
    "\n",
    "# Key Considerations\n",
    "    # Memory Constraints: LLM training and inference are memory-intensive processes. Techniques like PEFT, QAT, and gradient \n",
    "        # checkpointing can help mitigate memory limitations.\n",
    "    # Model Size: Models with billions of parameters may require distributed training or inference strategies to handle the memory demands.\n",
    "    # Efficiency: Methods like mixed precision, distributed training, and dynamic batching are key to improving efficiency in \n",
    "        # training and inference.\n",
    "    # Latency: Techniques like dynamic batching and continuous batching can help reduce inference latency, especially for real-time \n",
    "        # applications.\n",
    "    # Throughput: Distributed inference and model partitioning can improve throughput by leveraging multiple devices for \n",
    "        # parallel processing.\n",
    "    # Resource Optimization: Techniques like TensorRT-LLM and vLLM optimize memory usage and improve performance for large models.\n",
    "    # Scalability: Distributed training and inference methods enable scaling LLMs to handle larger models and datasets efficiently.\n",
    "    # Model Serving: Techniques like DeepSpeed-Fastgen provide end-to-end solutions for training and serving large language \n",
    "        # models effectively.\n",
    "    # Performance Trade-offs: Quantization and distributed strategies may impact model accuracy, so careful monitoring and tuning \n",
    "        # are essential to maintain performance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
