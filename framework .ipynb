{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Table of Content:\n",
    "* [Import Libraries](#1)\n",
    "* [Load Data](#2)\n",
    "* [Exploratory Data Analysis (EDA)](#3)\n",
    "* [Data Preprocessing](#4)\n",
    "    * [Data Cleaning](#4a)\n",
    "    * [Data Transformation](#4b)\n",
    "    * [Handling Imbalanced Data](#4c)\n",
    "    * [Data Reduction](#4d)\n",
    "* [Selecting and Training the Model](#5) \n",
    "* [Model Evaluation](#6) \n",
    "* [Model Optimization](#7) \n",
    "* [Model Deployment](#8) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis      \n",
    "import pandas as pd          # data analysis library for handling structured data   \n",
    "from pandas import DataFrame            \n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "from pandas.plotting import parallel_coordinates \n",
    "import ydata_profiling\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose # library for performing statistical analysis\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from typing import List\n",
    "import pickle\n",
    "import calendar \n",
    "import scipy.stats as stats\n",
    "import datetime \n",
    "import dtale \n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "import plotly.io as pio\n",
    "import plotly.express as px   # interactive data visualization library\n",
    "import plotly.graph_objects as go   # library for creating interactive graphs and charts\n",
    "import matplotlib \n",
    "import kaleido \n",
    "import missingno as msno \n",
    "\n",
    "# Machine Learning/Time Series \n",
    "import evalml \n",
    "import tensorflow as tf\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor \n",
    "#ML - Preprocessing data \n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer #variable transformation \n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder # Preprocessing feature scaling/categorical encoding\n",
    "from sklearn.preprocessing import Normalizer, Binarizer \n",
    "from sklearn.model_selection import train_test_split    #split data into train and test \n",
    "#ML - Handling imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler  \n",
    "#ML - Create your model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  #linear and logistics regression models\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "#ML - Evaluate model performance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, accuracy_score, classification_report \n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score, homogeneity_score \n",
    "#ML - Tune your model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/household_power_consumption.txt\", sep=\";\",  parse_dates={'Datetime' : ['Date', 'Time']}, \n",
    "                          infer_datetime_format=True, low_memory=False, index_col='Datetime')\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df.info() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Visualize Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_df(df):       #best for time series analysis when your index is in datetime \n",
    "    \"\"\"\n",
    "    Creates an interactive plot of the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data for the PJM Interconnection region.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays an interactive plot of the energy consumption data using Plotly.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_df(my_df)\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    fig = go.Figure(layout=go.Layout(\n",
    "        height=500,\n",
    "        width=800,\n",
    "    ))\n",
    "\n",
    "    for col in df.columns:\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'PJM Energy Consumption',\n",
    "            'font': {'size': 25, 'family': 'Arial', 'color': 'black'}\n",
    "        },\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Energy Consumption (MW)'\n",
    "    )\n",
    "\n",
    "    return fig.show(renderer='svg')\n",
    "\n",
    "visualize_df(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = ydata_profiling.ProfileReport(df) #replacing pandas_profiling with ydata_profiling\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "do_data_profiling(df, data_profiling.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize with individual plots \n",
    "def visualize_subplots_boxplots(df: DataFrame, columns: List[str], nrows: int, ncols: int) -> None:\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots containing boxplots of daily average energy consumption.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data.\n",
    "        columns: A list of column names to include in the boxplots.\n",
    "        nrows: The number of rows in the subplot grid.\n",
    "        ncols: The number of columns in the subplot grid.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a grid of subplots containing boxplots of daily average energy consumption.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_subplots_boxplots(my_df, ['Consumption', 'Generation'], 3, 4)\n",
    "    \"\"\"\n",
    "    from typing import List\n",
    "    from pandas import DataFrame\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 12))\n",
    "    fig.suptitle('Hourly Average Energy Consumption', weight='bold', fontsize=25)\n",
    "\n",
    "    # We just need 11 figures, so we delete the last one\n",
    "    if nrows*ncols > len(columns):\n",
    "        fig.delaxes(axes[nrows-1][ncols-1])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.boxplot(data=df, x='Hour', y=col, ax=axes.flatten()[i], color='#cc444b')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"Images/xxx.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_subplots_boxplots(df=result_2, columns=['AEP_MW', 'COMED_MW', 'DAYTON_MW', 'DEOK_MW', 'DOM_MW', 'DUQ_MW',\n",
    "        'EKPC_MW', 'FE_MW', 'NI_MW', 'PJME_MW', 'PJMW_MW'], nrows=6, ncols=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Auto EDA using dtale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtale.show(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4a\"></a> <br>\n",
    "### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Basic formating (renaming cols, duplicates detection, datetime etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates \n",
    "df.drop_duplicates(inplace=True) \n",
    "\n",
    "#formating columns \n",
    "df.\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'price': 'selling_price', 'bedrooms': 'num_bedrooms'}, inplace=True)\n",
    "\n",
    "#replace non numeric columns\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> format datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "convert_timestamp(ts) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Remove unwanted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "df.drop(['id', 'date'], axis=1, inplace=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True: \n",
    "            print(f\"there is {df[col].isnull().sum()} NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)  \n",
    "\n",
    "missing_value_analysis (df)\n",
    "view_NaN(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values \n",
    "# df.fillna(df.mean(), inplace=True) \n",
    "\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(5)\n",
    "\n",
    "\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sklearn to handle missing values - (SimpleImputer, KNN-Imputer, Iterative Imputer, )\n",
    "\n",
    "#IterativeImputer: This function estimates missing values using a predictive model.\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "def impute_missing_values_iteratively(X): #or (X, Columns)\n",
    "    imputer = IterativeImputer(estimator = RandomForestRegressor())\n",
    "        \n",
    "    # select only the columns with missing values to be imputed\n",
    "    # X_cols = X[columns]\n",
    "    X_imputed = imputer.fit_transform(X) #or X_cols\n",
    "    return X_imputed\n",
    "\n",
    "impute_missing_values_iteratively(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize outliers\n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "visualize_outlier (df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualizing Regression outliers using Cook's distance\n",
    "from yellowbrick.regressor import CooksDistance\n",
    "\n",
    "\n",
    "# Instantiate and fit the visualizer\n",
    "visualizer = CooksDistance()\n",
    "visualizer.fit(X, y)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to handle outliers, use any of Tukey's test, Kernel density estimation, Z-score method, Mahalanobis distance method,\n",
    "#Isolation Forest model, EllipticEnvelope.\n",
    "\n",
    "\n",
    "# Tukey's test: This statistical method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the median and works well for univariate datasets with normal distributions.\n",
    "\n",
    "# Kernel density estimation: This non-parametric method estimates the probability density function of a dataset \n",
    "#     and identifies outliers as values with low probability density, making it suitable for non-normal datasets.\n",
    "\n",
    "# Z-score method: This simple method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the mean and is widely used for datasets with normal distributions.\n",
    "\n",
    "# Mahalanobis distance method: This multivariate method identifies outliers based on the distance of each point from the \n",
    "#     centroid of the dataset and is effective for datasets with multivariate normal distributions.\n",
    "\n",
    "# Isolation Forest model: This machine learning algorithm identifies outliers by isolating them into a separate tree \n",
    "#     structure, making it suitable for high-dimensional feature spaces with both linear and non-linear relationships \n",
    "#     between features.\n",
    "\n",
    "# EllipticEnvelope: This multivariate method identifies outliers by fitting an ellipse to the data and identifying \n",
    "#     points that are outside the ellipse, making it effective for datasets with multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4b\"></a> <br>\n",
    "### Data Transformation (scaling, encoding categorical data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Extracting features from Dates, Mixed Variables etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df.resample('H').mean() \n",
    "df_day = df.resample('D').mean() \n",
    "df_month = df.resample('M').mean() \n",
    "df_year = df.resample('Y').mean()\n",
    "\n",
    "\n",
    "df['hour'] = df.index.hour \n",
    "df['day'] = df.index.day \n",
    "df['weekday'] = df.index.day_name() \n",
    "df['month'] = df.index.month \n",
    "df['year'] = df.index.year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Categorical Variable Encoding (data transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding: replaces each category with a numerical label. This technique is suitable for data \n",
    "#     where the categories have an intrinsic order, such as \"low,\" \"medium,\" and \"high.\" Works well with linear models\n",
    "\n",
    "# Ordinal encoding: assigns a numerical value to each category based on their frequency. This technique is suitable \n",
    "#     for data where the categories do not have an intrinsic order, but where their frequency may be informative.\n",
    "#     Suitable for non-linear models\n",
    "\n",
    "# One-hot encoding: creates a binary variable for each category, indicating its presence or absence. \n",
    "#     This technique is suitable for data where the categories do not have an intrinsic order and the \n",
    "#     number of categories is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def holdout_cv(X,y,size = 0.3, seed = 1): \n",
    "#     X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = size, random_state = seed) \n",
    "#     X_train = X_train.reset_index(drop='index') \n",
    "#     X_test = X_test.reset_index(drop='index') \n",
    "#     return X_train, X_test, y_train, y_test \n",
    "\n",
    "# X_train, X_test, y_train, y_test = holdout_cv(X, y, size=0.3, seed=1) \n",
    "\n",
    "# #OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(dataset, test_size=0.2, val_size=0.2):\n",
    "    \"\"\"\n",
    "    Split a dataset into training, validation, and test sets.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : array-like of shape (n_samples, n_features)\n",
    "        The input dataset.\n",
    "    test_size : float, optional\n",
    "        The proportion of the dataset to include in the test set.\n",
    "    val_size : float, optional\n",
    "        The proportion of the dataset to include in the validation set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_train : array-like of shape (n_train_samples, n_features)\n",
    "        The training input samples.\n",
    "    X_val : array-like of shape (n_val_samples, n_features)\n",
    "        The validation input samples.\n",
    "    X_test : array-like of shape (n_test_samples, n_features)\n",
    "        The test input samples.\n",
    "    y_train : array-like of shape (n_train_samples,)\n",
    "        The target values (class labels) for the training input samples.\n",
    "    y_val : array-like of shape (n_val_samples,)\n",
    "        The target values (class labels) for the validation input samples.\n",
    "    y_test : array-like of shape (n_test_samples,)\n",
    "        The target values (class labels) for the test input samples.\n",
    "    \"\"\"\n",
    "    # Split the dataset into train and test sets\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(dataset.data, dataset.target, test_size=test_size, random_state=42)\n",
    "\n",
    "    # Split the train set into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=val_size/(1-test_size), random_state=42)\n",
    "\n",
    "    #show the shapes\n",
    "    print(X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape)\n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "X_train, X_val, X_test, y_train, y_val, y_test = split_data(df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Scaling (data transformation) - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use any of standard scaler (z-score), min-max scaler \n",
    "\n",
    "#Z-score standardized feature scaling (most common)\n",
    "#min-max scaling (common) \n",
    "\n",
    "def standardize_data(X_train, X_test): \n",
    "    \"\"\"\n",
    "    Standardizes the training and testing data using the mean and standard deviation\n",
    "    learned from the training set.\n",
    "    \n",
    "    Args:\n",
    "    - X_train: numpy array or pandas dataframe, training data\n",
    "    - X_test: numpy array or pandas dataframe, testing data\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled: numpy array or pandas dataframe, standardized training data\n",
    "    - X_test_scaled: numpy array or pandas dataframe, standardized testing data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    # Set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the training set\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    # Transform the training and testing sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "standardize_data(X_test, X_test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Variable Transformation - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable transformation involves transforming the values of variables to make them more suitable for analysis\n",
    "#the idea is to make the variables normally/gaussian distributed. Hence, \n",
    "\n",
    "#first step is to assess normality using a histogram or QQ-plot (to explore the variable distribution)\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.title(f\"Histogram of {variable}\")\n",
    "\n",
    "    # q-q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot of {variable}\")\n",
    "\n",
    "    # check for skewness\n",
    "    skewness = df[variable].skew()\n",
    "    if skewness > 0:\n",
    "        skew_type = \"positively skewed\"\n",
    "    elif skewness < 0:\n",
    "        skew_type = \"negatively skewed\"\n",
    "    else:\n",
    "        skew_type = \"approximately symmetric\"\n",
    "        \n",
    "    # print message indicating skewness type\n",
    "    print(f\"The variable {variable} is {skew_type} (skewness = {skewness:.2f})\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Check function output\n",
    "diagnostic_plots(X, \"MedInc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the variables are NOT normally distributed, we then transform it. It is necessary to test several variable \n",
    "# transformation methods, and choose the best for that feature. One variable transformation method is log_transform\n",
    "\n",
    "#log transform \n",
    "def log_transform(df, columns):\n",
    "     \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the natural logarithm function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_log = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_log\n",
    "\n",
    "df_log = log_transform(df, columns)\n",
    "\n",
    "diagnostic_plots(df_log, columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Discretization (data transformation - apply to train, and then to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization in machine learning is the process of transforming continuous variables into discrete or \n",
    "# categorical variables. This process involves dividing the range of a continuous variable into a finite number of \n",
    "# intervals or bins, and then assigning each observation to a particular bin based on the value of the continuous \n",
    "# variable. \n",
    "\n",
    "#Discretization approaches: equal width, equal frequency, K means, Decision Trees\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4c\"></a> <br>\n",
    "### Handling Imbalanced Data and Biases (Class Imbalance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to train, and then to test \n",
    "\n",
    "#Imbalanced data refers to a situation where the number of observations in one class or category is much larger \n",
    "# or smaller than the number of observations in other classes or categories. Imbalanced data can pose challenges \n",
    "# in machine learning because it can lead to biased models that perform poorly on the minority class.\n",
    "\n",
    "#we typically look at the 'target data' when checking for imbalanced data. However, it is also important \n",
    "# to consider the features\n",
    "\n",
    "def check_imbalance(dataset, columns=None, threshold=10):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and one or more columns as input and returns True if any of the specified columns\n",
    "    are imbalanced, False otherwise. A column is considered imbalanced if the percentage of the minority class is less\n",
    "    than the specified threshold.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, use all columns except for the last one as the features\n",
    "    if columns is None:\n",
    "        features = dataset.iloc[:, :-1]\n",
    "        columns = features.columns\n",
    "    \n",
    "    # Check the imbalance of each specified column\n",
    "    for col in columns:\n",
    "        # Get the counts of each class in the column\n",
    "        class_counts = dataset[col].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each class in the column\n",
    "        class_percentages = class_counts / len(dataset) * 100\n",
    "\n",
    "        # Plot the class percentages\n",
    "        plt.bar(class_counts.index, class_percentages)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the column is imbalanced\n",
    "        minority_class = class_counts.index[-1]\n",
    "        minority_class_percentage = class_percentages.iloc[-1]\n",
    "        if minority_class_percentage < threshold:\n",
    "            print(f'{col} is imbalanced. Minority class: {minority_class}, Percentage: {minority_class_percentage:.2f}%')\n",
    "            return True\n",
    "\n",
    "    # If none of the specified columns are imbalanced, return False\n",
    "    print('No imbalance found.')\n",
    "    return False\n",
    "\n",
    "check_imbalance(df, columns=['target'], threshold=10) \n",
    "\n",
    "\n",
    "#OR\n",
    "\n",
    "from yellowbrick.target import ClassBalance \n",
    "# Instantiate the visualizer\n",
    "visualizer = ClassBalance(labels=[\"draw\", \"loss\", \"win\"])\n",
    "visualizer.fit(y_train, y_test)        # Fit the data to the visualizer (you can also use visualizer.fit(y))\n",
    "visualizer.show()                       # Finalize and render the figure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is imbalance, you can handle it by over-sampling or under-sampling the dataset\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def handle_imbalanced_data(X, y, strategy='over-sampling'):\n",
    "    \"\"\"\n",
    "    Handle imbalanced data using imblearn library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array-like of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y: array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    strategy: str, default='over-sampling'\n",
    "        The strategy to use for handling imbalanced data. Possible values are\n",
    "        'over-sampling' and 'under-sampling'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled: array-like of shape (n_samples_new, n_features)\n",
    "        The resampled input data.\n",
    "    y_resampled: array-like of shape (n_samples_new,)\n",
    "        The resampled target values.\n",
    "    \"\"\"\n",
    "    if strategy == 'over-sampling':\n",
    "        # Initialize the RandomOverSampler object\n",
    "        ros = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    elif strategy == 'under-sampling':\n",
    "        # Initialize the RandomUnderSampler object\n",
    "        rus = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Possible values are 'over-sampling' and 'under-sampling'.\")\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4d\"></a> <br>\n",
    "### Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## Selecting and Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before you choose a ML technique to train the model, consider if your ML technique is resistant to the following:\n",
    "#     Missing data\n",
    "#     Data imbalance\n",
    "#     Feature Scaling\n",
    "#     Categorical Data\n",
    "#     Outliers\n",
    "#     Dimensionality (refers to the number of features or variables in the dataset)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Use Pycaret and EvalMl to test multiple ML methods (AutoML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pycaret\n",
    "\n",
    "from pycaret.classification import setup, compare_models, create_model, tune_model, plot_model, evaluate_model\n",
    "\n",
    "X, y = load_iris(return_X_y=True, as_frame=True) \n",
    "X['target'] = y \n",
    "# Initialize classification setup \n",
    "# clf1 = setup(data=X, target='target', train_size = 0.8, \n",
    "#              preprocess = True, polynomial_features = True, \n",
    "#              polynomial_degree = 2, fix_imbalance = True,\n",
    "#              fix_imbalance_method = 'SMOTE', feature_selection = True,\n",
    "#              feature_selection_method = ' ', feature_selection_estimator = ,\n",
    "#              n_features_to_select = 0.2) \n",
    "\n",
    "\n",
    "clf1 = setup(data=X, target='target', train_size = 0.8)\n",
    "\n",
    "# Compare models \n",
    "compare_results = compare_models(n_select=5)    #the best 5 models will be highlighted\n",
    "\n",
    "# Create a model\n",
    "model = create_model('knn')     #change knn to any of the top 5 models from above\n",
    "\n",
    "# # Tune the model\n",
    "tuned_model = tune_model(model)\n",
    "\n",
    "# # Evaluate the model\n",
    "evaluate_model(tuned_model)\n",
    "\n",
    "# # Fit the model\n",
    "final_model = tune_model(tuned_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EvalML (AutoML using EvalML doesn't just give you the best model, it also gives the best pipeline)\n",
    "import evalml\n",
    "\n",
    "X, y = evalml.demos.load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = evalml.preprocessing.split_data(X, y, problem_type='binary') #change the problem type\n",
    "                    #to see all problem types, use (evalml.problem_types.ProblemTypes.all_problem_types)\n",
    "\n",
    "#Use EvalML's AutoML to perform the following steps:\n",
    "\n",
    "#Step 1: search multiple ML methods and parameters\n",
    "from evalml.automl import AutoMLSearch\n",
    "automl = AutoMLSearch(X_train=X_train, y_train=y_train, problem_type='binary')  #also change the problem type\n",
    "automl.search()\n",
    "\n",
    "#Step 2: Rank each of the multiple ML algorithms to see their parameters and then choose the best\n",
    "automl.rankings\n",
    "# automl.describe_pipeline(automl.rankings.iloc[0][\"id\"]) #use this to describe each of the model/pipeline. change 0 to other values\n",
    "\n",
    "#Step 3: Choose the best pipeline\n",
    "best_pipeline=automl.best_pipeline\n",
    "best_pipeline\n",
    "\n",
    "#Step 4: You can evaluate other objective functions, or optimize the model for a specific objective\n",
    "best_pipeline.score(X_test, y_test, objectives=[\"auc\",\"f1\",\"Precision\",\"Recall\"]) #evaluate other objective functions\n",
    "automl_auc = AutoMLSearch(X_train=X_train, y_train=y_train,             #optimize step 1 for a specific objective\n",
    "                          problem_type='binary',\n",
    "                          objective='auc',\n",
    "                          additional_objectives=['f1', 'precision'],\n",
    "                          max_batches=1,\n",
    "                          optimize_thresholds=True)\n",
    "\n",
    "automl_auc.search()\n",
    "\n",
    "#Step 5: Make predictions, save and load the model\n",
    "best_pipeline.predict_proba(X_test).to_dataframe()\n",
    "best_pipeline.save(\"model.pkl\")\n",
    "check_model=automl.load('model.pkl')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Then select the preferred model to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest classifier on the dataset\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "\n",
    "# # Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
    "# X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[1,1], random_state=1)\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = roc_auc_score(train_y, train_preds)\n",
    "        test_loss = roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model\n",
    "\n",
    "y_pred, test_loss, test_preds2, clf = runRF(\n",
    "                                            X_train,\n",
    "                                            y_train,\n",
    "                                            X_val,\n",
    "                                            y_val=None,\n",
    "                                            test_X2=None,\n",
    "                                            rounds=100,\n",
    "                                            depth=20,\n",
    "                                            leaf=10,\n",
    "                                            feat=0.2,\n",
    "                                            min_data_split_val=2,\n",
    "                                            seed_val=0,\n",
    "                                            job=-1,\n",
    "                                        )\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Compare predicted values with the actual test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_pred, y_test): \n",
    "    \"\"\"\n",
    "    Plots the predicted and actual values on separate scatter plots.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot the actual values\n",
    "    ax1.scatter(range(len(y_test)), y_test, label='Actual Values')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Actual Values')\n",
    "    ax1.set_title('Scatter plot of Actual Values')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot the predicted values\n",
    "    ax2.scatter(range(len(y_pred)), y_pred, label='Predicted Values')\n",
    "    ax2.set_xlabel('Index')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title('Scatter plot of Predicted Values')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(y_pred, y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Class Prediction Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "# classes = [\"apple\", \"kiwi\", \"pear\", \"banana\", \"orange\"]\n",
    "\n",
    "# Instantiate the classification model and visualizer\n",
    "visualizer = ClassPredictionError(model, classes=classes)\n",
    "\n",
    "# Fit the training data to the visualizer\n",
    "visualizer.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "visualizer.score(X_test, y_test)\n",
    "\n",
    "# Draw visualization\n",
    "visualizer.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model evaluation using scoring parameter\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "print(f' Score: {clf.score(X_val, y_val)}')   #R^2 for Regression; accuracy for classification. similar to cross_val_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def accuracy(model, data, labels):\n",
    "    \n",
    "    predictions = model.predict(data)\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "    # Error = Actual value - Predicted value\n",
    "\n",
    "    # MSE (Mean Square Error)\n",
    "        # The square of the error over all samples is called Mean Squarred Error(MSE).\n",
    "        # MSE = SQUARE(Actual value - Predicted value)/Number of Samples\n",
    "    #RMSE (Root Mean Square Error)\n",
    "    # MAE (Mean Absolute Error)\n",
    "        # MAE = ABSOLUTE (Actual value - Predicted Value)\n",
    "\n",
    "\n",
    "#Classification\n",
    "    #Accuracy\n",
    "    #Precision\n",
    "    #Recall\n",
    "    #F1 score \n",
    "    #confusion matrix\n",
    "    #AUC ROC curve\n",
    "\n",
    "# True Positives(TP): Number of samples that are correctly classified as positive, and their actual label is positive.\n",
    "\n",
    "# False Positives (FP): Number of samples that are incorrectly classified as positive, when in fact their actual label \n",
    "#     is negative.\n",
    "\n",
    "# True Negatives (TN): Number of samples that are correctly classified as negative, and their actual label is negative.\n",
    "\n",
    "# False Negatives (FN): Number of samples that are incorrectly classified as negative, when in fact their actual label \n",
    "#     is positive.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Mean Square Error (MSE), Root Mean Square Error (RMSE), Mean Absolute Error (MAE), R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def predict(X_test, model, y_test):\n",
    "    \"\"\"\n",
    "    Take the input data, model and labels and return predictions and evaluation metrics\n",
    "    \"\"\"\n",
    "    preds = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, preds)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "\n",
    "    print(\"Mean Squared Error: \", mse)\n",
    "    print(\"Root Mean Squared Error: \", rmse)\n",
    "    print(\"Mean Absolute Error: \", mae)\n",
    "    print(\"R-squared: \", r2)\n",
    "    \n",
    "    return preds, mse, rmse, mae, r2\n",
    "\n",
    "\n",
    "predict(X_test, model, y_test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Confusion Matrix, Precision, Recall, Accuracy, F1-Score, AUC ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "class_names = digits.target_names\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    \n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    \n",
    "    for i, j in np.ndindex(cm.shape):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred, classes=class_names,\n",
    "                      title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AUC ROC curve (use this for binary classification)\n",
    "\n",
    "def plot_roc(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    fpr, tpr, thresholds = roc_curve(y_actual, y_pred)\n",
    "    plt.plot(\n",
    "        fpr,\n",
    "        tpr,\n",
    "        color=\"b\",\n",
    "        label=r\"Model (AUC = %0.2f)\" % (roc_auc_score(y_actual, y_pred)),\n",
    "        lw=2,\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.plot(\n",
    "        [0, 1],\n",
    "        [0, 1],\n",
    "        linestyle=\"--\",\n",
    "        lw=2,\n",
    "        color=\"r\",\n",
    "        label=\"Luck (AUC = 0.5)\",\n",
    "        alpha=0.8,\n",
    "    )\n",
    "    plt.xlim([-0.05, 1.05])\n",
    "    plt.ylim([-0.05, 1.05])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"Receiver operating characteristic example\")\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    #To choose the threshold value that maximizes the Youden's J statistic\n",
    "    # calculate Youden's J statistic for each threshold value\n",
    "    J = tpr - fpr\n",
    "    best_threshold = thresholds[np.argmax(J)]\n",
    "    print('Best threshold:', best_threshold)\n",
    "    \n",
    "plot_roc(y_actual, y_pred) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#OR\n",
    "from yellowbrick.classifier import ROCAUC #yellow brick can be used for multiclass classification\n",
    "\n",
    "visualizer = ROCAUC(model, classes=[\"win\", \"loss\", \"draw\"])\n",
    "visualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "visualizer.show()                       # Finalize and render the figure\n",
    "\n",
    "# roc_auc(model, X_train, y_train, X_test=X_test, y_test=y_test, classes=['not_defaulted', 'defaulted']) #quick_method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classification Report (Precision, Recall, F1-Score)\n",
    "from sklearn.metrics import classification_report\n",
    "from yellowbrick.classifier import classification_report \n",
    "\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# precision measures how many of the positive predictions made by the model are actually correct. \n",
    "# A high precision score indicates that the model is making very few false positive predictions.\n",
    "\n",
    "# recall measures how many of the actual positive instances in the dataset are correctly predicted as positive \n",
    "# by the model. A high recall score indicates that the model is correctly identifying a large proportion of the \n",
    "# positive instances in the dataset.\n",
    "\n",
    "#F1- Score is used to compare precision/recall numbers\n",
    "\n",
    "\n",
    "#OR\n",
    "# Instantiate the visualizer\n",
    "visualizer = classification_report(\n",
    "    model, X_train, y_train, X_test, y_test, classes=classes, support=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Precision - Recall\n",
    "\n",
    "def plot_precisionrecall(y_actual, y_pred):\n",
    "    \"\"\"\n",
    "    Function to plot AUC-ROC curve\n",
    "    \"\"\"\n",
    "    average_precision = average_precision_score(y_actual, y_pred)\n",
    "    precision, recall, _ = precision_recall_curve(y_actual, y_pred)\n",
    "    # In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
    "    step_kwargs = (\n",
    "        {\"step\": \"post\"} if \"step\" in signature(plt.fill_between).parameters else {}\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.step(recall, precision, color=\"b\", alpha=0.2, where=\"post\")\n",
    "    plt.fill_between(recall, precision, alpha=0.2, color=\"b\", **step_kwargs)\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title(\"Precision-Recall curve: AP={0:0.2f}\".format(average_precision))\n",
    "\n",
    "plot_precisionrecall(y_actual, y_pred)\n",
    "\n",
    "#from the plot, we can pick a trade-off threshold where both precision and recall are high\n",
    "\n",
    "\n",
    "\n",
    "#OR\n",
    "from yellowbrick.classifier import PrecisionRecallCurve\n",
    "# Create the visualizer, fit, score, and show it\n",
    "viz = PrecisionRecallCurve(model, per_class=True,\n",
    "                            cmap=\"Set1\", iso_f1_curves=True, \n",
    "                            micro=False)\n",
    "viz.fit(X_train, y_train)\n",
    "viz.score(X_test, y_test)\n",
    "viz.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Access the Bias and Variance of the Model (to check for underfitting or overfitting) - Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, cross_val_score, KFold, train_test_split\n",
    "from yellowbrick.model_selection import CVScores #visualizing the cross validation scores\n",
    "\n",
    "#check Bias and Variance using Cross Validation\n",
    "\n",
    "cv = 5  #or # Create a cross-validation object: cv = KFold(n_splits=5, shuffle=True, random_state=42) \n",
    "\n",
    "def cv_bias_variance(model, X, y, cv):\n",
    "    scores = cross_val_score(model, X, y, cv=cv, n_jobs=-1, scoring='neg_mean_squared_error')   \n",
    "    train_error = -scores.mean()\n",
    "    val_error = -scores.std()\n",
    "    return train_error, val_error, scores\n",
    "\n",
    "# options to replace scoring:\n",
    "#     regression: r2, neg_mean_absolute_error, explained_variance, neg_root_mean_squared_error, etc.\n",
    "#     classification: accuracy, f1, roc_auc, precision, recall, etc.  \n",
    "\n",
    "\n",
    "# Calculate the mean training and validation error scores\n",
    "train_error, val_error, scores = cv_bias_variance(model, X, y, cv)\n",
    "print(\"Mean training error:\", train_error)\n",
    "print(\"Mean validation error:\", val_error)\n",
    "\n",
    "\n",
    "visualizer = CVScores(model, cv=cv, scoring='r2')\n",
    "\n",
    "visualizer.fit(X, y)        # Fit the data to the visualizer\n",
    "visualizer.show()           # Finalize and render the figure\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check Bias and Variance using Learnng Curve\n",
    "\n",
    "train_sizes = np.linspace(0.1, 1.0, 10) # Define the training set sizes to plot the learning curve\n",
    "\n",
    "def cv_learning_curve(model, X, y, cv, train_sizes):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv=cv, n_jobs=-1, train_sizes=train_sizes, scoring='neg_mean_squared_error')\n",
    "                                                #scoring parameter -  #https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter \n",
    "    train_mean = np.mean(-train_scores, axis=1)\n",
    "    train_std = np.std(-train_scores, axis=1)\n",
    "    test_mean = np.mean(-test_scores, axis=1)\n",
    "    test_std = np.std(-test_scores, axis=1)\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', color='blue', label='Training Error')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', color='green', label='Validation Error')\n",
    "    plt.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    plt.fill_between(train_sizes, test_mean - test_std, test_mean + test_std, alpha=0.1, color='green')\n",
    "    plt.xlabel('Number of Training Examples')\n",
    "    plt.ylabel('Mean Squared Error')\n",
    "    plt.title('Learning Curve')\n",
    "    plt.legend(loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    return train_sizes, train_mean, train_std, test_mean, test_std\n",
    "\n",
    "\n",
    "\n",
    "# Generate learning curve plot\n",
    "train_sizes, train_mean, train_std, test_mean, test_std = cv_learning_curve(model, X, y, cv, train_sizes)\n",
    "\n",
    "# If the training and validation errors are both high and close to each other, it indicates that the model is underfitting \n",
    "# and has high bias. If the training error is low, but the validation error is high and there is a large gap between them, \n",
    "# it indicates that the model is overfitting and has high variance.\n",
    "\n",
    "# Jtrain -> Jcv -> high and close to each other (high bias) - Jcv and Jtrain flatten out after a certain no of training samples, so getting more data would not likely help\n",
    "# Jcv >> Jtrain (high variance) - \n",
    "# Jcv >> Jtrain -> high (high bias and high v_measure_score)\n",
    "\n",
    "# To fix high bias:\n",
    "#     get additional features or increase the model size. First perform a feature matrix before adding additional features\n",
    "#     try adding polynomial features (feature engineering: create new features or transform existing features)\n",
    "#     decrease the regularization parameter (lambda)\n",
    "\n",
    "# To fix high variance:\n",
    "#     get more training samples (more data)\n",
    "#     simplify the data by reducing the number of features\n",
    "#     Increase the regularization parameter (lambda)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Analyze Error Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the errors are normally distributed around zero, it may indicate that the model is making unbiased predictions. \n",
    "# If there is a pattern or trend in the errors, it may suggest that the model has systematic biases or is making \n",
    "# consistent errors in certain regions of the input space\n",
    "\n",
    "\n",
    "\n",
    "def analyze_error_distribution(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to analyze the error distribution by plotting histograms and scatter plots.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Array of true labels or ground truth.\n",
    "    y_pred : array-like\n",
    "        Array of predicted values.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Calculate errors\n",
    "    errors = y_true - y_pred\n",
    "\n",
    "    # Plot histogram of errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.hist(errors, bins=20, alpha=0.75)\n",
    "    plt.xlabel('Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Error Distribution (Histogram)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot scatter plot of true labels vs. errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_true, errors, alpha=0.75)\n",
    "    plt.xlabel('True Labels')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Error Distribution (Scatter Plot)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Plot scatter plot of predicted values vs. errors\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(y_pred, errors, alpha=0.75)\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.ylabel('Error')\n",
    "    plt.title('Error Distribution (Scatter Plot)')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "analyze_error_distribution(y_val, y_pred)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Error Analysis - Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Error analysis is the process of analyzing the errors made by a machine learning model and identifying the patterns \n",
    "# or trends that may be causing the errors. The goal of error analysis is to gain insight into the behavior of the \n",
    "# model and identify areas for improvement.\n",
    "\n",
    "# The steps involved in error analysis:\n",
    "    # Collect error data\n",
    "    # Categorize errors\n",
    "    # Identify patterns\n",
    "    # Analyze causes\n",
    "    # Prioritize fixes\n",
    "    \n",
    "# Based on the insights gained from the error analysis, you can perform the following.\n",
    "# False negatives:\n",
    "# False negatives occur when the model predicts that a customer will not churn when they actually do churn. \n",
    "# To fix this issue, you may consider the following:\n",
    "#     Increase the weight of the features that are more indicative of churn for low-usage customers, \n",
    "#         such as frequency of usage or specific product usage. (adjust the model parameters)\n",
    "#     Add new features that may be predictive of churn, such as customer sentiment or customer service interactions.\n",
    "#     Use a different model architecture that is better suited for handling imbalanced data, such as a decision tree \n",
    "#         or ensemble model.\n",
    "# False positives:\n",
    "# False positives occur when the model predicts that a customer will churn when they actually do not churn. \n",
    "# To fix this issue, you may consider the following:\n",
    "#     Decrease the weight of features that are causing false positives, such as age or income, if they are not as \n",
    "#         indicative of churn for low-usage customers. (adjust the model parameters)\n",
    "#     Remove features that are causing false positives altogether, if they are not providing significant value to the \n",
    "#         model.\n",
    "#     Increase the size of the training dataset to capture a more representative sample of customers who do not churn, \n",
    "#         which may help the model learn more accurately which customers are likely to churn.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot confusion matrix to visualize false positives and false negatives\n",
    "    #By default, scikit-learn will assume that the \"positive\" class is the last label (or highest label value) \n",
    "    # in the list of labels. [0, 1] where 1 is Positive and is the class_of_interest.\n",
    "\n",
    "\n",
    "class_names = [0, 1] #or iris().target_names #this is an example and should be edited. [0, 1] for binary classification\n",
    "class_of_interest = 1 #this selects a specific class of interest other than 1 or the highest value. \n",
    "                        #always select the highest one because that is what Scikit learn uses. \n",
    "\n",
    "def false_positives(X_test, y_true, y_pred, classes):\n",
    "    \"\"\" \n",
    "    This function identifies and plots the false positives in a classification problem. \n",
    "    \"\"\" \n",
    "    fp_indices = np.where((y_true != class_of_interest) & (y_pred == class_of_interest))[0] \n",
    "    fp_features = X_test[fp_indices] # assuming X_test is a numpy array of input data \n",
    "    # fp_features = X_test.iloc[fp_indices]\n",
    "    fp_labels = y_pred[fp_indices] # assuming y_pred is a numpy array of predicted labels \n",
    "    # fp_labels = pd.Series(y_pred).iloc[fp_indices]\n",
    "\n",
    "    print(\"False positives: \", len(fp_indices))\n",
    "    return fp_features, fp_labels\n",
    "\n",
    "\n",
    "#false negatives \n",
    "def false_negatives(X_test, y_true, y_pred, classes):\n",
    "    \"\"\" \n",
    "    This function identifies and plots the false negatives in a classification problem. \n",
    "    \"\"\" \n",
    "    fn_indices = np.where((y_true == class_of_interest) & (y_pred != class_of_interest))[0] \n",
    "    fn_features = X_test[fn_indices] # assuming X_test is a numpy array of input data\n",
    "    # fn_features = X_test.iloc[fn_indices] \n",
    "    fn_labels = y_pred[fn_indices] # assuming y_pred is a numpy array of predicted labels \n",
    "    # fn_labels = pd.Series(y_pred).iloc[fn_indices]\n",
    "\n",
    "    print(\"False negatives: \", len(fn_indices))\n",
    "    return fn_features, fn_labels\n",
    "\n",
    "\n",
    "# Plot the confusion matrix to evaluate the performance of the model\n",
    "plot_confusion_matrix(y_test, y_pred, classes=classes,\n",
    "                      title='Confusion matrix, Accuracy = {:.2f}'.format(accuracy))\n",
    "\n",
    "# Identify and plot the false positives\n",
    "X_fp, y_fp = false_positives(X_test, y_test, y_pred, class_names)\n",
    "\n",
    "# Identify and plot the false negatives\n",
    "X_fn, y_fn = false_negatives(X_test, y_test, y_pred, class_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Selection/Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#access the most important features in the model\n",
    "\n",
    "#depending on the results from the bias and variance tests, there may be need to assess which features\n",
    "# are the most important in the ML model\n",
    "\n",
    "\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "feature_importance(model,X_train)  \n",
    "\n",
    "\n",
    "\n",
    "#OR\n",
    "from yellowbrick.model_selection import FeatureImportances\n",
    "\n",
    "viz = FeatureImportances(model, labels=labels, relative=False)\n",
    "viz.fit(X, y)\n",
    "viz.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Selection/Extraction (to fix high bias and high variance) - (apply to train, and then to test)\n",
    "\n",
    "# having accessed the feature importance, you may then Add or Remove features, or make polynomial features\n",
    "\n",
    "#Step 1: Manually adding or removing features\n",
    "\n",
    "#AND/OR\n",
    "\n",
    "#Step 2: Performing any of the following: VarianceThreshold, SelectKBest, Principal COmponent Analysis, \n",
    "# Independent Component Analysis (ICA), t-distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "# some common ones are these two transformers: PCA for feature extraction and SelectKBest for feature selection\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Polynomial Features (to Fix High Variance) - do this only if there is high variance\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "def add_polynomial_features_sklearn(df, degree, columns=None):\n",
    "    \"\"\"\n",
    "    Adds polynomial features up to a specified degree to a subset of columns in a Pandas DataFrame using Scikit-Learn's PolynomialFeatures.\n",
    "    \n",
    "    Parameters:\n",
    "        df (Pandas DataFrame): The DataFrame to which the polynomial features will be added.\n",
    "        degree (int): The maximum degree of polynomial features to add.\n",
    "        columns (list of str): The names of the columns to which polynomial features will be added. If None, all columns will be used.\n",
    "        \n",
    "    Returns:\n",
    "        Pandas DataFrame: A new DataFrame with the original columns and polynomial features up to the specified degree.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the columns to which polynomial features will be added\n",
    "    if columns is None:\n",
    "        columns = df.columns\n",
    "    df_subset = df[columns]\n",
    "    \n",
    "    # Create a copy of the original DataFrame to avoid modifying it\n",
    "    new_df = df.copy()\n",
    "    \n",
    "    # Create a PolynomialFeatures transformer\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    \n",
    "    # Transform the subset of the DataFrame with polynomial features\n",
    "    poly_df = poly.fit_transform(df_subset)\n",
    "    \n",
    "    # Create column names for the new DataFrame\n",
    "    col_names = poly.get_feature_names(df_subset.columns)\n",
    "    \n",
    "    # Create a new DataFrame with the polynomial features\n",
    "    poly_df = pd.DataFrame(poly_df, columns=col_names, index=df_subset.index)\n",
    "    \n",
    "    # Merge the original DataFrame with the new DataFrame\n",
    "    new_df = pd.concat([new_df, poly_df], axis=1)\n",
    "    \n",
    "    return new_df \n",
    "\n",
    "df_polynomial = add_polynomial_features_sklearn(df, degree, columns=None)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Performing Reglarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Another way to fix High Bias or Variance is to perform regularization on the model.\n",
    "#this would involve increasing or decreasing the regularization parameter (lambda) to fix high variance or bias\n",
    "\n",
    "\n",
    "# By tuning the hyperparameters of the model using cross-validation, \n",
    "# we would have effectively applied regularization to the model, which can help to reduce overfitting and improve \n",
    "# its generalization performance.\n",
    "\n",
    "#Hence the next step is MODEL OPTIMIZATION. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "## Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Steps to Model Optimization: \n",
    "\n",
    "#1.  Define the objective:\n",
    "#     The objective of this example is to classify the iris flowers into three species (setosa, versicolor, and virginica) \n",
    "#     based on the four features (sepal length, sepal width, petal length, and petal width).\n",
    "\n",
    "#2.  Choose hyperparameters to optimize:\n",
    "    # In this example, we will optimize the hyperparameters of a Random Forest Classifier. We will tune the number of \n",
    "    # estimators, maximum depth of the tree, and minimum number of samples required to split a node.\n",
    "\n",
    "#3.  Choose an optimization algorithm:\n",
    "#     We will use Random Search to optimize the hyperparameters. Random Search randomly selects combinations of \n",
    "#     hyperparameters from a pre-defined search space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.get_params()  #to get the parameters of the models in order to improve it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "RandomForestClassifier().get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.  Train and evaluate the model:\n",
    "    #     We will train the model using the training set and evaluate it on the validation set. We will use \n",
    "    #     RandomizedSearchCV to perform the random search.\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV \n",
    "import numpy as np\n",
    "\n",
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# rfc.get_params()  #to get the parameters of the models in order to improve it\n",
    "\n",
    "# Define the search space (i.e., the hyperparameters to be tunes). This would vary depending on the model being used\n",
    "n_estimators = [int(x) for x in np.linspace(start=200, stop=2000, num=10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n",
    "max_depth.append(None)\n",
    "min_samples_split = [2, 5, 10]\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "# Perform random search\n",
    "rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "\n",
    "rfc_random.fit(X_train, y_train)\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(rfc_random.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#After performing RandomSearchCV, it can be a good idea to perform GridSearchCV to get the best estimators\n",
    "#we can then streamline the results as use it as an input for the GridSearchCV\n",
    "\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "\n",
    "rfc_grid = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), random_grid, verbose=2, cv=3, refit=True)\n",
    "\n",
    "rfc_grid.fit(X_train, y_train)\n",
    "print(rfc_grid.best_params_) # Print the best hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=70, min_samples_leaf=2, min_samples_split=5,\n",
      "                       n_estimators=1800, random_state=42)\n"
     ]
    }
   ],
   "source": [
    "best_params = rfc_grid.best_estimator_\n",
    "\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'n_estimators': 1800, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_depth': 70}\n"
     ]
    }
   ],
   "source": [
    "# The best hyperparameters found by RandomizedSearchCV are:\n",
    "#     n_estimators=1800, \n",
    "#     min_samples_split=5, \n",
    "#     min_samples_leaf=1, and \n",
    "#     max_depth=30.\n",
    "\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(rfc_grid.best_params_) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#6.  Fine-tune the model:\n",
    "#        We will use the best hyperparameters found by RandomizedSearchCV to fine-tune the model.\n",
    "\n",
    "rfc = RandomForestClassifier(n_estimators=1800,\n",
    "                              min_samples_split=5,\n",
    "                              min_samples_leaf=1,\n",
    "                              max_depth=30,\n",
    "                              random_state=42)\n",
    "\n",
    "rfc.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = rfc.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7.   Test the Model\n",
    "  #     Finally, after optimizing and fine-tuning the model, we can use it to make predictions on new, unseen data\n",
    "\n",
    "# Use the optimized model to make predictions on the new dataset\n",
    "y_pred_new = rfc.predict(X_test)\n",
    "\n",
    "# Print the predicted class labels and the actual class labels of the new dataset\n",
    "print(\"Predicted class labels:\", y_pred_new)\n",
    "print(\"Actual class labels:   \", y_test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "## Model Deployment "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Save the ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "def save_model(model, filename):\n",
    "    \"\"\"\n",
    "    Save a trained scikit-learn model to disk using joblib.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        joblib.dump(model, filename)\n",
    "        print(f\"Model saved to {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {filename}: {e}\")\n",
    "\n",
    "save_model(model, 'model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OR (save the model along with the preprcessor) - check the bottom to find out how preprocessor is used. \n",
    "\n",
    "# def save_model(model, preprocessor, filename):\n",
    "#     \"\"\"\n",
    "#     Save a trained scikit-learn model and its preprocessor to disk using joblib.\n",
    "#     \"\"\"\n",
    "#     try:\n",
    "#         joblib.dump((model, preprocessor), filename)\n",
    "#         print(f\"Model saved to {filename}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error saving model to {filename}: {e}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Deploy the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Load the trained model from disk\n",
    "    model = joblib.load('model.joblib')\n",
    "\n",
    "    # Get the request data and convert to numpy array\n",
    "    request_data = request.get_json()\n",
    "    input_data = np.array(request_data['data'])\n",
    "\n",
    "    # Make predictions using the trained model\n",
    "    predictions = model.predict(input_data)\n",
    "\n",
    "    # Return the predictions as a JSON response\n",
    "    response = {'predictions': predictions.tolist()}\n",
    "    return jsonify(response)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deploy the model here\n",
    "\n",
    "# https://dashboard.render.com/\n",
    "# sign-in with Github\n",
    "#or use Heroku, GCP, AWS, AZUre, IBM Watson etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Train a scikit-learn model with preprocessor\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "# from sklearn.datasets import load_iris\n",
    "\n",
    "# iris = load_iris()\n",
    "# X, y = iris.data, iris.target\n",
    "\n",
    "# numeric_transformer = Pipeline(steps=[\n",
    "#     ('scaler', StandardScaler())\n",
    "# ])\n",
    "\n",
    "# categorical_transformer = Pipeline(steps=[\n",
    "#     ('onehot', OneHotEncoder())\n",
    "# ])\n",
    "\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     transformers=[\n",
    "#         ('num', numeric_transformer, [0, 1]),\n",
    "#         ('cat', categorical_transformer, [2])\n",
    "#     ])\n",
    "\n",
    "# model = Pipeline(steps=[\n",
    "#     ('preprocessor', preprocessor),\n",
    "#     ('classifier', LogisticRegression())\n",
    "# ])\n",
    "\n",
    "# model.fit(X, y)\n",
    "\n",
    "# # Save the trained model and preprocessor to disk\n",
    "# save_model(model, preprocessor, \"model.joblib\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">>Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a synthetic dataset with 1000 samples, 20 features, and 2 classes\n",
    "X, y = make_classification(n_samples=1000, n_features=20, n_classes=2, weights=[1,1], random_state=1)\n",
    "feature_names = ['Feature_{}'.format(i) for i in range(X.shape[1])] # Create feature names\n",
    "\n",
    "\n",
    "#you can use both RandomGridSearch and GridSearch to optimize a model. Use RandomGridSearch first, then GridSearch\n",
    "rfc_random = RandomizedSearchCV(estimator=rfc, param_distributions=random_grid, n_iter=100, cv=3, verbose=2, random_state=42, n_jobs=-1)\n",
    "rfc_random = GridSearchCV(RandomForestClassifier(random_state=42, class_weight='balanced'), random_grid, verbose=2, cv=3, refit=True)\n",
    "        # estimator: This is the machine learning model or estimator that you want to optimize using hyperparameter tuning\n",
    "        # param_distributions: This parameter specifies the hyperparameter space to be searched during the random search\n",
    "        # n_iter: This specifies the number of random combinations of hyperparameter values to try during the search.\n",
    "        # cv: This parameter determines the number of folds in the cross-validation process.\n",
    "        # verbose: This controls the verbosity of the output during the hyperparameter search. \n",
    "        #     A higher value, such as verbose=2, means more detailed output will be displayed during the search.\n",
    "        # random_state: This parameter sets the random seed for reproducibility\n",
    "        # n_jobs: This specifies the number of CPU cores to use for parallelization during the hyperparameter search. \n",
    "        #     A value of -1 (n_jobs=-1) means that all available CPU cores will be used.\n",
    "        # scoring: This parameter specifies the scoring metric used to evaluate the performance of the model \n",
    "        #     with different hyperparameter values. It can be set to a string representing a scoring metric, \n",
    "        #     such as 'accuracy', 'precision', 'recall', 'f1', etc., or an object of a custom scoring function\n",
    "        # refit: This parameter determines whether the best hyperparameters found during the search should be used to \n",
    "        #     refit the model on the entire dataset after the search is complete.\n",
    "        \n",
    "# Cross-validation techniques (three of the most common):\n",
    "#     K-fold Cross-Validation: It divides the data into k equally sized folds and performs training and testing on \n",
    "#         k iterations. It is widely used due to its simplicity and provides a good balance between bias and variance.\n",
    "        from sklearn.model_selection import KFold\n",
    "        kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     Stratified K-fold Cross-Validation: It is similar to K-fold cross-validation, but it ensures that each fold has an \n",
    "#         approximately equal distribution of target classes, making it suitable for imbalanced datasets.\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "        skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     Time Series Cross-Validation: It is used for time series data, where the order of data points matters. \n",
    "#         It involves using a sliding time window to create overlapping train and test sets, taking into account \n",
    "#         temporal dependencies.\n",
    "        from sklearn.model_selection import TimeSeriesSplit\n",
    "        tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "#      Leave-One-Out Cross-Validation (LOOCV): It is a special case of K-fold cross-validation where k is set to the \n",
    "#         total number of samples, resulting in each sample being used as a test set once. It is computationally expensive \n",
    "#         but can be useful for small datasets.\n",
    "        from sklearn.model_selection import LeaveOneOut\n",
    "        loo = LeaveOneOut()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c1065e887cc437d9280cab66f73a21fdac543e65443791bfb846601e6c934655"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
