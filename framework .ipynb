{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>Table of Content:\n",
    "* [Import Libraries](#1)\n",
    "* [Load Data](#2)\n",
    "* [Data Preprocessing](#3)\n",
    "    * [Data Cleaning](#3a)\n",
    "    * [Data Transformation](#3b)\n",
    "    * [Feature Selection/Extraction](#3c)\n",
    "    * [Handling Imbalanced Data](#3d)\n",
    "    * [Data Reduction](#3e)\n",
    "* [Exploratory Data Analysis (EDA)](#4)\n",
    "* [Selecting and Training the Model](#5) \n",
    "* [Model Evaluation](#6) \n",
    "* [Model Optimization](#7) \n",
    "* [Model Deployment](#8) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis     \n",
    "import pandas as pd          # data analysis library for handling structured data   \n",
    "from pandas import DataFrame            \n",
    "import numpy as np           # mathematical library for working with numerical data\n",
    "from pandas.plotting import parallel_coordinates \n",
    "from statsmodels.tsa.seasonal import seasonal_decompose # library for performing statistical analysis\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from typing import List\n",
    "import pickle\n",
    "import calendar \n",
    "import scipy.stats as stats\n",
    "import datetime \n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt     # data visualization library for creating graphs and charts\n",
    "%matplotlib inline\n",
    "import seaborn as sns        # data visualization library based on matplotlib for creating more attractive visualizations\n",
    "import plotly.io as pio\n",
    "import plotly.express as px   # interactive data visualization library\n",
    "import plotly.graph_objects as go   # library for creating interactive graphs and charts\n",
    "import matplotlib \n",
    "import kaleido \n",
    "import missingno as msno \n",
    "\n",
    "# Machine Learning/Time Series \n",
    "import tensorflow as tf\n",
    "from prophet import Prophet\n",
    "from xgboost import XGBRegressor \n",
    "#ML - Preprocessing data\n",
    "from sklearn.preprocessing import FunctionTransformer, PowerTransformer #variable transformation \n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, OrdinalEncoder, OneHotEncoder # Preprocessing feature scaling/categorical encoding\n",
    "from sklearn.preprocessing import Normalizer, Binarizer \n",
    "from sklearn.model_selection import train_test_split    #split data into train and test \n",
    "#ML - Handling imbalanced data\n",
    "from imblearn.over_sampling import RandomOverSampler \n",
    "from imblearn.under_sampling import RandomUnderSampler  \n",
    "#ML - Create your model\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression  #linear and logistics regression models\n",
    "from sklearn.tree import DecisionTreeRegressor \n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "#ML - Evaluate model performance\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, confusion_matrix, accuracy_score, classification_report \n",
    "from sklearn.metrics import adjusted_rand_score, v_measure_score, homogeneity_score \n",
    "#ML - Tune your model\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") \n",
    "\n",
    "pd.set_option('display.max_rows', 15)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Dataset/household_power_consumption.txt\", sep=\";\",  parse_dates={'Datetime' : ['Date', 'Time']}, \n",
    "                          infer_datetime_format=True, low_memory=False, index_col='Datetime')\n",
    "\n",
    "df.head(4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data DESCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "df.info() "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Visualize Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_df(df):       #best for time series analysis when your index is in datetime \n",
    "    \"\"\"\n",
    "    Creates an interactive plot of the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data for the PJM Interconnection region.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays an interactive plot of the energy consumption data using Plotly.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_df(my_df)\n",
    "    \"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "\n",
    "    fig = go.Figure(layout=go.Layout(\n",
    "        height=500,\n",
    "        width=800,\n",
    "    ))\n",
    "\n",
    "    for col in df.columns:\n",
    "        fig.add_trace(go.Scatter(x=df.index, y=df[col], name=col))\n",
    "\n",
    "    fig.update_layout(\n",
    "        title={\n",
    "            'text': 'PJM Energy Consumption',\n",
    "            'font': {'size': 25, 'family': 'Arial', 'color': 'black'}\n",
    "        },\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Energy Consumption (MW)'\n",
    "    )\n",
    "\n",
    "    return fig.show(renderer='svg')\n",
    "\n",
    "visualize_df(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_data_profiling(df, filename):\n",
    "    '''\n",
    "    Function to do basic data profiling\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - filename = Path for output file with a .html extension\n",
    "    Expected Output -\n",
    "        - HTML file with data profiling summary\n",
    "    '''\n",
    "    profile = pandas_profiling.ProfileReport(df)\n",
    "    profile.to_file(output_file = filename)\n",
    "    print(\"Data profiling done\")\n",
    "\n",
    "do_data_profiling(df, data_profiling.html) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "## Data Pre-processing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3a\"></a> <br>\n",
    "### Data Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Basic formating (renaming cols, duplicates detection, datetime etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates \n",
    "df.drop_duplicates(inplace=True) \n",
    "\n",
    "#formating columns \n",
    "df.\n",
    "\n",
    "\n",
    "\n",
    "# Rename columns\n",
    "df.rename(columns={'price': 'selling_price', 'bedrooms': 'num_bedrooms'}, inplace=True)\n",
    "\n",
    "#replace non numeric columns\n",
    "def replace_non_numeric(df: pd.DataFrame, columns):\n",
    "    \"\"\"\n",
    "    Replaces non-numeric values in the specified columns of a Pandas dataframe with NaN.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataframe to process.\n",
    "        columns (list): A list of column names to replace non-numeric values in.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The updated dataframe with non-numeric values replaced by NaN.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        df.dropna(subset = col, inplace= True)\n",
    "        if df[col].dtype == 'object' or df[col].dtype == 'float':\n",
    "            # df.dropna(subset = col, inplace= True)\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "        else:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "            df.dropna(subset = col, inplace= True)\n",
    "    return df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> format datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_timestamp(ts):\n",
    "    \"\"\"\n",
    "    Converts a Unix timestamp to a formatted date and time string.\n",
    "\n",
    "    Args:\n",
    "        ts (int): The Unix timestamp to convert.\n",
    "\n",
    "    Returns:\n",
    "        str: A formatted date and time string in the format 'YYYY-MM-DD HH:MM:SS'.\n",
    "    \"\"\"\n",
    "    utc_datetime = datetime.datetime.utcfromtimestamp(ts)\n",
    "    formatted_datetime = utc_datetime.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    formatted_datetime = pd.to_datetime(formatted_datetime, infer_datetime_format=True) \n",
    "    return formatted_datetime\n",
    "\n",
    "convert_timestamp(ts) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Remove unwanted Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove irrelevant columns\n",
    "df.drop(['id', 'date'], axis=1, inplace=True) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_value_analysis(df):\n",
    "    '''\n",
    "    Function to do basic missing value analysis\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "    Expected Output -\n",
    "        - Chart of Missing value co-occurance\n",
    "        - Chart of Missing value heatmap\n",
    "    '''\n",
    "    msno.matrix(df)\n",
    "    msno.heatmap(df)\n",
    "\n",
    "def view_NaN(df):\n",
    "    \"\"\"\n",
    "    Prints the name of any column in a Pandas DataFrame that contains NaN values.\n",
    "\n",
    "    Parameters:\n",
    "        - df: Pandas DataFrame\n",
    "\n",
    "    Returns:\n",
    "        - None\n",
    "    \"\"\"\n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any() == True: \n",
    "            print(f\"there is {df[col].isnull().sum()} NaN present in column:\", col)\n",
    "        else:\n",
    "            print(\"No NaN present in column:\", col)  \n",
    "\n",
    "missing_value_analysis (df)\n",
    "view_NaN(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values \n",
    "# df.fillna(df.mean(), inplace=True) \n",
    "\n",
    "def treat_missing_numeric(df,columns,how = 'mean', value = None):\n",
    "    '''\n",
    "    Function to treat missing values in numeric columns\n",
    "    Required Input - \n",
    "        - df = Pandas DataFrame\n",
    "        - columns = List input of all the columns need to be imputed\n",
    "        - how = valid values are 'mean', 'mode', 'median','ffill', numeric value\n",
    "    Expected Output -\n",
    "        - Pandas dataframe with imputed missing value in mentioned columns\n",
    "    '''\n",
    "    if how == 'mean':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mean for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mean())\n",
    "            \n",
    "    elif how == 'mode':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with mode for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].mode())\n",
    "    \n",
    "    elif how == 'median':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with median for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(df[i].median())\n",
    "    \n",
    "    elif how == 'ffill':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with forward fill for columns - {0}\".format(i))\n",
    "            df[i] = df[i].fillna(method ='ffill')\n",
    "    \n",
    "    elif how == 'digit':\n",
    "        for i in columns:\n",
    "            print(\"Filling missing values with {0} for columns - {1}\".format(how, i))\n",
    "            df[i] = df[i].fillna(str(value)) \n",
    "      \n",
    "    else:\n",
    "        print(\"Missing value fill cannot be completed\")\n",
    "    return df.head(5)\n",
    "\n",
    "\n",
    "treat_missing_numeric(smart_home, [\"cloudCover\"], how=\"digit\", value = 0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using Sklearn to handle missing values - (SimpleImputer, KNN-Imputer, Iterative Imputer, )\n",
    "\n",
    "#IterativeImputer: This function estimates missing values using a predictive model.\n",
    "from sklearn.experimental import enable_iterative_imputer \n",
    "from sklearn.impute import IterativeImputer\n",
    "from sklearn.ensemble import RandomForestRegressor \n",
    "\n",
    "def impute_missing_values_iteratively(X): #or (X, Columns)\n",
    "    imputer = IterativeImputer(estimator = RandomForestRegressor())\n",
    "        \n",
    "    # select only the columns with missing values to be imputed\n",
    "    # X_cols = X[columns]\n",
    "    X_imputed = imputer.fit_transform(X) #or X_cols\n",
    "    return X_imputed\n",
    "\n",
    "impute_missing_values_iteratively(df) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize outliers\n",
    "def visualize_outlier (df: pd.DataFrame):\n",
    "    # Select only numeric columns\n",
    "    numeric_cols = df.select_dtypes(include=[\"float64\", \"int64\"])\n",
    "    # Set figure size and create boxplot\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    numeric_cols.boxplot(ax=ax, rot=90)\n",
    "    # Set x-axis label\n",
    "    ax.set_xlabel(\"Numeric Columns\")\n",
    "    # Adjust subplot spacing to prevent x-axis labels from being cut off\n",
    "    plt.subplots_adjust(bottom=0.4) \n",
    "    # Increase the size of the plot\n",
    "    fig.set_size_inches(10, 6)\n",
    "    # Show the plot\n",
    "    plt.show()\n",
    "\n",
    "visualize_outlier (df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to handle outliers, use any of Tukey's test, Kernel density estimation, Z-score method, Mahalanobis distance method,\n",
    "#Isolation Forest model, EllipticEnvelope.\n",
    "\n",
    "\n",
    "# Tukey's test: This statistical method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the median and works well for univariate datasets with normal distributions.\n",
    "\n",
    "# Kernel density estimation: This non-parametric method estimates the probability density function of a dataset \n",
    "#     and identifies outliers as values with low probability density, making it suitable for non-normal datasets.\n",
    "\n",
    "# Z-score method: This simple method identifies outliers as values more than a certain number of standard deviations \n",
    "#     away from the mean and is widely used for datasets with normal distributions.\n",
    "\n",
    "# Mahalanobis distance method: This multivariate method identifies outliers based on the distance of each point from the \n",
    "#     centroid of the dataset and is effective for datasets with multivariate normal distributions.\n",
    "\n",
    "# Isolation Forest model: This machine learning algorithm identifies outliers by isolating them into a separate tree \n",
    "#     structure, making it suitable for high-dimensional feature spaces with both linear and non-linear relationships \n",
    "#     between features.\n",
    "\n",
    "# EllipticEnvelope: This multivariate method identifies outliers by fitting an ellipse to the data and identifying \n",
    "#     points that are outside the ellipse, making it effective for datasets with multivariate normal distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3b\"></a> <br>\n",
    "### Data Transformation (scaling, encoding categorical data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Extracting features from Dates, Mixed Variables etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hour = df.resample('H').mean() \n",
    "df_day = df.resample('D').mean() \n",
    "df_month = df.resample('M').mean() \n",
    "df_year = df.resample('Y').mean()\n",
    "\n",
    "\n",
    "df['hour'] = df.index.hour \n",
    "df['day'] = df.index.day \n",
    "df['weekday'] = df.index.day_name() \n",
    "df['month'] = df.index.month \n",
    "df['year'] = df.index.year "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Categorical Variable Encoding (data transformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoding: replaces each category with a numerical label. This technique is suitable for data \n",
    "#     where the categories have an intrinsic order, such as \"low,\" \"medium,\" and \"high.\" Works well with linear models\n",
    "\n",
    "# Ordinal encoding: assigns a numerical value to each category based on their frequency. This technique is suitable \n",
    "#     for data where the categories do not have an intrinsic order, but where their frequency may be informative.\n",
    "#     Suitable for non-linear models\n",
    "\n",
    "# One-hot encoding: creates a binary variable for each category, indicating its presence or absence. \n",
    "#     This technique is suitable for data where the categories do not have an intrinsic order and the \n",
    "#     number of categories is small\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Data Spliting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def holdout_cv(X,y,size = 0.3, seed = 1): \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = size, random_state = seed) \n",
    "    X_train = X_train.reset_index(drop='index') \n",
    "    X_test = X_test.reset_index(drop='index') \n",
    "    return X_train, X_test, y_train, y_test \n",
    "\n",
    "X_train, X_test, y_train, y_test = holdout_cv(X, y, size=0.3, seed=1) \n",
    "\n",
    "#OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(X, y, test_size=0.2, val_size=0.2, random_state=42): #split data into train, test, and validation\n",
    "    \"\"\"\n",
    "    This function splits the data into train and test sets, and further splits the train set into training and validation sets.\n",
    "    \n",
    "    df : pandas DataFrame\n",
    "        The dataframe containing the input data.\n",
    "    target_col : str\n",
    "        The name of the target column in the dataframe.\n",
    "    test_size : float, optional (default=0.2)\n",
    "        The proportion of the data to be used for testing.\n",
    "    val_size : float, optional (default=0.2)\n",
    "        The proportion of the training data to be used for validation.\n",
    "    random_state : int, optional (default=42)\n",
    "        The seed used by the random number generator.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    xtrain : pandas DataFrame\n",
    "        The training input data.\n",
    "    ytrain : pandas Series\n",
    "        The training target data.\n",
    "    xvalid : pandas DataFrame\n",
    "        The validation input data.\n",
    "    yvalid : pandas Series\n",
    "        The validation target data.\n",
    "    xtest : pandas DataFrame\n",
    "        The test input data.\n",
    "    ytest : pandas Series\n",
    "        The test target data.\n",
    "    \"\"\" \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=val_size, random_state=random_state)\n",
    "    \n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = split_data(X, y, test_size=0.2, val_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_splitting(train, test): \n",
    "    \"\"\" \n",
    "    Plots the training and test sets of a time series.\n",
    "\n",
    "    Args:\n",
    "    train (pandas.DataFrame): DataFrame containing the training set with a DatetimeIndex and a 'PJME_MW' column.\n",
    "    test (pandas.DataFrame): DataFrame containing the test set with a DatetimeIndex and a 'PJME_MW' column.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(20,8))\n",
    "\n",
    "    plt.plot(train.index, train['PJME_MW'], label='Training Set')\n",
    "    plt.plot(test.index, test['PJME_MW'], label='Test Set')\n",
    "\n",
    "    plt.title('Data Splitting', weight='bold', fontsize=25, loc= \"center\", pad=20)\n",
    "    plt.axvline('2015-09-01', color='black', ls='--', lw=3) \n",
    "    plt.legend()\n",
    "    plt.show() \n",
    "\n",
    "plot_data_splitting(train, test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Feature Scaling (data transformation) - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use any of standard scaler (z-score), min-max scaler \n",
    "\n",
    "#Z-score standardized feature scaling (most common)\n",
    "#min-max scaling (common) \n",
    "\n",
    "def standardize_data(X_train, X_test): \n",
    "    \"\"\"\n",
    "    Standardizes the training and testing data using the mean and standard deviation\n",
    "    learned from the training set.\n",
    "    \n",
    "    Args:\n",
    "    - X_train: numpy array or pandas dataframe, training data\n",
    "    - X_test: numpy array or pandas dataframe, testing data\n",
    "    \n",
    "    Returns:\n",
    "    - X_train_scaled: numpy array or pandas dataframe, standardized training data\n",
    "    - X_test_scaled: numpy array or pandas dataframe, standardized testing data\n",
    "    \"\"\"\n",
    "    from sklearn.preprocessing import StandardScaler \n",
    "    # Set up the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Fit the scaler to the training set\n",
    "    scaler.fit(X_train) \n",
    "    \n",
    "    # Transform the training and testing sets\n",
    "    X_train_scaled = scaler.transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled\n",
    "\n",
    "standardize_data(X_test, X_test) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Variable Transformation - apply to train, and then to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variable transformation involves transforming the values of variables to make them more suitable for analysis\n",
    "#the idea is to make the variables normally/gaussian distributed. Hence, \n",
    "\n",
    "#first step is to assess normality using a histogram or QQ-plot (to explore the variable distribution)\n",
    "\n",
    "def diagnostic_plots(df, variable):\n",
    "\n",
    "    # function to plot a histogram and a Q-Q plot\n",
    "    # side by side, for a certain variable\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    df[variable].hist(bins=30)\n",
    "    plt.title(f\"Histogram of {variable}\")\n",
    "\n",
    "    # q-q plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    stats.probplot(df[variable], dist=\"norm\", plot=plt)\n",
    "    plt.title(f\"Q-Q plot of {variable}\")\n",
    "\n",
    "    # check for skewness\n",
    "    skewness = df[variable].skew()\n",
    "    if skewness > 0:\n",
    "        skew_type = \"positively skewed\"\n",
    "    elif skewness < 0:\n",
    "        skew_type = \"negatively skewed\"\n",
    "    else:\n",
    "        skew_type = \"approximately symmetric\"\n",
    "        \n",
    "    # print message indicating skewness type\n",
    "    print(f\"The variable {variable} is {skew_type} (skewness = {skewness:.2f})\")\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Check function output\n",
    "diagnostic_plots(X, \"MedInc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If the variables are NOT normally distributed, we then transform it. It is necessary to test several variable \n",
    "# transformation methods, and choose the best for that feature. One variable transformation method is log_transform\n",
    "\n",
    "#log transform \n",
    "def log_transform(df, columns):\n",
    "     \"\"\"\n",
    "    Transforms specified columns of a pandas DataFrame using the natural logarithm function.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The DataFrame to transform.\n",
    "    columns : list\n",
    "        A list of column names to transform.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pandas DataFrame\n",
    "        The transformed DataFrame.\n",
    "    \"\"\"\n",
    "    transformer = FunctionTransformer(np.log1p, validate=True)\n",
    "    X = df.values.copy()\n",
    "    X[:, df.columns.isin(columns)] = transformer.transform(X[:, df.columns.isin(columns)])\n",
    "    X_log = pd.DataFrame(X, index=df.index, columns=df.columns)\n",
    "    return X_log\n",
    "\n",
    "df_log = log_transform(df, columns)\n",
    "\n",
    "diagnostic_plots(df_log, columns) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Discretization (data transformation - apply to train, and then to test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discretization in machine learning is the process of transforming continuous variables into discrete or \n",
    "# categorical variables. This process involves dividing the range of a continuous variable into a finite number of \n",
    "# intervals or bins, and then assigning each observation to a particular bin based on the value of the continuous \n",
    "# variable. \n",
    "\n",
    "#Discretization approaches: equal width, equal frequency, K means, Decision Trees\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3c\"></a> <br>\n",
    "### Feature Selection/Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to train, and then to test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3d\"></a> <br>\n",
    "### Handling Imbalanced Data and Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply to train, and then to test\n",
    "\n",
    "#Imbalanced data refers to a situation where the number of observations in one class or category is much larger \n",
    "# or smaller than the number of observations in other classes or categories. Imbalanced data can pose challenges \n",
    "# in machine learning because it can lead to biased models that perform poorly on the minority class.\n",
    "\n",
    "#we typically look at the 'target data' when checking for imbalanced data. However, it is also important \n",
    "# to consider the features\n",
    "\n",
    "def check_imbalance(dataset, columns=None, threshold=10):\n",
    "    \"\"\"\n",
    "    This function takes a dataset and one or more columns as input and returns True if any of the specified columns\n",
    "    are imbalanced, False otherwise. A column is considered imbalanced if the percentage of the minority class is less\n",
    "    than the specified threshold.\n",
    "    \"\"\"\n",
    "    # If no columns are specified, use all columns except for the last one as the features\n",
    "    if columns is None:\n",
    "        features = dataset.iloc[:, :-1]\n",
    "        columns = features.columns\n",
    "    \n",
    "    # Check the imbalance of each specified column\n",
    "    for col in columns:\n",
    "        # Get the counts of each class in the column\n",
    "        class_counts = dataset[col].value_counts()\n",
    "\n",
    "        # Calculate the percentage of each class in the column\n",
    "        class_percentages = class_counts / len(dataset) * 100\n",
    "\n",
    "        # Plot the class percentages\n",
    "        plt.bar(class_counts.index, class_percentages)\n",
    "        plt.xlabel(col)\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.title(f'{col} Distribution')\n",
    "        plt.show()\n",
    "\n",
    "        # Check if the column is imbalanced\n",
    "        minority_class = class_counts.index[-1]\n",
    "        minority_class_percentage = class_percentages.iloc[-1]\n",
    "        if minority_class_percentage < threshold:\n",
    "            print(f'{col} is imbalanced. Minority class: {minority_class}, Percentage: {minority_class_percentage:.2f}%')\n",
    "            return True\n",
    "\n",
    "    # If none of the specified columns are imbalanced, return False\n",
    "    print('No imbalance found.')\n",
    "    return False\n",
    "\n",
    "check_imbalance(df, columns=['target'], threshold=10) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if there is imbalance, you can handle it by over-sampling or under-sampling the dataset\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "def handle_imbalanced_data(X, y, strategy='over-sampling'):\n",
    "    \"\"\"\n",
    "    Handle imbalanced data using imblearn library.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X: array-like of shape (n_samples, n_features)\n",
    "        The input data.\n",
    "    y: array-like of shape (n_samples,)\n",
    "        The target values.\n",
    "    strategy: str, default='over-sampling'\n",
    "        The strategy to use for handling imbalanced data. Possible values are\n",
    "        'over-sampling' and 'under-sampling'.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_resampled: array-like of shape (n_samples_new, n_features)\n",
    "        The resampled input data.\n",
    "    y_resampled: array-like of shape (n_samples_new,)\n",
    "        The resampled target values.\n",
    "    \"\"\"\n",
    "    if strategy == 'over-sampling':\n",
    "        # Initialize the RandomOverSampler object\n",
    "        ros = RandomOverSampler(sampling_strategy='minority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = ros.fit_resample(X, y)\n",
    "    elif strategy == 'under-sampling':\n",
    "        # Initialize the RandomUnderSampler object\n",
    "        rus = RandomUnderSampler(sampling_strategy='majority', random_state=0)\n",
    "        # Resample the data\n",
    "        X_resampled, y_resampled = rus.fit_resample(X, y)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid strategy. Possible values are 'over-sampling' and 'under-sampling'.\")\n",
    "    \n",
    "    return X_resampled, y_resampled"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"3e\"></a> <br>\n",
    "### Data Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize with individual plots \n",
    "def visualize_subplots_boxplots(df: DataFrame, columns: List[str], nrows: int, ncols: int) -> None:\n",
    "    \"\"\"\n",
    "    Creates a grid of subplots containing boxplots of daily average energy consumption.\n",
    "\n",
    "    Args:\n",
    "        df: A Pandas DataFrame containing energy consumption data.\n",
    "        columns: A list of column names to include in the boxplots.\n",
    "        nrows: The number of rows in the subplot grid.\n",
    "        ncols: The number of columns in the subplot grid.\n",
    "\n",
    "    Returns:\n",
    "        None. Displays a grid of subplots containing boxplots of daily average energy consumption.\n",
    "\n",
    "    Example:\n",
    "        >>> visualize_subplots_boxplots(my_df, ['Consumption', 'Generation'], 3, 4)\n",
    "    \"\"\"\n",
    "    from typing import List\n",
    "    from pandas import DataFrame\n",
    "    fig, axes = plt.subplots(nrows=nrows, ncols=ncols, figsize=(16, 12))\n",
    "    fig.suptitle('Hourly Average Energy Consumption', weight='bold', fontsize=25)\n",
    "\n",
    "    # We just need 11 figures, so we delete the last one\n",
    "    if nrows*ncols > len(columns):\n",
    "        fig.delaxes(axes[nrows-1][ncols-1])\n",
    "\n",
    "    for i, col in enumerate(columns):\n",
    "        sns.boxplot(data=df, x='Hour', y=col, ax=axes.flatten()[i], color='#cc444b')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(\"Images/xxx.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "visualize_subplots_boxplots(df=result_2, columns=['AEP_MW', 'COMED_MW', 'DAYTON_MW', 'DEOK_MW', 'DOM_MW', 'DUQ_MW',\n",
    "        'EKPC_MW', 'FE_MW', 'NI_MW', 'PJME_MW', 'PJMW_MW'], nrows=6, ncols=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "## Selecting and Training the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a random forest classifier on the dataset\n",
    "\n",
    "### Running Random Forest\n",
    "def runRF(\n",
    "    train_X,\n",
    "    train_y,\n",
    "    test_X,\n",
    "    test_y=None,\n",
    "    test_X2=None,\n",
    "    rounds=100,\n",
    "    depth=20,\n",
    "    leaf=10,\n",
    "    feat=0.2,\n",
    "    min_data_split_val=2,\n",
    "    seed_val=0,\n",
    "    job=-1,\n",
    "):\n",
    "    model = RandomForestClassifier(\n",
    "        n_estimators=rounds,\n",
    "        max_depth=depth,\n",
    "        min_samples_split=min_data_split_val,\n",
    "        min_samples_leaf=leaf,\n",
    "        max_features=feat,\n",
    "        n_jobs=job,\n",
    "        random_state=seed_val,\n",
    "    )\n",
    "    model.fit(train_X, train_y)\n",
    "    train_preds = model.predict_proba(train_X)[:, 1]\n",
    "    test_preds = model.predict_proba(test_X)[:, 1]\n",
    "\n",
    "    test_preds2 = 0\n",
    "    if test_X2 is not None:\n",
    "        test_preds2 = model.predict_proba(test_X2)[:, 1]\n",
    "\n",
    "    test_loss = 0\n",
    "    if test_y is not None:\n",
    "        train_loss = metrics.roc_auc_score(train_y, train_preds)\n",
    "        test_loss = metrics.roc_auc_score(test_y, test_preds)\n",
    "        print(\"Train and Test loss : \", train_loss, test_loss)\n",
    "    return test_preds, test_loss, test_preds2, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Compare predicted values with the actual test values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(y_pred, y_test): \n",
    "    \"\"\"\n",
    "    Plots the predicted and actual values on separate scatter plots.\n",
    "    \"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "    \n",
    "    # Plot the actual values\n",
    "    ax1.scatter(range(len(y_test)), y_test, label='Actual Values')\n",
    "    ax1.set_xlabel('Index')\n",
    "    ax1.set_ylabel('Actual Values')\n",
    "    ax1.set_title('Scatter plot of Actual Values')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # Plot the predicted values\n",
    "    ax2.scatter(range(len(y_pred)), y_pred, label='Predicted Values')\n",
    "    ax2.set_xlabel('Index')\n",
    "    ax2.set_ylabel('Predicted Values')\n",
    "    ax2.set_title('Scatter plot of Predicted Values')\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Show the plots\n",
    "    plt.show()\n",
    "\n",
    "plot_predictions(y_pred_sgd, y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">> Assess the most important features in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#before evaluating the model, there is need to assess which features are the most important in the ML model\n",
    "\n",
    "def feature_importance(model,X):\n",
    "    feature_importance = model.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.figure(figsize=(15, 15))\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.title('Variable Importance')\n",
    "    plt.show()\n",
    "\n",
    "feature_importance(model,X_train)  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regression\n",
    "    # Error = Actual value - Predicted value\n",
    "\n",
    "    # MSE (Mean Square Error)\n",
    "        # The square of the error over all samples is called Mean Squarred Error(MSE).\n",
    "        # MSE = SQUARE(Actual value - Predicted value)/Number of Samples\n",
    "    #RMSE (Root Mean Square Error)\n",
    "    # MAE (Mean Absolute Error)\n",
    "        # MAE = ABSOLUTE (Actual value - Predicted Value)\n",
    "\n",
    "\n",
    "#Classification\n",
    "    #Accuracy\n",
    "    #Precision\n",
    "    #Recall\n",
    "    #F1 score \n",
    "    #confusion matrix\n",
    "    #AUC ROC curve\n",
    "\n",
    "# True Positives(TP): Number of samples that are correctly classified as positive, and their actual label is positive.\n",
    "\n",
    "# False Positives (FP): Number of samples that are incorrectly classified as positive, when in fact their actual label \n",
    "#     is negative.\n",
    "\n",
    "# True Negatives (TN): Number of samples that are correctly classified as negative, and their actual label is negative.\n",
    "\n",
    "# False Negatives (FN): Number of samples that are incorrectly classified as negative, when in fact their actual label \n",
    "#     is positive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"7\"></a> <br>\n",
    "## Model Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"8\"></a> <br>\n",
    "## Model Deployment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
